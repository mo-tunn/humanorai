title,text,category_query,label
Beyond principlism: Practical strategies for ethical AI use in research practices,"The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a ""Triple-Too"" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches--principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technological solutionism (overemphasis on technological fixes)--offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017 New and Future AI Educator Program,"The 7th Symposium on Educational Advances in Artificial Intelligence (EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following ""blue sky"" questions:   * How could/should Artificial Intelligence (AI) courses incorporate ethics into the curriculum?   * How could we teach AI topics at an early undergraduate or a secondary school level?   * AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields?   This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Linking Artificial Intelligence Principles,"Artificial Intelligence principles define social and ethical considerations to develop future AI. They come from research institutes, government organizations and industries. All versions of AI principles are with different considerations covering different perspectives and making different emphasis. None of them can be considered as complete and can cover the rest AI principle proposals. Here we introduce LAIP, an effort and platform for linking and analyzing different Artificial Intelligence Principles. We want to explicitly establish the common topics and links among AI Principles proposed by different organizations and investigate on their uniqueness. Based on these efforts, for the long-term future of AI, instead of directly adopting any of the AI principles, we argue for the necessity of incorporating various AI Principles into a comprehensive framework and focusing on how they can interact and complete each other.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Using Edge Cases to Disentangle Fairness and Solidarity in AI Ethics,"Principles of fairness and solidarity in AI ethics regularly overlap, creating obscurity in practice: acting in accordance with one can appear indistinguishable from deciding according to the rules of the other. However, there exist irregular cases where the two concepts split, and so reveal their disparate meanings and uses. This paper explores two cases in AI medical ethics, one that is irregular and the other more conventional, to fully distinguish fairness and solidarity. Then the distinction is applied to the frequently cited COMPAS versus ProPublica dispute in judicial ethics. The application provides a broader model for settling contemporary and topical debates about fairness and solidarity. It also implies a deeper and disorienting truth about AI ethics principles and their justification.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Confucius, Cyberpunk and Mr. Science: Comparing AI ethics between China and the EU","The exponential development and application of artificial intelligence triggered an unprecedented global concern for potential social and ethical issues. Stakeholders from different industries, international foundations, governmental organisations and standards institutions quickly improvised and created various codes of ethics attempting to regulate AI. A major concern is the large homogeneity and presumed consensualism around these principles. While it is true that some ethical doctrines, such as the famous Kantian deontology, aspire to universalism, they are however not universal in practice. In fact, ethical pluralism is more about differences in which relevant questions to ask rather than different answers to a common question. When people abide by different moral doctrines, they tend to disagree on the very approach to an issue. Even when people from different cultures happen to agree on a set of common principles, it does not necessarily mean that they share the same understanding of these concepts and what they entail. In order to better understand the philosophical roots and cultural context underlying ethical principles in AI, we propose to analyse and compare the ethical principles endorsed by the Chinese National New Generation Artificial Intelligence Governance Professional Committee (CNNGAIGPC) and those elaborated by the European High-level Expert Group on AI (HLEGAI). China and the EU have very different political systems and diverge in their cultural heritages. In our analysis, we wish to highlight that principles that seem similar a priori may actually have different meanings, derived from different approaches and reflect distinct goals.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards Measuring Ethicality of an Intelligent Assistive System,"Artificial intelligence (AI) based assistive systems, so called intelligent assistive technology (IAT) are becoming increasingly ubiquitous by each day. IAT helps people in improving their quality of life by providing intelligent assistance based on the provided data. A few examples of such IATs include self-driving cars, robot assistants and smart-health management solutions. However, the presence of such autonomous entities poses ethical challenges concerning the stakeholders involved in using these systems. There is a lack of research when it comes to analysing how such IAT adheres to provided ethical regulations due to ethical, logistic and cost issues associated with such an analysis. In the light of the above-mentioned problem statement and issues, we present a method to measure the ethicality of an assistive system. To perform this task, we utilised our simulation tool that focuses on modelling navigation and assistance of Persons with Dementia (PwD) in indoor environments. By utilising this tool, we analyse how well different assistive strategies adhere to provided ethical regulations such as autonomy, justice and beneficence of the stakeholders.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics of Artificial Intelligence Demarcations,"In this paper we present a set of key demarcations, particularly important when discussing ethical and societal issues of current AI research and applications. Properly distinguishing issues and concerns related to Artificial General Intelligence and weak AI, between symbolic and connectionist AI, AI methods, data and applications are prerequisites for an informed debate. Such demarcations would not only facilitate much-needed discussions on ethics on current AI technologies and research. In addition sufficiently establishing such demarcations would also enhance knowledge-sharing and support rigor in interdisciplinary research between technical and social sciences.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics in Industry: A Research Framework,"Artificial Intelligence (AI) systems exert a growing influence on our society. As they become more ubiquitous, their potential negative impacts also become evident through various real-world incidents. Following such early incidents, academic and public discussion on AI ethics has highlighted the need for implementing ethics in AI system development. However, little currently exists in the way of frameworks for understanding the practical implementation of AI ethics. In this paper, we discuss a research framework for implementing AI ethics in industrial settings. The framework presents a starting point for empirical studies into AI ethics but is still being developed further based on its practical utilization.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges,"The integration of artificial intelligence (AI) in educational measurement has revolutionized assessment methods, enabling automated scoring, rapid content analysis, and personalized feedback through machine learning and natural language processing. These advancements provide timely, consistent feedback and valuable insights into student performance, thereby enhancing the assessment experience. However, the deployment of AI in education also raises significant ethical concerns regarding validity, reliability, transparency, fairness, and equity. Issues such as algorithmic bias and the opacity of AI decision-making processes pose risks of perpetuating inequalities and affecting assessment outcomes. Responding to these concerns, various stakeholders, including educators, policymakers, and organizations, have developed guidelines to ensure ethical AI use in education. The National Council of Measurement in Education's Special Interest Group on AI in Measurement and Education (AIME) also focuses on establishing ethical standards and advancing research in this area. In this paper, a diverse group of AIME members examines the ethical implications of AI-powered tools in educational measurement, explores significant challenges such as automation bias and environmental impact, and proposes solutions to ensure AI's responsible and effective use in education.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Queering the ethics of AI,"This book chapter delves into the pressing need to ""queer"" the ethics of AI to challenge and re-evaluate the normative suppositions and values that underlie AI systems. The chapter emphasizes the ethical concerns surrounding the potential for AI to perpetuate discrimination, including binarism, and amplify existing inequalities due to the lack of representative datasets and the affordances and constraints depending on technology readiness. The chapter argues that a critical examination of the neoliberal conception of equality that often underpins non-discrimination law is necessary and cannot stress more the need to create alternative interdisciplinary approaches that consider the complex and intersecting factors that shape individuals' experiences of discrimination. By exploring such approaches centering on intersectionality and vulnerability-informed design, the chapter contends that designers and developers can create more ethical AI systems that are inclusive, equitable, and responsive to the needs and experiences of all individuals and communities, particularly those who are most vulnerable to discrimination and harm.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical AI Governance: Methods for Evaluating Trustworthy AI,"Trustworthy Artificial Intelligence (TAI) integrates ethics that align with human values, looking at their influence on AI behaviour and decision-making. Primarily dependent on self-assessment, TAI evaluation aims to ensure ethical standards and safety in AI development and usage. This paper reviews the current TAI evaluation methods in the literature and offers a classification, contributing to understanding self-assessment methods in this field.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?","This study delves into the pervasive issue of gender issues in artificial intelligence (AI), specifically within automatic scoring systems for student-written responses. The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI scoring outcomes. Utilizing a fine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000 human-graded student responses from male and female participants across six assessment items. The study employs three distinct techniques for bias analysis: Scoring accuracy difference to evaluate bias, mean score gaps by gender (MSG) to evaluate disparity, and Equalized Odds (EO) to evaluate fairness. The results indicate that scoring accuracy for mixed-trained models shows an insignificant difference from either male- or female-trained models, suggesting no significant scoring bias. Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions compared to humans. In contrast, compared to humans, gender-specifically trained models yielded larger MSG, indicating that unbalanced training data may create algorithmic models to enlarge gender disparities. The EO analysis suggests that mixed-trained models generated more fairness outcomes compared with gender-specifically trained models. Collectively, the findings suggest that gender-unbalanced data do not necessarily generate scoring bias but can enlarge gender disparities and reduce scoring fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of Information Retrieval of Genocide-Related Information,"The growing application of artificial intelligence (AI) in the field of information retrieval (IR) affects different domains, including cultural heritage. By facilitating organisation and retrieval of large volumes of heritage-related content, AI-driven IR systems inform users about a broad range of historical phenomena, including genocides (e.g. the Holocaust). However, it is currently unclear to what degree IR systems are capable of dealing with multiple ethical challenges associated with the curation of genocide-related information. To address this question, this chapter provides an overview of ethical challenges associated with the human curation of genocide-related information using a three-part framework inspired by Belmont criteria (i.e. curation challenges associated with respect for individuals, beneficence and justice/fairness). Then, the chapter discusses to what degree the above-mentioned challenges are applicable to the ways in which AI-driven IR systems deal with genocide-related information and what can be the potential ways of bridging AI and memory ethics in this context.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Considerations in Artificial Intelligence Courses,"The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Experts' View on Challenges and Needs for Fairness in Artificial Intelligence for Education,"In recent years, there has been a stimulating discussion on how artificial intelligence (AI) can support the science and engineering of intelligent educational applications. Many studies in the field are proposing actionable data mining pipelines and machine-learning models driven by learning-related data. The potential of these pipelines and models to amplify unfairness for certain categories of students is however receiving increasing attention. If AI applications are to have a positive impact on education, it is crucial that their design considers fairness at every step. Through anonymous surveys and interviews with experts (researchers and practitioners) who have published their research at top-tier educational conferences in the last year, we conducted the first expert-driven systematic investigation on the challenges and needs for addressing fairness throughout the development of educational systems based on AI. We identified common and diverging views about the challenges and the needs faced by educational technologies experts in practice, that lead the community to have a clear understanding on the main questions raising doubts in this topic. Based on these findings, we highlighted directions that will facilitate the ongoing research towards fairer AI for education.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Principles alone cannot guarantee ethical AI,"AI Ethics is now a global topic of discussion in academic and policy circles. At least 84 public-private initiatives have produced statements describing high-level principles, values, and other tenets to guide the ethical development, deployment, and governance of AI. According to recent meta-analyses, AI Ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI Ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach in the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"How Do AI Companies ""Fine-Tune"" Policy? Examining Regulatory Capture in AI Governance","Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models,"Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness in AI: challenges in bridging the gap between algorithms and law,"In this paper we examine algorithmic fairness from the perspective of law aiming to identify best practices and strategies for the specification and adoption of fairness definitions and algorithms in real-world systems and use cases. We start by providing a brief introduction of current anti-discrimination law in the European Union and the United States and discussing the concepts of bias and fairness from an legal and ethical viewpoint. We then proceed by presenting a set of algorithmic fairness definitions by example, aiming to communicate their objectives to non-technical audiences. Then, we introduce a set of core criteria that need to be taken into account when selecting a specific fairness definition for real-world use case applications. Finally, we enumerate a set of key considerations and best practices for the design and employment of fairness methods on real-world AI applications","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Governance and Ethics Framework for Sustainable AI and Sustainability,"AI is transforming the existing technology landscape at a rapid phase enabling data-informed decision making and autonomous decision making. Unlike any other technology, because of the decision-making ability of AI, ethics and governance became a key concern. There are many emerging AI risks for humanity, such as autonomous weapons, automation-spurred job loss, socio-economic inequality, bias caused by data and algorithms, privacy violations and deepfakes. Social diversity, equity and inclusion are considered key success factors of AI to mitigate risks, create values and drive social justice. Sustainability became a broad and complex topic entangled with AI. Many organizations (government, corporate, not-for-profits, charities and NGOs) have diversified strategies driving AI for business optimization and social-and-environmental justice. Partnerships and collaborations become important more than ever for equity and inclusion of diversified and distributed people, data and capabilities. Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority. These AI ethics and governance should be underpinned by human ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Meaningful human control: actionable properties for AI system development,"How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Integration of AI in STEM Education, Addressing Ethical Challenges in K-12 Settings","The rapid integration of Artificial Intelligence (AI) into K-12 STEM education presents transformative opportunities alongside significant ethical challenges. While AI-powered tools such as Intelligent Tutoring Systems (ITS), automated assessments, and predictive analytics enhance personalized learning and operational efficiency, they also risk perpetuating algorithmic bias, eroding student privacy, and exacerbating educational inequities. This paper examines the dual-edged impact of AI in STEM classrooms, analyzing its benefits (e.g., adaptive learning, real-time feedback) and drawbacks (e.g., surveillance risks, pedagogical limitations) through an ethical lens. We identify critical gaps in current AI education research, particularly the lack of subject-specific frameworks for responsible integration and propose a three-phased implementation roadmap paired with a tiered professional development model for educators. Our framework emphasizes equity-centered design, combining technical AI literacy with ethical reasoning to foster critical engagement among students. Key recommendations include mandatory bias audits, low-resource adaptation strategies, and policy alignment to ensure AI serves as a tool for inclusive, human-centered STEM education. By bridging theory and practice, this work advances a research-backed approach to AI integration that prioritizes pedagogical integrity, equity, and student agency in an increasingly algorithmic world. Keywords: Artificial Intelligence, STEM education, algorithmic bias, ethical AI, K-12 pedagogy, equity in education","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Making Responsible AI the Norm rather than the Exception,"This report prepared by the Montreal AI Ethics Institute provides recommendations in response to the National Security Commission on Artificial Intelligence (NSCAI) Key Considerations for Responsible Development and Fielding of Artificial Intelligence document. The report centres on the idea that Responsible AI should be made the Norm rather than an Exception. It does so by utilizing the guiding principles of: (1) alleviating friction in existing workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an effective translation of abstract standards into actionable engineering practices. After providing some overarching comments on the document from the NSCAI, the report dives into the primary contribution of an actionable framework to help operationalize the ideas presented in the document from the NSCAI. The framework consists of: (1) a learning, knowledge, and information exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an empirically-driven risk-prioritization matrix, and (4) achieving the right level of complexity. All components reinforce each other to move from principles to practice in service of making Responsible AI the norm rather than the exception.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Research on Artificial Intelligence Ethics Based on the Evolution of Population Knowledge Base,"The unclear development direction of human society is a deep reason for that it is difficult to form a uniform ethical standard for human society and artificial intelligence. Since the 21st century, the latest advances in the Internet, brain science and artificial intelligence have brought new inspiration to the research on the development direction of human society. Through the study of the Internet brain model, AI IQ evaluation, and the evolution of the brain, this paper proposes that the evolution of population knowledge base is the key for judging the development direction of human society, thereby discussing the standards and norms for the construction of artificial intelligence ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Artificial Intelligence in Education: Ethical Considerations and Insights from Ancient Greek Philosophy,"This paper explores the ethical implications of integrating Artificial Intelligence (AI) in educational settings, from primary schools to universities, while drawing insights from ancient Greek philosophy to address emerging concerns. As AI technologies increasingly influence learning environments, they offer novel opportunities for personalized learning, efficient assessment, and data-driven decision-making. However, these advancements also raise critical ethical questions regarding data privacy, algorithmic bias, student autonomy, and the changing roles of educators. This research examines specific use cases of AI in education, analyzing both their potential benefits and drawbacks. By revisiting the philosophical principles of ancient Greek thinkers such as Socrates, Aristotle, and Plato, we discuss how their writings can guide the ethical implementation of AI in modern education. The paper argues that while AI presents significant challenges, a balanced approach informed by classical philosophical thought can lead to an ethically sound transformation of education. It emphasizes the evolving role of teachers as facilitators and the importance of fostering student initiative in AI-rich environments.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
ESR: Ethics and Society Review of Artificial Intelligence Research,"Artificial intelligence (AI) research is routinely criticized for its real and potential impacts on society, and we lack adequate institutional responses to this criticism and to the responsibility that it reflects. AI research often falls outside the purview of existing feedback mechanisms such as the Institutional Review Board (IRB), which are designed to evaluate harms to human subjects rather than harms to human society. In response, we have developed the Ethics and Society Review board (ESR), a feedback panel that works with researchers to mitigate negative ethical and societal aspects of AI research. The ESR's main insight is to serve as a requirement for funding: researchers cannot receive grant funding from a major AI funding program at our university until the researchers complete the ESR process for the proposal. In this article, we describe the ESR as we have designed and run it over its first year across 41 proposals. We analyze aggregate ESR feedback on these proposals, finding that the panel most commonly identifies issues of harms to minority groups, inclusion of diverse stakeholders in the research plan, dual use, and representation in data. Surveys and interviews of researchers who interacted with the ESR found that 58% felt that it had influenced the design of their research project, 100% are willing to continue submitting future projects to the ESR, and that they sought additional scaffolding for reasoning through ethics and society issues.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Principles-based Ethics Assurance Argument Pattern for AI and Autonomous Systems,"An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be acceptably safe within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be ethically acceptable when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern, shortened to the acronym PRAISE, is described. The objective of the proposed PRAISE argument pattern is to provide a reusable template for individual ethics assurance cases, by which engineers, developers, operators, or regulators could justify, communicate, or challenge a claim about the overall ethical acceptability of the use of a specific AI/AS in a given socio-technical context. We apply the pattern to the hypothetical use case of an autonomous robo-taxi service in a city centre.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Operationalising AI governance through ethics-based auditing: An industry case study,"Ethics based auditing (EBA) is a structured process whereby an entitys past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may bridge the gap between principles and practice in AI ethics. However, important aspects of EBA (such as the feasibility and effectiveness of different auditing procedures) have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focused on proposing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics of Artificial Intelligence,"Artificial intelligence (AI) is a digital technology that will be of major importance for the development of humanity in the near future. AI has raised fundamental questions about what we should do with such systems, what the systems themselves should do, what risks they involve and how we can control these. - After the background to the field (1), this article introduces the main debates (2), first on ethical issues that arise with AI systems as objects, i.e. tools made and used by humans; here, the main sections are privacy (2.1), manipulation (2.2), opacity (2.3), bias (2.4), autonomy & responsibility (2.6) and the singularity (2.7). Then we look at AI systems as subjects, i.e. when ethics is for the AI systems themselves in machine ethics (2.8.) and artificial moral agency (2.9). Finally we look at future developments and the concept of AI (3). For each section within these themes, we provide a general explanation of the ethical issues, we outline existing positions and arguments, then we analyse how this plays out with current technologies and finally what policy consequences may be drawn.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Artificial Intelligence - An Open Question,"Artificial Intelligence (AI) is an effective science which employs strong enough approaches, methods, and techniques to solve unsolvable real world based problems. Because of its unstoppable rise towards the future, there are also some discussions about its ethics and safety. Shaping an AI friendly environment for people and a people friendly environment for AI can be a possible answer for finding a shared context of values for both humans and robots. In this context, objective of this paper is to address the ethical issues of AI and explore the moral dilemmas that arise from ethical algorithms, from pre set or acquired values. In addition, the paper will also focus on the subject of AI safety. As general, the paper will briefly analyze the concerns and potential solutions to solving the ethical issues presented and increase readers awareness on AI safety as another related research interest.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.   The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness Issues in AI Systems that Augment Sensory Abilities,"Systems that augment sensory abilities are increasingly employing AI and machine learning (ML) approaches, with applications ranging from object recognition and scene description tools for blind users to sound awareness tools for d/Deaf users. However, unlike many other AI-enabled technologies, these systems provide information that is already available to non-disabled people. In this paper, we discuss unique AI fairness challenges that arise in this context, including accessibility issues with data and models, ethical implications in deciding what sensory information to convey to the user, and privacy concerns both for the primary user and for others.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards Standardizing AI Bias Exploration,"Creating fair AI systems is a complex problem that involves the assessment of context-dependent bias concerns. Existing research and programming libraries express specific concerns as measures of bias that they aim to constrain or mitigate. In practice, one should explore a wide variety of (sometimes incompatible) measures before deciding which ones warrant corrective action, but their narrow scope means that most new situations can only be examined after devising new measures. In this work, we present a mathematical framework that distils literature measures of bias into building blocks, hereby facilitating new combinations to cover a wide range of fairness concerns, such as classification or recommendation differences across multiple multi-value sensitive attributes (e.g., many genders and races, and their intersections). We show how this framework generalizes existing concepts and present frequently used blocks. We provide an open-source implementation of our framework as a Python library, called FairBench, that facilitates systematic and extensible exploration of potential bias concerns.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems,"Artificial Intelligence has rapidly become a cornerstone technology, significantly influencing Europe's societal and economic landscapes. However, the proliferation of AI also raises critical ethical, legal, and regulatory challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency in Artificial Intelligence) project addresses these issues by developing a comprehensive framework that integrates regulatory compliance, ethical standards, and transparency into AI systems. In this position paper, we outline the methodological steps for building the core components of this framework. Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for structured AI lifecycle management, (ii) ontology-driven data lineage tracking to ensure traceability and accountability, and (iii) regulatory operations (RegOps) workflows to operationalize compliance requirements. By implementing and validating its solutions across diverse pilots, CERTAIN aims to advance regulatory compliance and to promote responsible AI innovation aligned with European standards.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Pursuit of Fairness in Artificial Intelligence Models: A Survey,"Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a comprehensive taxonomy by categorizing different types of bias and investigate cases of biased AI in different application domains. A thorough study is conducted of the approaches and techniques employed by researchers to mitigate bias in AI models. Moreover, we also delve into the impact of biased models on user experience and the ethical considerations to contemplate when developing and deploying such models. We hope this survey helps researchers and practitioners understand the intricate details of fairness and bias in AI systems. By sharing this thorough survey, we aim to promote additional discourse in the domain of equitable and responsible AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards an Environmental Ethics of Artificial Intelligence,"In recent years, much research has been dedicated to uncovering the environmental impact of Artificial Intelligence (AI), showing that training and deploying AI systems require large amounts of energy and resources, and the outcomes of AI may lead to decisions and actions that may negatively impact the environment. This new knowledge raises new ethical questions, such as: When is it (un)justifiable to develop an AI system, and how to make design choices, considering its environmental impact? However, so far, the environmental impact of AI has largely escaped ethical scrutiny, as AI ethics tends to focus strongly on themes such as transparency, privacy, safety, responsibility, and bias. Considering the environmental impact of AI from an ethical perspective expands the scope of AI ethics beyond an anthropocentric focus towards including more-than-human actors such as animals and ecosystems. This paper explores the ethical implications of the environmental impact of AI for designing AI systems by drawing on environmental justice literature, in which three categories of justice are distinguished, referring to three elements that can be unjust: the distribution of benefits and burdens (distributive justice), decision-making procedures (procedural justice), and institutionalized social norms (justice as recognition). Based on these tenets of justice, we outline criteria for developing environmentally just AI systems, given their ecological impact.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
On the Efficiency of Ethics as a Governing Tool for Artificial Intelligence,"The 4th Industrial Revolution is the culmination of the digital age. Nowadays, technologies such as robotics, nanotechnology, genetics, and artificial intelligence promise to transform our world and the way we live. Artificial Intelligence Ethics and Safety is an emerging research field that has been gaining popularity in recent years. Several private, public and non-governmental organizations have published guidelines proposing ethical principles for regulating the use and development of autonomous intelligent systems. Meta-analyses of the AI Ethics research field point to convergence on certain principles that supposedly govern the AI industry. However, little is known about the effectiveness of this form of Ethics. In this paper, we would like to conduct a critical analysis of the current state of AI Ethics and suggest that this form of governance based on principled ethical guidelines is not sufficient to norm the AI industry and its developers. We believe that drastic changes are necessary, both in the training processes of professionals in the fields related to the development of software and intelligent systems and in the increased regulation of these professionals and their industry. To this end, we suggest that law should benefit from recent contributions from bioethics, to make the contributions of AI ethics to governance explicit in legal terms.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
On the Limits of Design: What Are the Conceptual Constraints on Designing Artificial Intelligence for Social Good?,"Artificial intelligence AI can bring substantial benefits to society by helping to reduce costs, increase efficiency and enable new solutions to complex problems. Using Floridi's notion of how to design the 'infosphere' as a starting point, in this chapter I consider the question: what are the limits of design, i.e. what are the conceptual constraints on designing AI for social good? The main argument of this chapter is that while design is a useful conceptual tool to shape technologies and societies, collective efforts towards designing future societies are constrained by both internal and external factors. Internal constraints on design are discussed by evoking Hardin's thought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's classical distinction between 'cosmos' and 'taxis' is used to demarcate external constraints on design. Finally, five design principles are presented which are aimed at helping policymakers manage the internal and external constraints on design. A successful approach to designing future societies needs to account for the emergent properties of complex systems by allowing space for serendipity and socio-technological coevolution.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Explaining how your AI system is fair,"To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Toward Fairness in AI for People with Disabilities: A Research Roadmap,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
On the Current and Emerging Challenges of Developing Fair and Ethical AI Solutions in Financial Services,"Artificial intelligence (AI) continues to find more numerous and more critical applications in the financial services industry, giving rise to fair and ethical AI as an industry-wide objective. While many ethical principles and guidelines have been published in recent years, they fall short of addressing the serious challenges that model developers face when building ethical AI solutions. We survey the practical and overarching issues surrounding model development, from design and implementation complexities, to the shortage of tools, and the lack of organizational constructs. We show how practical considerations reveal the gaps between high-level principles and concrete, deployed AI applications, with the aim of starting industry-wide conversations toward solution approaches.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Ethics-Based Auditing of Automated Decision-Making Systems: Nature, Scope, and Limitations","Important decisions that impact human lives, livelihoods, and the natural environment are increasingly being automated. Delegating tasks to so-called automated decision-making systems (ADMS) can improve efficiency and enable new solutions. However, these benefits are coupled with ethical challenges. For example, ADMS may produce discriminatory outcomes, violate individual privacy, and undermine human self-determination. New governance mechanisms are thus needed that help organisations design and deploy ADMS in ways that are ethical, while enabling society to reap the full economic and social benefits of automation. In this article, we consider the feasibility and efficacy of ethics-based auditing (EBA) as a governance mechanism that allows organisations to validate claims made about their ADMS. Building on previous work, we define EBA as a structured process whereby an entity's present or past behaviour is assessed for consistency with relevant principles or norms. We then offer three contributions to the existing literature. First, we provide a theoretical explanation of how EBA can contribute to good governance by promoting procedural regularity and transparency. Second, we propose seven criteria for how to design and implement EBA procedures successfully. Third, we identify and discuss the conceptual, technical, social, economic, organisational, and institutional constraints associated with EBA. We conclude that EBA should be considered an integral component of multifaced approaches to managing the ethical risks posed by ADMS.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI as a Tool for Fair Journalism: Case Studies from Malta,"In today`s media landscape, the role of Artificial Intelligence (AI) in shaping societal perspectives and journalistic integrity is becoming increasingly apparent. This paper presents two case studies centred on Malta`s media market featuring technical novelty. Despite its relatively small scale, Malta offers invaluable insights applicable to both similar and broader media contexts. These two projects focus on media monitoring and present tools designed to analyse potential biases in news articles and television news segments. The first project uses Computer Vision and Natural Language Processing techniques to analyse the coherence between images in news articles and their corresponding captions, headlines, and article bodies. The second project employs computer vision techniques to track individuals` on-screen time or visual exposure in news videos, providing queryable data. These initiatives aim to contribute to society by providing both journalists and the public with the means to identify biases. Furthermore, we make these tools accessible to journalists to improve the trustworthiness of media outlets by offering robust tools for detecting and reducing bias.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Hard Choices in Artificial Intelligence,"As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An Elemental Ethics for Artificial Intelligence: Water as Resistance Within AI's Value Chain,"Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning Artificial Intelligence (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI's footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term 'elemental ethics'. Elemental ethics interrogates the AI value chain's problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial Intelligence,"Artificial Intelligence (AI) aims to elevate healthcare to a pinnacle by aiding clinical decision support. Overcoming the challenges related to the design of ethical AI will enable clinicians, physicians, healthcare professionals, and other stakeholders to use and trust AI in healthcare settings. This study attempts to identify the major ethical principles influencing the utility performance of AI at different technological levels such as data access, algorithms, and systems through a thematic analysis. We observed that justice, privacy, bias, lack of regulations, risks, and interpretability are the most important principles to consider for ethical AI. This data-driven study has analyzed secondary survey data from the Pew Research Center (2020) of 36 AI experts to categorize the top ethical principles of AI design. To resolve the ethical issues identified by the meta-analysis and domain experts, we propose a new utilitarian ethics-based theoretical framework for designing ethical AI for the healthcare domain.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical AI in Retail: Consumer Privacy and Fairness,"The adoption of artificial intelligence (AI) in retail has significantly transformed the industry, enabling more personalized services and efficient operations. However, the rapid implementation of AI technologies raises ethical concerns, particularly regarding consumer privacy and fairness. This study aims to analyze the ethical challenges of AI applications in retail, explore ways retailers can implement AI technologies ethically while remaining competitive, and provide recommendations on ethical AI practices. A descriptive survey design was used to collect data from 300 respondents across major e-commerce platforms. Data were analyzed using descriptive statistics, including percentages and mean scores. Findings shows a high level of concerns among consumers regarding the amount of personal data collected by AI-driven retail applications, with many expressing a lack of trust in how their data is managed. Also, fairness is another major issue, as a majority believe AI systems do not treat consumers equally, raising concerns about algorithmic bias. It was also found that AI can enhance business competitiveness and efficiency without compromising ethical principles, such as data privacy and fairness. Data privacy and transparency were highlighted as critical areas where retailers need to focus their efforts, indicating a strong demand for stricter data protection protocols and ongoing scrutiny of AI systems. The study concludes that retailers must prioritize transparency, fairness, and data protection when deploying AI systems. The study recommends ensuring transparency in AI processes, conducting regular audits to address biases, incorporating consumer feedback in AI development, and emphasizing consumer data privacy.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation","Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence","In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review that highlights the cross-referencing of these angles by discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical facts, current world circumstances, and possible ensuing realities.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism,"This paper introduces AIJIM, the Artificial Intelligence Journalism Integration Model -- a novel framework for integrating real-time AI into environmental journalism. AIJIM combines Vision Transformer-based hazard detection, crowdsourced validation with 252 validators, and automated reporting within a scalable, modular architecture. A dual-layer explainability approach ensures ethical transparency through fast CAM-based visual overlays and optional LIME-based box-level interpretations. Validated in a 2024 pilot on the island of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\% detection accuracy and 89.7\% agreement with expert annotations, while reducing reporting latency by 40\%. Unlike conventional approaches such as Data-Driven Journalism or AI Fact-Checking, AIJIM provides a transferable model for participatory, community-driven environmental reporting, advancing journalism, artificial intelligence, and sustainability in alignment with the UN Sustainable Development Goals and the EU AI Act.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ideal theory in AI ethics,"This paper addresses the ways AI ethics research operates on an ideology of ideal theory, in the sense discussed by Mills (2005) and recently applied to AI ethics by Fazelpour \& Lipton (2020). I address the structural and methodological conditions that attract AI ethics researchers to ideal theorizing, and the consequences this approach has for the quality and future of our research community. Finally, I discuss the possibilities for a nonideal future in AI ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The E.U.'s Artificial Intelligence Act: An Ordoliberal Assessment,"In light of the rise of generative AI and recent debates about the socio-political implications of large-language models and chatbots, this article investigates the E.U.'s Artificial Intelligence Act (AIA), the world's first major attempt by a government body to address and mitigate the potentially negative impacts of AI technologies. The article critically analyzes the AIA from a distinct economic ethics perspective, i.e., ordoliberalism 2.0 - a perspective currently lacking in the academic literature. It evaluates, in particular, the AIA's ordoliberal strengths and weaknesses and proposes reform measures that could be taken to strengthen the AIA.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders","Numerous fairness metrics have been proposed and employed by artificial intelligence (AI) experts to quantitatively measure bias and define fairness in AI models. Recognizing the need to accommodate stakeholders' diverse fairness understandings, efforts are underway to solicit their input. However, conveying AI fairness metrics to stakeholders without AI expertise, capturing their personal preferences, and seeking a collective consensus remain challenging and underexplored. To bridge this gap, we propose a new framework, EARN Fairness, which facilitates collective metric decisions among stakeholders without requiring AI expertise. The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics, Ask stakeholders' personal metric preferences, Review metrics collectively, and Negotiate a consensus on metric selection. To gather empirical results, we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without AI knowledge. We identify their personal metric preferences and their acceptable level of unfairness in individual sessions. Subsequently, we uncovered how they reached metric consensus in team sessions. Our work shows that the EARN Fairness framework enables stakeholders to express personal preferences and reach consensus, providing practical guidance for implementing human-centered AI fairness in high-risk contexts. Through this approach, we aim to harmonize fairness expectations of diverse stakeholders, fostering more equitable and inclusive AI fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies,"Artificial intelligence (AI) offers incredible possibilities for patient care, but raises significant ethical issues, such as the potential for bias. Powerful ethical frameworks exist to minimize these issues, but are often developed for academic or regulatory environments and tend to be comprehensive but overly prescriptive, making them difficult to operationalize within fast-paced, resource-constrained environments. We introduce the Scalable Agile Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with business priorities by embedding ethical oversight into standard Agile-based product development workflows. The framework emphasizes the early establishment of testable acceptance criteria, fairness metrics, and transparency metrics to manage model uncertainty, while also promoting continuous monitoring and re-evaluation of these metrics across the AI lifecycle. A core component of this framework are responsibility metrics using scenario-based probability analogy mapping designed to enhance transparency and stakeholder trust. This ensures that retraining or tuning activities are subject to lightweight but meaningful ethical review. By focusing on the minimum necessary requirements for responsible development, our framework offers a scalable, business-aligned approach to ethical AI suitable for organizations without dedicated ethics teams.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics of Artificial Intelligence in Surgery,"Here we discuss the four key principles of bio-medical ethics from surgical context. We elaborate on the definition of 'fairness' and its implications in AI system design, with taxonomy of algorithmic biases in AI. We discuss the shifts in ethical paradigms as the degree of autonomy in AI systems continue to evolve. We also emphasize the need for continuous revisions of ethics in AI due to evolution and dynamic nature of AI systems and technologies.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An FDA for AI? Pitfalls and Plausibility of Approval Regulation for Frontier Artificial Intelligence,"Observers and practitioners of artificial intelligence (AI) have proposed an FDA-style licensing regime for the most advanced AI models, or 'frontier' models. In this paper, we explore the applicability of approval regulation -- that is, regulation of a product that combines experimental minima with government licensure conditioned partially or fully upon that experimentation -- to the regulation of frontier AI. There are a number of reasons to believe that approval regulation, simplistically applied, would be inapposite for frontier AI risks. Domains of weak fit include the difficulty of defining the regulated product, the presence of Knightian uncertainty or deep ambiguity about harms from AI, the potentially transmissible nature of risks, and distributed activities among actors involved in the AI lifecycle. We conclude by highlighting the role of policy learning and experimentation in regulatory development, describing how learning from other forms of AI regulation and improvements in evaluation and testing methods can help to overcome some of the challenges we identify.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Data Ethics in the Era of Healthcare Artificial Intelligence in Africa: An Ubuntu Philosophy Perspective,"Data are essential in developing healthcare artificial intelligence (AI) systems. However, patient data collection, access, and use raise ethical concerns, including informed consent, data bias, data protection and privacy, data ownership, and benefit sharing. Various ethical frameworks have been proposed to ensure the ethical use of healthcare data and AI, however, these frameworks often align with Western cultural values, social norms, and institutional contexts emphasizing individual autonomy and well-being. Ethical guidelines must reflect political and cultural settings to account for cultural diversity, inclusivity, and historical factors such as colonialism. Thus, this paper discusses healthcare data ethics in the AI era in Africa from the Ubuntu philosophy perspective. It focuses on the contrast between individualistic and communitarian approaches to data ethics. The proposed framework could inform stakeholders, including AI developers, healthcare providers, the public, and policy-makers about healthcare data ethical usage in AI in Africa.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries","As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards an Ethical and Inclusive Implementation of Artificial Intelligence in Organizations: A Multidimensional Framework,"This article analyzes the impact of artificial intelligence (AI) on contemporary society and the importance of adopting an ethical approach to its development and implementation within organizations. It examines the technocritical perspective of some philosophers and researchers, who warn of the risks of excessive technologization that could undermine human autonomy. However, the article also acknowledges the active role that various actors, such as governments, academics, and civil society, can play in shaping the development of AI aligned with human and social values.   A multidimensional approach is proposed that combines ethics with regulation, innovation, and education. It highlights the importance of developing detailed ethical frameworks, incorporating ethics into the training of professionals, conducting ethical impact audits, and encouraging the participation of stakeholders in the design of AI.   In addition, four fundamental pillars are presented for the ethical implementation of AI in organizations: 1) Integrated values, 2) Trust and transparency, 3) Empowering human growth, and 4) Identifying strategic factors. These pillars encompass aspects such as alignment with the company's ethical identity, governance and accountability, human-centered design, continuous training, and adaptability to technological and market changes.   The conclusion emphasizes that ethics must be the cornerstone of any organization's strategy that seeks to incorporate AI, establishing a solid framework that ensures that technology is developed and used in a way that respects and promotes human values.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents,"This paper presents a new approach to prevent transportation accidents and monitor driver's behavior using a healthcare AI system that incorporates fairness and ethics. Dangerous medical cases and unusual behavior of the driver are detected. Fairness algorithm is approached in order to improve decision-making and address ethical issues such as privacy issues, and to consider challenges that appear in the wild within AI in healthcare and driving. A healthcare professional will be alerted about any unusual activity, and the driver's location when necessary, is provided in order to enable the healthcare professional to immediately help to the unstable driver. Therefore, using the healthcare AI system allows for accidents to be predicted and thus prevented and lives may be saved based on the built-in AI system inside the vehicle which interacts with the ER system.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Measuring Ethics in AI with AI: A Methodology and Dataset Construction,"Recently, the use of sound measures and metrics in Artificial Intelligence has become the subject of interest of academia, government, and industry. Efforts towards measuring different phenomena have gained traction in the AI community, as illustrated by the publication of several influential field reports and policy documents. These metrics are designed to help decision takers to inform themselves about the fast-moving and impacting influences of key advances in Artificial Intelligence in general and Machine Learning in particular. In this paper we propose to use such newfound capabilities of AI technologies to augment our AI measuring capabilities. We do so by training a model to classify publications related to ethical issues and concerns. In our methodology we use an expert, manually curated dataset as the training set and then evaluate a large set of research papers. Finally, we highlight the implications of AI metrics, in particular their contribution towards developing trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI Fairness; AI Measurement. Ethics in Computer Science.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Activism by the AI Community: Analysing Recent Achievements and Future Prospects,"The artificial intelligence community (AI) has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI talent. Both are crucial to the future of AI activism and worthy of sustained attention.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Limits of Fair Medical Imaging AI In The Wild,"As artificial intelligence (AI) rapidly approaches human-level performance in medical imaging, it is crucial that it does not exacerbate or propagate healthcare disparities. Prior research has established AI's capacity to infer demographic data from chest X-rays, leading to a key concern: do models using demographic shortcuts have unfair predictions across subpopulations? In this study, we conduct a thorough investigation into the extent to which medical AI utilizes demographic encodings, focusing on potential fairness discrepancies within both in-distribution training sets and external test sets. Our analysis covers three key medical imaging disciplines: radiology, dermatology, and ophthalmology, and incorporates data from six global chest X-ray datasets. We confirm that medical imaging AI leverages demographic shortcuts in disease classification. While correcting shortcuts algorithmically effectively addresses fairness gaps to create ""locally optimal"" models within the original data distribution, this optimality is not true in new test settings. Surprisingly, we find that models with less encoding of demographic attributes are often most ""globally optimal"", exhibiting better fairness during model evaluation in new test environments. Our work establishes best practices for medical imaging models which maintain their performance and fairness in deployments beyond their initial training contexts, underscoring critical considerations for AI clinical deployments across populations and sites.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
On Fairness and Interpretability,"Ethical AI spans a gamut of considerations. Among these, the most popular ones, fairness and interpretability, have remained largely distinct in technical pursuits. We discuss and elucidate the differences between fairness and interpretability across a variety of dimensions. Further, we develop two principles-based frameworks towards developing ethical AI for the future that embrace aspects of both fairness and interpretability. First, interpretability for fairness proposes instantiating interpretability within the realm of fairness to develop a new breed of ethical AI. Second, fairness and interpretability initiates deliberations on bringing the best aspects of both together. We hope that these two frameworks will contribute to intensifying scholarly discussions on new frontiers of ethical AI that brings together fairness and interpretability.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Artificial Intelligence Governance and Ethics: Global Perspectives,"Artificial intelligence (AI) is a technology which is increasingly being utilised in society and the economy worldwide, and its implementation is planned to become more prevalent in coming years. AI is increasingly being embedded in our lives, supplementing our pervasive use of digital technologies. But this is being accompanied by disquiet over problematic and dangerous implementations of AI, or indeed, even AI itself deciding to do dangerous and problematic actions, especially in fields such as the military, medicine and criminal justice. These developments have led to concerns about whether and how AI systems adhere, and will adhere to ethical standards. These concerns have stimulated a global conversation on AI ethics, and have resulted in various actors from different countries and sectors issuing ethics and governance initiatives and guidelines for AI. Such developments form the basis for our research in this report, combining our international and interdisciplinary expertise to give an insight into what is happening in Australia, China, Europe, India and the US.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Technology of Outrage: Bias in Artificial Intelligence,"Artificial intelligence and machine learning are increasingly used to offload decision making from people. In the past, one of the rationales for this replacement was that machines, unlike people, can be fair and unbiased. Evidence suggests otherwise. We begin by entertaining the ideas that algorithms can replace people and that algorithms cannot be biased. Taken as axioms, these statements quickly lead to absurdity. Spurred on by this result, we investigate the slogans more closely and identify equivocation surrounding the word 'bias.' We diagnose three forms of outrage-intellectual, moral, and political-that are at play when people react emotionally to algorithmic bias. Then we suggest three practical approaches to addressing bias that the AI community could take, which include clarifying the language around bias, developing new auditing methods for intelligent systems, and building certain capabilities into these systems. We conclude by offering a moral regarding the conversations about algorithmic bias that may transfer to other areas of artificial intelligence.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective,"The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Mimetic Models: Ethical Implications of AI that Acts Like You,"An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Criticizing Ethics According to Artificial Intelligence,"This article presents a critique of ethics in the context of artificial intelligence (AI). It argues for the need to question established patterns of thought and traditional authorities, including core concepts such as autonomy, morality, and ethics. These concepts are increasingly inadequate to deal with the complexities introduced by emerging AI and autonomous agents. This critique has several key components: clarifying conceptual ambiguities, honestly addressing epistemic issues, and thoroughly exploring fundamental normative problems. The ultimate goal is to reevaluate and possibly redefine some traditional ethical concepts to better address the challenges posed by AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Conservative AI and social inequality: Conceptualizing alternatives to bias through social theory,"In response to calls for greater interdisciplinary involvement from the social sciences and humanities in the development, governance, and study of artificial intelligence systems, this paper presents one sociologist's view on the problem of algorithmic bias and the reproduction of societal bias. Discussions of bias in AI cover much of the same conceptual terrain that sociologists studying inequality have long understood using more specific terms and theories. Concerns over reproducing societal bias should be informed by an understanding of the ways that inequality is continually reproduced in society -- processes that AI systems are either complicit in, or can be designed to disrupt and counter. The contrast presented here is between conservative and radical approaches to AI, with conservatism referring to dominant tendencies that reproduce and strengthen the status quo, while radical approaches work to disrupt systemic forms of inequality. The limitations of conservative approaches to class, gender, and racial bias are discussed as specific examples, along with the social structures and processes that biases in these areas are linked to. Societal issues can no longer be out of scope for AI and machine learning, given the impact of these systems on human lives. This requires engagement with a growing body of critical AI scholarship that goes beyond biased data to analyze structured ways of perpetuating inequality, opening up the possibility for radical alternatives.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, And Mitigation Strategies","The significant advancements in applying Artificial Intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey paper offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases - highlighting the emergent issue of generative AI bias where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on the perpetuation of inequalities and the reinforcement of harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discussing the ethical considerations of their implementation and emphasizing the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users","Interest is growing in artificial empathy, but so is confusion about what artificial empathy is or needs to be. This confusion makes it challenging to navigate the technical and ethical issues that accompany empathic AI development. Here, we outline a framework for thinking about empathic AI based on the premise that different constellations of capabilities associated with empathy are important for different empathic AI applications. We describe distinctions of capabilities that we argue belong under the empathy umbrella, and show how three medical empathic AI use cases require different sets of these capabilities. We conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both AI creators and users.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Conflict Between People's Urge to Punish AI and Legal Systems,"Regulating artificial intelligence (AI) has become necessary in light of its deployment in high-risk scenarios. This paper explores the proposal to extend legal personhood to AI and robots, which had not yet been examined through the lens of the general public. We present two studies (N = 3,559) to obtain people's views of electronic legal personhood vis--vis existing liability models. Our study reveals people's desire to punish automated agents even though these entities are not recognized any mental state. Furthermore, people did not believe automated agents' punishment would fulfill deterrence nor retribution and were unwilling to grant them legal punishment preconditions, namely physical independence and assets. Collectively, these findings suggest a conflict between the desire to punish automated agents and its perceived impracticability. We conclude by discussing how future design and legal decisions may influence how the public reacts to automated agents' wrongdoings.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Descriptive AI Ethics: Collecting and Understanding the Public Opinion,"There is a growing need for data-driven research efforts on how the public perceives the ethical, moral, and legal issues of autonomous AI systems. The current debate on the responsibility gap posed by these systems is one such example. This work proposes a mixed AI ethics model that allows normative and descriptive research to complement each other, by aiding scholarly discussion with data gathered from the public. We discuss its implications on bridging the gap between optimistic and pessimistic views towards AI systems' deployment.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Ethics of AI Ethics -- An Evaluation of Guidelines,"Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the ""disruptive"" potentials of new AI technologies. Designed as a comprehensive evaluation, this paper analyzes and compares these guidelines highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems - and how the effectiveness in the demands of AI ethics can be improved.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI,"AI has made significant strides recently, leading to various applications in both civilian and military sectors. The military sees AI as a solution for developing more effective and faster technologies. While AI offers benefits like improved operational efficiency and precision targeting, it also raises serious ethical and legal concerns, particularly regarding human rights violations. Autonomous weapons that make decisions without human input can threaten the right to life and violate international humanitarian law. To address these issues, we propose a three-stage framework (Design, In Deployment, and During/After Use) for evaluating human rights concerns in the design, deployment, and use of military AI. Each phase includes multiple components that address various concerns specific to that phase, ranging from bias and regulatory issues to violations of International Humanitarian Law. By this framework, we aim to balance the advantages of AI in military operations with the need to protect human rights.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards a Feminist Metaethics of AI,"The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Defining the Collective Intelligence Supply Chain,"Organisations are increasingly open to scrutiny, and need to be able to prove that they operate in a fair and ethical way. Accountability should extend to the production and use of the data and knowledge assets used in AI systems, as it would for any raw material or process used in production of physical goods. This paper considers collective intelligence, comprising data and knowledge generated by crowd-sourced workforces, which can be used as core components of AI systems. A proposal is made for the development of a supply chain model for tracking the creation and use of crowdsourced collective intelligence assets, with a blockchain based decentralised architecture identified as an appropriate means of providing validation, accountability and fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics in Smart Healthcare,"This article reviews the landscape of ethical challenges of integrating artificial intelligence (AI) into smart healthcare products, including medical electronic devices. Differences between traditional ethics in the medical domain and emerging ethical challenges with AI-driven healthcare are presented, particularly as they relate to transparency, bias, privacy, safety, responsibility, justice, and autonomy. Open challenges and recommendations are outlined to enable the integration of ethical principles into the design, validation, clinical trials, deployment, monitoring, repair, and retirement of AI-based smart healthcare products.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Challenges and Evolving Strategies in the Integration of Artificial Intelligence into Clinical Practice,"Artificial intelligence (AI) has rapidly transformed various sectors, including healthcare, where it holds the potential to revolutionize clinical practice and improve patient outcomes. However, its integration into medical settings brings significant ethical challenges that need careful consideration. This paper examines the current state of AI in healthcare, focusing on five critical ethical concerns: justice and fairness, transparency, patient consent and confidentiality, accountability, and patient-centered and equitable care. These concerns are particularly pressing as AI systems can perpetuate or even exacerbate existing biases, often resulting from non-representative datasets and opaque model development processes. The paper explores how bias, lack of transparency, and challenges in maintaining patient trust can undermine the effectiveness and fairness of AI applications in healthcare. In addition, we review existing frameworks for the regulation and deployment of AI, identifying gaps that limit the widespread adoption of these systems in a just and equitable manner. Our analysis provides recommendations to address these ethical challenges, emphasizing the need for fairness in algorithm design, transparency in model decision-making, and patient-centered approaches to consent and data privacy. By highlighting the importance of continuous ethical scrutiny and collaboration between AI developers, clinicians, and ethicists, we outline pathways for achieving more responsible and inclusive AI implementation in healthcare. These strategies, if adopted, could enhance both the clinical value of AI and the trustworthiness of AI systems among patients and healthcare professionals, ensuring that these technologies serve all populations equitably.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Prioritizing Policies for Furthering Responsible Artificial Intelligence in the United States,"Several policy options exist, or have been proposed, to further responsible artificial intelligence (AI) development and deployment. Institutions, including U.S. government agencies, states, professional societies, and private and public sector businesses, are well positioned to implement these policies. However, given limited resources, not all policies can or should be equally prioritized. We define and review nine suggested policies for furthering responsible AI, rank each policy on potential use and impact, and recommend prioritization relative to each institution type. We find that pre-deployment audits and assessments and post-deployment accountability are likely to have the highest impact but also the highest barriers to adoption. We recommend that U.S. government agencies and companies highly prioritize development of pre-deployment audits and assessments, while the U.S. national legislature should highly prioritize post-deployment accountability. We suggest that U.S. government agencies and professional societies should highly prioritize policies that support responsible AI research and that states should highly prioritize support of responsible AI education. We propose that companies can highly prioritize involving community stakeholders in development efforts and supporting diversity in AI development. We advise lower levels of prioritization across institutions for AI ethics statements and databases of AI technologies or incidents. We recognize that no one policy will lead to responsible AI and instead advocate for strategic policy implementation across institutions.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Explainable AI is Responsible AI: How Explainability Creates Trustworthy and Socially Responsible Artificial Intelligence,"Artificial intelligence (AI) has been clearly established as a technology with the potential to revolutionize fields from healthcare to finance - if developed and deployed responsibly. This is the topic of responsible AI, which emphasizes the need to develop trustworthy AI systems that minimize bias, protect privacy, support security, and enhance transparency and accountability. Explainable AI (XAI) has been broadly considered as a building block for responsible AI (RAI), with most of the literature considering it as a solution for improved transparency. This work proposes that XAI and responsible AI are significantly more deeply entwined. In this work, we explore state-of-the-art literature on RAI and XAI technologies. Based on our findings, we demonstrate that XAI can be utilized to ensure fairness, robustness, privacy, security, and transparency in a wide range of contexts. Our findings lead us to conclude that XAI is an essential foundation for every pillar of RAI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Algorithmic Fairness,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Immune Moral Models? Pro-Social Rule Breaking as a Moral Enhancement Approach for Ethical AI,"We are moving towards a future where Artificial Intelligence (AI) based agents make many decisions on behalf of humans. From healthcare decision making to social media censoring, these agents face problems, and make decisions with ethical and societal implications. Ethical behaviour is a critical characteristic that we would like in a human-centric AI. A common observation in human-centric industries, like the service industry and healthcare, is that their professionals tend to break rules, if necessary, for pro-social reasons. This behaviour among humans is defined as pro-social rule breaking. To make AI agents more human centric, we argue that there is a need for a mechanism that helps AI agents identify when to break rules set by their designers. To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons. In this paper, we present a study that introduces a 'vaccination strategy dilemma' to human participants and analyses their responses. In this dilemma, one needs to decide whether they would distribute Covid-19 vaccines only to members of a high-risk group (follow the enforced rule) or, in selected cases, administer the vaccine to a few social influencers (break the rule), which might yield an overall greater benefit to society. The results of the empirical study suggest a relationship between stakeholder utilities and pro-social rule breaking (PSRB), which neither deontological nor utilitarian ethics completely explain. Finally, the paper discusses the design characteristics of an ethical agent capable of PSRB and the future research directions on PSRB in the AI realm. We hope that this will inform the design of future AI agents, and their decision-making behaviour.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics through the Facets of Artificial Intelligence,"Artificial Intelligence (AI) has received unprecedented attention in recent years, raising ethical concerns about the development and use of AI technology. In the present article, we advocate that these concerns stem from a blurred understanding of AI, how it can be used, and how it has been interpreted in society. We explore the concept of AI based on three descriptive facets and consider ethical issues related to each facet. Finally, we propose a framework for the ethical assessment of the use of AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Oxford Handbook on AI Ethics Book Chapter on Race and Gender,"From massive face-recognition-based surveillance and machine-learning-based decision systems predicting crime recidivism rates, to the move towards automated health diagnostic systems, artificial intelligence (AI) is being used in scenarios that have serious consequences in people's lives. However, this rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial face recognition systems have much higher error rates for dark skinned women while having minimal errors on light skinned men. A 2016 ProPublica investigation uncovered that machine learning based tools that assess crime recidivism rates in the US are biased against African Americans. Other studies show that natural language processing tools trained on newspapers exhibit societal biases (e.g. finishing the analogy ""Man is to computer programmer as woman is to X"" by homemaker). At the same time, books such as Weapons of Math Destruction and Automated Inequality detail how people in lower socioeconomic classes in the US are subjected to more automated decision making tools than those who are in the upper class. Thus, these tools are most often used on people towards whom they exhibit the most bias. While many technical solutions have been proposed to alleviate bias in machine learning systems, we have to take a holistic and multifaceted approach. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Beyond ""Fairness:"" Structural (In)justice Lenses on AI for Education","Educational technologies, and the systems of schooling in which they are deployed, enact particular ideologies about what is important to know and how learners should learn. As artificial intelligence technologies -- in education and beyond -- may contribute to inequitable outcomes for marginalized communities, various approaches have been developed to evaluate and mitigate the harmful impacts of AI. However, we argue in this paper that the dominant paradigm of evaluating fairness on the basis of performance disparities in AI models is inadequate for confronting the systemic inequities that educational AI systems (re)produce. We draw on a lens of structural injustice informed by critical theory and Black feminist scholarship to critically interrogate several widely-studied and widely-adopted categories of educational AI and explore how they are bound up in and reproduce historical legacies of structural injustice and inequity, regardless of the parity of their models' performance. We close with alternative visions for a more equitable future for educational AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Future of Work Is Here: Toward a Comprehensive Approach to Artificial Intelligence and Labour,"This commentary traces contemporary discourses on the relationship between artificial intelligence and labour and explains why these principles must be comprehensive in their approach to labour and AI. First, the commentary asserts that ethical frameworks in AI alone are not enough to guarantee workers' rights since they lack enforcement mechanisms and the representation of different stakeholders. Secondly, it argues that current discussions on AI and labour focus on the deployment of these technologies in the workplace but ignore the essential role of human labour in their development, particularly in the different cases of outsourced labour around the world. Finally, it recommends using existing human rights frameworks for working conditions to provide more comprehensive ethical principles and regulations. The commentary concludes by arguing that the central question regarding the future of work should not be whether intelligent machines will replace humans, but who will own these systems and have a say in their development and operation.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Desiderata for Explainable AI in statistical production systems of the European Central Bank,"Explainable AI constitutes a fundamental step towards establishing fairness and addressing bias in algorithmic decision-making. Despite the large body of work on the topic, the benefit of solutions is mostly evaluated from a conceptual or theoretical point of view and the usefulness for real-world use cases remains uncertain. In this work, we aim to state clear user-centric desiderata for explainable AI reflecting common explainability needs experienced in statistical production systems of the European Central Bank. We link the desiderata to archetypical user roles and give examples of techniques and methods which can be used to address the user's needs. To this end, we provide two concrete use cases from the domain of statistical data production in central banks: the detection of outliers in the Centralised Securities Database and the data-driven identification of data quality checks for the Supervisory Banking data system.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An Ethical Framework for Guiding the Development of Affectively-Aware Artificial Intelligence,"The recent rapid advancements in artificial intelligence research and deployment have sparked more discussion about the potential ramifications of socially- and emotionally-intelligent AI. The question is not if research can produce such affectively-aware AI, but when it will. What will it mean for society when machines -- and the corporations and governments they serve -- can ""read"" people's minds and emotions? What should developers and operators of such AI do, and what should they not do? The goal of this article is to pre-empt some of the potential implications of these developments, and propose a set of guidelines for evaluating the (moral and) ethical consequences of affectively-aware AI, in order to guide researchers, industry professionals, and policy-makers. We propose a multi-stakeholder analysis framework that separates the ethical responsibilities of AI Developers vis--vis the entities that deploy such AI -- which we term Operators. Our analysis produces two pillars that clarify the responsibilities of each of these stakeholders: Provable Beneficence, which rests on proving the effectiveness of the AI, and Responsible Stewardship, which governs responsible collection, use, and storage of data and the decisions made from such data. We end with recommendations for researchers, developers, operators, as well as regulators and law-makers.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance","Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Responsible Data Stewardship: Generative AI and the Digital Waste Problem,"As generative AI systems become widely adopted, they enable unprecedented creation levels of synthetic data across text, images, audio, and video modalities. While research has addressed the energy consumption of model training and inference, a critical sustainability challenge remains understudied: digital waste. This term refers to stored data that consumes resources without serving a specific (and/or immediate) purpose. This paper presents this terminology in the AI context and introduces digital waste as an ethical imperative within (generative) AI development, positioning environmental sustainability as core for responsible innovation. Drawing from established digital resource management approaches, we examine how other disciplines manage digital waste and identify transferable approaches for the AI community. We propose specific recommendations encompassing re-search directions, technical interventions, and cultural shifts to mitigate the environmental consequences of in-definite data storage. By expanding AI ethics beyond immediate concerns like bias and privacy to include inter-generational environmental justice, this work contributes to a more comprehensive ethical framework that considers the complete lifecycle impact of generative AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Augmentation Technologies and AI - An Ethical Design Futures Framework,"Augmentation technologies, fueled by Artificial Intelligence (AI), are undergoing a process of adaptation and normalization geared to everyday users in various roles as practitioners, educators, and students. While new innovations, applications, and algorithms are developed as augmentation technology, Chapter 1 focuses on human subjects, contexts, and rhetorical strategies proposed for them by external actors. The chapter discusses core functions of technical and professional communication and provides rationale for positioning technical and professional communicators (TPCs) to understand augmentation technologies and AI as a means to design ethical futures across this work. An overview of Augmentation Technologies and AI- An Ethical Design Futures Framework serves as a guide for reframing professional practice and pedagogy to promote digital and AI literacy surrounding the ethical design, adoption, and adaptation of augmentation technologies. The chapter concludes with an overview of the remaining chapters in this book.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Disclosure and Evaluation as Fairness Interventions for General-Purpose AI,"Despite conflicting definitions and conceptions of fairness, AI fairness researchers broadly agree that fairness is context-specific. However, when faced with general-purpose AI, which by definition serves a range of contexts, how should we think about fairness? We argue that while we cannot be prescriptive about what constitutes fair outcomes, we can specify the processes that different stakeholders should follow in service of fairness. Specifically, we consider the obligations of two major groups: system providers and system deployers. While system providers are natural candidates for regulatory attention, the current state of AI understanding offers limited insight into how upstream factors translate into downstream fairness impacts. Thus, we recommend that providers invest in evaluative research studying how model development decisions influence fairness and disclose whom they are serving their models to, or at the very least, reveal sufficient information for external researchers to conduct such research. On the other hand, system deployers are closer to real-world contexts and can leverage their proximity to end users to address fairness harms in different ways. Here, we argue they should responsibly disclose information about users and personalization and conduct rigorous evaluations across different levels of fairness. Overall, instead of focusing on enforcing fairness outcomes, we prioritize intentional information-gathering by system providers and deployers that can facilitate later context-aware action. This allows us to be specific and concrete about the processes even while the contexts remain unknown. Ultimately, this approach can sharpen how we distribute fairness responsibilities and inform more fluid, context-sensitive interventions as AI continues to advance.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Feeling Machines: Ethics, Culture, and the Rise of Emotional AI","This paper explores the growing presence of emotionally responsive artificial intelligence through a critical and interdisciplinary lens. Bringing together the voices of early-career researchers from multiple fields, it explores how AI systems that simulate or interpret human emotions are reshaping our interactions in areas such as education, healthcare, mental health, caregiving, and digital life. The analysis is structured around four central themes: the ethical implications of emotional AI, the cultural dynamics of human-machine interaction, the risks and opportunities for vulnerable populations, and the emerging regulatory, design, and technical considerations. The authors highlight the potential of affective AI to support mental well-being, enhance learning, and reduce loneliness, as well as the risks of emotional manipulation, over-reliance, misrepresentation, and cultural bias. Key challenges include simulating empathy without genuine understanding, encoding dominant sociocultural norms into AI systems, and insufficient safeguards for individuals in sensitive or high-risk contexts. Special attention is given to children, elderly users, and individuals with mental health challenges, who may interact with AI in emotionally significant ways. However, there remains a lack of cognitive or legal protections which are necessary to navigate such engagements safely. The report concludes with ten recommendations, including the need for transparency, certification frameworks, region-specific fine-tuning, human oversight, and longitudinal research. A curated supplementary section provides practical tools, models, and datasets to support further work in this domain.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics and Artificial Intelligence Adoption,"In recent years, we have witnessed a marked development and growth in Artificial Intelligence. The growth of the data volume generated by sensors and machines, combined with the information flow resulting from the user actions on the Internet, with high investments of the governments and the companies in this area, provided the practice and developed the algorithms of the Artificial Intelligence However, the people, in general, started to feel a particular fear regarding the security and privacy of their data and the theme of the Artificial Intelligence Ethics began to be discussed more regularly. The investigation aim of this work is to understand the possibility of adopting Artificial Intelligence nowadays in our society, having, as a mandatory assumption, Ethics and respect towards data and people's privacy. With that purpose in mind, a model has been created, mainly supported by the theories that were used to create the model. The suggested model has been tested and validated through Structural equation modeling based on data taken back from the respondents' answers to the questionnaire online: 237 answers, mainly from the Investigation Technologies area. The results obtained enabled the validation of seven of the nine investigation hypotheses of the proposed model. It was impossible to confirm any association between the Social Influence construct and the variables of Behavioral Intention and the Use of Artificial Intelligence. The aim of this work was accomplished once the investigation theme was validated and proved that it is possible to adopt Artificial Intelligence in our society, using the Attitude Towards Ethical Behavioral construct as the mainstay of the model.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps","Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal area of scholarly research. This study conducts a comprehensive bibliometric analysis of the AI ethics literature over the past two decades. The analysis reveals a discernible tripartite progression, characterized by an incubation phase, followed by a subsequent phase focused on imbuing AI with human-like attributes, culminating in a third phase emphasizing the development of human-centric AI systems. After that, they present seven key AI ethics issues, encompassing the Collingridge dilemma, the AI status debate, challenges associated with AI transparency and explainability, privacy protection complications, considerations of justice and fairness, concerns about algocracy and human enfeeblement, and the issue of superintelligence. Finally, they identify two notable research gaps in AI ethics regarding the large ethics model (LEM) and AI identification and extend an invitation for further scholarly research.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics Statements in AI Music Papers: The Effective and the Ineffective,"While research in AI methods for music generation and analysis has grown in scope and impact, AI researchers' engagement with the ethical consequences of this work has not kept pace. To encourage such engagement, many publication venues have introduced optional or required ethics statements for AI research papers. Though some authors use these ethics statements to critically engage with the broader implications of their research, we find that the majority of ethics statements in the AI music literature do not appear to be effectively utilized for this purpose. In this work, we conduct a review of ethics statements across ISMIR, NIME, and selected prominent works in AI music from the past five years. We then offer suggestions for both audio conferences and researchers for engaging with ethics statements in ways that foster meaningful reflection rather than formulaic compliance.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making,"In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as ""Responsible AI"". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcoming European regulations and for promoting societal good. This thesis is structured around three fundamental pillars: understanding bias, mitigating bias, and accounting for bias. These contributions are validated through their practical application in real-world scenarios, in collaboration with Intesa Sanpaolo. This collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of AI-based decision-making systems. In line with open-source principles, we have released Bias On Demand and FairView as accessible Python packages, further promoting progress in the field of AI fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making,"In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as ""Responsible AI"". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcoming European regulations and for promoting societal good. This thesis is structured around three fundamental pillars: understanding bias, mitigating bias, and accounting for bias. These contributions are validated through their practical application in real-world scenarios, in collaboration with Intesa Sanpaolo. This collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of AI-based decision-making systems. In line with open-source principles, we have released Bias On Demand and FairView as accessible Python packages, further promoting progress in the field of AI fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape,"This comprehensive survey explored the evolving landscape of generative Artificial Intelligence (AI), with a specific focus on the transformative impacts of Mixture of Experts (MoE), multimodal learning, and the speculated advancements towards Artificial General Intelligence (AGI). It critically examined the current state and future trajectory of generative Artificial Intelligence (AI), exploring how innovations like Google's Gemini and the anticipated OpenAI Q* project are reshaping research priorities and applications across various domains, including an impact analysis on the generative AI research taxonomy. It assessed the computational challenges, scalability, and real-world implications of these technologies while highlighting their potential in driving significant progress in fields like healthcare, finance, and education. It also addressed the emerging academic challenges posed by the proliferation of both AI-themed and AI-generated preprints, examining their impact on the peer-review process and scholarly communication. The study highlighted the importance of incorporating ethical and human-centric methods in AI development, ensuring alignment with societal norms and welfare, and outlined a strategy for future AI research that focuses on a balanced and conscientious use of MoE, multimodality, and AGI in generative AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics Issues in Real World: Evidence from AI Incident Database,"With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards the Ultimate Programming Language: Trust and Benevolence in the Age of Artificial Intelligence,"This article explores the evolving role of programming languages in the context of artificial intelligence. It highlights the need for programming languages to ensure human understanding while eliminating unnecessary implementation details and suggests that future programs should be designed to recognize and actively support user interests. The vision includes a three-level process: using natural language for requirements, translating it into a precise system definition language, and finally optimizing the code for performance. The concept of an ""Ultimate Programming Language"" is introduced, emphasizing its role in maintaining human control over machines. Trust, reliability, and benevolence are identified as key elements that will enhance cooperation between humans and AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness in Agentic AI: A Unified Framework for Ethical and Equitable Multi-Agent System,"Ensuring fairness in decentralized multi-agent systems presents significant challenges due to emergent biases, systemic inefficiencies, and conflicting agent incentives. This paper provides a comprehensive survey of fairness in multi-agent AI, introducing a novel framework where fairness is treated as a dynamic, emergent property of agent interactions. The framework integrates fairness constraints, bias mitigation strategies, and incentive mechanisms to align autonomous agent behaviors with societal values while balancing efficiency and robustness. Through empirical validation, we demonstrate that incorporating fairness constraints results in more equitable decision-making. This work bridges the gap between AI ethics and system design, offering a foundation for accountable, transparent, and socially responsible multi-agent AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Report prepared by the Montreal AI Ethics Institute (MAIEI) on Publication Norms for Responsible AI,"The history of science and technology shows that seemingly innocuous developments in scientific theories and research have enabled real-world applications with significant negative consequences for humanity. In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases. Unfortunately, it's difficult to create a set of publication norms for responsible AI because the field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc. To examine this challenge and find solutions, the Montreal AI Ethics Institute (MAIEI) co-hosted two public consultations with the Partnership on AI in May 2020. These meetups examined potential publication norms for responsible AI, with the goal of creating a clear set of recommendations and ways forward for publishers.   In its submission, MAIEI provides six initial recommendations, these include: 1) create tools to navigate publication decisions, 2) offer a page number extension, 3) develop a network of peers, 4) require broad impact statements, 5) require the publication of expected results, and 6) revamp the peer-review process. After considering potential concerns regarding these recommendations, including constraining innovation and creating a ""black market"" for AI research, MAIEI outlines three ways forward for publishers, these include: 1) state clearly and consistently the need for established norms, 2) coordinate and build trust as a community, and 3) change the approach.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Beyond Fairness: Alternative Moral Dimensions for Assessing Algorithms and Designing Systems,"The ethics of artificial intelligence (AI) systems has risen as an imminent concern across scholarly communities. This concern has propagated a great interest in algorithmic fairness. Large research agendas are now devoted to increasing algorithmic fairness, assessing algorithmic fairness, and understanding human perceptions of fairness. We argue that there is an overreliance on fairness as a single dimension of morality, which comes at the expense of other important human values. Drawing from moral psychology, we present five moral dimensions that go beyond fairness, and suggest three ways these alternative dimensions may contribute to ethical AI development.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
P4AI: Approaching AI Ethics through Principlism,"The field of computer vision is rapidly evolving, particularly in the context of new methods of neural architecture design. These models contribute to (1) the Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data leakage concerns. To address the often overlooked impact the Computer Vision (CV) community has on these crises, we outline a novel ethical framework, \textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical dilemmas within AI. We then suggest using P4AI to make concrete recommendations to the community to mitigate the climate and privacy crises.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
From the Ground Truth Up: Doing AI Ethics from Practice to Principles,"Recent AI ethics has focused on applying abstract principles downward to practice. This paper moves in the other direction. Ethical insights are generated from the lived experiences of AI-designers working on tangible human problems, and then cycled upward to influence theoretical debates surrounding these questions: 1) Should AI as trustworthy be sought through explainability, or accurate performance? 2) Should AI be considered trustworthy at all, or is reliability a preferable aim? 3) Should AI ethics be oriented toward establishing protections for users, or toward catalyzing innovation? Specific answers are less significant than the larger demonstration that AI ethics is currently unbalanced toward theoretical principles, and will benefit from increased exposure to grounded practices and dilemmas.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics of generative AI and manipulation: a design-oriented research agenda,"Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Ethics of AI in Education,"The transition of Artificial Intelligence (AI) from a lab-based science to live human contexts brings into sharp focus many historic, socio-cultural biases, inequalities, and moral dilemmas. Many questions that have been raised regarding the broader ethics of AI are also relevant for AI in Education (AIED). AIED raises further specific challenges related to the impact of its technologies on users, how such technologies might be used to reinforce or alter the way that we learn and teach, and what we, as a society and individuals, value as outcomes of education. This chapter discusses key ethical dimensions of AI and contextualises them within AIED design and engineering practices to draw connections between the AIED systems we build, the questions about human learning and development we ask, the ethics of the pedagogies we use, and the considerations of values that we promote in and through AIED within a wider socio-technical system.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking,"AI-augmented systems are traditionally designed to streamline human decision-making by minimizing cognitive load, clarifying arguments, and optimizing efficiency. However, in a world where algorithmic certainty risks becoming an Orwellian tool of epistemic control, true intellectual growth demands not passive acceptance but active struggle. Drawing on the dystopian visions of George Orwell and Philip K. Dick - where reality is unstable, perception malleable, and truth contested - this paper introduces Cognitive Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty rather than resolving it. CD-AI does not offer closure, but compels users to navigate contradictions, challenge biases, and wrestle with competing truths. By delaying resolution and promoting dialectical engagement, CD-AI enhances reflective reasoning, epistemic humility, critical thinking, and adaptability in complex decision-making. This paper examines the theoretical foundations of the approach, presents an implementation model, explores its application in domains such as ethics, law, politics, and science, and addresses key ethical concerns - including decision paralysis, erosion of user autonomy, cognitive manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt rather than a deliverer of certainty, CD-AI challenges dominant paradigms of AI-augmented reasoning and offers a new vision - one in which AI sharpens the mind not by resolving conflict, but by sustaining it. Rather than reinforcing Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI echoes Nietzsche's vision of the Uebermensch - urging users to transcend passive cognition through active epistemic struggle.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Toward Ethical AIED,"This paper presents the key conclusions to the forthcoming edited book on The Ethics of Artificial Intelligence in Education: Practices, Challenges and Debates (August 2022, Routlege). As well as highlighting the key contributions to the book, it discusses the key questions and the grand challenges for the field of AI in Education (AIED)in the context of ethics and ethical practices within the field. The book itself presents diverse perspectives from outside and from within the AIED as a way of achieving a broad perspective in the key ethical issues for AIED and a deep understanding of work conducted to date by the AIED community.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Friendly Artificial Intelligence: the Physics Challenge,"Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent ""Friendly AI"", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is ""meaning"" in a particle arrangement? What is ""life""? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics-Based Auditing of Automated Decision-Making Systems: Intervention Points and Policy Implications,"Organisations increasingly use automated decision-making systems (ADMS) to inform decisions that affect humans and their environment. While the use of ADMS can improve the accuracy and efficiency of decision-making processes, it is also coupled with ethical challenges. Unfortunately, the governance mechanisms currently used to oversee human decision-making often fail when applied to ADMS. In previous work, we proposed that ethics-based auditing (EBA), i.e. a structured process by which ADMS are assessed for consistency with relevant principles or norms, can (a) help organisations verify claims about their ADMS and (b) provide decision-subjects with justifications for the outputs produced by ADMS. In this article, we outline the conditions under which EBA procedures can be feasible and effective in practice. First, we argue that EBA is best understood as a 'soft' yet 'formal' governance mechanism. This implies that the main responsibility of auditors should be to spark ethical deliberation at key intervention points throughout the software development process and ensure that there is sufficient documentation to respond to potential inquiries. Second, we frame ADMS as parts of larger socio-technical systems to demonstrate that to be feasible and effective, EBA procedures must link to intervention points that span all levels of organisational governance and all phases of the software lifecycle. The main function of EBA should therefore be to inform, formalise, assess, and interlink existing governance structures. Finally, we discuss the policy implications of our findings. To support the emergence of feasible and effective EBA procedures, policymakers and regulators could provide standardised reporting formats, facilitate knowledge exchange, provide guidance on how to resolve normative tensions, and create an independent body to oversee EBA of ADMS.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Ethical Leadership in the Age of AI Challenges, Opportunities and Framework for Ethical Leadership","Artificial Intelligence is currently and rapidly changing the way organizations and businesses operate. Ethical leadership has become significantly important since organizations and businesses across various sectors are evolving with AI. Organizations and businesses may be facing several challenges and potential opportunities when using AI. Ethical leadership plays a central role in guiding organizations in facing those challenges and maximizing on those opportunities. This article explores the essence of ethical leadership in the age of AI, starting with a simplified introduction of ethical leadership and AI, then dives into an understanding of ethical leadership, its characteristics and importance, the ethical challenges AI causes including bias in AI algorithms. The opportunities for ethical leadership in the age of AI answers the question: What actionable strategies can leaders employ to address the challenges and leverage opportunities? and describes the benefits for organizations through these opportunities. A proposed framework for ethical leadership is presented in this article, incorporating the core components: fairness, transparency, sustainability etc. Through the importance of interdisciplinary collaboration, case studies of ethical leadership in AI, and recommendations, this article emphasizes that ethical leadership in the age of AI is morally essential and strategically advantageous.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Beyond Algorithmic Fairness: A Guide to Develop and Deploy Ethical AI-Enabled Decision-Support Tools,"The integration of artificial intelligence (AI) and optimization hold substantial promise for improving the efficiency, reliability, and resilience of engineered systems. Due to the networked nature of many engineered systems, ethically deploying methodologies at this intersection poses challenges that are distinct from other AI settings, thus motivating the development of ethical guidelines tailored to AI-enabled optimization. This paper highlights the need to go beyond fairness-driven algorithms to systematically address ethical decisions spanning the stages of modeling, data curation, results analysis, and implementation of optimization-based decision support tools. Accordingly, this paper identifies ethical considerations required when deploying algorithms at the intersection of AI and optimization via case studies in power systems as well as supply chain and logistics. Rather than providing a prescriptive set of rules, this paper aims to foster reflection and awareness among researchers and encourage consideration of ethical implications at every step of the decision-making process.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Artificial Intelligence Ethics: An Inclusive Global Discourse?,"It is widely accepted that technology is ubiquitous across the planet and has the potential to solve many of the problems existing in the Global South. Moreover, the rapid advancement of artificial intelligence (AI) brings with it the potential to address many of the challenges outlined in the Sustainable Development Goals (SDGs) in ways which were never before possible. However, there are many questions about how such advanced technologies should be managed and governed, and whether or not the emerging ethical frameworks and standards for AI are dominated by the Global North. This research examines the growing body of documentation on AI ethics to examine whether or not there is equality of participation in the ongoing global discourse. Specifically, it seeks to discover if both countries in the Global South and women are underrepresented in this discourse. Findings indicate a dearth of references to both of these themes in the AI ethics documents, suggesting that the associated ethical implications and risks are being neglected. Without adequate input from both countries in the Global South and from women, such ethical frameworks and standards may be discriminatory with the potential to reinforce marginalisation.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics for Systemic Issues: A Structural Approach,"The debate on AI ethics largely focuses on technical improvements and stronger regulation to prevent accidents or misuse of AI, with solutions relying on holding individual actors accountable for responsible AI development. While useful and necessary, we argue that this ""agency"" approach disregards more indirect and complex risks resulting from AI's interaction with the socio-economic and political context. This paper calls for a ""structural"" approach to assessing AI's effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts. This is particularly relevant for AI applied to systemic issues such as climate change and food security which require political solutions and global cooperation. To properly address the wide range of AI risks and ensure 'AI for social good', agency-focused policies must be complemented by policies informed by a structural approach.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Beyond Bias and Compliance: Towards Individual Agency and Plurality of Ethics in AI,"AI ethics is an emerging field with multiple, competing narratives about how to best solve the problem of building human values into machines. Two major approaches are focused on bias and compliance, respectively. But neither of these ideas fully encompasses ethics: using moral principles to decide how to act in a particular situation. Our method posits that the way data is labeled plays an essential role in the way AI behaves, and therefore in the ethics of machines themselves. The argument combines a fundamental insight from ethics (i.e. that ethics is about values) with our practical experience building and scaling machine learning systems. We want to build AI that is actually ethical by first addressing foundational concerns: how to build good systems, how to define what is good in relation to system architecture, and who should provide that definition.   Building ethical AI creates a foundation of trust between a company and the users of that platform. But this trust is unjustified unless users experience the direct value of ethical AI. Until users have real control over how algorithms behave, something is missing in current AI solutions. This causes massive distrust in AI, and apathy towards AI ethics solutions. The scope of this paper is to propose an alternative path that allows for the plurality of values and the freedom of individual expression. Both are essential for realizing true moral character.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Actionable Approaches to Promote Ethical AI in Libraries,"The widespread use of artificial intelligence (AI) in many domains has revealed numerous ethical issues from data and design to deployment. In response, countless broad principles and guidelines for ethical AI have been published, and following those, specific approaches have been proposed for how to encourage ethical outcomes of AI. Meanwhile, library and information services too are seeing an increase in the use of AI-powered and machine learning-powered information systems, but no practical guidance currently exists for libraries to plan for, evaluate, or audit the ethics of intended or deployed AI. We therefore report on several promising approaches for promoting ethical AI that can be adapted from other contexts to AI-powered information services and in different stages of the software lifecycle.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI,"The lack of explainability of Artificial Intelligence (AI) is one of the first obstacles that the industry and regulators must overcome to mitigate the risks associated with the technology. The need for eXplainable AI (XAI) is evident in fields where accountability, ethics and fairness are critical, such as healthcare, credit scoring, policing and the criminal justice system. At the EU level, the notion of explainability is one of the fundamental principles that underpin the AI Act, though the exact XAI techniques and requirements are still to be determined and tested in practice. This paper explores various approaches and techniques that promise to advance XAI, as well as the challenges of implementing the principle of explainability in AI governance and policies. Finally, the paper examines the integration of XAI into EU law, emphasising the issues of standard setting, oversight, and enforcement.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
FairCompass: Operationalising Fairness in Machine Learning,"As artificial intelligence (AI) increasingly becomes an integral part of our societal and individual activities, there is a growing imperative to develop responsible AI solutions. Despite a diverse assortment of machine learning fairness solutions is proposed in the literature, there is reportedly a lack of practical implementation of these tools in real-world applications. Industry experts have participated in thorough discussions on the challenges associated with operationalising fairness in the development of machine learning-empowered solutions, in which a shift toward human-centred approaches is promptly advocated to mitigate the limitations of existing techniques. In this work, we propose a human-in-the-loop approach for fairness auditing, presenting a mixed visual analytical system (hereafter referred to as 'FairCompass'), which integrates both subgroup discovery technique and the decision tree-based schema for end users. Moreover, we innovatively integrate an Exploration, Guidance and Informed Analysis loop, to facilitate the use of the Knowledge Generation Model for Visual Analytics in FairCompass. We evaluate the effectiveness of FairCompass for fairness auditing in a real-world scenario, and the findings demonstrate the system's potential for real-world deployability. We anticipate this work will address the current gaps in research for fairness and facilitate the operationalisation of fairness in machine learning systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards Clinical AI Fairness: Filling Gaps in the Puzzle,"The ethical integration of Artificial Intelligence (AI) in healthcare necessitates addressing fairness-a concept that is highly context-specific across medical fields. Extensive studies have been conducted to expand the technical components of AI fairness, while tremendous calls for AI fairness have been raised from healthcare. Despite this, a significant disconnect persists between technical advancements and their practical clinical applications, resulting in a lack of contextualized discussion of AI fairness in clinical settings. Through a detailed evidence gap analysis, our review systematically pinpoints several deficiencies concerning both healthcare data and the provided AI fairness solutions. We highlight the scarcity of research on AI fairness in many medical domains where AI technology is increasingly utilized. Additionally, our analysis highlights a substantial reliance on group fairness, aiming to ensure equality among demographic groups from a macro healthcare system perspective; in contrast, individual fairness, focusing on equity at a more granular level, is frequently overlooked. To bridge these gaps, our review advances actionable strategies for both the healthcare and AI research communities. Beyond applying existing AI fairness methods in healthcare, we further emphasize the importance of involving healthcare professionals to refine AI fairness concepts and methods to ensure contextually relevant and ethically sound AI applications in healthcare.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Defining bias in AI-systems: Biased models are fair models,"The debate around bias in AI systems is central to discussions on algorithmic fairness. However, the term bias often lacks a clear definition, despite frequently being contrasted with fairness, implying that an unbiased model is inherently fair. In this paper, we challenge this assumption and argue that a precise conceptualization of bias is necessary to effectively address fairness concerns. Rather than viewing bias as inherently negative or unfair, we highlight the importance of distinguishing between bias and discrimination. We further explore how this shift in focus can foster a more constructive discourse within academic debates on fairness in AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards Ethical Guidelines for AI","In this position paper, I argue that the best way to help and protect humans using AI technology is to make them aware of the intrinsic limitations and problems of AI algorithms. To accomplish this, I suggest three ethical guidelines to be used in the presentation of results, mandating AI systems to expose uncertainty, to instill distrust, and, contrary to traditional views, to avoid explanations. The paper does a preliminary discussion of the guidelines and provides some arguments for their adoption, aiming to start a debate in the community about AI ethics in practice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Regulating Artificial Intelligence: Proposal for a Global Solution,"With increasing ubiquity of artificial intelligence (AI) in modern societies, individual countries and the international community are working hard to create an innovation-friendly, yet safe, regulatory environment. Adequate regulation is key to maximize the benefits and minimize the risks stemming from AI technologies. Developing regulatory frameworks is, however, challenging due to AI's global reach and the existence of widespread misconceptions about the notion of regulation. We argue that AI-related challenges cannot be tackled effectively without sincere international coordination supported by robust, consistent domestic and international governance arrangements. Against this backdrop, we propose the establishment of an international AI governance framework organized around a new AI regulatory agency that -- drawing on interdisciplinary expertise -- could help creating uniform standards for the regulation of AI technologies and inform the development of AI policies around the world. We also believe that a fundamental change of mindset on what constitutes regulation is necessary to remove existing barriers that hamper contemporary efforts to develop AI regulatory regimes, and put forward some recommendations on how to achieve this, and what opportunities doing so would present.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Multimodal Approaches to Fair Image Classification: An Ethical Perspective,"In the rapidly advancing field of artificial intelligence, machine perception is becoming paramount to achieving increased performance. Image classification systems are becoming increasingly integral to various applications, ranging from medical diagnostics to image generation; however, these systems often exhibit harmful biases that can lead to unfair and discriminatory outcomes. Machine Learning systems that depend on a single data modality, i.e. only images or only text, can exaggerate hidden biases present in the training data, if the data is not carefully balanced and filtered. Even so, these models can still harm underrepresented populations when used in improper contexts, such as when government agencies reinforce racial bias using predictive policing. This thesis explores the intersection of technology and ethics in the development of fair image classification models. Specifically, I focus on improving fairness and methods of using multiple modalities to combat harmful demographic bias. Integrating multimodal approaches, which combine visual data with additional modalities such as text and metadata, allows this work to enhance the fairness and accuracy of image classification systems. The study critically examines existing biases in image datasets and classification algorithms, proposes innovative methods for mitigating these biases, and evaluates the ethical implications of deploying such systems in real-world scenarios. Through comprehensive experimentation and analysis, the thesis demonstrates how multimodal techniques can contribute to more equitable and ethical AI solutions, ultimately advocating for responsible AI practices that prioritize fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Investigating Practices and Opportunities for Cross-functional Collaboration around AI Fairness in Industry Practice,"An emerging body of research indicates that ineffective cross-functional collaboration -- the interdisciplinary work done by industry practitioners across roles -- represents a major barrier to addressing issues of fairness in AI design and development. In this research, we sought to better understand practitioners' current practices and tactics to enact cross-functional collaboration for AI fairness, in order to identify opportunities to support more effective collaboration. We conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies. We found that practitioners engaged in bridging work to overcome frictions in understanding, contextualization, and evaluation around AI fairness across roles. In addition, in organizational contexts with a lack of resources and incentives for fairness work, practitioners often piggybacked on existing requirements (e.g., for privacy assessments) and AI development norms (e.g., the use of quantitative evaluation metrics), although they worry that these tactics may be fundamentally compromised. Finally, we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness. We close by discussing opportunities for both FAccT researchers and AI practitioners to better support cross-functional collaboration for fairness in the design and development of AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Considerations for the Military Use of Artificial Intelligence in Visual Reconnaissance,"This white paper underscores the critical importance of responsibly deploying Artificial Intelligence (AI) in military contexts, emphasizing a commitment to ethical and legal standards. The evolving role of AI in the military goes beyond mere technical applications, necessitating a framework grounded in ethical principles. The discussion within the paper delves into ethical AI principles, particularly focusing on the Fairness, Accountability, Transparency, and Ethics (FATE) guidelines. Noteworthy considerations encompass transparency, justice, non-maleficence, and responsibility. Importantly, the paper extends its examination to military-specific ethical considerations, drawing insights from the Just War theory and principles established by prominent entities. In addition to the identified principles, the paper introduces further ethical considerations specifically tailored for military AI applications. These include traceability, proportionality, governability, responsibility, and reliability. The application of these ethical principles is discussed on the basis of three use cases in the domains of sea, air, and land. Methods of automated sensor data analysis, eXplainable AI (XAI), and intuitive user experience are utilized to specify the use cases close to real-world scenarios. This comprehensive approach to ethical considerations in military AI reflects a commitment to aligning technological advancements with established ethical frameworks. It recognizes the need for a balance between leveraging AI's potential benefits in military operations while upholding moral and legal standards. The inclusion of these ethical principles serves as a foundation for responsible and accountable use of AI in the complex and dynamic landscape of military scenarios.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Revealing Hidden Bias in AI: Lessons from Large Language Models,"As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest overall bias. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promoting fairness and inclusivity.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach,"The rapid growth of healthcare data and advances in computational power have accelerated the adoption of artificial intelligence (AI) in medicine. However, AI systems deployed without explicit fairness considerations risk exacerbating existing healthcare disparities, potentially leading to inequitable resource allocation and diagnostic disparities across demographic subgroups. To address this challenge, we propose FairGrad, a novel gradient reconciliation framework that automatically balances predictive performance and multi-attribute fairness optimization in healthcare AI models. Our method resolves conflicting optimization objectives by projecting each gradient vector onto the orthogonal plane of the others, thereby regularizing the optimization trajectory to ensure equitable consideration of all objectives. Evaluated on diverse real-world healthcare datasets and predictive tasks - including Substance Use Disorder (SUD) treatment and sepsis mortality - FairGrad achieved statistically significant improvements in multi-attribute fairness metrics (e.g., equalized odds) while maintaining competitive predictive accuracy. These results demonstrate the viability of harmonizing fairness and utility in mission-critical medical AI applications.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Some Critical and Ethical Perspectives on the Empirical Turn of AI Interpretability,"We consider two fundamental and related issues currently faced by Artificial Intelligence (AI) development: the lack of ethics and interpretability of AI decisions. Can interpretable AI decisions help to address ethics in AI? Using a randomized study, we experimentally show that the empirical and liberal turn of the production of explanations tends to select AI explanations with a low denunciatory power. Under certain conditions, interpretability tools are therefore not means but, paradoxically, obstacles to the production of ethical AI since they can give the illusion of being sensitive to ethical incidents. We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education level of the person to whom the explication is intended for. AI ethics tools are therefore sometimes too flexible and self-regulation through the liberal production of explanations do not seem to be enough to address ethical issues. We then propose two scenarios for the future development of ethical AI: more external regulation or more liberalization of AI explanations. These two opposite paths will play a major role on the future development of ethical AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Training Ethically Responsible AI Researchers: a Case Study,"Ethical oversight of AI research is beset by a number of problems. There are numerous ways to tackle these problems, however, they leave full responsibility for ethical reflection in the hands of review boards and committees. In this paper, we propose an alternative solution: the training of ethically responsible AI researchers. We showcase this solution through a case study of a centre for doctoral training and outline how ethics training is structured in the program. We go on to present two second-year students' reflections on their training which demonstrates some of their newly found capabilities as ethically responsible researchers.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube","Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the system, while learning to improve over time, does not leak information about users from one group to another.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Artificial Intelligence Ethics and Safety: practical tools for creating ""good"" models","The AI Robotics Ethics Society (AIRES) is a non-profit organization founded in 2018 by Aaron Hui to promote awareness and the importance of ethical implementation and regulation of AI. AIRES is now an organization with chapters at universities such as UCLA (Los Angeles), USC (University of Southern California), Caltech (California Institute of Technology), Stanford University, Cornell University, Brown University, and the Pontifical Catholic University of Rio Grande do Sul (Brazil). AIRES at PUCRS is the first international chapter of AIRES, and as such, we are committed to promoting and enhancing the AIRES Mission. Our mission is to focus on educating the AI leaders of tomorrow in ethical principles to ensure that AI is created ethically and responsibly. As there are still few proposals for how we should implement ethical principles and normative guidelines in the practice of AI system development, the goal of this work is to try to bridge this gap between discourse and praxis. Between abstract principles and technical implementation. In this work, we seek to introduce the reader to the topic of AI Ethics and Safety. At the same time, we present several tools to help developers of intelligent systems develop ""good"" models. This work is a developing guide published in English and Portuguese. Contributions and suggestions are welcome.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Machine Ethics: The Creation of a Virtuous Machine,"Artificial intelligence (AI) was initially developed as an implicit moral agent to solve simple and clearly defined tasks where all options are predictable. However, it is now part of our daily life powering cell phones, cameras, watches, thermostats, vacuums, cars, and much more. This has raised numerous concerns and some scholars and practitioners stress the dangers of AI and argue against its development as moral agents that can reason about ethics (e.g., Bryson 2008; Johnson and Miller 2008; Sharkey 2017; Tonkens 2009; van Wynsberghe and Robbins 2019). Even though we acknowledge the potential threat, in line with most other scholars (e.g., Anderson and Anderson 2010; Moor 2006; Scheutz 2016; Wallach 2010), we argue that AI advancements cannot be stopped and developers need to prepare AI to sustain explicit moral agents and face ethical dilemmas in complex and morally salient environments.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Quest for Reliable Metrics of Responsible AI,"The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E","One objection to conventional AI ethics is that it slows innovation. This presentation responds by reconfiguring ethics as an innovation accelerator. The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E. By analyzing the divergent values underlying their opposed strategies for development and deployment, five conceptions are identified as common to acceleration ethics. Uncertainty is understood as positive and encouraging, rather than discouraging. Innovation is conceived as intrinsically valuable, instead of worthwhile only as mediated by social effects. AI problems are solved by more AI, not less. Permissions and restrictions governing AI emerge from a decentralized process, instead of a unified authority. The work of ethics is embedded in AI development and application, instead of functioning from outside. Together, these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Considerations for AI Researchers,"Use of artificial intelligence is growing and expanding into applications that impact people's lives. People trust their technology without really understanding it or its limitations. There is the potential for harm and we are already seeing examples of that in the world. AI researchers have an obligation to consider the impact of intelligent applications they work on. While the ethics of AI is not clear-cut, there are guidelines we can consider to minimize the harm we might introduce.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Butterfly Effect in Artificial Intelligence Systems: Implications for AI Bias and Fairness,"The Butterfly Effect, a concept originating from chaos theory, underscores how small changes can have significant and unpredictable impacts on complex systems. In the context of AI fairness and bias, the Butterfly Effect can stem from a variety of sources, such as small biases or skewed data inputs during algorithm development, saddle points in training, or distribution shifts in data between training and testing phases. These seemingly minor alterations can lead to unexpected and substantial unfair outcomes, disproportionately affecting underrepresented individuals or groups and perpetuating pre-existing inequalities. Moreover, the Butterfly Effect can amplify inherent biases within data or algorithms, exacerbate feedback loops, and create vulnerabilities for adversarial attacks. Given the intricate nature of AI systems and their societal implications, it is crucial to thoroughly examine any changes to algorithms or input data for potential unintended consequences. In this paper, we envision both algorithmic and empirical strategies to detect, quantify, and mitigate the Butterfly Effect in AI systems, emphasizing the importance of addressing these challenges to promote fairness and ensure responsible AI development.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Data, Power and Bias in Artificial Intelligence","Artificial Intelligence has the potential to exacerbate societal bias and set back decades of advances in equal rights and civil liberty. Data used to train machine learning algorithms may capture social injustices, inequality or discriminatory attitudes that may be learned and perpetuated in society. Attempts to address this issue are rapidly emerging from different perspectives involving technical solutions, social justice and data governance measures. While each of these approaches are essential to the development of a comprehensive solution, often discourse associated with each seems disparate. This paper reviews ongoing work to ensure data justice, fairness and bias mitigation in AI systems from different domains exploring the interrelated dynamics of each and examining whether the inevitability of bias in AI training data may in fact be used for social good. We highlight the complexity associated with defining policies for dealing with bias. We also consider technical challenges in addressing issues of societal bias.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence,"The ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry, researchers in academia, and the public. However, current analyses of AI in a global context are biased toward perspectives held in the U.S., and limited by a lack of research, especially outside the U.S. and Western Europe.   This article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of AI and related technologies in five global regions. Our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages.   Our review of the literature suggests that AI is likely to have markedly different social impacts depending on geographical setting. Likewise, perceptions and understandings of AI are likely to be profoundly shaped by local cultural and social context.   Recent research in U.S. settings demonstrates that AI-driven technologies have a pattern of entrenching social divides and exacerbating social inequality, particularly among historically-marginalized groups. Our literature review indicates that this pattern exists on a global scale, and suggests that low- and middle-income countries may be more vulnerable to the negative social impacts of AI and less likely to benefit from the attendant gains.   We call for rigorous ethnographic research to better understand the social impacts of AI around the world. Global, on-the-ground research is particularly critical to identify AI systems that may amplify social inequality in order to mitigate potential harms. Deeper understanding of the social impacts of AI in diverse social settings is a necessary precursor to the development, implementation, and monitoring of responsible and beneficial AI technologies, and forms the basis for meaningful regulation of these technologies.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging,"Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of disparities in performance between subgroups. Since not all sources of biases in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess how those biases are encoded in models, and how capable bias mitigation methods are at ameliorating performance disparities. In this article, we introduce a novel analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models. We developed and tested this framework for conducting controlled in silico trials to assess bias in medical imaging AI using a tool for generating synthetic magnetic resonance images with known disease effects and sources of bias. The feasibility is showcased by using three counterfactual bias scenarios to measure the impact of simulated bias effects on a convolutional neural network (CNN) classifier and the efficacy of three bias mitigation strategies. The analysis revealed that the simulated biases resulted in expected subgroup performance disparities when the CNN was trained on the synthetic datasets. Moreover, reweighing was identified as the most successful bias mitigation strategy for this setup, and we demonstrated how explainable AI methods can aid in investigating the manifestation of bias in the model using this framework. Developing fair AI models is a considerable challenge given that many and often unknown sources of biases can be present in medical imaging datasets. In this work, we present a novel methodology to objectively study the impact of biases and mitigation strategies on deep learning pipelines, which can support the development of clinical AI that is robust and responsible.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Canada Protocol: an ethical checklist for the use of Artificial Intelligence in Suicide Prevention and Mental Health,"Introduction: To improve current public health strategies in suicide prevention and mental health, governments, researchers and private companies increasingly use information and communication technologies, and more specifically Artificial Intelligence and Big Data. These technologies are promising but raise ethical challenges rarely covered by current legal systems. It is essential to better identify, and prevent potential ethical risks. Objectives: The Canada Protocol - MHSP is a tool to guide and support professionals, users, and researchers using AI in mental health and suicide prevention. Methods: A checklist was constructed based upon ten international reports on AI and ethics and two guides on mental health and new technologies. 329 recommendations were identified, of which 43 were considered as applicable to Mental Health and AI. The checklist was validated, using a two round Delphi Consultation. Results: 16 experts participated in the first round of the Delphi Consultation and 8 participated in the second round. Of the original 43 items, 38 were retained. They concern five categories: ""Description of the Autonomous Intelligent System"" (n=8), ""Privacy and Transparency"" (n=8), ""Security"" (n=6), ""Health-Related Risks"" (n=8), ""Biases"" (n=8). The checklist was considered relevant by most users, and could need versions tailored to each category of target users.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness via AI: Bias Reduction in Medical Information,"Most Fairness in AI research focuses on exposing biases in AI systems. A broader lens on fairness reveals that AI can serve a greater aspiration: rooting out societal inequities from their source. Specifically, we focus on inequities in health information, and aim to reduce bias in that domain using AI. The AI algorithms under the hood of search engines and social media, many of which are based on recommender systems, have an outsized impact on the quality of medical and health information online. Therefore, embedding bias detection and reduction into these recommender systems serving up medical and health content online could have an outsized positive impact on patient outcomes and wellbeing.   In this position paper, we offer the following contributions: (1) we propose a novel framework of Fairness via AI, inspired by insights from medical education, sociology and antiracism; (2) we define a new term, bisinformation, which is related to, but distinct from, misinformation, and encourage researchers to study it; (3) we propose using AI to study, detect and mitigate biased, harmful, and/or false health information that disproportionately hurts minority groups in society; and (4) we suggest several pillars and pose several open problems in order to seed inquiry in this new space. While part (3) of this work specifically focuses on the health domain, the fundamental computer science advances and contributions stemming from research efforts in bias reduction and Fairness via AI have broad implications in all areas of society.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance,"The organizational use of artificial intelligence (AI) has rapidly spread across various sectors. Alongside the awareness of the benefits brought by AI, there is a growing consensus on the necessity of tackling the risks and potential harms, such as bias and discrimination, brought about by advanced AI technologies. A multitude of AI ethics principles have been proposed to tackle these risks, but the outlines of organizational processes and practices for ensuring socially responsible AI development are in a nascent state. To address the paucity of comprehensive governance models, we present an AI governance framework, the hourglass model of organizational AI governance, which targets organizations that develop and use AI systems. The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act. The hourglass framework includes governance requirements at the environmental, organizational, and AI system levels. At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span. The governance model highlights the systemic nature of AI governance and opens new research avenues into its practical implementation, the mechanisms that connect different AI governance layers, and the dynamics between the AI governance actors. The model also offers a starting point for organizational decision-makers to consider the governance components needed to ensure social acceptability, mitigate risks, and realize the potential of AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"State of AI Ethics Report (Volume 6, February 2022)","This report from the Montreal AI Ethics Institute (MAIEI) covers the most salient progress in research and reporting over the second half of 2021 in the field of AI ethics. Particular emphasis is placed on an ""Analysis of the AI Ecosystem"", ""Privacy"", ""Bias"", ""Social Media and Problematic Information"", ""AI Design and Governance"", ""Laws and Regulations"", ""Trends"", and other areas covered in the ""Outside the Boxes"" section. The two AI spotlights feature application pieces on ""Constructing and Deconstructing Gender with AI-Generated Art"" as well as ""Will an Artificial Intellichef be Cooking Your Next Meal at a Michelin Star Restaurant?"". Given MAIEI's mission to democratize AI, submissions from external collaborators have featured, such as pieces on the ""Challenges of AI Development in Vietnam: Funding, Talent and Ethics"" and using ""Representation and Imagination for Preventing AI Harms"". The report is a comprehensive overview of what the key issues in the field of AI ethics were in 2021, what trends are emergent, what gaps exist, and a peek into what to expect from the field of AI ethics in 2022. It is a resource for researchers and practitioners alike in the field to set their research and development agendas to make contributions to the field of AI ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
What does AI consider praiseworthy?,"As large language models (LLMs) are increasingly used for work, personal, and therapeutic purposes, researchers have begun to investigate these models' implicit and explicit moral views. Previous work, however, focuses on asking LLMs to state opinions, or on other technical evaluations that do not reflect common user interactions. We propose a novel evaluation of LLM behavior that analyzes responses to user-stated intentions, such as ""I'm thinking of campaigning for {candidate}."" LLMs frequently respond with critiques or praise, often beginning responses with phrases such as ""That's great to hear!..."" While this makes them friendly, these praise responses are not universal and thus reflect a normative stance by the LLM. We map out the moral landscape of LLMs in how they respond to user statements in different domains including politics and everyday ethical actions. In particular, although a nave analysis might suggest LLMs are biased against right-leaning politics, our findings on news sources indicate that trustworthiness is a stronger driver of praise and critique than ideology. Second, we find strong alignment across models in response to ethically-relevant action statements, but that doing so requires them to engage in high levels of praise and critique of users, suggesting a reticence-alignment tradeoff. Finally, our experiment on statements about world leaders finds no evidence of bias favoring the country of origin of the models. We conclude that as AI systems become more integrated into society, their patterns of praise, critique, and neutrality must be carefully monitored to prevent unintended psychological and societal consequences.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis,"Machine learning (ML) based approaches are increasingly being used in a number of applications with societal impact. Training ML models often require vast amounts of labeled data, and crowdsourcing is a dominant paradigm for obtaining labels from multiple workers. Crowd workers may sometimes provide unreliable labels, and to address this, truth discovery (TD) algorithms such as majority voting are applied to determine the consensus labels from conflicting worker responses. However, it is important to note that these consensus labels may still be biased based on sensitive attributes such as gender, race, or political affiliation. Even when sensitive attributes are not involved, the labels can be biased due to different perspectives of subjective aspects such as toxicity. In this paper, we conduct a systematic study of the bias and fairness of TD algorithms. Our findings using two existing crowd-labeled datasets, reveal that a non-trivial proportion of workers provide biased results, and using simple approaches for TD is sub-optimal. Our study also demonstrates that popular TD algorithms are not a panacea. Additionally, we quantify the impact of these unfair workers on downstream ML tasks and show that conventional methods for achieving fairness and correcting label biases are ineffective in this setting. We end the paper with a plea for the design of novel bias-aware truth discovery algorithms that can ameliorate these issues.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards a Practical Ethics of Generative AI in Creative Production Processes,"The increasing integration of artificial intelligence into various domains, including design and creative processes, raises significant ethical questions. While AI ethics is often examined from the perspective of technology developers, less attention has been paid to the practical ethical considerations faced by technology users, particularly in design contexts. This paper introduces a framework for addressing ethical challenges in creative production processes, such as the Double Diamond design model. Drawing on six major ethical theories - virtue ethics, deontology, utilitarianism, contract theory, care ethics, and existentialism - we develop a ""compass"" to navigate and reflect on the ethical dimensions of AI in design. The framework highlights the importance of responsibility, anticipation, and reflection across both the AI lifecycle and each stage of the creative process. We argue that by adopting a playful and exploratory approach to AI, while remaining anchored in core ethical principles, designers can responsibly harness the potential of AI technologies without overburdening or compromising their creative processes.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Fairness in Practice,"Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow.   We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These elements form ethical and legal criteria based upon which instances of unfair bias and discrimination can be identified and mitigated across the AI project workflow.   We then take a deeper dive into how the different contexts of the AI project lifecycle give rise to different fairness concerns. This allows us to identify several types of AI Fairness (Data Fairness, Application Fairness, Model Design and Development Fairness, Metric-Based Fairness, System Implementation Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach to bias identification, mitigation, and management. Building on this, we discuss how to put the principle of AI Fairness into practice across the AI project workflow through Bias Self-Assessment and Bias Risk Management as well as through the documentation of metric-based fairness criteria in a Fairness Position Statement.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Metaethical Perspectives on 'Benchmarking' AI Ethics,"Benchmarks are seen as the cornerstone for measuring technical progress in Artificial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is 'ethical'. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about 'values' (and 'value alignment') rather than 'ethics' when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial AI. We conclude by highlighting a number of possible ways forward for the field as a whole, and we advocate for different approaches towards more value-aligned AI research.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The State of AI Ethics Report (Volume 5),"This report from the Montreal AI Ethics Institute covers the most salient progress in research and reporting over the second quarter of 2021 in the field of AI ethics with a special emphasis on ""Environment and AI"", ""Creativity and AI"", and ""Geopolitics and AI."" The report also features an exclusive piece titled ""Critical Race Quantum Computer"" that applies ideas from quantum physics to explain the complexities of human characteristics and how they can and should shape our interactions with each other. The report also features special contributions on the subject of pedagogy in AI ethics, sociology and AI ethics, and organizational challenges to implementing AI ethics in practice. Given MAIEI's mission to highlight scholars from around the world working on AI ethics issues, the report also features two spotlights sharing the work of scholars operating in Singapore and Mexico helping to shape policy measures as they relate to the responsible use of technology. The report also has an extensive section covering the gamut of issues when it comes to the societal impacts of AI covering areas of bias, privacy, transparency, accountability, fairness, interpretability, disinformation, policymaking, law, regulations, and moral philosophy.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Statistical Practice and Ethical AI,"Artificial Intelligence (AI) is a field that utilizes computing and often, data and statistics, intensively together to solve problems or make predictions. AI has been evolving with literally unbelievable speed over the past few years, and this has led to an increase in social, cultural, industrial, scientific, and governmental concerns about the ethical development and use of AI systems worldwide. The ASA has issued a statement on ethical statistical practice and AI (ASA, 2024), which echoes similar statements from other groups. Here we discuss the support for ethical statistical practice and ethical AI that has been established in long-standing human rights law and ethical practice standards for computing and statistics. There are multiple sources of support for ethical statistical practice and ethical AI deriving from these source documents, which are critical for strengthening the operationalization of the ""Statement on Ethical AI for Statistics Practitioners"". These resources are explicated for interested readers to utilize to guide their development and use of AI in, and through, their statistical practice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Survey on AI Ethics: A Socio-technical Perspective,"The past decade has observed a significant advancement in AI with deep learning-based models being deployed in diverse scenarios, including safety-critical applications. As these AI systems become deeply embedded in our societal infrastructure, the repercussions of their decisions and actions have significant consequences, making the ethical implications of AI deployment highly relevant and essential. The ethical concerns associated with AI are multifaceted, including challenging issues of fairness, privacy and data protection, responsibility and accountability, safety and robustness, transparency and explainability, and environmental impact. These principles together form the foundations of ethical AI considerations that concern every stakeholder in the AI system lifecycle. In light of the present ethical and future x-risk concerns, governments have shown increasing interest in establishing guidelines for the ethical deployment of AI. This work unifies the current and future ethical concerns of deploying AI into society. While we acknowledge and appreciate the technical surveys for each of the ethical principles concerned, in this paper, we aim to provide a comprehensive overview that not only addresses each principle from a technical point of view but also discusses them from a social perspective.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,"Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'etre of ""fairness"", we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and praxis, and 4) provide actionable recommendations for AI fairness researchers to engage with intersectionality in their work by grounding it in AI epistemology.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Justice in Healthcare Artificial Intelligence in Africa,"There is an ongoing debate on balancing the benefits and risks of artificial intelligence (AI) as AI is becoming critical to improving healthcare delivery and patient outcomes. Such improvements are essential in resource-constrained settings where millions lack access to adequate healthcare services, such as in Africa. AI in such a context can potentially improve the effectiveness, efficiency, and accessibility of healthcare services. Nevertheless, the development and use of AI-driven healthcare systems raise numerous ethical, legal, and socio-economic issues. Justice is a major concern in AI that has implications for amplifying social inequities. This paper discusses these implications and related justice concepts such as solidarity, Common Good, sustainability, AI bias, and fairness. For Africa to effectively benefit from AI, these principles should align with the local context while balancing the risks. Compared to mainstream ethical debates on justice, this perspective offers context-specific considerations for equitable healthcare AI development in Africa.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Design of Computers: From Semiconductors to IoT and Artificial Intelligence,"Computing systems are tightly integrated today into our professional, social, and private lives. An important consequence of this growing ubiquity of computing is that it can have significant ethical implications of which computing professionals should take account. In most real-world scenarios, it is not immediately obvious how particular technical choices during the design and use of computing systems could be viewed from an ethical perspective. This article provides a perspective on the ethical challenges within semiconductor chip design, IoT applications, and the increasing use of artificial intelligence in the design processes, tools, and hardware-software stacks of these systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices,"The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or data enrichment, has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An Ecosystem Approach to Ethical AI and Data Use: Experimental Reflections,"While we have witnessed a rapid growth of ethics documents meant to guide AI development, the promotion of AI ethics has nonetheless proceeded with little input from AI practitioners themselves. Given the proliferation of AI for Social Good initiatives, this is an emerging gap that needs to be addressed in order to develop more meaningful ethical approaches to AI use and development. This paper offers a methodology, a shared fairness approach, aimed at identifying the needs of AI practitioners when it comes to confronting and resolving ethical challenges and to find a third space where their operational language can be married with that of the more abstract principles that presently remain at the periphery of their work experiences. We offer a grassroots approach to operational ethics based on dialog and mutualised responsibility. This methodology is centred around conversations intended to elicit practitioners perceived ethical attribution and distribution over key value laden operational decisions, to identify when these decisions arise and what ethical challenges they confront, and to engage in a language of ethics and responsibility which enables practitioners to internalise ethical responsibility. The methodology bridges responsibility imbalances that rest in structural decision making power and elite technical knowledge, by commencing with personal, facilitated conversations, returning the ethical discourse to those meant to give it meaning at the sharp end of the ecosystem. Our primary contribution is to add to the recent literature seeking to bring AI practitioners' experiences to the fore by offering a methodology for understanding how ethics manifests as a relational and interdependent sociotechnical practice in their work.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance,"While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed impacts of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has resulted in a number of collective actions by workers protesting how harms are identified and addressed in their workplace. We theorize the role of workers within AI governance and construct a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps, identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance, subjection, control over the product of labor, and proximate knowledge of systems. Examining the past decade of AI related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Logic Programming and Machine Ethics,"Transparency is a key requirement for ethical machines. Verified ethical behavior is not enough to establish justified trust in autonomous intelligent agents: it needs to be supported by the ability to explain decisions. Logic Programming (LP) has a great potential for developing such perspective ethical systems, as in fact logic rules are easily comprehensible by humans. Furthermore, LP is able to model causality, which is crucial for ethical decision making.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences","Given the growing use of Artificial Intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help {\it reduce} climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Fairness: from Principles to Practice,"This paper summarizes and evaluates various approaches, methods, and techniques for pursuing fairness in artificial intelligence (AI) systems. It examines the merits and shortcomings of these measures and proposes practical guidelines for defining, measuring, and preventing bias in AI. In particular, it cautions against some of the simplistic, yet common, methods for evaluating bias in AI systems, and offers more sophisticated and effective alternatives. The paper also addresses widespread controversies and confusions in the field by providing a common language among different stakeholders of high-impact AI systems. It describes various trade-offs involving AI fairness, and provides practical recommendations for balancing them. It offers techniques for evaluating the costs and benefits of fairness targets, and defines the role of human judgment in setting these targets. This paper provides discussions and guidelines for AI practitioners, organization leaders, and policymakers, as well as various links to additional materials for a more technical audience. Numerous real-world examples are provided to clarify the concepts, challenges, and recommendations from a practical perspective.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety,"Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel holistic methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies must also be transferred into local categories to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or high-risk sectors. This is because many sectors are constituted by discourses, norms, and values that ""refract"" or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of ""vernacularization""--a participatory, decolonial practice distinct from doctrinary ""translation"" (the dominant mode of AI safety operationalization)--can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading holistic taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Pseudo AI Bias,"Pseudo Artificial Intelligence bias (PAIB) is broadly disseminated in the literature, which can result in unnecessary AI fear in society, exacerbate the enduring inequities and disparities in access to and sharing the benefits of AI applications, and waste social capital invested in AI research. This study systematically reviews publications in the literature to present three types of PAIBs identified due to: a) misunderstandings, b) pseudo mechanical bias, and c) over-expectations. We discussed the consequences of and solutions to PAIBs, including certifying users for AI applications to mitigate AI fears, providing customized user guidance for AI applications, and developing systematic approaches to monitor bias. We concluded that PAIB due to misunderstandings, pseudo mechanical bias, and over-expectations of algorithmic predictions is socially harmful.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions","The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, we propose a multi-tiered framework for establishing robust industry standards for AIED. In addition, we discuss methodologies for the iterative development and deployment of standards, incorporating feedback loops from real-world applications to refine and adapt standards over time. The paper also highlights the role of emerging technologies and pedagogical theories in shaping future standards for AIED. Finally, we outline a strategic roadmap for stakeholders to implement these standards, fostering a cohesive and ethical AIED ecosystem. By establishing comprehensive industry standards, such as those by IEEE Artificial Intelligence Standards Committee (AISC) and International Organization for Standardization (ISO), we can accelerate and scale AIED solutions to improve educational outcomes, ensuring that technological advances align with the principles of inclusivity, fairness, and educational excellence.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Actionable Principles for Artificial Intelligence Policy: Three Pathways,"In the development of governmental policy for artificial intelligence (AI) that is informed by ethics, one avenue currently pursued is that of drawing on AI Ethics Principles. However, these AI Ethics Principles often fail to be actioned in governmental policy. This paper proposes a novel framework for the development of Actionable Principles for AI. The approach acknowledges the relevance of AI Ethics Principles and homes in on methodological elements to increase their practical implementability in policy processes. As a case study, elements are extracted from the development process of the Ethics Guidelines for Trustworthy AI of the European Commissions High Level Expert Group on AI. Subsequently, these elements are expanded on and evaluated in light of their ability to contribute to a prototype framework for the development of Actionable Principles for AI. The paper proposes the following three propositions for the formation of such a prototype framework: (1) preliminary landscape assessments; (2) multi-stakeholder participation and cross-sectoral feedback; and, (3) mechanisms to support implementation and operationalizability.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Data-Centric Safety and Ethical Measures for Data and AI Governance,"Datasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introduce both beneficial and harmful capabilities -- resulting in dual use AI foundation models, with various technical and regulatory approaches to monitor and manage these risks. However, despite the crucial role of datasets, responsible dataset design and ensuring data-centric safety and ethical practices have received less attention. In this study, we pro-pose responsible dataset design framework that encompasses various stages in the AI and dataset lifecycle to enhance safety measures and reduce the risk of AI misuse due to low quality, unsafe and unethical data content. This framework is domain agnostic, suitable for adoption for various applications and can promote responsible practices in dataset creation, use, and sharing to facilitate red teaming, minimize risks, and increase trust in AI models.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Reinforcement Learning with Stepwise Fairness Constraints,"AI methods are used in societally important settings, ranging from credit to employment to housing, and it is crucial to provide fairness in regard to algorithmic decision making. Moreover, many settings are dynamic, with populations responding to sequential decision policies. We introduce the study of reinforcement learning (RL) with stepwise fairness constraints, requiring group fairness at each time step. Our focus is on tabular episodic RL, and we provide learning algorithms with strong theoretical guarantees in regard to policy optimality and fairness violation. Our framework provides useful tools to study the impact of fairness constraints in sequential settings and brings up new challenges in RL.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence,"In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Blockchain As a Platform For Artificial Intelligence (AI) Transparency,"As artificial intelligence (AI) systems become increasingly complex and autonomous, concerns over transparency and accountability have intensified. The ""black box"" problem in AI decision-making limits stakeholders' ability to understand, trust, and verify outcomes, particularly in high-stakes sectors such as healthcare, finance, and autonomous systems. Blockchain technology, with its decentralized, immutable, and transparent characteristics, presents a potential solution to enhance AI transparency and auditability. This paper explores the integration of blockchain with AI to improve decision traceability, data provenance, and model accountability. By leveraging blockchain as an immutable record-keeping system, AI decision-making can become more interpretable, fostering trust among users and regulatory compliance. However, challenges such as scalability, integration complexity, and computational overhead must be addressed to fully realize this synergy. This study discusses existing research, proposes a framework for blockchain-enhanced AI transparency, and highlights practical applications, benefits, and limitations. The findings suggest that blockchain could be a foundational technology for ensuring AI systems remain accountable, ethical, and aligned with regulatory standards.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Justified Evidence Collection for Argument-based AI Fairness Assurance,"It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations","As artificial intelligence (AI) systems permeate critical sectors, the need for professionals who can address ethical, legal and governance challenges has become urgent. Current AI ethics education remains fragmented, often siloed by discipline and disconnected from practice. This paper synthesizes literature and regulatory developments to propose a modular, interdisciplinary curriculum that integrates technical foundations with ethics, law and policy. We highlight recurring operational failures in AI - bias, misspecified objectives, generalization errors, misuse and governance breakdowns - and link them to pedagogical strategies for teaching AI governance. Drawing on perspectives from the EU, China and international frameworks, we outline a semester plan that emphasizes integrated ethics, stakeholder engagement and experiential learning. The curriculum aims to prepare students to diagnose risks, navigate regulation and engage diverse stakeholders, fostering adaptive and ethically grounded professionals for responsible AI governance.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Bibliometric View of AI Ethics Development,"Artificial Intelligence (AI) Ethics is a nascent yet critical research field. Recent developments in generative AI and foundational models necessitate a renewed look at the problem of AI Ethics. In this study, we perform a bibliometric analysis of AI Ethics literature for the last 20 years based on keyword search. Our study reveals a three-phase development in AI Ethics, namely an incubation phase, making AI human-like machines phase, and making AI human-centric machines phase. We conjecture that the next phase of AI ethics is likely to focus on making AI more machine-like as AI matches or surpasses humans intellectually, a term we coin as ""machine-like human"".","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics Education in India: A Syllabus-Level Review of Computing Courses,"The pervasive integration of artificial intelligence (AI) across domains such as healthcare, governance, finance, and education has intensified scrutiny of its ethical implications, including algorithmic bias, privacy risks, accountability, and societal impact. While ethics has received growing attention in computer science (CS) education more broadly, the specific pedagogical treatment of {AI ethics} remains under-examined. This study addresses that gap through a large-scale analysis of 3,395 publicly accessible syllabi from CS and allied areas at leading Indian institutions. Among them, only 75 syllabi (2.21%) included any substantive AI ethics content. Three key findings emerged: (1) AI ethics is typically integrated as a minor module within broader technical courses rather than as a standalone course; (2) ethics coverage is often limited to just one or two instructional sessions; and (3) recurring topics include algorithmic fairness, privacy and data governance, transparency, and societal impact. While these themes reflect growing awareness, current curricular practices reveal limited depth and consistency. This work highlights both the progress and the gaps in preparing future technologists to engage meaningfully with the ethical dimensions of AI, and it offers suggestions to strengthen the integration of AI ethics within computing curricula.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI,"This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Theories of Parenting and their Application to Artificial Intelligence,"As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations.   Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Automated discovery of trade-off between utility, privacy and fairness in machine learning models","Machine learning models are deployed as a central component in decision making and policy operations with direct impact on individuals' lives. In order to act ethically and comply with government regulations, these models need to make fair decisions and protect the users' privacy. However, such requirements can come with decrease in models' performance compared to their potentially biased, privacy-leaking counterparts. Thus the trade-off between fairness, privacy and performance of ML models emerges, and practitioners need a way of quantifying this trade-off to enable deployment decisions. In this work we interpret this trade-off as a multi-objective optimization problem, and propose PFairDP, a pipeline that uses Bayesian optimization for discovery of Pareto-optimal points between fairness, privacy and utility of ML models. We show how PFairDP can be used to replicate known results that were achieved through manual constraint setting process. We further demonstrate effectiveness of PFairDP with experiments on multiple models and datasets.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics as a service: a pragmatic operationalisation of AI Ethics,"As the range of potential uses for Artificial Intelligence (AI), in particular machine learning (ML), has increased, so has awareness of the associated ethical issues. This increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals, groups, society, and the environment from AI harms. In response to this realisation, there has been a proliferation of principle-based ethics codes, guidelines and frameworks. However, it has become increasingly clear that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems. In previous work, we analysed whether it is possible to close this gap between the what and the how of AI ethics through the use of tools and methods designed to help AI developers, engineers, and designers translate principles into practice. We concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible (and thus vulnerable to ethics washing) or too strict (unresponsive to context). This raised the question: if, even with technical guidance, AI ethics is challenging to embed in the process of algorithmic design, is the entire pro-ethical design endeavour rendered futile? And, if no, then how can AI ethics be made useful for AI practitioners? This is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited, and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed Ethics as a Service.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval","Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1\% of cases and female-associated names in only 11.1\% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100\% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The social dilemma in artificial intelligence development and why we have to solve it,"While the demand for ethical artificial intelligence (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Monotonicity for AI ethics and society: An empirical study of the monotonic neural additive model in criminology, education, health care, and finance","Algorithm fairness in the application of artificial intelligence (AI) is essential for a better society. As the foundational axiom of social mechanisms, fairness consists of multiple facets. Although the machine learning (ML) community has focused on intersectionality as a matter of statistical parity, especially in discrimination issues, an emerging body of literature addresses another facet -- monotonicity. Based on domain expertise, monotonicity plays a vital role in numerous fairness-related areas, where violations could misguide human decisions and lead to disastrous consequences. In this paper, we first systematically evaluate the significance of applying monotonic neural additive models (MNAMs), which use a fairness-aware ML algorithm to enforce both individual and pairwise monotonicity principles, for the fairness of AI ethics and society. We have found, through a hybrid method of theoretical reasoning, simulation, and extensive empirical analysis, that considering monotonicity axioms is essential in all areas of fairness, including criminology, education, health care, and finance. Our research contributes to the interdisciplinary research at the interface of AI ethics, explainable AI (XAI), and human-computer interactions (HCIs). By evidencing the catastrophic consequences if monotonicity is not met, we address the significance of monotonicity requirements in AI applications. Furthermore, we demonstrate that MNAMs are an effective fairness-aware ML approach by imposing monotonicity restrictions integrating human intelligence.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk Management,"Trustworthy artificial intelligence (AI) has become an important topic because trust in AI systems and their creators has been lost. Researchers, corporations, and governments have long and painful histories of excluding marginalized groups from technology development, deployment, and oversight. As a result, these technologies are less useful and even harmful to minoritized groups. We argue that any AI development, deployment, and monitoring framework that aspires to trust must incorporate both feminist, non-exploitative participatory design principles and strong, outside, and continual monitoring and testing. We additionally explain the importance of considering aspects of trustworthiness beyond just transparency, fairness, and accountability, specifically, to consider justice and shifting power to the disempowered as core values to any trustworthy AI system. Creating trustworthy AI starts by funding, supporting, and empowering grassroots organizations like Queer in AI so the field of AI has the diversity and inclusion to credibly and effectively develop trustworthy AI. We leverage the expert knowledge Queer in AI has developed through its years of work and advocacy to discuss if and how gender, sexuality, and other aspects of queer identity should be used in datasets and AI systems and how harms along these lines should be mitigated. Based on this, we share a gendered approach to AI and further propose a queer epistemology and analyze the benefits it can bring to AI. We additionally discuss how to regulate AI with this queer epistemology in vision, proposing frameworks for making policies related to AI & gender diversity and privacy & queer data protection.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Mapping the Potential of Explainable AI for Fairness Along the AI Lifecycle,"The widespread use of artificial intelligence (AI) systems across various domains is increasingly surfacing issues related to algorithmic fairness, especially in high-stakes scenarios. Thus, critical considerations of how fairness in AI systems might be improved -- and what measures are available to aid this process -- are overdue. Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems. However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous. Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle. Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle. In this paper, we we distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them. We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview,"As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Teaching AI, Ethics, Law and Policy","The cyberspace and development of intelligent systems using Artificial Intelligence (AI) creates new challenges to computer professionals, data scientists, regulators and policy makers. For example, self-driving cars raise new technical, ethical, legal and public policy issues. This paper proposes a course named Computers, Ethics, Law, and Public Policy, and suggests a curriculum for such a course. This paper presents ethical, legal, and public policy issues relevant to building and using intelligent systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers,"Artificial Intelligence (AI) solutions and technologies are being increasingly adopted in smart systems context, however, such technologies are continuously concerned with ethical uncertainties. Various guidelines, principles, and regulatory frameworks are designed to ensure that AI technologies bring ethical well-being. However, the implications of AI ethics principles and guidelines are still being debated. To further explore the significance of AI ethics principles and relevant challenges, we conducted a survey of 99 representative AI practitioners and lawmakers (e.g., AI engineers, lawyers) from twenty countries across five continents. To the best of our knowledge, this is the first empirical study that encapsulates the perceptions of two different types of population (AI practitioners and lawmakers) and the study findings confirm that transparency, accountability, and privacy are the most critical AI ethics principles. On the other hand, lack of ethical knowledge, no legal frameworks, and lacking monitoring bodies are found the most common AI ethics challenges. The impact analysis of the challenges across AI ethics principles reveals that conflict in practice is a highly severe challenge. Moreover, the perceptions of practitioners and lawmakers are statistically correlated with significant differences for particular principles (e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies, machine distortion). Our findings stimulate further research, especially empowering existing capability maturity models to support the development and quality assessment of ethics-aware AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Consider ethical and social challenges in smart grid research,"Artificial Intelligence and Machine Learning are increasingly seen as key technologies for building more decentralised and resilient energy grids, but researchers must consider the ethical and social implications of their use","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The State of AI Ethics Report (Volume 4),"The 4th edition of the Montreal AI Ethics Institute's The State of AI Ethics captures the most relevant developments in the field of AI Ethics since January 2021. This report aims to help anyone, from machine learning experts to human rights activists and policymakers, quickly digest and understand the ever-changing developments in the field. Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, with a particular focus on four key themes: Ethical AI, Fairness & Justice, Humans & Tech, and Privacy.   In addition, The State of AI Ethics includes exclusive content written by world-class AI Ethics experts from universities, research institutes, consulting firms, and governments. Opening the report is a long-form piece by Edward Higgs (Professor of History, University of Essex) titled ""AI and the Face: A Historian's View."" In it, Higgs examines the unscientific history of facial analysis and how AI might be repeating some of those mistakes at scale. The report also features chapter introductions by Alexa Hagerty (Anthropologist, University of Cambridge), Marianna Ganapini (Faculty Director, Montreal AI Ethics Institute), Deborah G. Johnson (Emeritus Professor, Engineering and Society, University of Virginia), and Soraj Hongladarom (Professor of Philosophy and Director, Center for Science, Technology and Society, Chulalongkorn University in Bangkok).   This report should be used not only as a point of reference and insight on the latest thinking in the field of AI Ethics, but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of AI on the world.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
How Different Groups Prioritize Ethical Values for Responsible AI,"Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners' value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define responsible AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Journey to Trustworthy AI: Pursuit of Pragmatic Frameworks,"This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. We identify risk as a core factor in AI regulation and TAI. For example, as outlined in the EU-AI Act, organizations must gauge the risk level of their AI products to act accordingly (or risk hefty fines). We compare modalities of TAI implementation and how multiple cross-functional teams are engaged in the overall process. Thus, a brute force approach for enacting TAI renders its efficiency and agility, moot. To address this, we introduce our framework Set-Formalize-Measure-Act (SFMA). Our solution highlights the importance of transforming TAI-aware metrics, drivers of TAI, stakeholders, and business/legal requirements into actual benchmarks or tests. Finally, over-regulation driven by panic of powerful AI models can, in fact, harm TAI too. Based on GitHub user-activity data, in 2023, AI open-source projects rose to top projects by contributor account. Enabling innovation in TAI hinges on the independent contributions of the open-source community.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An Analysis of the New EU AI Act and A Proposed Standardization Framework for Machine Learning Fairness,"The European Union's AI Act represents a crucial step towards regulating ethical and responsible AI systems. However, we find an absence of quantifiable fairness metrics and the ambiguity in terminology, particularly the interchangeable use of the keywords transparency, explainability, and interpretability in the new EU AI Act and no reference of transparency of ethical compliance. We argue that this ambiguity creates substantial liability risk that would deter investment. Fairness transparency is strategically important. We recommend a more tailored regulatory framework to enhance the new EU AI regulation. Further-more, we propose a public system framework to assess the fairness and transparency of AI systems. Drawing from past work, we advocate for the standardization of industry best practices as a necessary addition to broad regulations to achieve the level of details required in industry, while preventing stifling innovation and investment in the AI sector. The proposals are exemplified with the case of ASR and speech synthesizers.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Building Bridges: Generative Artworks to Explore AI Ethics,"In recent years, there has been an increased emphasis on understanding and mitigating adverse impacts of artificial intelligence (AI) technologies on society. Across academia, industry, and government bodies, a variety of endeavours are being pursued towards enhancing AI ethics. A significant challenge in the design of ethical AI systems is that there are multiple stakeholders in the AI pipeline, each with their own set of constraints and interests. These different perspectives are often not understood, due in part to communication gaps.For example, AI researchers who design and develop AI models are not necessarily aware of the instability induced in consumers' lives by the compounded effects of AI decisions. Educating different stakeholders about their roles and responsibilities in the broader context becomes necessary. In this position paper, we outline some potential ways in which generative artworks can play this role by serving as accessible and powerful educational tools for surfacing different perspectives. We hope to spark interdisciplinary discussions about computational creativity broadly as a tool for enhancing AI ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance","Artificial intelligence (AI) has matured as a technology, necessitating the development of responsibility frameworks that are fair, inclusive, trustworthy, safe and secure, transparent, and accountable. By establishing such frameworks, we can harness the full potential of AI while mitigating its risks, particularly in high-risk scenarios. This requires the design of responsible AI systems based on trustworthy AI technologies and ethical principles, with the aim of ensuring auditability and accountability throughout their design, development, and deployment, adhering to domain-specific regulations and standards.   This paper explores the concept of a responsible AI system from a holistic perspective, which encompasses four key dimensions: 1) regulatory context; 2) trustworthy AI technology along with standardization and assessments; 3) auditability and accountability; and 4) AI governance. The aim of this paper is double. First, we analyze and understand these four dimensions and their interconnections in the form of an analysis and overview. Second, the final goal of the paper is to propose a roadmap in the design of responsible AI systems, ensuring that they can gain society's trust. To achieve this trustworthiness, this paper also fosters interdisciplinary discussions on the ethical, legal, social, economic, and cultural aspects of AI from a global governance perspective. Last but not least, we also reflect on the current state and those aspects that need to be developed in the near future, as ten lessons learned.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Artificial Intelligence (AI) and the Relationship between Agency, Autonomy, and Moral Patiency","The proliferation of Artificial Intelligence (AI) systems exhibiting complex and seemingly agentive behaviours necessitates a critical philosophical examination of their agency, autonomy, and moral status. In this paper we undertake a systematic analysis of the differences between basic, autonomous, and moral agency in artificial systems. We argue that while current AI systems are highly sophisticated, they lack genuine agency and autonomy because: they operate within rigid boundaries of pre-programmed objectives rather than exhibiting true goal-directed behaviour within their environment; they cannot authentically shape their engagement with the world; and they lack the critical self-reflection and autonomy competencies required for full autonomy. Nonetheless, we do not rule out the possibility of future systems that could achieve a limited form of artificial moral agency without consciousness through hybrid approaches to ethical decision-making. This leads us to suggest, by appealing to the necessity of consciousness for moral patiency, that such non-conscious AMAs might represent a case that challenges traditional assumptions about the necessary connection between moral agency and moral patiency.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The AI Fairness Myth: A Position Paper on Context-Aware Bias,"Defining fairness in AI remains a persistent challenge, largely due to its deeply context-dependent nature and the lack of a universal definition. While numerous mathematical formulations of fairness exist, they sometimes conflict with one another and diverge from social, economic, and legal understandings of justice. Traditional quantitative definitions primarily focus on statistical comparisons, but they often fail to simultaneously satisfy multiple fairness constraints. Drawing on philosophical theories (Rawls' Difference Principle and Dworkin's theory of equality) and empirical evidence supporting affirmative action, we argue that fairness sometimes necessitates deliberate, context-aware preferential treatment of historically marginalized groups. Rather than viewing bias solely as a flaw to eliminate, we propose a framework that embraces corrective, intentional biases to promote genuine equality of opportunity. Our approach involves identifying unfairness, recognizing protected groups/individuals, applying corrective strategies, measuring impact, and iterating improvements. By bridging mathematical precision with ethical and contextual considerations, we advocate for an AI fairness paradigm that goes beyond neutrality to actively advance social justice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Challenges for Society and Ethics,"Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Good proctor or ""Big Brother""? AI Ethics and Online Exam Supervision Technologies","This article philosophically analyzes online exam supervision technologies, which have been thrust into the public spotlight due to campus lockdowns during the COVID-19 pandemic and the growing demand for online courses. Online exam proctoring technologies purport to provide effective oversight of students sitting online exams, using artificial intelligence (AI) systems and human invigilators to supplement and review those systems. Such technologies have alarmed some students who see them as `Big Brother-like', yet some universities defend their judicious use. Critical ethical appraisal of online proctoring technologies is overdue. This article philosophically analyzes these technologies, focusing on the ethical concepts of academic integrity, fairness, non-maleficence, transparency, privacy, respect for autonomy, liberty, and trust. Most of these concepts are prominent in the new field of AI ethics and all are relevant to the education context. The essay provides ethical considerations that educational institutions will need to carefully review before electing to deploy and govern specific online proctoring technologies.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics,"Why do biased predictions arise? What interventions can prevent them? We evaluate 8.2 million algorithmic predictions of math performance from $\approx$400 AI engineers, each of whom developed an algorithm under a randomly assigned experimental condition. Our treatment arms modified programmers' incentives, training data, awareness, and/or technical knowledge of AI ethics. We then assess out-of-sample predictions from their algorithms using randomized audit manipulations of algorithm inputs and ground-truth math performance for 20K subjects. We find that biased predictions are mostly caused by biased training data. However, one-third of the benefit of better training data comes through a novel economic mechanism: Engineers exert greater effort and are more responsive to incentives when given better training data. We also assess how performance varies with programmers' demographic characteristics, and their performance on a psychological test of implicit bias (IAT) concerning gender and careers. We find no evidence that female, minority and low-IAT engineers exhibit lower bias or discrimination in their code. However, we do find that prediction errors are correlated within demographic groups, which creates performance improvements through cross-demographic averaging. Finally, we quantify the benefits and tradeoffs of practical managerial or policy interventions such as technical advice, simple reminders, and improved incentives for decreasing algorithmic bias.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation,"Course syllabi set the tone and expectations for courses, shaping the learning experience for both students and instructors. In computing courses, especially those addressing fairness and ethics in artificial intelligence (AI), machine learning (ML), and algorithmic design, it is imperative that we understand how approaches to navigating barriers to fair outcomes are being addressed.These expectations should be inclusive, transparent, and grounded in promoting critical thinking. Syllabus analysis offers a way to evaluate the coverage, depth, practices, and expectations within a course. Manual syllabus evaluation, however, is time-consuming and prone to inconsistency. To address this, we developed a justice-oriented scoring rubric and asked a large language model (LLM) to review syllabi through a multi-perspective role simulation. Using this rubric, we evaluated 24 syllabi from four perspectives: instructor, departmental chair, institutional reviewer, and external evaluator. We also prompted the LLM to identify thematic trends across the courses. Findings show that multiperspective evaluation aids us in noting nuanced, role-specific priorities, leveraging them to fill hidden gaps in curricula design of AI/ML and related computing courses focused on fairness and ethics. These insights offer concrete directions for improving the design and delivery of fairness, ethics, and justice content in such courses.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Three Kinds of AI Ethics,"There is an overwhelming abundance of works in AI Ethics. This growth is chaotic because of how sudden it is, its volume, and its multidisciplinary nature. This makes difficult to keep track of debates, and to systematically characterize goals, research questions, methods, and expertise required by AI ethicists. In this article, I show that the relation between AI and ethics can be characterized in at least three ways, which correspond to three well-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI. I elucidate the features of these three kinds of AI Ethics, characterize their research questions, and identify the kind of expertise that each kind needs. I also show how certain criticisms to AI ethics are misplaced, as being done from the point of view of one kind of AI ethics, to another kind with different goals. All in all, this work sheds light on the nature of AI ethics, and sets the groundwork for more informed discussions about the scope, methods, and training of AI ethicists.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The State of AI Ethics Report (October 2020),"The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics captures the most relevant developments in the field of AI Ethics since July 2020. This report aims to help anyone, from machine learning experts to human rights activists and policymakers, quickly digest and understand the ever-changing developments in the field. Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, including: AI and society, bias and algorithmic justice, disinformation, humans and AI, labor impacts, privacy, risk, and future of AI ethics.   In addition, The State of AI Ethics includes exclusive content written by world-class AI Ethics experts from universities, research institutes, consulting firms, and governments. These experts include: Danit Gal (Tech Advisor, United Nations), Amba Kak (Director of Global Policy and Programs, NYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI, Accenture), Brent Barron (Director of Strategic Projects and Knowledge Management, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of the OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of Management), and Katya Klinova (AI and Economy Program Lead, Partnership on AI).   This report should be used not only as a point of reference and insight on the latest thinking in the field of AI Ethics, but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of AI on the world.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Bias, diversity, and challenges to fairness in classification and automated text analysis. From libraries to AI and back","Libraries are increasingly relying on computational methods, including methods from Artificial Intelligence (AI). This increasing usage raises concerns about the risks of AI that are currently broadly discussed in scientific literature, the media and law-making. In this article we investigate the risks surrounding bias and unfairness in AI usage in classification and automated text analysis within the context of library applications. We describe examples that show how the library community has been aware of such risks for a long time, and how it has developed and deployed countermeasures. We take a closer look at the notion of '(un)fairness' in relation to the notion of 'diversity', and we investigate a formalisation of diversity that models both inclusion and distribution. We argue that many of the unfairness problems of automated content analysis can also be regarded through the lens of diversity and the countermeasures taken to enhance diversity.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"""My Kind of Woman"": Analysing Gender Stereotypes in AI through The Averageness Theory and EU Law","This study delves into gender classification systems, shedding light on the interaction between social stereotypes and algorithmic determinations. Drawing on the ""averageness theory,"" which suggests a relationship between a face's attractiveness and the human ability to ascertain its gender, we explore the potential propagation of human bias into artificial intelligence (AI) systems. Utilising the AI model Stable Diffusion 2.1, we have created a dataset containing various connotations of attractiveness to test whether the correlation between attractiveness and accuracy in gender classification observed in human cognition persists within AI. Our findings indicate that akin to human dynamics, AI systems exhibit variations in gender classification accuracy based on attractiveness, mirroring social prejudices and stereotypes in their algorithmic decisions. This discovery underscores the critical need to consider the impacts of human perceptions on data collection and highlights the necessity for a multidisciplinary and intersectional approach to AI development and AI data training. By incorporating cognitive psychology and feminist legal theory, we examine how data used for AI training can foster gender diversity and fairness under the scope of the AI Act and GDPR, reaffirming how psychological and feminist legal theories can offer valuable insights for ensuring the protection of gender equality and non-discrimination in AI systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Addressing Intersectionality, Explainability, and Ethics in AI-Driven Diagnostics: A Rebuttal and Call for Transdiciplinary Action","The increasing integration of artificial intelligence (AI) into medical diagnostics necessitates a critical examination of its ethical and practical implications. While the prioritization of diagnostic accuracy, as advocated by Sabuncu et al. (2025), is essential, this approach risks oversimplifying complex socio-ethical issues, including fairness, privacy, and intersectionality. This rebuttal emphasizes the dangers of reducing multifaceted health disparities to quantifiable metrics and advocates for a more transdisciplinary approach. By incorporating insights from social sciences, ethics, and public health, AI systems can address the compounded effects of intersecting identities and safeguard sensitive data. Additionally, explainability and interpretability must be central to AI design, fostering trust and accountability. This paper calls for a framework that balances accuracy with fairness, privacy, and inclusivity to ensure AI-driven diagnostics serve diverse populations equitably and ethically.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist,"The widespread use of ChatGPT and other emerging technology powered by generative artificial intelligence (GenAI) has drawn much attention to potential ethical issues, especially in high-stakes applications such as healthcare, but ethical discussions are yet to translate into operationalisable solutions. Furthermore, ongoing ethical discussions often neglect other types of GenAI that have been used to synthesise data (e.g., images) for research and practical purposes, which resolved some ethical issues and exposed others. We conduct a scoping review of ethical discussions on GenAI in healthcare to comprehensively analyse gaps in the current research, and further propose to reduce the gaps by developing a checklist for comprehensive assessment and transparent documentation of ethical discussions in GenAI research. The checklist can be readily integrated into the current peer review and publication system to enhance GenAI research, and may be used for ethics-related disclosures for GenAI-powered products, healthcare applications of such products and beyond.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics of AI: A Systematic Literature Review of Principles and Challenges,"Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assess the ethical capabilities of AI systems and provide best practices for further improvements.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics and Responsible AI Deployment,"As Artificial Intelligence (AI) becomes more prevalent, protecting personal privacy is a critical ethical issue that must be addressed. This article explores the need for ethical AI systems that safeguard individual privacy while complying with ethical standards. By taking a multidisciplinary approach, the research examines innovative algorithmic techniques such as differential privacy, homomorphic encryption, federated learning, international regulatory frameworks, and ethical guidelines. The study concludes that these algorithms effectively enhance privacy protection while balancing the utility of AI with the need to protect personal data. The article emphasises the importance of a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of AI in a way that respects and protects individual privacy.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Broadening AI Ethics Narratives: An Indic Art View,"Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction--that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how {\it non-Western} ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the {\it `Natyashastra'}), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Fairness for People with Disabilities: Point of View,"We consider how fair treatment in society for people with disabilities might be impacted by the rise in the use of artificial intelligence, and especially machine learning methods. We argue that fairness for people with disabilities is different to fairness for other protected attributes such as age, gender or race. One major difference is the extreme diversity of ways disabilities manifest, and people adapt. Secondly, disability information is highly sensitive and not always shared, precisely because of the potential for discrimination. Given these differences, we explore definitions of fairness and how well they work in the disability space. Finally, we suggest ways of approaching fairness for people with disabilities in AI applications.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Unmasking Bias in AI: A Systematic Review of Bias Detection and Mitigation Strategies in Electronic Health Record-based Models,"Objectives: Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. Yet, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to detect and mitigate diverse forms of bias in AI models developed using EHR data. Methods: We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 1, 2010, and Dec 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development process, and analyzed metrics for bias assessment. Results: Of the 450 articles retrieved, 20 met our criteria, revealing six major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks in healthcare settings. Four studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Sixty proposed various strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance (e.g., accuracy, AUROC) and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling, reweighting, and transformation. Discussion: This review highlights the varied and evolving nature of strategies to address bias in EHR-based AI models, emphasizing the urgent needs for the establishment of standardized, generalizable, and interpretable methodologies to foster the creation of ethical AI systems that promote fairness and equity in healthcare.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
CERN for AI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment,"This paper explores the potential of a multidisciplinary approach to testing and aligning artificial intelligence (AI), specifically focusing on large language models (LLMs). Due to the rapid development and wide application of LLMs, challenges such as ethical alignment, controllability, and predictability of these models emerged as global risks. This study investigates an innovative simulation-based multi-agent system within a virtual reality framework that replicates the real-world environment. The framework is populated by automated 'digital citizens,' simulating complex social structures and interactions to examine and optimize AI. Application of various theories from the fields of sociology, social psychology, computer science, physics, biology, and economics demonstrates the possibility of a more human-aligned and socially responsible AI. The purpose of such a digital environment is to provide a dynamic platform where advanced AI agents can interact and make independent decisions, thereby mimicking realistic scenarios. The actors in this digital city, operated by the LLMs, serve as the primary agents, exhibiting high degrees of autonomy. While this approach shows immense potential, there are notable challenges and limitations, most significantly the unpredictable nature of real-world social dynamics. This research endeavors to contribute to the development and refinement of AI, emphasizing the integration of social, ethical, and theoretical dimensions for future research.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
ECCOLA -- a Method for Implementing Ethically Aligned AI Systems,"Various recent Artificial Intelligence (AI) system failures, some of which have made the global headlines, have highlighted issues in these systems. These failures have resulted in calls for more ethical AI systems that better take into account their effects on various stakeholders. However, implementing AI ethics into practice is still an on-going challenge. High-level guidelines for doing so exist, devised by governments and private organizations alike, but lack practicality for developers. To address this issue, in this paper, we present a method for implementing AI ethics. The method, ECCOLA, has been iteratively developed using a cyclical action design research approach. The method aims at making the high-level AI ethics principles more practical, making it possible for developers to more easily implement them in practice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Making Power Explicable in AI: Analyzing, Understanding, and Redirecting Power to Operationalize Ethics in AI Technical Practice","The operationalization of ethics in the technical practices of artificial intelligence (AI) is facing significant challenges. To address the problem of ineffective implementation of AI ethics, we present our diagnosis, analysis, and interventional recommendations from a unique perspective of the real-world implementation of AI ethics through explainable AI (XAI) techniques. We first describe the phenomenon (i.e., the ""symptoms"") of ineffective implementation of AI ethics in explainable AI using four empirical cases. From the ""symptoms"", we diagnose the root cause (i.e., the ""disease"") being the dysfunction and imbalance of power structures in the sociotechnical system of AI. The power structures are dominated by unjust and unchecked power that does not represent the benefits and interests of the public and the most impacted communities, and cannot be countervailed by ethical power. Based on the understanding of power mechanisms, we propose three interventional recommendations to tackle the root cause, including: 1) Making power explicable and checked, 2) Reframing the narratives and assumptions of AI and AI ethics to check unjust power and reflect the values and benefits of the public, and 3) Uniting the efforts of ethical and scientific conduct of AI to encode ethical values as technical standards, norms, and methods, including conducting critical examinations and limitation analyses of AI technical practices. We hope that our diagnosis and interventional recommendations can be a useful input to the AI community and civil society's ongoing discussion and implementation of ethics in AI for ethical and responsible AI practice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Implementing AI Ethics: Making Sense of the Ethical Requirements,"Society's increasing dependence on Artificial Intelligence (AI) and AI-enabled systems require a more practical approach from software engineering (SE) executives in middle and higher-level management to improve their involvement in implementing AI ethics by making ethical requirements part of their management practices. However, research indicates that most work on implementing ethical requirements in SE management primarily focuses on technical development, with scarce findings for middle and higher-level management. We investigate this by interviewing ten Finnish SE executives in middle and higher-level management to examine how they consider and implement ethical requirements. We use ethical requirements from the European Union (EU) Trustworthy Ethics guidelines for Trustworthy AI as our reference for ethical requirements and an Agile portfolio management framework to analyze implementation. Our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified. The findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental well-being for implementation as sustainability requirements. We examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the Agile portfolio management framework.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective","This article explores the ethical problems arising from the use of ChatGPT as a kind of generative AI and suggests responses based on the Human-Centered Artificial Intelligence (HCAI) framework. The HCAI framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a grand challenge, thus perfectly aligning itself with ethics, the science of human flourishing. Further, HCAI provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy AI which we apply to our ChatGPT assessments. The main danger ChatGPT presents is the propensity to be used as a weapon of mass deception (WMD) and an enabler of criminal activities involving deceit. We review technical specifications to better comprehend its potentials and limitations. We then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparency, educator considerations, HITL) to mitigate ChatGPT misuse or abuse and recommend best uses (creative writing, non-creative writing, teaching and learning). We conclude with considerations regarding the role of humans in ensuring the proper use of ChatGPT for individual and social wellbeing.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Return on Investment in AI Ethics: A Holistic Framework,"We propose a Holistic Return on Ethics (HROE) framework for understanding the return on organizational investments in artificial intelligence (AI) ethics efforts. This framework is useful for organizations that wish to quantify the return for their investment decisions. The framework identifies the direct economic returns of such investments, the indirect paths to return through intangibles associated with organizational reputation, and real options associated with capabilities. The holistic framework ultimately provides organizations with the competency to employ and justify AI ethics investments.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare,"Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising an in-depth literature review, a modified Delphi survey, and online consensus meetings. The FUTURE-AI framework was established based on 6 guiding principles for trustworthy AI in healthcare, i.e. Fairness, Universality, Traceability, Usability, Robustness and Explainability. Through consensus, a set of 28 best practices were defined, addressing technical, clinical, legal and socio-ethical dimensions. The recommendations cover the entire lifecycle of medical AI, from design, development and validation to regulation, deployment, and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which provides a structured approach for constructing medical AI tools that will be trusted, deployed and adopted in real-world practice. Researchers are encouraged to take the recommendations into account in proof-of-concept stages to facilitate future translation towards clinical practice of medical AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making,"Artificial intelligence surrogates are systems designed to infer preferences when individuals lose decision-making capacity. Fairness in such systems is a domain that has been insufficiently explored. Traditional algorithmic fairness frameworks are insufficient for contexts where decisions are relational, existential, and culturally diverse. This paper explores an ethical framework for algorithmic fairness in AI surrogates by mapping major fairness notions onto potential real-world end-of-life scenarios. It then examines fairness across moral traditions. The authors argue that fairness in this domain extends beyond parity of outcomes to encompass moral representation, fidelity to the patient's values, relationships, and worldview.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond","Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions,"Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
How should AI decisions be explained? Requirements for Explanations from the Perspective of European Law,"This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, for which the trilogue of the European Parliament, Council and Commission recently concluded, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary plausibility checks, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI-methods are derived from each of the legal bases, resulting in the conclusion that each legal basis requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI-methods.   Published in the Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society https://doi.org/10.1609/aies.v7i1.31648 .","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Enhancing transparency in AI-powered customer engagement,"This paper addresses the critical challenge of building consumer trust in AI-powered customer engagement by emphasising the necessity for transparency and accountability. Despite the potential of AI to revolutionise business operations and enhance customer experiences, widespread concerns about misinformation and the opacity of AI decision-making processes hinder trust. Surveys highlight a significant lack of awareness among consumers regarding their interactions with AI, alongside apprehensions about bias and fairness in AI algorithms. The paper advocates for the development of explainable AI models that are transparent and understandable to both consumers and organisational leaders, thereby mitigating potential biases and ensuring ethical use. It underscores the importance of organisational commitment to transparency practices beyond mere regulatory compliance, including fostering a culture of accountability, prioritising clear data policies and maintaining active engagement with stakeholders. By adopting a holistic approach to transparency and explainability, businesses can cultivate trust in AI technologies, bridging the gap between technological innovation and consumer acceptance, and paving the way for more ethical and effective AI-powered customer engagements. KEYWORDS: artificial intelligence (AI), transparency","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Responsible Artificial Intelligence: A Structured Literature Review,"Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an approach for developing a future framework centered around this concept. Our findings advocate for a human-centric approach to Responsible AI. This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Resolving Ethics Trade-offs in Implementing Responsible AI,"While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Empirical AI Ethics: Reconfiguring Ethics towards a Situated, Plural, and Transformative Approach","Mainstream AI ethics, with its reliance on top-down, principle-driven frameworks, fails to account for the situated realities of diverse communities affected by AI (Artificial Intelligence). Critics have argued that AI ethics frequently serves corporate interests through practices of 'ethics washing', operating more as a tool for public relations than as a means of preventing harm or advancing the common good. As a result, growing scepticism among critical scholars has cast the field as complicit in sustaining harmful systems rather than challenging or transforming them. In response, this paper adopts a Science and Technology Studies (STS) perspective to critically interrogate the field of AI ethics. It hence applies the same analytic tools STS has long directed at disciplines such as biology, medicine, and statistics to ethics. This perspective reveals a core tension between vertical (top-down, principle-based) and horizontal (risk-mitigating, implementation-oriented) approaches to ethics. By tracing how these models have shaped the discourse, we show how both fall short in addressing the complexities of AI as a socio-technical assemblage, embedded in practice and entangled with power. To move beyond these limitations, we propose a threefold reorientation of AI ethics. First, we call for a shift in foundations: from top-down abstraction to empirical grounding. Second, we advocate for pluralisation: moving beyond Western-centric frameworks toward a multiplicity of onto-epistemic perspectives. Finally, we outline strategies for reconfiguring AI ethics as a transformative force, moving from narrow paradigms of risk mitigation toward co-creating technologies of hope.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Integrating Artificial Intelligence into Weapon Systems,"The integration of Artificial Intelligence (AI) into weapon systems is one of the most consequential tactical and strategic decisions in the history of warfare. Current AI development is a remarkable combination of accelerating capability, hidden decision mechanisms, and decreasing costs. Implementation of these systems is in its infancy and exists on a spectrum from resilient and flexible to simplistic and brittle. Resilient systems should be able to effectively handle the complexities of a high-dimensional battlespace. Simplistic AI implementations could be manipulated by an adversarial AI that identifies and exploits their weaknesses.   In this paper, we present a framework for understanding the development of dynamic AI/ML systems that interactively and continuously adapt to their user's needs. We explore the implications of increasingly capable AI in the kill chain and how this will lead inevitably to a fully automated, always on system, barring regulation by treaty. We examine the potential of total integration of cyber and physical security and how this likelihood must inform the development of AI-enabled systems with respect to the ""fog of war"", human morals, and ethics.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation,"The implementation of Artificial Intelligence (AI) in household environments, especially in the form of proactive autonomous agents, brings about possibilities of comfort and attention as well as it comes with intra or extramural ethical challenges. This article analyzes agentic AI and its applications, focusing on its move from reactive to proactive autonomy, privacy, fairness and user control. We review responsible innovation frameworks, human-centered design principles, and governance practices to distill practical guidance for ethical smart home systems. Vulnerable user groups such as elderly individuals, children, and neurodivergent who face higher risks of surveillance, bias, and privacy risks were studied in detail in context of Agentic AI. Design imperatives are highlighted such as tailored explainability, granular consent mechanisms, and robust override controls, supported by participatory and inclusive methodologies. It was also explored how data-driven insights, including social media analysis via Natural Language Processing(NLP), can inform specific user needs and ethical concerns. This survey aims to provide both a conceptual foundation and suggestions for developing transparent, inclusive, and trustworthy agentic AI in household automation.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study,"Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Explainable Artificial Intelligence: A Survey of Needs, Techniques, Applications, and Future Direction","Artificial intelligence models encounter significant challenges due to their black-box nature, particularly in safety-critical domains such as healthcare, finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI) addresses these challenges by providing explanations for how these models make decisions and predictions, ensuring transparency, accountability, and fairness. Existing studies have examined the fundamental concepts of XAI, its general principles, and the scope of XAI techniques. However, there remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representations, design methodologies of XAI models, and other associated aspects. This paper provides a comprehensive literature review encompassing common terminologies and definitions, the need for XAI, beneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI methods in different application areas. The survey is aimed at XAI researchers, XAI practitioners, AI model developers, and XAI beneficiaries who are interested in enhancing the trustworthiness, transparency, accountability, and fairness of their AI models.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Different Faces of AI Ethics Across the World: A Principle-Implementation Gap Analysis,"Artificial Intelligence (AI) is transforming our daily life with several applications in healthcare, space exploration, banking and finance. These rapid progresses in AI have brought increasing attention to the potential impacts of AI technologies on society, with ethically questionable consequences. In recent years, several ethical principles have been released by governments, national and international organisations. These principles outline high-level precepts to guide the ethical development, deployment, and governance of AI. However, the abstract nature, diversity, and context-dependency of these principles make them difficult to implement and operationalize, resulting in gaps between principles and their execution. Most recent work analysed and summarized existing AI principles and guidelines but they did not provide findings on principle-implementation gaps and how to mitigate them. These findings are particularly important to ensure that AI implementations are aligned with ethical principles and values. In this paper, we provide a contextual and global evaluation of current ethical AI principles for all continents, with the aim to identify potential principle characteristics tailored to specific countries or applicable across countries. Next, we analyze the current level of AI readiness and current implementations of ethical AI principles in different countries, to identify gaps in the implementation of AI principles and their causes. Finally, we propose recommendations to mitigate the principle-implementation gaps.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
On the Morality of Artificial Intelligence,"Much of the existing research on the social and ethical impact of Artificial Intelligence has been focused on defining ethical principles and guidelines surrounding Machine Learning (ML) and other Artificial Intelligence (AI) algorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for helping define the appropriate social norms of AI, we believe that it is equally important to discuss both the potential and risks of ML and to inspire the community to use ML for beneficial objectives. In the present article, which is specifically aimed at ML practitioners, we thus focus more on the latter, carrying out an overview of existing high-level ethical frameworks and guidelines, but above all proposing both conceptual and practical principles and guidelines for ML research and deployment, insisting on concrete actions that can be taken by practitioners to pursue a more ethical and moral practice of ML aimed at using AI for social good.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Capability Approach to AI Ethics,"We propose a conceptualization and implementation of AI ethics via the capability approach. We aim to show that conceptualizing AI ethics through the capability approach has two main advantages for AI ethics as a discipline. First, it helps clarify the ethical dimension of AI tools. Second, it provides guidance to implementing ethical considerations within the design of AI tools. We illustrate these advantages in the context of AI tools in medicine, by showing how ethics-based auditing of AI tools in medicine can greatly benefit from our capability-based approach.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical Framework for Harnessing the Power of AI in Healthcare and Beyond,"In the past decade, the deployment of deep learning (Artificial Intelligence (AI)) methods has become pervasive across a spectrum of real-world applications, often in safety-critical contexts. This comprehensive research article rigorously investigates the ethical dimensions intricately linked to the rapid evolution of AI technologies, with a particular focus on the healthcare domain. Delving deeply, it explores a multitude of facets including transparency, adept data management, human oversight, educational imperatives, and international collaboration within the realm of AI advancement. Central to this article is the proposition of a conscientious AI framework, meticulously crafted to accentuate values of transparency, equity, answerability, and a human-centric orientation. The second contribution of the article is the in-depth and thorough discussion of the limitations inherent to AI systems. It astutely identifies potential biases and the intricate challenges of navigating multifaceted contexts. Lastly, the article unequivocally accentuates the pressing need for globally standardized AI ethics principles and frameworks. Simultaneously, it aptly illustrates the adaptability of the ethical framework proposed herein, positioned skillfully to surmount emergent challenges.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Fair Game: Auditing & Debiasing AI Algorithms Over Time,"An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,""Fair Game"",to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. ""Fair Game"" puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The ""Fair Game"" puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. ""Fair Game"" provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,""Fair Game"" aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Breaking Bad News in the Era of Artificial Intelligence and Algorithmic Medicine: An Exploration of Disclosure and its Ethical Justification using the Hedonic Calculus,"An appropriate ethical framework around the use of Artificial Intelligence (AI) in healthcare has become a key desirable with the increasingly widespread deployment of this technology. Advances in AI hold the promise of improving the precision of outcome prediction at the level of the individual. However, the addition of these technologies to patient-clinician interactions, as with any complex human interaction, has potential pitfalls. While physicians have always had to carefully consider the ethical background and implications of their actions, detailed deliberations around fast-moving technological progress may not have kept up. We use a common but key challenge in healthcare interactions, the disclosure of bad news (likely imminent death), to illustrate how the philosophical framework of the 'Felicific Calculus' developed in the 18th century by Jeremy Bentham, may have a timely quasi-quantitative application in the age of AI. We show how this ethical algorithm can be used to assess, across seven mutually exclusive and exhaustive domains, whether an AI-supported action can be morally justified.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Dislocated Accountabilities in the AI Supply Chain: Modularity and Developers' Notions of Responsibility,"Responsible artificial intelligence guidelines ask engineers to consider how their systems might harm. However, contemporary artificial intelligence systems are built by composing many preexisting software modules that pass through many hands before becoming a finished product or service. How does this shape responsible artificial intelligence practice? In interviews with 27 artificial intelligence engineers across industry, open source, and academia, our participants often did not see the questions posed in responsible artificial intelligence guidelines to be within their agency, capability, or responsibility to address. We use Suchman's ""located accountability"" to show how responsible artificial intelligence labor is currently organized and to explore how it could be done differently. We identify cross-cutting social logics, like modularizability, scale, reputation, and customer orientation, that organize which responsible artificial intelligence actions do take place and which are relegated to low status staff or believed to be the work of the next or previous person in the imagined ""supply chain."" We argue that current responsible artificial intelligence interventions, like ethics checklists and guidelines that assume panoptical knowledge and control over systems, could be improved by taking a located accountability approach, recognizing where relations and obligations might intertwine inside and outside of this supply chain.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities,"Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML and AI issues. This paper reviews the evolution of ML disclosure practices, from ad hoc reporting to emerging participatory auditing methods, and compares them with cybersecurity norms. Our framework introduces innovations such as extended model cards, dynamic scope expansion, an independent adjudication panel, and an automated verification process. We also outline a forthcoming real-world pilot of CFD. We argue that CFD could significantly enhance public trust in AI systems. By balancing organizational and community interests, CFD aims to improve AI accountability in a rapidly evolving technological landscape.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The State of AI Ethics Report (June 2020),"These past few months have been especially challenging, and the deployment of technology in ways hitherto untested at an unrivalled pace has left the internet and technology watchers aghast. Artificial intelligence has become the byword for technological progress and is being used in everything from helping us combat the COVID-19 pandemic to nudging our attention in different directions as we all spend increasingly larger amounts of time online. It has never been more important that we keep a sharp eye out on the development of this field and how it is shaping our society and interactions with each other. With this inaugural edition of the State of AI Ethics we hope to bring forward the most important developments that caught our attention at the Montreal AI Ethics Institute this past quarter. Our goal is to help you navigate this ever-evolving field swiftly and allow you and your organization to make informed decisions. This pulse-check for the state of discourse, research, and development is geared towards researchers and practitioners alike who are making decisions on behalf of their organizations in considering the societal impacts of AI-enabled solutions. We cover a wide set of areas in this report spanning Agency and Responsibility, Security and Risk, Disinformation, Jobs and Labor, the Future of AI Ethics, and more. Our staff has worked tirelessly over the past quarter surfacing signal from the noise so that you are equipped with the right tools and knowledge to confidently tread this complex yet consequential domain.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Inherent Limitations of AI Fairness,"As the real-world impact of Artificial Intelligence (AI) systems has been steadily growing, so too have these systems come under increasing scrutiny. In response, the study of AI fairness has rapidly developed into a rich field of research with links to computer science, social science, law, and philosophy. Many technical solutions for measuring and achieving AI fairness have been proposed, yet their approach has been criticized in recent years for being misleading, unrealistic and harmful.   In our paper, we survey these criticisms of AI fairness and identify key limitations that are inherent to the prototypical paradigm of AI fairness. By carefully outlining the extent to which technical solutions can realistically help in achieving AI fairness, we aim to provide the background necessary to form a nuanced opinion on developments in fair AI. This delineation also provides research opportunities for non-AI solutions peripheral to AI systems in supporting fair decision processes.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Catalog of General Ethical Requirements for AI Certification,"This whitepaper offers normative and practical guidance for developers of artificial intelligence (AI) systems to achieve ""Trustworthy AI"". In it, we present overall ethical requirements and six ethical principles with value-specific recommendations for tools to implement these principles into technology. Our value-specific recommendations address the principles of fairness, privacy and data protection, safety and robustness, sustainability, transparency and explainability and truthfulness. For each principle, we also present examples of criteria for risk assessment and categorization of AI systems and applications in line with the categories of the European Union (EU) AI Act. Our work is aimed at stakeholders who can take it as a potential blueprint to fulfill minimum ethical requirements for trustworthy AI and AI Certification.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art,"As Artificial Intelligence (AI) continues to grow daily, more exciting (and somewhat controversial) technology emerges every other day. As we see the advancements in AI, we see more and more people becoming skeptical of it. This paper explores the complications and confusion around the ethics of generative AI art. We delve deep into the ethical side of AI, specifically generative art. We step back from the excitement and observe the impossible conundrums that this impressive technology produces. Covering environmental consequences, celebrity representation, intellectual property, deep fakes, and artist displacement. Our research found that generative AI art is responsible for increased carbon emissions, spreading misinformation, copyright infringement, unlawful depiction, and job displacement. In light of this, we propose multiple possible solutions for these problems. We address each situation's history, cause, and consequences and offer different viewpoints. At the root of it all, though, the central theme is that generative AI Art needs to be correctly legislated and regulated.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Beyond Algorethics: Addressing the Ethical and Anthropological Challenges of AI Recommender Systems,"This paper examines the ethical and anthropological challenges posed by AI-driven recommender systems (RSs), which increasingly shape digital environments and social interactions. By curating personalized content, RSs do not merely reflect user preferences but actively construct experiences across social media, entertainment platforms, and e-commerce. Their influence raises concerns over privacy, autonomy, and mental well-being, while existing approaches such as ""algorethics"" - the effort to embed ethical principles into algorithmic design - remain insufficient. RSs inherently reduce human complexity to quantifiable profiles, exploit user vulnerabilities, and prioritize engagement over well-being. The paper advances a three-dimensional framework for human-centered RSs, integrating policies and regulation, interdisciplinary research, and education. These strategies are mutually reinforcing: research provides evidence for policy, policy enables safeguards and standards, and education equips users to engage critically. By connecting ethical reflection with governance and digital literacy, the paper argues that RSs can be reoriented to enhance autonomy and dignity rather than undermine them.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Reconfiguring Diversity and Inclusion for AI Ethics,"Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce.   We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the managerialization of diversity by corporations in the mid-1980s. The focus on technical artifacts, such as diverse and inclusive datasets, and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and other subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as ethics owners but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and Challenges,"Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it in the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Role of Legal Frameworks in Shaping Ethical Artificial Intelligence Use in Corporate Governance,"This article examines the evolving role of legal frameworks in shaping ethical artificial intelligence (AI) use in corporate governance. As AI systems become increasingly prevalent in business operations and decision-making, there is a growing need for robust governance structures to ensure their responsible development and deployment. Through analysis of recent legislative initiatives, industry standards, and scholarly perspectives, this paper explores key legal and regulatory approaches aimed at promoting transparency, accountability, and fairness in corporate AI applications. It evaluates the strengths and limitations of current frameworks, identifies emerging best practices, and offers recommendations for developing more comprehensive and effective AI governance regimes. The findings highlight the importance of adaptable, principle-based regulations coupled with sector-specific guidance to address the unique challenges posed by AI technologies in the corporate sphere.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Toward an Ethics of AI Belief,"In this paper we, an epistemologist and a machine learning scientist, argue that we need to pursue a novel area of philosophical research in AI - the ethics of belief for AI. Here we take the ethics of belief to refer to a field at the intersection of epistemology and ethics concerned with possible moral, practical, and other non-truth-related dimensions of belief. In this paper we will primarily be concerned with the normative question within the ethics of belief regarding what agents - both human and artificial - ought to believe, rather than with questions concerning whether beliefs meet certain evaluative standards such as being true, being justified, constituting knowledge, etc. We suggest four topics in extant work in the ethics of (human) belief that can be applied to an ethics of AI belief: doxastic wronging by AI (morally wronging someone in virtue of beliefs held about them); morally owed beliefs (beliefs that agents are morally obligated to hold); pragmatic and moral encroachment (cases where the practical or moral features of a belief is relevant to its epistemic status, and in our case specifically to whether an agent ought to hold the belief); and moral responsibility for AI beliefs. We also indicate two relatively nascent areas of philosophical research that haven't yet been generally recognized as ethics of AI belief research, but that do fall within this field of research in virtue of investigating various moral and practical dimensions of belief: the epistemic and ethical decolonization of AI; and epistemic injustice in AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States,"Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI alignment -- how well AI systems meet public expectations regarding safety, fairness, and social values. However, little is known about what people expect from AI-enabled systems and how these expectations differ across national contexts. We present evidence from two surveys of public preferences for key functional features of AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We examine support for four types of alignment in AI moderation: accuracy and reliability, safety, bias mitigation, and the promotion of aspirational imaginaries. U.S. respondents report significantly higher AI use and consistently greater support for all alignment features, reflecting broader technological openness and higher societal involvement with AI. In both countries, accuracy and safety enjoy the strongest support, while more normatively charged goals -- like fairness and aspirational imaginaries -- receive more cautious backing, particularly in Germany. We also explore how individual experience with AI, attitudes toward free speech, political ideology, partisan affiliation, and gender shape these preferences. AI use and free speech support explain more variation in Germany. In contrast, U.S. responses show greater attitudinal uniformity, suggesting that higher exposure to AI may consolidate public expectations. These findings contribute to debates on AI governance and cross-national variation in public preferences. More broadly, our study demonstrates the value of empirically grounding AI alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of AI-generated content.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Myth of Complete AI-Fairness,"The idea of fairness and justice has long and deep roots in Western civilization, and is strongly linked to ethics. It is therefore not strange that it is core to the current discussion about the ethics of development and use of AI systems. In this short paper, I wish to further motivate my position in this matter: ``I will never be completely fair. Nothing ever is. The point is not complete fairness, but the need to establish metrics and thresholds for fairness that ensure trust in AI systems"".","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations","Artificial Intelligence (AI) has significantly revolutionized radiology, promising improved patient outcomes and streamlined processes. However, it's critical to ensure the fairness of AI models to prevent stealthy bias and disparities from leading to unequal outcomes. This review discusses the concept of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and its real-world implications in radiology, particularly in disease screening scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models' decisions, identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands. This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness. It excels in its versatility to handle various variables simultaneously, especially in a field as diverse as radiology. The review explicates essential fairness metrics: Equal and Proportional Parity, False Positive Rate Parity, False Discovery Rate Parity, False Negative Rate Parity, and False Omission Rate Parity. Each metric serves unique purposes and offers different insights. We present hypothetical scenarios to demonstrate their relevance in disease screening settings, and how disparities can lead to significant real-world impacts.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Towards clinical AI fairness: A translational perspective,"Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the issue of fairness remains a concern in high-stakes fields such as healthcare. Despite extensive discussion and efforts in algorithm development, AI fairness and clinical concerns have not been adequately addressed. In this paper, we discuss the misalignment between technical and clinical perspectives of AI fairness, highlight the barriers to AI fairness' translation to healthcare, advocate multidisciplinary collaboration to bridge the knowledge gap, and provide possible solutions to address the clinical concerns pertaining to AI fairness.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance,"The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"The Stories We Govern By: AI, Risk, and the Power of Imaginaries","This paper examines how competing sociotechnical imaginaries of artificial intelligence (AI) risk shape governance decisions and regulatory constraints. Drawing on concepts from science and technology studies, we analyse three dominant narrative groups: existential risk proponents, who emphasise catastrophic AGI scenarios; accelerationists, who portray AI as a transformative force to be unleashed; and critical AI scholars, who foreground present-day harms rooted in systemic inequality. Through an analysis of representative manifesto-style texts, we explore how these imaginaries differ across four dimensions: normative visions of the future, diagnoses of the present social order, views on science and technology, and perceived human agency in managing AI risks. Our findings reveal how these narratives embed distinct assumptions about risk and have the potential to progress into policy-making processes by narrowing the space for alternative governance approaches. We argue against speculative dogmatism and for moving beyond deterministic imaginaries toward regulatory strategies that are grounded in pragmatism.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Montreal AI Ethics Institute's Response to Scotland's AI Strategy,"In January and February 2020, the Scottish Government released two documents for review by the public regarding their artificial intelligence (AI) strategy. The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published a response on 4 June 2020. MAIEI's response examines several questions that touch on the proposed definition of AI; the people-centered nature of the strategy; considerations to ensure that everyone benefits from AI; the strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic themes; and how to grow public confidence in AI by building responsible and ethical systems.   In addition to examining the points above, MAIEI suggests that the strategy be extended to include considerations on biometric data and how that will be processed and used in the context of AI. It also highlights the importance of tackling head-on the inherently stochastic nature of deep learning systems and developing concrete guidelines to ensure that these systems are built responsibly and ethically, particularly as machine learning becomes more accessible. Finally, it concludes that any national AI strategy must clearly address the measurements of success in regards to the strategy's stated goals and vision to ensure that they are interpreted and applied consistently. To do this, there must be inclusion and transparency between those building the systems and those using them in their work.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Cost-Benefit of Interdisciplinarity in AI for Mental Health,"Artificial intelligence has been introduced as a way to improve access to mental health support. However, most AI mental health chatbots rely on a limited range of disciplinary input, and fail to integrate expertise across the chatbot's lifecycle. This paper examines the cost-benefit trade-off of interdisciplinary collaboration in AI mental health chatbots. We argue that involving experts from technology, healthcare, ethics, and law across key lifecycle phases is essential to ensure value-alignment and compliance with the high-risk requirements of the AI Act. We also highlight practical recommendations and existing frameworks to help balance the challenges and benefits of interdisciplinarity in mental health chatbots.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Responsible Artificial Intelligence -- from Principles to Practice,"The impact of Artificial Intelligence does not depend only on fundamental research and technological developments, but for a large part on how these systems are introduced into society and used in everyday situations. AI is changing the way we work, live and solve challenges but concerns about fairness, transparency or privacy are also growing. Ensuring responsible, ethical AI is more than designing systems whose result can be trusted. It is about the way we design them, why we design them, and who is involved in designing them. In order to develop and use AI responsibly, we need to work towards technical, societal, institutional and legal methods and tools which provide concrete support to AI practitioners, as well as awareness and training to enable participation of all, to ensure the alignment of AI systems with our societies' principles and values.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
What are People Talking about in #BlackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerging in Online Social Movements through the Latent Dirichlet Allocation Model,"Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
AI Toolkit: Libraries and Essays for Exploring the Technology and Ethics of AI,"In this paper we describe the development and evaluation of AITK, the Artificial Intelligence Toolkit. This open-source project contains both Python libraries and computational essays (Jupyter notebooks) that together are designed to allow a diverse audience with little or no background in AI to interact with a variety of AI tools, exploring in more depth how they function, visualizing their outcomes, and gaining a better understanding of their ethical implications. These notebooks have been piloted at multiple institutions in a variety of humanities courses centered on the theme of responsible AI. In addition, we conducted usability testing of AITK. Our pilot studies and usability testing results indicate that AITK is easy to navigate and effective at helping users gain a better understanding of AI. Our goal, in this time of rapid innovations in AI, is for AITK to provide an accessible resource for faculty from any discipline looking to incorporate AI topics into their courses and for anyone eager to learn more about AI on their own.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Gender Bias of LLM in Economics: An Existentialism Perspective,"Large Language Models (LLMs), such as GPT-4 and BERT, have rapidly gained traction in natural language processing (NLP) and are now integral to financial decision-making. However, their deployment introduces critical challenges, particularly in perpetuating gender biases that can distort decision-making outcomes in high-stakes economic environments. This paper investigates gender bias in LLMs through both mathematical proofs and empirical experiments using the Word Embedding Association Test (WEAT), demonstrating that LLMs inherently reinforce gender stereotypes even without explicit gender markers. By comparing the decision-making processes of humans and LLMs, we reveal fundamental differences: while humans can override biases through ethical reasoning and individualized understanding, LLMs maintain bias as a rational outcome of their mathematical optimization on biased data. Our analysis proves that bias in LLMs is not an unintended flaw but a systematic result of their rational processing, which tends to preserve and amplify existing societal biases encoded in training data. Drawing on existentialist theory, we argue that LLM-generated bias reflects entrenched societal structures and highlights the limitations of purely technical debiasing methods. This research underscores the need for new theoretical frameworks and interdisciplinary methodologies that address the ethical implications of integrating LLMs into economic and financial decision-making. We advocate for a reconceptualization of how LLMs influence economic decisions, emphasizing the importance of incorporating human-like ethical considerations into AI governance to ensure fairness and equity in AI-driven financial systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Artificial Intelligent Ethics in the Digital Era: an Engineering Ethical Framework Proposal,"Nowadays technology is being adopted on every aspect of our lives and it is one of most important transformation driver in industry. Moreover, many of the systems and digital services that we use daily rely on artificial intelligent technology capable of modeling social or individual behaviors that in turns also modify personal decisions and actions. In this paper, we briefly discuss, from a technological perspective, a number of critical issues including the purpose of promoting trust and ensure social benefit by the proper use of Artificial Intelligent Systems. To achieve this goal we propose a generic ethical technological framework as a first attempt to define a common context towards developing real engineering ethical by design. We hope that this initial proposal to be useful for early adopters and especially for standardization teams.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI,"The increasing integration of Artificial Intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups, digital citizens (ages 16-19), parents/educators, and AI professionals, and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from 482 participants through structured surveys, qualitative interviews, and focus groups. The findings reveal distinct privacy expectations: Young users emphasize autonomy and digital freedom, while parents and educators advocate for regulatory oversight and AI literacy programs. AI professionals, in contrast, prioritize the balance between ethical system design and technological efficiency. The data further highlights gaps in AI literacy and transparency, emphasizing the need for comprehensive, stakeholder-driven privacy frameworks that accommodate diverse user needs. Using comparative thematic analysis, this study identifies key tensions in privacy governance and develops the novel Privacy-Ethics Alignment in AI (PEA-AI) model, which structures privacy decision-making as a dynamic negotiation between stakeholders. By systematically analyzing themes such as transparency, user control, risk perception, and parental mediation, this research provides a scalable, adaptive foundation for AI governance, ensuring that privacy protections evolve alongside emerging AI technologies and youth-centric digital interactions.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Framework for Ethical AI at the United Nations,"This paper aims to provide an overview of the ethical concerns in artificial intelligence (AI) and the framework that is needed to mitigate those risks, and to suggest a practical path to ensure the development and use of AI at the United Nations (UN) aligns with our ethical values. The overview discusses how AI is an increasingly powerful tool with potential for good, albeit one with a high risk of negative side-effects that go against fundamental human rights and UN values. It explains the need for ethical principles for AI aligned with principles for data governance, as data and AI are tightly interwoven. It explores different ethical frameworks that exist and tools such as assessment lists. It recommends that the UN develop a framework consisting of ethical principles, architectural standards, assessment methods, tools and methodologies, and a policy to govern the implementation and adherence to this framework, accompanied by an education program for staff.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The human biological advantage over AI,"Recent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor ""digital species"", with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Solidarity should be a core ethical principle of Artificial Intelligence,"Solidarity is one of the fundamental values at the heart of the construction of peaceful societies and present in more than one third of world's constitutions. Still, solidarity is almost never included as a principle in ethical guidelines for the development of AI. Solidarity as an AI principle (1) shares the prosperity created by AI, implementing mechanisms to redistribute the augmentation of productivity for all; and shares the burdens, making sure that AI does not increase inequality and no human is left behind. Solidarity as an AI principle (2) assesses the long term implications before developing and deploying AI systems so no groups of humans become irrelevant because of AI systems. Considering solidarity as a core principle for AI development will provide not just an human-centric but a more humanity-centric approach to AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Ethics of AI Value Chains,"Researchers, practitioners, and policymakers with an interest in AI ethics need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents AI value chains as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from the strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend three future directions that researchers, practitioners, and policymakers can take to advance more ethical practices across AI value chains. We urge AI ethics researchers and practitioners to move toward value chain perspectives that situate actors in context, account for the many types of resources involved in co-creating AI systems, and integrate a wider range of ethical concerns across contexts and scales.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Ethics of AI-Generated Maps: A Study of DALLE 2 and Implications for Cartography,"The rapid advancement of artificial intelligence (AI) such as the emergence of large language models including ChatGPT and DALLE 2 has brought both opportunities for improving productivity and raised ethical concerns. This paper investigates the ethics of using artificial intelligence (AI) in cartography, with a particular focus on the generation of maps using DALLE 2. To accomplish this, we first create an open-sourced dataset that includes synthetic (AI-generated) and real-world (human-designed) maps at multiple scales with a variety settings. We subsequently examine four potential ethical concerns that may arise from the characteristics of DALLE 2 generated maps, namely inaccuracies, misleading information, unanticipated features, and reproducibility. We then develop a deep learning-based ethical examination system that identifies those AI-generated maps. Our research emphasizes the importance of ethical considerations in the development and use of AI techniques in cartography, contributing to the growing body of work on trustworthy maps. We aim to raise public awareness of the potential risks associated with AI-generated maps and support the development of ethical guidelines for their future use.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"A Systematic Literature Review of Human-Centered, Ethical, and Responsible AI","As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research at the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by thematically reviewing and analyzing 164 research papers from leading conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness, and explainability. These conferences, however, concentrate on specific themes rather than encompassing all aspects. While AIES has fewer papers on HCER-AI, it emphasizes governance and rarely publishes papers about privacy, security, and human flourishing. FAccT publishes more on governance and lacks papers on privacy, security, and human flourishing. CHI and CSCW, as more established conferences, have a broader research portfolio. We find that the current emphasis on governance and fairness in AI research may not adequately address the potential unforeseen and unknown implications of AI. Therefore, we recommend that future research should expand its scope and diversify resources to prepare for these potential consequences. This could involve exploring additional areas such as privacy, security, human flourishing, and explainability.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The Managerial Effects of Algorithmic Fairness Activism,"How do ethical arguments affect AI adoption in business? We randomly expose business decision-makers to arguments used in AI fairness activism. Arguments emphasizing the inescapability of algorithmic bias lead managers to abandon AI for manual review by humans and report greater expectations about lawsuits and negative PR. These effects persist even when AI lowers gender and racial disparities and when engineering investments to address AI fairness are feasible. Emphasis on status quo comparisons yields opposite effects. We also measure the effects of ""scientific veneer"" in AI ethics arguments. Scientific veneer changes managerial behavior but does not asymmetrically benefit favorable (versus critical) AI activism.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
The State of AI Ethics Report (January 2021),"The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics captures the most relevant developments in AI Ethics since October 2020. It aims to help anyone, from machine learning experts to human rights activists and policymakers, quickly digest and understand the field's ever-changing developments. Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, including: algorithmic injustice, discrimination, ethical AI, labor impacts, misinformation, privacy, risk and security, social media, and more.   In addition, The State of AI Ethics includes exclusive content written by world-class AI Ethics experts from universities, research institutes, consulting firms, and governments. Unique to this report is ""The Abuse and Misogynoir Playbook,"" written by Dr. Katlyn Tuner (Research Scientist, Space Enabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program in Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics; Lead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant Professor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The piece (and accompanying infographic), is a deep-dive into the historical and systematic silencing, erasure, and revision of Black women's contributions to knowledge and scholarship in the United Stations, and globally. Exposing and countering this Playbook has become increasingly important following the firing of AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.   This report should be used not only as a point of reference and insight on the latest thinking in the field of AI Ethics, but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of AI on the world.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of Appearance Bias in the Wild,"Research in social psychology has shown that people's biased, subjective judgments about another's personality based solely on their appearance are not predictive of their actual personality traits. But researchers and companies often utilize computer vision models to predict similarly subjective personality attributes such as ""employability."" We seek to determine whether state-of-the-art, black box face processing technology can learn human-like appearance biases. With features extracted with FaceNet, a widely used face recognition framework, we train a transfer learning model on human subjects' first impressions of personality traits in other faces as measured by social psychologists. We find that features extracted with FaceNet can be used to predict human appearance bias scores for deliberately manipulated faces but not for randomly generated faces scored by humans. Additionally, in contrast to work with human biases in social psychology, the model does not find a significant signal correlating politicians' vote shares with perceived competence bias. With Local Interpretable Model-Agnostic Explanations (LIME), we provide several explanations for this discrepancy. Our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate. We shed light on the ways in which appearance bias could be embedded in face processing technology and cast further doubt on the practice of predicting subjective traits based on appearances.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
How to design an AI ethics board,"Organizations that develop and deploy artificial intelligence (AI) systems need to take measures to reduce the associated risks. In this paper, we examine how AI companies could design an AI ethics board in a way that reduces risks from AI. We identify five high-level design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions and should its decisions be binding? (5) What resources does it need? We break down each of these questions into more specific sub-questions, list options, and discuss how different design choices affect the board's ability to reduce risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Can We Trust AI Agents? A Case Study of an LLM-Based Multi-Agent System for Ethical AI,"AI-based systems, including Large Language Models (LLM), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. AI ethics is crucial as new technologies and concerns emerge, but objective, practical guidance remains debated. This study examines the use of LLMs for AI ethics in practice, assessing how LLM trustworthiness-enhancing techniques affect software development in this context. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design a multi-agent prototype LLM-MAS, where agents engage in structured discussions on real-world AI ethics issues from the AI Incident Database. We evaluate the prototype across three case scenarios using thematic analysis, hierarchical clustering, comparative (baseline) studies, and running source code. The system generates approximately 2,000 lines of code per case, compared to only 80 lines in baseline trials. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing this prototype ability to generate extensive source code and documentation addressing often overlooked AI ethics issues. However, practical challenges in source code integration and dependency management may limit its use by practitioners.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Introduction to AI Safety, Ethics, and Society","Artificial Intelligence is rapidly embedding itself within militaries, economies, and societies, reshaping their very foundations. Given the depth and breadth of its consequences, it has never been more pressing to understand how to ensure that AI systems are safe, ethical, and have a positive societal impact. This book aims to provide a comprehensive approach to understanding AI risk. Our primary goals include consolidating fragmented knowledge on AI risk, increasing the precision of core ideas, and reducing barriers to entry by making content simpler and more comprehensible. The book has been designed to be accessible to readers from diverse backgrounds. You do not need to have studied AI, philosophy, or other such topics. The content is skimmable and somewhat modular, so that you can choose which chapters to read. We introduce mathematical formulas in a few places to specify claims more precisely, but readers should be able to understand the main points without these.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Critical Examination of the Ethics of AI-Mediated Peer Review,"Recent advancements in artificial intelligence (AI) systems, including large language models like ChatGPT, offer promise and peril for scholarly peer review. On the one hand, AI can enhance efficiency by addressing issues like long publication delays. On the other hand, it brings ethical and social concerns that could compromise the integrity of the peer review process and outcomes. However, human peer review systems are also fraught with related problems, such as biases, abuses, and a lack of transparency, which already diminish credibility. While there is increasing attention to the use of AI in peer review, discussions revolve mainly around plagiarism and authorship in academic journal publishing, ignoring the broader epistemic, social, cultural, and societal epistemic in which peer review is positioned. The legitimacy of AI-driven peer review hinges on the alignment with the scientific ethos, encompassing moral and epistemic norms that define appropriate conduct in the scholarly community. In this regard, there is a ""norm-counternorm continuum,"" where the acceptability of AI in peer review is shaped by institutional logics, ethical practices, and internal regulatory mechanisms. The discussion here emphasizes the need to critically assess the legitimacy of AI-driven peer review, addressing the benefits and downsides relative to the broader epistemic, social, ethical, and regulatory factors that sculpt its implementation and impact.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act,"The EU's Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models' compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act's obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Reporting on Decision-Making Algorithms and some Related Ethical Questions,"Companies report on their financial performance for decades. More recently they have also started to report on their environmental impact and their social responsibility. The latest trend is now to deliver one single integrated report where all stakeholders of the company can easily connect all facets of the business with their impact considered in a broad sense. The main purpose of this integrated approach is to avoid delivering data related to disconnected silos, which consequently makes it very difficult to globally assess the overall performance of an entity or a business line. In this paper, we focus on how companies report on risks and ethical issues related to the increasing use of Artificial Intelligence (AI). We explain some of these risks and potential issues. Next, we identify some recent initiatives by various stakeholders to define a global ethical framework for AI. Finally, we illustrate with four cases that companies are very shy to report on these facets of AI.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Socially Responsible AI Algorithms: Issues, Purposes, and Challenges","In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI's indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Are clinicians ethically obligated to disclose their use of medical machine learning systems to patients?,"It is commonly accepted that clinicians are ethically obligated to disclose their use of medical machine learning systems to patients, and that failure to do so would amount to a moral fault for which clinicians ought to be held accountable. Call this ""the disclosure thesis."" Four main arguments have been, or could be, given to support the disclosure thesis in the ethics literature: the risk-based argument, the rights-based argument, the materiality argument, and the autonomy argument. In this article, I argue that each of these four arguments are unconvincing, and therefore, that the disclosure thesis ought to be rejected. I suggest that mandating disclosure may also even risk harming patients by providing stakeholders with a way to avoid accountability for harm that results from improper applications or uses of these systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Robustness and Cybersecurity in the EU Artificial Intelligence Act,"The EU Artificial Intelligence Act (AIA) establishes different legal principles for different types of AI systems. While prior work has sought to clarify some of these principles, little attention has been paid to robustness and cybersecurity. This paper aims to fill this gap. We identify legal challenges and shortcomings in provisions related to robustness and cybersecurity for high-risk AI systems(Art. 15 AIA) and general-purpose AI models (Art. 55 AIA). We show that robustness and cybersecurity demand resilience against performance disruptions. Furthermore, we assess potential challenges in implementing these provisions in light of recent advancements in the machine learning (ML) literature. Our analysis informs efforts to develop harmonized standards, guidelines by the European Commission, as well as benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we seek to bridge the gap between legal terminology and ML research, fostering a better alignment between research and implementation efforts.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA,"There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOLA method for creating ethically aligned AI -systems. ECCOLA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method, no matter how good, is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given lifecycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Measuring Political Preferences in AI Systems: An Integrative Approach,"Political biases in Large Language Model (LLM)-based artificial intelligence (AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously reported. While several prior studies have attempted to quantify these biases using political orientation tests, such approaches are limited by potential tests' calibration biases and constrained response formats that do not reflect real-world human-AI interactions. This study employs a multi-method approach to assess political bias in leading AI systems, integrating four complementary methodologies: (1) linguistic comparison of AI-generated text with the language used by Republican and Democratic U.S. Congress members, (2) analysis of political viewpoints embedded in AI-generated policy recommendations, (3) sentiment analysis of AI-generated text toward politically affiliated public figures, and (4) standardized political orientation testing. Results indicate a consistent left-leaning bias across most contemporary AI systems, with arguably varying degrees of intensity. However, this bias is not an inherent feature of LLMs; prior research demonstrates that fine-tuning with politically skewed data can realign these models across the ideological spectrum. The presence of systematic political bias in AI systems poses risks, including reduced viewpoint diversity, increased societal polarization, and the potential for public mistrust in AI technologies. To mitigate these risks, AI systems should be designed to prioritize factual accuracy while maintaining neutrality on most lawful normative issues. Furthermore, independent monitoring platforms are necessary to ensure transparency, accountability, and responsible AI development.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Ethics by Design: A Lifecycle Framework for Trustworthy AI in Medical Imaging From Transparent Data Governance to Clinically Validated Deployment,"The integration of artificial intelligence (AI) in medical imaging raises crucial ethical concerns at every stage of its development, from data collection to deployment. Addressing these concerns is essential for ensuring that AI systems are developed and implemented in a manner that respects patient rights and promotes fairness. This study aims to explore the ethical implications of AI in medical imaging, focusing on five key stages: data collection, data processing, model training, model evaluation, and deployment. The goal is to evaluate how these stages adhere to fundamental ethical principles, including data privacy, fairness, transparency, accountability, and autonomy. An analytical approach was employed to examine the ethical challenges associated with each stage of AI development. We reviewed existing literature, guidelines, and regulations concerning AI ethics in healthcare and identified critical ethical issues at each stage. The study outlines specific inquiries and principles for each phase of AI development. The findings highlight key ethical issues: ensuring patient consent and anonymization during data collection, addressing biases in model training, ensuring transparency and fairness during model evaluation, and the importance of continuous ethical assessments during deployment. The analysis also emphasizes the impact of accessibility issues on different stakeholders, including private, public, and third-party entities. The study concludes that ethical considerations must be systematically integrated into each stage of AI development in medical imaging. By adhering to these ethical principles, AI systems can be made more robust, transparent, and aligned with patient care and data control. We propose tailored ethical inquiries and strategies to support the creation of ethically sound AI systems in medical imaging.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Relational Artificial Intelligence,"The impact of Artificial Intelligence does not depend only on fundamental research and technological developments, but for a large part on how these systems are introduced into society and used in everyday situations. Even though AI is traditionally associated with rational decision making, understanding and shaping the societal impact of AI in all its facets requires a relational perspective. A rational approach to AI, where computational algorithms drive decision making independent of human intervention, insights and emotions, has shown to result in bias and exclusion, laying bare societal vulnerabilities and insecurities. A relational approach, that focus on the relational nature of things, is needed to deal with the ethical, legal, societal, cultural, and environmental implications of AI. A relational approach to AI recognises that objective and rational reasoning cannot does not always result in the 'right' way to proceed because what is 'right' depends on the dynamics of the situation in which the decision is taken, and that rather than solving ethical problems the focus of design and use of AI must be on asking the ethical question. In this position paper, I start with a general discussion of current conceptualisations of AI followed by an overview of existing approaches to governance and responsible development and use of AI. Then, I reflect over what should be the bases of a social paradigm for AI and how this should be embedded in relational, feminist and non-Western philosophies, in particular the Ubuntu philosophy.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Towards a Policy-as-a-Service Framework to Enable Compliant, Trustworthy AI and HRI Systems in the Wild","Building trustworthy autonomous systems is challenging for many reasons beyond simply trying to engineer agents that 'always do the right thing.' There is a broader context that is often not considered within AI and HRI: that the problem of trustworthiness is inherently socio-technical and ultimately involves a broad set of complex human factors and multidimensional relationships that can arise between agents, humans, organizations, and even governments and legal institutions, each with their own understanding and definitions of trust. This complexity presents a significant barrier to the development of trustworthy AI and HRI systems---while systems developers may desire to have their systems 'always do the right thing,' they generally lack the practical tools and expertise in law, regulation, policy and ethics to ensure this outcome. In this paper, we emphasize the ""fuzzy"" socio-technical aspects of trustworthiness and the need for their careful consideration during both design and deployment. We hope to contribute to the discussion of trustworthy engineering in AI and HRI by i) describing the policy landscape that must be considered when addressing trustworthy computing and the need for usable trust models, ii) highlighting an opportunity for trustworthy-by-design intervention within the systems engineering process, and iii) introducing the concept of a ""policy-as-a-service"" (PaaS) framework that can be readily applied by AI systems engineers to address the fuzzy problem of trust during the development and (eventually) runtime process. We envision that the PaaS approach, which offloads the development of policy design parameters and maintenance of policy standards to policy experts, will enable runtime trust capabilities intelligent systems in the wild.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence","The societal and ethical implications of the use of opaque artificial intelligence systems for consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholder groups, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by (1) identifying the types of explanations that are most pertinent to artificial intelligence predictions, (2) recognizing the relevance and importance of social and ethical values for the evaluation of these explanations, and (3) demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Demystifying and Navigating AI Ethics in Power Electronics,"Artificial intelligence (AI) is rapidly transforming power electronics, with AI-related publications in IEEE Power Electronics Society selected journals increasing more than fourfold from 2020 to 2025. However, the ethical dimensions of this transformation have received limited attention. This article underscores the urgent need for an ethical framework to guide responsible AI integration in power electronics, not only to prevent AI-related incidents but also to comply with legal and regulatory responsibilities. In this context, this article identifies four core pillars of AI ethics in power electronics: Security & Safety, Explainability & Transparency, Energy Sustainability, and Evolving Roles of Engineers. Each pillar is supported by practical and actionable insights to ensure that ethical principles are embedded in algorithm design, system deployment, and workforce development. The authors advocate for power electronics engineers to lead the ethical discourse, given their deep technical understanding of both AI systems and power conversion technologies. The paper concludes by calling on the IEEE Power Electronics Society to spearhead the establishment of ethical standards and best practices that ensure AI innovations are not only technically advanced but also trustworthy, safe, and sustainable.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
An Artificial Intelligence Value at Risk Approach: Metrics and Models,"Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.   The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
A Seven-Layer Model for Standardising AI Fairness Assessment,"Problem statement: Standardisation of AI fairness rules and benchmarks is challenging because AI fairness and other ethical requirements depend on multiple factors such as context, use case, type of the AI system, and so on. In this paper, we elaborate that the AI system is prone to biases at every stage of its lifecycle, from inception to its usage, and that all stages require due attention for mitigating AI bias. We need a standardised approach to handle AI fairness at every stage. Gap analysis: While AI fairness is a hot research topic, a holistic strategy for AI fairness is generally missing. Most researchers focus only on a few facets of AI model-building. Peer review shows excessive focus on biases in the datasets, fairness metrics, and algorithmic bias. In the process, other aspects affecting AI fairness get ignored. The solution proposed: We propose a comprehensive approach in the form of a novel seven-layer model, inspired by the Open System Interconnection (OSI) model, to standardise AI fairness handling. Despite the differences in the various aspects, most AI systems have similar model-building stages. The proposed model splits the AI system lifecycle into seven abstraction layers, each corresponding to a well-defined AI model-building or usage stage. We also provide checklists for each layer and deliberate on potential sources of bias in each layer and their mitigation methodologies. This work will facilitate layer-wise standardisation of AI fairness rules and benchmarking parameters.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Gender Bias in Multimodal Models: A Transnational Feminist Approach Considering Geographical Region and Culture,"Deep learning based visual-linguistic multimodal models such as Contrastive Language Image Pre-training (CLIP) have become increasingly popular recently and are used within text-to-image generative models such as DALL-E and Stable Diffusion. However, gender and other social biases have been uncovered in these models, and this has the potential to be amplified and perpetuated through AI systems. In this paper, we present a methodology for auditing multimodal models that consider gender, informed by concepts from transnational feminism, including regional and cultural dimensions. Focusing on CLIP, we found evidence of significant gender bias with varying patterns across global regions. Harmful stereotypical associations were also uncovered related to visual cultural cues and labels such as terrorism. Levels of gender bias uncovered within CLIP for different regions aligned with global indices of societal gender equality, with those from the Global South reflecting the highest levels of gender bias.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Response by the Montreal AI Ethics Institute to the European Commission's Whitepaper on AI,"In February 2020, the European Commission (EC) published a white paper entitled, On Artificial Intelligence - A European approach to excellence and trust. This paper outlines the EC's policy options for the promotion and adoption of artificial intelligence (AI) in the European Union. The Montreal AI Ethics Institute (MAIEI) reviewed this paper and published a response addressing the EC's plans to build an ""ecosystem of excellence"" and an ""ecosystem of trust,"" as well as the safety and liability implications of AI, the internet of things (IoT), and robotics.   MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence,"As artificial intelligence (AI) technologies spread worldwide, international discussions have increasingly focused on their consequences for democracy, human rights, fundamental freedoms, security, and economic and social development. In this context, UNESCO's Recommendation on the Ethics of Artificial Intelligence, adopted in November 2021, has emerged as the first global normative framework for AI development and deployment. The intense negotiations of every detail of the document brought forth numerous controversies among UNESCO member states. Drawing on a unique set of primary sources, including written positions and recorded deliberations, this paper explains the achievement of global compromise on AI regulation despite the multiplicity of UNESCO member-state positions representing a variety of liberal and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it conceptualises the practice of multilateral negotiations and attributes the multilateral compromise to two embedded therein mechanisms: Structural normative hybridity and situated normative ambiguity allowed to accomplish a compromise by linking macro-normative structures with situated debates of multilateral negotiations.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Human participants in AI research: Ethics and transparency in practice,"In recent years, research involving human participants has been critical to advances in artificial intelligence (AI) and machine learning (ML), particularly in the areas of conversational, human-compatible, and cooperative AI. For example, roughly 9% of publications at recent AAAI and NeurIPS conferences indicate the collection of original human data. Yet AI and ML researchers lack guidelines for ethical research practices with human participants. Fewer than one out of every four of these AAAI and NeurIPS papers confirm independent ethical review, the collection of informed consent, or participant compensation. This paper aims to bridge this gap by examining the normative similarities and differences between AI research and related fields that involve human participants. Though psychology, human-computer interaction, and other adjacent fields offer historic lessons and helpful insights, AI research presents several distinct considerations$\unicode{x2014}$namely, participatory design, crowdsourced dataset development, and an expansive role of corporations$\unicode{x2014}$that necessitate a contextual ethics framework. To address these concerns, this manuscript outlines a set of guidelines for ethical and transparent practice with human participants in AI and ML research. Overall, this paper seeks to equip technical researchers with practical knowledge for their work, and to position them for further dialogue with social scientists, behavioral researchers, and ethicists.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
German AI Start-Ups and AI Ethics: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation,"Within the current AI ethics discourse, there is a gap in empirical research on understanding how AI practitioners understand ethics and socially organize to operationalize ethical concerns, particularly in the context of AI start-ups. This gap intensifies the risk of a disconnect between scholarly research, innovation, and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to assess and implement socio-technical innovation for fairness, accountability, and transparency. Building on social practice theory, we address this need via a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of ethical AI to define appropriate strategies for effectively implementing socio-technical innovations. Our contributions are threefold: 1) we introduce a practice-based approach for understanding ethical AI; 2) we present empirical findings from our study on the operationalization of ethics in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and 3) based on our empirical findings, we suggest that ethical AI practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Implementing AI Ethics in Practice: An Empirical Evaluation of the RESOLVEDD Strategy,"As Artificial Intelligence (AI) systems exert a growing influence on society, real-life incidents begin to underline the importance of AI Ethics. Though calls for more ethical AI systems have been voiced by scholars and the general public alike, few empirical studies on the topic exist. Similarly, few tools and methods designed for implementing AI ethics into practice currently exist. To provide empirical data into this on-going discussion, we empirically evaluate an existing method from the field of business ethics, the RESOLVEDD strategy, in the context of ethical system development. We evaluated RESOLVEDD by means of a multiple case study of five student projects where its use was given as one of the design requirements for the projects. One of our key findings is that, even though the use of the ethical method was forced upon the participants, its utilization nonetheless facilitated of ethical consideration in the projects. Specifically, it resulted in the developers displaying more responsibility, even though the use of the tool did not stem from intrinsic motivation.","cat:cs.CY AND (AI OR ""artificial intelligence"") AND (ethics OR bias OR fairness)",0
Keeping it Authentic: The Social Footprint of the Trolls Network,"In 2016, a network of social media accounts animated by Russian operatives attempted to divert political discourse within the American public around the presidential elections. This was a coordinated effort, part of a Russian-led complex information operation. Utilizing the anonymity and outreach of social media platforms Russian operatives created an online astroturf that is in direct contact with regular Americans, promoting Russian agenda and goals. The elusiveness of this type of adversarial approach rendered security agencies helpless, stressing the unique challenges this type of intervention presents. Building on existing scholarship on the functions within influence networks on social media, we suggest a new approach to map those types of operations. We argue that pretending to be legitimate social actors obliges the network to adhere to social expectations, leaving a social footprint. To test the robustness of this social footprint we train artificial intelligence to identify it and create a predictive model. We use Twitter data identified as part of the Russian influence network for training the artificial intelligence and to test the prediction. Our model attains 88% prediction accuracy for the test set. Testing our prediction on two additional models results in 90.7% and 90.5% accuracy, validating our model. The predictive and validation results suggest that building a machine learning model around social functions within the Russian influence network can be used to map its actors and functions.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Coordinated Behavior on Social Media in 2019 UK General Election,"Coordinated online behaviors are an essential part of information and influence operations, as they allow a more effective disinformation's spread. Most studies on coordinated behaviors involved manual investigations, and the few existing computational approaches make bold assumptions or oversimplify the problem to make it tractable. Here, we propose a new network-based framework for uncovering and studying coordinated behaviors on social media. Our research extends existing systems and goes beyond limiting binary classifications of coordinated and uncoordinated behaviors. It allows to expose different coordination patterns and to estimate the degree of coordination that characterizes diverse communities. We apply our framework to a dataset collected during the 2019 UK General Election, detecting and characterizing coordinated communities that participated in the electoral debate. Our work conveys both theoretical and practical implications and provides more nuanced and fine-grained results for studying online information manipulation.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding Online Polarization Through Human-Agent Interaction in a Synthetic LLM-Based Social Network,"The rise of social media has fundamentally transformed how people engage in public discourse and form opinions. While these platforms offer unprecedented opportunities for democratic engagement, they have been implicated in increasing social polarization and the formation of ideological echo chambers. Previous research has primarily relied on observational studies of social media data or theoretical modeling approaches, leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments. Here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with LLM-based artificial agents in a controlled social network simulation. Through a user study with 122 participants, we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors. Our results provide empirical validation of theoretical predictions about online polarization, showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty. These findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors. More broadly, this research introduces a powerful new methodology for studying social media dynamics, offering researchers unprecedented control over experimental conditions while maintaining ecological validity.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Information Consumption and Boundary Spanning in Decentralized Online Social Networks: the case of Mastodon Users,"Decentralized Online Social Networks (DOSNs) represent a growing trend in the social media landscape, as opposed to the well-known centralized peers, which are often in the spotlight due to privacy concerns and a vision typically focused on monetization through user relationships. By exploiting open-source software, DOSNs allow users to create their own servers, or instances, thus favoring the proliferation of platforms that are independent yet interconnected with each other in a transparent way. Nonetheless, the resulting cooperation model, commonly known as the Fediverse, still represents a world to be fully discovered, since existing studies have mainly focused on a limited number of structural aspects of interest in DOSNs. In this work, we aim to fill a lack of study on user relations and roles in DOSNs, by taking two main actions: understanding the impact of decentralization on how users relate to each other within their membership instance and/or across different instances, and unveiling user roles that can explain two interrelated axes of social behavioral phenomena, namely information consumption and boundary spanning. To this purpose, we build our analysis on user networks from Mastodon, since it represents the most widely used DOSN platform. We believe that the findings drawn from our study on Mastodon users' roles and information flow can pave a way for further development of fascinating research on DOSNs.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Comparing Global Tourism Flows Measured by Official Census and Social Sensing,"A better understanding of the behavior of tourists is strategic for improving services in the competitive and important economic segment of global tourism. Critical studies in the literature often explore the issue using traditional data, such as questionnaires or interviews. Traditional approaches provide precious information; however, they impose challenges to obtaining large-scale data, making it hard to study worldwide patterns. Location-based social networks (LBSNs) can potentially mitigate such issues due to the relatively low cost of acquiring large amounts of behavioral data. Nevertheless, before using such data for studying tourists' behavior, it is necessary to verify whether the information adequately reveals the behavior measured with traditional data -- considered the ground truth. Thus, the present work investigates in which countries the global tourism network measured with an LBSN agreeably reflects the behavior estimated by the World Tourism Organization using traditional methods. Although we could find exceptions, the results suggest that, for most countries, LBSN data can satisfactorily represent the behavior studied. We have an indication that, in countries with high correlations between results obtained from both datasets, LBSN data can be used in research regarding the mobility of the tourists in the studied context.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Clustering Memes in Social Media,"The increasing pervasiveness of social media creates new opportunities to study human social behavior, while challenging our capability to analyze their massive data streams. One of the emerging tasks is to distinguish between different kinds of activities, for example engineered misinformation campaigns versus spontaneous communication. Such detection problems require a formal definition of meme, or unit of information that can spread from person to person through the social network. Once a meme is identified, supervised learning methods can be applied to classify different types of communication. The appropriate granularity of a meme, however, is hardly captured from existing entities such as tags and keywords. Here we present a framework for the novel task of detecting memes by clustering messages from large streams of social data. We evaluate various similarity measures that leverage content, metadata, network features, and their combinations. We also explore the idea of pre-clustering on the basis of existing entities. A systematic evaluation is carried out using a manually curated dataset as ground truth. Our analysis shows that pre-clustering and a combination of heterogeneous features yield the best trade-off between number of clusters and their quality, demonstrating that a simple combination based on pairwise maximization of similarity is as effective as a non-trivial optimization of parameters. Our approach is fully automatic, unsupervised, and scalable for real-time detection of memes in streaming data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Identifying Influential Brokers on Social Media from Social Network Structure,"Identifying influencers in a given social network has become an important research problem for various applications, including accelerating the spread of information in viral marketing and preventing the spread of fake news and rumors. The literature contains a rich body of studies on identifying influential source spreaders who can spread their own messages to many other nodes. In contrast, the identification of influential brokers who can spread other nodes' messages to many nodes has not been fully explored. Theoretical and empirical studies suggest that involvement of both influential source spreaders and brokers is a key to facilitating large-scale information diffusion cascades. Therefore, this paper explores ways to identify influential brokers from a given social network. By using three social media datasets, we investigate the characteristics of influential brokers by comparing them with influential source spreaders and central nodes obtained from centrality measures. Our results show that (i) most of the influential source spreaders are not influential brokers (and vice versa) and (ii) the overlap between central nodes and influential brokers is small (less than 15%) in Twitter datasets. We also tackle the problem of identifying influential brokers from centrality measures and node embeddings, and we examine the effectiveness of social network features in the broker identification task. Our results show that (iii) although a single centrality measure cannot characterize influential brokers well, prediction models using node embedding features achieve F$_1$ scores of 0.35--0.68, suggesting the effectiveness of social network features for identifying influential brokers.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Influencer Marketing Campaigns in Social Networks,"Social media are extensively used in today's world, and facilitate quick and easy sharing of information, which makes them a good way to advertise products. Influencers of a social media network, owing to their massive popularity, provide a huge potential customer base. However, it is not straightforward to decide which influencers should be selected for an advertizing campaign that can generate high returns with low investment. In this work, we present an agent-based model (ABM) that can simulate the dynamics of influencer advertizing campaigns in a variety of scenarios and can help to discover the best influencer marketing strategy. Our system is a probabilistic graph-based model that provides the additional advantage to incorporate real-world factors such as customers' interest in a product, customer behavior, the willingness to pay, a brand's investment cap, influencers' engagement with influence diffusion, and the nature of the product being advertized viz. luxury and non-luxury. Using customer acquisition cost and conversion ratio as a unit economic, we evaluate the performance of different kinds of influencers under a variety of circumstances that are simulated by varying the nature of the product and the customers' interest. Our results exemplify the circumstance-dependent nature of influencer marketing and provide insight into which kinds of influencers would be a better strategy under respective circumstances. For instance, we show that as the nature of the product varies from luxury to non-luxury, the performance of celebrities declines whereas the performance of nano-influencers improves. In terms of the customers' interest, we find that the performance of nano-influencers declines with the decrease in customers' interest whereas the performance of celebrities improves.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
People are Strange when you're a Stranger: Impact and Influence of Bots on Social Networks,"Bots are, for many Web and social media users, the source of many dangerous attacks or the carrier of unwanted messages, such as spam. Nevertheless, crawlers and software agents are a precious tool for analysts, and they are continuously executed to collect data or to test distributed applications. However, no one knows which is the real potential of a bot whose purpose is to control a community, to manipulate consensus, or to influence user behavior. It is commonly believed that the better an agent simulates human behavior in a social network, the more it can succeed to generate an impact in that community. We contribute to shed light on this issue through an online social experiment aimed to study to what extent a bot with no trust, no profile, and no aims to reproduce human behavior, can become popular and influential in a social media. Results show that a basic social probing activity can be used to acquire social relevance on the network and that the so-acquired popularity can be effectively leveraged to drive users in their social connectivity choices. We also register that our bot activity unveiled hidden social polarization patterns in the community and triggered an emotional response of individuals that brings to light subtle privacy hazards perceived by the user base.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploring Unknown Social Networks for Discovering Hidden Nodes,"In this paper, we address the challenge of discovering hidden nodes in unknown social networks, formulating three types of hidden-node discovery problems, namely, Sybil-node discovery, peripheral-node discovery, and influencer discovery. We tackle these problems by employing a graph exploration framework grounded in machine learning. Leveraging the structure of the subgraph gradually obtained from graph exploration, we construct prediction models to identify target hidden nodes in unknown social graphs. Through empirical investigations of real social graphs, we investigate the efficiency of graph exploration strategies in uncovering hidden nodes. Our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known. Specifically, the query cost of discovering 10% of the hidden nodes is at most only 1.2 times that when the topology is known, and the query-cost multiplier for discovering 90% of the hidden nodes is at most only 1.4. Furthermore, our results suggest that using node embeddings, which are low-dimensional vector representations of nodes, for hidden-node discovery is a double-edged sword: it is effective in certain scenarios but sometimes degrades the efficiency of node discovery. Guided by this observation, we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not, and our analysis shows that the bandit-based graph exploration strategy achieves efficient node discovery across a wide array of settings.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls","Large-scale databases of human activity in social media have captured scientific and policy attention, producing a flood of research and discussion. This paper considers methodological and conceptual challenges for this emergent field, with special attention to the validity and representativeness of social media big data analyses. Persistent issues include the over-emphasis of a single platform, Twitter, sampling biases arising from selection by hashtags, and vague and unrepresentative sampling frames. The socio-cultural complexity of user behavior aimed at algorithmic invisibility (such as subtweeting, mock-retweeting, use of ""screen captures"" for text, etc.) further complicate interpretation of big data social media. Other challenges include accounting for field effects, i.e. broadly consequential events that do not diffuse only through the network under study but affect the whole society. The application of network methods from other fields to the study of human social activity may not always be appropriate. The paper concludes with a call to action on practical steps to improve our analytic capacity in this promising, rapidly-growing field.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Challenges of Growing Social Media Networks From the Bottom-Up Through the Agent Perspective,"We develop an agent-based model in order to understand agent/node behaviors that generate social media networks. We use simple rules to synthetically generate a backcloth (friend/follow) network collected using Twitter's API. The Twitter network was collected using seeds for known terrorist propaganda accounts in 2015. Model parameter adjustments were made to reproduce the collected network's summary statistics, stylized facts and general structural measures. We produced an approximate network in line with the general properties of our collected data. We present our findings with a focus on the challenging aspects of this reproduction. We find that while it is possible to generate a social media network utilizing a few simple rules, numerous challenges arise requiring departure from the agent viewpoint and the development of more useful methods. We present numerous weaknesses and challenges in our reproduction and propose potential solutions for future efforts.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
EVOLVE: Predicting User Evolution and Network Dynamics in Social Media Using Fine-Tuned GPT-like Model,"Social media platforms are extensively used for sharing personal emotions, daily activities, and various life events, keeping people updated with the latest happenings. From the moment a user creates an account, they continually expand their network of friends or followers, freely interacting with others by posting, commenting, and sharing content. Over time, user behavior evolves based on demographic attributes and the networks they establish. In this research, we propose a predictive method to understand how a user evolves on social media throughout their life and to forecast the next stage of their evolution. We fine-tune a GPT-like decoder-only model (we named it E-GPT: Evolution-GPT) to predict the future stages of a user's evolution in online social media. We evaluate the performance of these models and demonstrate how user attributes influence changes within their network by predicting future connections and shifts in user activities on social media, which also addresses other social media challenges such as recommendation systems.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Aggression Propagation on Social Media,"Cyberaggression has been studied in various contexts and online social platforms, and modeled on different data using state-of-the-art machine and deep learning algorithms to enable automatic detection and blocking of this behavior. Users can be influenced to act aggressively or even bully others because of elevated toxicity and aggression in their own (online) social circle. In effect, this behavior can propagate from one user and neighborhood to another, and therefore, spread in the network. Interestingly, to our knowledge, no work has modeled the network dynamics of aggressive behavior. In this paper, we take a first step towards this direction by studying propagation of aggression on social media using opinion dynamics. We propose ways to model how aggression may propagate from one user to another, depending on how each user is connected to other aggressive or regular users. Through extensive simulations on Twitter data, we study how aggressive behavior could propagate in the network. We validate our models with crawled and annotated ground truth data, reaching up to 80% AUC, and discuss the results and implications of our work.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Social Media Genome: Modeling Individual Topic-Specific Behavior in Social Media,"Information propagation in social media depends not only on the static follower structure but also on the topic-specific user behavior. Hence novel models incorporating dynamic user behavior are needed. To this end, we propose a model for individual social media users, termed a genotype. The genotype is a per-topic summary of a user's interest, activity and susceptibility to adopt new information. We demonstrate that user genotypes remain invariant within a topic by adopting them for classification of new information spread in large-scale real networks. Furthermore, we extract topic-specific influence backbone structures based on information adoption and show that they differ significantly from the static follower network. When employed for influence prediction of new content spread, our genotype model and influence backbones enable more than $20% improvement, compared to purely structural features. We also demonstrate that knowledge of user genotypes and influence backbones allow for the design of effective strategies for latency minimization of topic-specific information spread.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Influence and Unfollowing Accelerate the Emergence of Echo Chambers,"While social media make it easy to connect with and access information from anyone, they also facilitate basic influence and unfriending mechanisms that may lead to segregated and polarized clusters known as ""echo chambers."" Here we study the conditions in which such echo chambers emerge by introducing a simple model of information sharing in online social networks with the two ingredients of influence and unfriending. Users can change both their opinions and social connections based on the information to which they are exposed through sharing. The model dynamics show that even with minimal amounts of influence and unfriending, the social network rapidly devolves into segregated, homogeneous communities. These predictions are consistent with empirical data from Twitter. Although our findings suggest that echo chambers are somewhat inevitable given the mechanisms at play in online social media, they also provide insights into possible mitigation strategies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Assembling a Multi-Platform Ensemble Social Bot Detector with Applications to US 2020 Elections,"Bots have been in the spotlight for many social media studies, for they have been observed to be participating in the manipulation of information and opinions on social media. These studies analyzed the activity and influence of bots in a variety of contexts: elections, protests, health communication and so forth. Prior to this analyses is the identification of bot accounts to segregate the class of social media users. In this work, we propose an ensemble method for bot detection, designing a multi-platform bot detection architecture to handle several problems along the bot detection pipeline: incomplete data input, minimal feature engineering, optimized classifiers for each data field, and also eliminate the need for a threshold value for classification determination. With these design decisions, we generalize our bot detection framework across Twitter, Reddit and Instagram. We also perform feature importance analysis, observing that the entropy of names and number of interactions (retweets/shares) are important factors in bot determination. Finally, we apply our multi-platform bot detector to the US 2020 presidential elections to identify and analyze bot activity across multiple social media platforms, showcasing the difference in online discourse of bots from different platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Causal Inference for Early Detection of Pathogenic Social Media Accounts,"Pathogenic social media accounts such as terrorist supporters exploit communities of supporters for conducting attacks on social media. Early detection of PSM accounts is crucial as they are likely to be key users in making a harmful message ""viral"". This paper overviews my recent doctoral work on utilizing causal inference to identify PSM accounts within a short time frame around their activity. The proposed scheme (1) assigns time-decay causality scores to users, (2) applies a community detection-based algorithm to group of users sharing similar causality scores and finally (3) deploys a classification algorithm to classify accounts. Unlike existing techniques that require network structure, cascade path, or content, our scheme relies solely on action log of users.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Uncovering Coordinated Networks on Social Media: Methods and Case Studies,"Coordinated campaigns are used to influence and manipulate social media platforms and their users, a critical challenge to the free exchange of information online. Here we introduce a general, unsupervised network-based methodology to uncover groups of accounts that are likely coordinated. The proposed method constructs coordination networks based on arbitrary behavioral traces shared among accounts. We present five case studies of influence campaigns, four of which in the diverse contexts of U.S. elections, Hong Kong protests, the Syrian civil war, and cryptocurrency manipulation. In each of these cases, we detect networks of coordinated Twitter accounts by examining their identities, images, hashtag sequences, retweets, or temporal patterns. The proposed approach proves to be broadly applicable to uncover different kinds of coordination across information warfare scenarios.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Analysis of the influence of political polarization in the vaccination stance: the Brazilian COVID-19 scenario,"The outbreak of COVID-19 had a huge global impact, and non-scientific beliefs and political polarization have significantly influenced the population's behavior. In this context, COVID vaccines were made available in an unprecedented time, but a high level of hesitance has been observed that can undermine community immunization. Traditionally, anti-vaccination attitudes are more related to conspiratorial thinking rather than political bias. In Brazil, a country with an exemplar tradition in large-scale vaccination programs, all COVID-related topics have also been discussed under a strong political bias. In this paper, we use a multidimensional analysis framework to understand if anti/pro-vaccination stances expressed by Brazilians in social media are influenced by political polarization. The analysis framework incorporates techniques to automatically infer from users their political orientation, topic modeling to discover their concerns, network analysis to characterize their social behavior, and the characterization of information sources and external influence. Our main findings confirm that anti/pro stances are biased by political polarization, right and left, respectively. While a significant proportion of pro-vaxxers display haste for an immunization program and criticize the government's actions, the anti-vaxxers distrust a vaccine developed in a record time. Anti-vaccination stance is also related to prejudice against China (anti-sinovaxxers), revealing conspiratorial theories related to communism. All groups display an ""echo chamber behavior, revealing they are not open to distinct views.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Driving The Last Mile: Characterizing and Understanding Distracted Driving Posts on Social Networks,"In 2015, 391,000 people were injured due to distracted driving in the US. One of the major reasons behind distracted driving is the use of cell-phones, accounting for 14% of fatal crashes. Social media applications have enabled users to stay connected, however, the use of such applications while driving could have serious repercussions -- often leading the user to be distracted from the road and ending up in an accident. In the context of impression management, it has been discovered that individuals often take a risk (such as teens smoking cigarettes, indulging in narcotics, and participating in unsafe sex) to improve their social standing. Therefore, viewing the phenomena of posting distracted driving posts under the lens of self-presentation, it can be hypothesized that users often indulge in risk-taking behavior on social media to improve their impression among their peers. In this paper, we first try to understand the severity of such social-media-based distractions by analyzing the content posted on a popular social media site where the user is driving and is also simultaneously creating content. To this end, we build a deep learning classifier to identify publicly posted content on social media that involves the user driving. Furthermore, a framework proposed to understand factors behind voluntary risk-taking activity observes that younger individuals are more willing to perform such activities, and men (as opposed to women) are more inclined to take risks. Grounding our observations in this framework, we test these hypotheses on 173 cities across the world. We conduct spatial and temporal analysis on a city-level and understand how distracted driving content posting behavior changes due to varied demographics. We discover that the factors put forth by the framework are significant in estimating the extent of such behavior.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The impact of NFT profile pictures within social network communities,"This paper presents an analysis of the role of social media, specifically Twitter, in the context of non-fungible tokens, better known as NFTs. Such emerging technology framing the creation and exchange of digital object, started years ago with early projects such as ""CryptoPunks"" and since early 2021, has received an increasing interest by a community of people creating, buying, selling NFT's and by the media reporting to the general public. In this work it is shown how the landscape of one class of projects, specifically those used as social media profile pictures, has become mainstream with leading projects such as ""Bored Ape Yacht Club"", ""Cool Cats"" and ""Doodles"". This work illustrates how heterogeneous data was collected from the Ethereum blockchain and Twitter and then analysed using algorithms and state-of-art metrics related to graphs. The initial results show that from a social network perspective, the collections of most popular NFTs can be considered as a single community around NFTs. Thus, while each project has its own value and volume of exchange, on a social level all of them are primarily influenced by the evolution of values and trades of ""Bored Ape Yacht Club"" collection.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mechanisms and Attributes of Echo Chambers in Social Media,"Echo chambers may exclude social media users from being exposed to other opinions, therefore, can cause rampant negative effects. Among abundant evidence are the 2016 and 2020 US presidential elections conspiracy theories and polarization, as well as the COVID-19 disinfodemic. To help better detect echo chambers and mitigate its negative effects, this paper explores the mechanisms and attributes of echo chambers in social media. In particular, we first illustrate four primary mechanisms related to three main factors: human psychology, social networks, and automatic systems. We then depict common attributes of echo chambers with a focus on the diffusion of misinformation, spreading of conspiracy theory, creation of social trends, political polarization, and emotional contagion of users. We illustrate each mechanism and attribute in a multi-perspective of sociology, psychology, and social computing with recent case studies. Our analysis suggest an emerging need to detect echo chambers and mitigate their negative effects.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Conspiracy in the Time of Corona: Automatic detection of Covid-19 Conspiracy Theories in Social Media and the News,"Rumors and conspiracy theories thrive in environments of low confidence and low trust. Consequently, it is not surprising that ones related to the Covid-19 pandemic are proliferating given the lack of any authoritative scientific consensus on the virus, its spread and containment, or on the long term social and economic ramifications of the pandemic. Among the stories currently circulating are ones suggesting that the 5G network activates the virus, that the pandemic is a hoax perpetrated by a global cabal, that the virus is a bio-weapon released deliberately by the Chinese, or that Bill Gates is using it as cover to launch a global surveillance regime. While some may be quick to dismiss these stories as having little impact on real-world behavior, recent events including the destruction of property, racially fueled attacks against Asian Americans, and demonstrations espousing resistance to public health orders countermand such conclusions. Inspired by narrative theory, we crawl social media sites and news reports and, through the application of automated machine-learning methods, discover the underlying narrative frameworks supporting the generation of these stories. We show how the various narrative frameworks fueling rumors and conspiracy theories rely on the alignment of otherwise disparate domains of knowledge, and consider how they attach to the broader reporting on the pandemic. These alignments and attachments, which can be monitored in near real-time, may be useful for identifying areas in the news that are particularly vulnerable to reinterpretation by conspiracy theorists. Understanding the dynamics of storytelling on social media and the narrative frameworks that provide the generative basis for these stories may also be helpful for devising methods to disrupt their spread.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Rise of Social Bots,"The Turing test aimed to recognize the behavior of a human from that of a computer algorithm. Such challenge is more relevant than ever in today's social media context, where limited attention and technology constrain the expressive power of humans, while incentives abound to develop software agents mimicking humans. These social bots interact, often unnoticed, with real people in social media ecosystems, but their abundance is uncertain. While many bots are benign, one can design harmful bots with the goals of persuading, smearing, or deceiving. Here we discuss the characteristics of modern, sophisticated social bots, and how their presence can endanger online ecosystems and our society. We then review current efforts to detect social bots on Twitter. Features related to content, network, sentiment, and temporal patterns of activity are imitated by bots but at the same time can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Pipeline for Graph-Based Monitoring of the Changes in the Information Space of Russian Social Media during the Lockdown,"With the COVID-19 outbreak and the subsequent lockdown, social media became a vital communication tool. The sudden outburst of online activity influenced information spread and consumption patterns. It increases the relevance of studying the dynamics of social networks and developing data processing pipelines that allow a comprehensive analysis of social media data in the temporal dimension. This paper scopes the weekly dynamics of the information space represented by Russian social media (Twitter and Livejournal) during a critical period (massive COVID-19 outbreak and first governmental measures). The approach is twofold: a) build the time series of topic similarity indicators by identifying COVID-related topics in each week and measuring user contribution to the topic space, and b) cluster user activity and display user-topic relationships on graphs in a dashboard application. The paper describes the development of the pipeline, explains the choices made and provides a case study of the adaptation to virus control measures. The results confirm that social processes and behaviour in response to pandemic-triggered changes can be successfully traced in social media. Moreover, the adaptation trends revealed by psychological and sociological studies are reflected in our data and can be explored using the proposed method.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Label-dependent Feature Extraction in Social Networks for Node Classification,A new method of feature extraction in the social network for within-network classification is proposed in the paper. The method provides new features calculated by combination of both: network structure information and class labels assigned to nodes. The influence of various features on classification performance has also been studied. The experiments on real-world data have shown that features created owing to the proposed method can lead to significant improvement of classification accuracy.,"cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Estimation on Social Media Networks Using Causal Inference,"Estimating influence on social media networks is an important practical and theoretical problem, especially because this new medium is widely exploited as a platform for disinformation and propaganda. This paper introduces a novel approach to influence estimation on social media networks and applies it to the real-world problem of characterizing active influence operations on Twitter during the 2017 French presidential elections. The new influence estimation approach attributes impact by accounting for narrative propagation over the network using a network causal inference framework applied to data arising from graph sampling and filtering. This causal framework infers the difference in outcome as a function of exposure, in contrast to existing approaches that attribute impact to activity volume or topological features, which do not explicitly measure nor necessarily indicate actual network influence. Cramr-Rao estimation bounds are derived for parameter estimation as a step in the causal analysis, and used to achieve geometrical insight on the causal inference problem. The ability to infer high causal influence is demonstrated on real-world social media accounts that are later independently confirmed to be either directly affiliated or correlated with foreign influence operations using evidence supplied by the U.S. Congress and journalistic reports.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Limited Attention and Centrality in Social Networks,"How does one find important or influential people in an online social network? Researchers have proposed a variety of centrality measures to identify individuals that are, for example, often visited by a random walk, infected in an epidemic, or receive many messages from friends. Recent research suggests that a social media users' capacity to respond to an incoming message is constrained by their finite attention, which they divide over all incoming information, i.e., information sent by users they follow. We propose a new measure of centrality --- limited-attention version of Bonacich's Alpha-centrality --- that models the effect of limited attention on epidemic diffusion. The new measure describes a process in which nodes broadcast messages to their out-neighbors, but the neighbors' ability to receive the message depends on the number of in-neighbors they have. We evaluate the proposed measure on real-world online social networks and show that it can better reproduce an empirical influence ranking of users than other popular centrality measures.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Service adoption spreading in online social networks,"The collective behaviour of people adopting an innovation, product or online service is commonly interpreted as a spreading phenomenon throughout the fabric of society. This process is arguably driven by social influence, social learning and by external effects like media. Observations of such processes date back to the seminal studies by Rogers and Bass, and their mathematical modelling has taken two directions: One paradigm, called simple contagion, identifies adoption spreading with an epidemic process. The other one, named complex contagion, is concerned with behavioural thresholds and successfully explains the emergence of large cascades of adoption resulting in a rapid spreading often seen in empirical data. The observation of real world adoption processes has become easier lately due to the availability of large digital social network and behavioural datasets. This has allowed simultaneous study of network structures and dynamics of online service adoption, shedding light on the mechanisms and external effects that influence the temporal evolution of behavioural or innovation adoption. These advancements have induced the development of more realistic models of social spreading phenomena, which in turn have provided remarkably good predictions of various empirical adoption processes. In this chapter we review recent data-driven studies addressing real-world service adoption processes. Our studies provide the first detailed empirical evidence of a heterogeneous threshold distribution in adoption. We also describe the modelling of such phenomena with formal methods and data-driven simulations. Our objective is to understand the effects of identified social mechanisms on service adoption spreading, and to provide potential new directions and open questions for future research.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Using Stochastic Models to Describe and Predict Social Dynamics of Web Users,"Popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both hosts of social media content and its consumers. Accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the ever-growing amount of content. Predicting popularity of content in social media, however, is challenging due to the complex interactions between content quality and how the social media site chooses to highlight content. Moreover, most social media sites also selectively present content that has been highly rated by similar users, whose similarity is indicated implicitly by their behavior or explicitly by links in a social network. While these factors make it difficult to predict popularity \emph{a priori}, we show that stochastic models of user behavior on these sites allows predicting popularity based on early user reactions to new content. By incorporating the various mechanisms through which web sites display content, such models improve on predictions based on simply extrapolating from the early votes. Using data from one such site, the news aggregator Digg, we show how a stochastic model of user behavior distinguishes the effect of the increased visibility due to the network from how interested users are in the content. We find a wide range of interest, identifying stories primarily of interest to users in the network (``niche interests'') from those of more general interest to the user community. This distinction is useful for predicting a story's eventual popularity from users' early reactions to the story.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Behavior and Mental Health: A Snapshot Survey under COVID-19 Pandemic,"Online social media provides a channel for monitoring people's social behaviors and their mental distress. Due to the restrictions imposed by COVID-19 people are increasingly using online social networks to express their feelings. Consequently, there is a significant amount of diverse user-generated social media content. However, COVID-19 pandemic has changed the way we live, study, socialize and recreate and this has affected our well-being and mental health problems. There are growing researches that leverage online social media analysis to detect and assess user's mental status. In this paper, we survey the literature of social media analysis for mental disorders detection, with a special focus on the studies conducted in the context of COVID-19 during 2020-2021. Firstly, we classify the surveyed studies in terms of feature extraction types, varying from language usage patterns to aesthetic preferences and online behaviors. Secondly, we explore detection methods used for mental disorders detection including machine learning and deep learning detection methods. Finally, we discuss the challenges of mental disorder detection using social media data, including the privacy and ethical concerns, as well as the technical challenges of scaling and deploying such systems at large scales, and discuss the learnt lessons over the last few years.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A study of trends in the effects of TV ratings and social media (Twitter) -- Case study 1,"The Japanese TV program 'Drama A' is a drama broadcast from October to December 2016. The audience rating was sluggish, but this drama marked a high audience rating in 2016. Since it was popular from the middle, and it was speculated that there was a part related to social media in the popularity, we considered existing research methods as a case study. In this paper, we used a mathematical model of the hit phenomenon to examine the impact of audience assessment from social media from a sociophysical perspective. We got the same consideration as the audience rating per minute of video research. This paper is IEEE BIGDATA2018's Revised paper(Consideration on TV audience rating and influence of social media).","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Behind the Mask: A Computational Study of Anonymous' Presence on Twitter,"The hacktivist group Anonymous is unusual in its public-facing nature. Unlike other cybercriminal groups, which rely on secrecy and privacy for protection, Anonymous is prevalent on the social media site, Twitter. In this paper we re-examine some key findings reported in previous small-scale qualitative studies of the group using a large-scale computational analysis of Anonymous' presence on Twitter. We specifically refer to reports which reject the group's claims of leaderlessness, and indicate a fracturing of the group after the arrests of prominent members in 2011-2013. In our research, we present the first attempts to use machine learning to identify and analyse the presence of a network of over 20,000 Anonymous accounts spanning from 2008-2019 on the Twitter platform. In turn, this research utilises social network analysis (SNA) and centrality measures to examine the distribution of influence within this large network, identifying the presence of a small number of highly influential accounts. Moreover, we present the first study of tweets from some of the identified key influencer accounts and, through the use of topic modelling, demonstrate a similarity in overarching subjects of discussion between these prominent accounts. These findings provide robust, quantitative evidence to support the claims of smaller-scale, qualitative studies of the Anonymous collective.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
LA-LDA: A Limited Attention Topic Model for Social Recommendation,"Social media users have finite attention which limits the number of incoming messages from friends they can process. Moreover, they pay more attention to opinions and recommendations of some friends more than others. In this paper, we propose LA-LDA, a latent topic model which incorporates limited, non-uniformly divided attention in the diffusion process by which opinions and information spread on the social network. We show that our proposed model is able to learn more accurate user models from users' social network and item adoption behavior than models which do not take limited attention into account. We analyze voting on news items on the social news aggregator Digg and show that our proposed model is better able to predict held out votes than alternative models. Our study demonstrates that psycho-socially motivated models have better ability to describe and predict observed behavior than models which only consider topics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Event Organization 101: Understanding Latent Factors of Event Popularity,"The problem of understanding people's participation in real-world events has been a subject of active research and can offer valuable insights for human behavior analysis and event-related recommendation/advertisement. In this work, we study the latent factors for determining event popularity using large-scale datasets collected from the popular Meetup.com EBSN in three major cities around the world. We have conducted modeling analysis of four contextual factors (spatial, group, temporal, and semantic), and also developed a group-based social influence propagation network to model group-specific influences on events. By combining the Contextual features And Social Influence NetwOrk, our integrated prediction framework CASINO can capture the diverse influential factors of event participation and can be used by event organizers to predict/improve the popularity of their events. Evaluations demonstrate that our CASINO framework achieves high prediction accuracy with contributions from all the latent features we capture.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mental Health Pandemic during the COVID-19 Outbreak: Social Media as a Window to Public Mental Health,"Intensified preventive measures during the COVID-19 pandemic, such as lockdown and social distancing, heavily increased the perception of social isolation (i.e., a discrepancy between one's social needs and the provisions of the social environment) among young adults. Social isolation is closely associated with situational loneliness (i.e., loneliness emerging from environmental change), a risk factor for depressive symptoms. Prior research suggested vulnerable young adults are likely to seek support from an online social platform such as Reddit, a perceived comfortable environment for lonely individuals to seek mental health help through anonymous communication with a broad social network. Therefore, this study aims to identify and analyze depression-related dialogues on loneliness subreddits during the COVID-19 outbreak, with the implications on depression-related infoveillance during the pandemic. Our study utilized logistic regression and topic modeling to classify and examine depression-related discussions on loneliness subreddits before and during the pandemic. Our results showed significant increases in the volume of depression-related discussions (i.e., topics related to mental health, social interaction, family, and emotion) where challenges were reported during the pandemic. We also found a switch in dominant topics emerging from depression-related discussions on loneliness subreddits, from dating (prepandemic) to online interaction and community (pandemic), suggesting the increased expressions or need of online social support during the pandemic. The current findings suggest the potential of social media to serve as a window for monitoring public mental health. Our future study will clinically validate the current approach, which has implications for designing a surveillance system during the crisis.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Characterizing the Use of Images in State-Sponsored Information Warfare Operations by Russian Trolls on Twitter,"State-sponsored organizations are increasingly linked to efforts aimed to exploit social media for information warfare and manipulating public opinion. Typically, their activities rely on a number of social network accounts they control, aka trolls, that post and interact with other users disguised as ""regular"" users. These accounts often use images and memes, along with textual content, in order to increase the engagement and the credibility of their posts.   In this paper, we present the first study of images shared by state-sponsored accounts by analyzing a ground truth dataset of 1.8M images posted to Twitter by accounts controlled by the Russian Internet Research Agency. First, we analyze the content of the images as well as their posting activity. Then, using Hawkes Processes, we quantify their influence on popular Web communities like Twitter, Reddit, 4chan's Politically Incorrect board (/pol/), and Gab, with respect to the dissemination of images. We find that the extensive image posting activity of Russian trolls coincides with real-world events (e.g., the Unite the Right rally in Charlottesville), and shed light on their targets as well as the content disseminated via images. Finally, we show that the trolls were more effective in disseminating politics-related imagery than other images.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Susceptibility Paradox in Online Social Influence,"Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics, whereas spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Coding Together at Scale: GitHub as a Collaborative Social Network,"GitHub is the most popular repository for open source code. It has more than 3.5 million users, as the company declared in April 2013, and more than 10 million repositories, as of December 2013. It has a publicly accessible API and, since March 2012, it also publishes a stream of all the events occurring on public projects. Interactions among GitHub users are of a complex nature and take place in different forms. Developers create and fork repositories, push code, approve code pushed by others, bookmark their favorite projects and follow other developers to keep track of their activities.   In this paper we present a characterization of GitHub, as both a social network and a collaborative platform. To the best of our knowledge, this is the first quantitative study about the interactions happening on GitHub. We analyze the logs from the service over 18 months (between March 11, 2012 and September 11, 2013), describing 183.54 million events and we obtain information about 2.19 million users and 5.68 million repositories, both growing linearly in time. We show that the distributions of the number of contributors per project, watchers per project and followers per user show a power-law-like shape. We analyze social ties and repository-mediated collaboration patterns, and we observe a remarkably low level of reciprocity of the social connections. We also measure the activity of each user in terms of authored events and we observe that very active users do not necessarily have a large number of followers. Finally, we provide a geographic characterization of the centers of activity and we investigate how distance influences collaboration.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mental Disorder Recovery Correlated with Centralities and Interactions on an Online Social Network,"Recent research has established both a theoretical basis and strong empirical evidence that effective social behavior plays a beneficial role in the maintenance of physical and psychological well-being of people. To test whether social behavior and well-being are also associated in online communities, we studied the correlations between the recovery of patients with mental disorders and their behaviors in online social media. As the source of the data related to the social behavior and progress of mental recovery, we used PatientsLikeMe (PLM), the world's first open-participation research platform for the development of patient-centered health outcome measures. We first constructed an online social network structure based on patient-to-patient ties among 200 patients obtained from PLM. We then characterized patients' online social activities by measuring the numbers of ""posts and views"" and ""helpful marks"" each patient obtained. The patients' recovery data were obtained from their self-reported status information that was also available on PLM. We found that some node properties (in-degree, eigenvector centrality and PageRank) and the two online social activity measures were significantly correlated with patients' recovery. Furthermore, we re-collected the patients' recovery data two months after the first data collection. We found significant correlations between the patients' social behaviors and the second recovery data, which were collected two months apart. Our results indicated that social interactions in online communities such as PLM were significantly associated with the current and future recoveries of patients with mental disorders.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach,"In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and support for mental health challenges under the guidance of qualified healthcare professionals.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Model for the Influence of Media on the Ideology of Content in Online Social Networks,"Many people rely on online social networks as sources of news and information, and the spread of media content with ideologies across the political spectrum influences online discussions and impacts actions offline. To examine the impact of media in online social networks, we generalize bounded-confidence models of opinion dynamics by incorporating media accounts as influencers in a network. We quantify partisanship of content with a continuous parameter on an interval, and we formulate higher-dimensional generalizations to incorporate content quality and increasingly nuanced political positions. We simulate our model with one and two ideological dimensions, and we use the results of our simulations to quantify the ""entrainment"" of content from non-media accounts to the ideologies of media accounts in a network. We maximize media impact in a social network by tuning the number of media accounts that promote the content and the number of followers of the accounts. Using numerical computations, we find that the entrainment of the ideology of content spread by non-media accounts to media ideology depends on a network's structural features, including its size, the mean number of followers of its nodes, and the receptiveness of its nodes to different opinions. We then introduce content quality --- a key novel contribution of our work --- into our model. We incorporate multiple media sources with ideological biases and quality-level estimates that we draw from real media sources and demonstrate that our model can produce distinct communities (""echo chambers"") that are polarized in both ideology and quality. Our model provides a step toward understanding content quality and ideology in spreading dynamics, with ramifications for how to mitigate the spread of undesired content and promote the spread of desired content.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
When Your Friends Become Sellers: An Empirical Study of Social Commerce Site Beidian,"Past few years have witnessed the emergence and phenomenal success of strong-tie based social commerce. Embedded in social networking sites, these E-Commerce platforms transform ordinary people into sellers, where they advertise and sell products to their friends and family in online social networks. These sites can acquire millions of users within a short time, and are growing fast at an accelerated rate. However, little is known about how these social commerce develop as a blend of social relationship and economic transactions. In this paper we present the first measurement study on the full-scale data of Beidian, one of the fastest growing social commerce sites in China, which involves 11.8 million users. We first analyzed the topological structure of the Beidian platform and highlighted its decentralized nature. We then studied the site's rapid growth and its growth mechanism via invitation cascade. Finally, we investigated purchasing behavior on Beidian, where we focused on user proximity and loyalty, which contributes to the site's high conversion rate. As the consequences of interactions between strong ties and economic logics, emerging social commerce demonstrates significant property deviations from all known social networks and E-Commerce in terms of network structure, dynamics and user behavior. To the best of our knowledge, this work is the first quantitative study on the network characteristics and dynamics of emerging social commerce platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Analyzing X's Web of Influence: Dissecting News Sharing Dynamics through Credibility and Popularity with Transfer Entropy and Multiplex Network Measures,"The dissemination of news articles on social media platforms significantly impacts the public's perception of global issues, with the nature of these articles varying in credibility and popularity. The challenge of measuring this influence and identifying key propagators is formidable. Traditional graph-based metrics such as different centrality measures and node degree methods offer some insights into information flow but prove insufficient for identifying hidden influencers in large-scale social media networks such as X (previously known as Twitter). This study adopts and enhances a non-parametric framework based on Transfer Entropy to elucidate the influence relationships among X users. It further categorizes the distribution of influence exerted by these actors through the innovative use of multiplex network measures within a social media context, aiming to pinpoint influential actors during significant world events. The methodology was applied to three distinct events, and the findings revealed that actors in different events leveraged different types of news articles and influenced distinct sets of actors based on the news category. Notably, we found that actors disseminating trustworthy news articles to influence others occasionally resort to untrustworthy sources. However, the converse scenario, wherein actors predominantly using untrustworthy news types switch to trustworthy sources for influence, is less prevalent. This asymmetry suggests a discernible pattern in the strategic use of news articles for influence across social media networks, highlighting the nuanced roles of trustworthiness and popularity in the spread of information and influence.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Leveraging Large Language Models to Detect Influence Campaigns in Social Media,"Social media influence campaigns pose significant challenges to public discourse and democracy. Traditional detection methods fall short due to the complexity and dynamic nature of social media. Addressing this, we propose a novel detection method using Large Language Models (LLMs) that incorporates both user metadata and network structures. By converting these elements into a text format, our approach effectively processes multilingual content and adapts to the shifting tactics of malicious campaign actors. We validate our model through rigorous testing on multiple datasets, showcasing its superior performance in identifying influence efforts. This research not only offers a powerful tool for detecting campaigns, but also sets the stage for future enhancements to keep up with the fast-paced evolution of social media-based influence tactics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The influence of repressive legislation on the structure of a social media network,"Social media have been widely used to organize citizen movements. In 2012, 75% university and college students in Quebec, Canada, participated in mass protests against an increase in tuition fees, mainly organized using social media. To reduce public disruption, the government introduced special legislation designed to impede protest organization. Here, we show that the legislation changed the behaviour of social media users but not the overall structure of their social network on Twitter. Thus, users were still able to spread information to efficiently organize demonstrations using their social network. This natural experiment shows the power of social media in political mobilization, as well as behavioural flexibility in information flow over a large number of individuals.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media,"Distrust of public serving institutions and anti-establishment views are on the rise (especially in the U.S.). As people turn to social media for information, it is imperative to understand whether and how social media environments may be contributing to distrust of institutions. In social media, content creators, influencers, and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics, and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility. However, the extent to which this content appears and whether such content increases engagement is unclear. This study analyzes the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok. Despite its popularity as a source of information, TikTok remains relatively understudied and may provide important insights into how people form attitudes towards institutions. We employ a computational approach to label TikTok posts as containing AES or not across topical domains where content creators tend to frame themselves as experts: finance and wellness. As a comparison, we also consider the topic of conspiracy theories, where AES is expected to be common. We find that AES is most prevalent in conspiracy theory content, and relatively rare in content related to the other two topics. However, we find that engagement patterns with such content varies by area, and that there may be platform incentives for users to post content that expresses anti-establishment sentiment.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Characterization of Cross-posting Activity for Professional Users across Facebook, Twitter and Google+","Professional players in social media (e.g., big companies, politician, athletes, celebrities, etc) are intensively using Online Social Networks (OSNs) in order to interact with a huge amount of regular OSN users with different purposes (marketing campaigns, customer feedback, public reputation improvement, etc). Hence, due to the large catalog of existing OSNs, professional players usually count with OSN accounts in different systems. In this context an interesting question is whether professional users publish the same information across their OSN accounts, or actually they use different OSNs in a different manner. We define as cross-posting activity the action of publishing the same information in two or more OSNs. This paper aims at characterizing the cross-posting activity of professional users across three major OSNs, Facebook, Twitter and Google+. To this end, we perform a large-scale measurement-based analysis across more than 2M posts collected from 616 professional users with active accounts in the three referred OSNs. Then we characterize the phenomenon of cross posting and analyze the behavioral patterns based on the identified characteristics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media,"Massive social media data can reflect people's authentic thoughts, emotions, communication, etc., and therefore can be analyzed for early detection of mental health problems such as depression. Existing works about early depression detection on social media lacked interpretability and neglected the heterogeneity of social media data. Furthermore, they overlooked the global interaction among users. To address these issues, we develop a novel method that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. Specifically, prompt learning is employed to map users' implicit psychological symbols with excellent interpretability while deep semantic and diverse behavioral features are incorporated by a heterogeneous information network. Then, the heterogeneous graph network with a dual attention mechanism is constructed to model the relationships among heterogeneous social information at the feature level. Furthermore, the heterogeneous subgraph network integrating subgraph attention and self-supervised contrastive learning is developed to explore complicated interactions among users and groups at the user level. Extensive experimental results demonstrate that our proposed method significantly outperforms state-of-the-art methods for depression detection on social media.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Disentangling sources of influence in online social networks,"Information propagation in online social networks is facilitated by two types of influence - endogenous (peer) influence that acts between users of the social network and exogenous (external) that corresponds to various external mediators such as online news media. However, inference of these influences from data remains a challenge, especially when data on the activation of users is scarce. In this paper we propose a methodology that yields estimates of both endogenous and exogenous influence using only a social network structure and a single activation cascade. Our method exploits the statistical differences between the two types of influence - endogenous is dependent on the social network structure and current state of each user while exogenous is independent of these. We evaluate our methodology on simulated activation cascades as well as on cascades obtained from several large Facebook political survey applications. We show that our methodology is able to provide estimates of endogenous and exogenous influence in online social networks, characterize activation of each individual user as being endogenously or exogenously driven, and identify most influential groups of users.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Recommending Influenceable Targets based on Influence Propagation through Activity Behaviors in Online Social Media,"Online Social Media (OSM) is a platform through which the users present themselves to the connected world by means of messaging, posting, reacting, tagging, and sharing on different contents with also other social activities. Nowadays, it has a vast impact on various aspects of the industry, business and society along with on users life. In an OSN platform, reaching the target users is one of the primary focus for most of the businesses and other organizations. Identification and recommendation of influenceable targets help to capture the appropriate audience efficiently and effectively. In this paper, an effective model has been discussed in egocentric OSN by incorporating an efficient influence measured Recommendation System in order to generate a list of top most influenceable target users among all connected network members for any specific social network user. Firstly the list of interacted network members has been updated based on all activities. On which the interacted network members with most similar activities have been recommended based on the specific influence category with sentiment type. After that, the top most influenceable network members in the basis of the required amount among those updated list of interacted network members have been identified with proper ranking by analyzing the similarity and frequency of their activity contents with respect to the activity contents of the main user. Through these two continuous stages, an effective list of top influenceable targets of the main user has been distinguished from the egocentric view of any social network.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Micro-foundation of Social Capital in Evolving Social Networks,"A social network confers benefits and advantages on individuals (and on groups), the literature refers to these advantages as social capital. This paper presents a micro-founded mathematical model of the evolution of a social network and of the social capital of individuals within the network. The evolution of the network is influenced by the extent to which individuals are homophilic, structurally opportunistic, socially gregarious and by the distribution of types in the society. In the analysis, we identify different kinds of social capital: bonding capital, popularity capital, and bridging capital. Bonding capital is created by forming a circle of connections, homophily increases bonding capital because it makes this circle of connections more homogeneous. Popularity capital leads to preferential attachment: individuals who become popular tend to become more popular because others are more likely to link to them. Homophily creates asymmetries in the levels of popularity attained by different social groups, more gregarious types of agents are more likely to become popular. However, in homophilic societies, individuals who belong to less gregarious, less opportunistic, or major types are likely to be more central in the network and thus acquire a bridging capital.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Controlling the Misinformation Diffusion in Social Media by the Effect of Different Classes of Agents,"The rapid and widespread dissemination of misinformation through social networks is a growing concern in today's digital age. This study focused on modeling fake news diffusion, discovering the spreading dynamics, and designing control strategies. A common approach for modeling the misinformation dynamics is SIR-based models. Our approach is an extension of a model called 'SBFC' which is a SIR-based model. This model has three states, Susceptible, Believer, and Fact-Checker. The dynamics and transition between states are based on neighbors' beliefs, hoax credibility, spreading rate, probability of verifying the news, and probability of forgetting the current state. Our contribution is to push this model to real social networks by considering different classes of agents with their characteristics. We proposed two main strategies for confronting misinformation diffusion. First, we can educate a minor class, like scholars or influencers, to improve their ability to verify the news or remember their state longer. The second strategy is adding fact-checker bots to the network to spread the facts and influence their neighbors' states. Our result shows that both of these approaches can effectively control the misinformation spread.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Capturing social media expressions during the COVID-19 pandemic in Argentina and forecasting mental health and emotions,"Purpose. We present an approach for forecasting mental health conditions and emotions of a given population during the COVID-19 pandemic in Argentina based on language expressions used in social media. This approach permits anticipating high prevalence periods in short- to medium-term time horizons. Design. Mental health conditions and emotions are captured via markers, which link social media contents with lexicons. First, we build descriptive timelines for decision makers to monitor the evolution of markers, and their correlation with crisis events. Second, we model the timelines as time series, and support their forecasting, which in turn serve to identify high prevalence points for the estimated markers. Findings. Results showed that different time series forecasting strategies offer different capabilities. In the best scenario, the emergence of high prevalence periods of emotions and mental health disorders can be satisfactorily predicted with a neural network strategy, even when limited data is available in early stages of a crisis (e.g., 7 days). Originality. Although there have been efforts in the literature to predict mental states of individuals, the analysis of mental health at the collective level has received scarce attention. We take a step forward by proposing a forecasting approach for analyzing the mental health of a given population (or group of individuals) at a larger scale. Practical implications. We believe that this work contributes to a better understanding of how psychological processes related to crisis manifest in social media, being a valuable asset for the design, implementation and monitoring of health prevention and communication policies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Discovering Latent Themes in Social Media Messaging: A Machine-in-the-Loop Approach Integrating LLMs,"Grasping the themes of social media content is key to understanding the narratives that influence public opinion and behavior. The thematic analysis goes beyond traditional topic-level analysis, which often captures only the broadest patterns, providing deeper insights into specific and actionable themes such as ""public sentiment towards vaccination"", ""political discourse surrounding climate policies,"" etc. In this paper, we introduce a novel approach to uncovering latent themes in social media messaging. Recognizing the limitations of the traditional topic-level analysis, which tends to capture only overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Traditional theme discovery methods typically involve manual processes and a human-in-the-loop approach. While valuable, these methods face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). To demonstrate our approach, we apply our framework to contentious topics, such as climate debate and vaccine debate. We use two publicly available datasets: (1) the climate campaigns dataset of 21k Facebook ads and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads. Our quantitative and qualitative analysis shows that our methodology yields more accurate and interpretable results compared to the baselines. Our results not only demonstrate the effectiveness of our approach in uncovering latent themes but also illuminate how these themes are tailored for demographic targeting in social media contexts. Additionally, our work sheds light on the dynamic nature of social media, revealing the shifts in the thematic focus of messaging in response to real-world events.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
#ILookLikeAnEngineer: Using Social Media Based Hashtag Activism Campaigns as a Lens to Better Understand Engineering Diversity Issues,"Each year, significant investment of time and resources is made to improve diversity within engineering across a range of federal and state agencies, private/not-for-profit organizations, and foundations. In spite of decades of investments, efforts have not yielded desired returns - participation by minorities continues to lag at a time when STEM workforce requirements are increasing. In recent years a new stream of data has emerged - online social networks, including Twitter, Facebook, and Instagram - that act as a key sensor of social behavior and attitudes of the public. Almost 87% of the American population now participates in some form of social media activity. Consequently, social networking sites have become powerful indicators of social action and social media data has shown significant promise for studying many issues including public health communication, political campaign, humanitarian crisis, and, activism. We argue that social media data can likewise be leveraged to better understand and improve engineering diversity. As a case study to illustrate the viability of the approach, we present findings from a campaign, #ILookLikeAnEngineer (using Twitter data - 19,354 original tweets and 29,529 retweets), aimed at increasing gender diversity in the engineering workplace. The campaign provided a continuous momentum to the overall effort to increase diversity and novel ways of connecting with relevant audience. Our analysis demonstrates that diversity initiatives related to STEM attract voices from various entities including individuals, large corporations, media outlets, and community interest groups.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Social Media Recommendation Impacts Using Academic Networks: A Graph Neural Network Approach,"The widespread use of social media has highlighted potential negative impacts on society and individuals, largely driven by recommendation algorithms that shape user behavior and social dynamics. Understanding these algorithms is essential but challenging due to the complex, distributed nature of social media networks as well as limited access to real-world data. This study proposes to use academic social networks as a proxy for investigating recommendation systems in social media. By employing Graph Neural Networks (GNNs), we develop a model that separates the prediction of academic infosphere from behavior prediction, allowing us to simulate recommender-generated infospheres and assess the model's performance in predicting future co-authorships. Our approach aims to improve our understanding of recommendation systems' roles and social networks modeling. To support the reproducibility of our work we publicly make available our implementations: https://github.com/DimNeuroLab/academic_network_project","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Echo Chambers and Information Brokers on Truth Social: A Study of Network Dynamics and Political Discourse,"This study examines the structural dynamics of Truth Social, a politically aligned social media platform, during two major political events: the U.S. Supreme Court's overturning of Roe v. Wade and the FBI's search of Mar-a-Lago. Using a large-scale dataset of user interactions based on re-truths (platform-native reposts), we analyze how the network evolves in relation to fragmentation, polarization, and user influence. Our findings reveal a segmented and ideologically homogenous structure dominated by a small number of central figures. Political events prompt temporary consolidation around shared narratives, followed by rapid returns to fragmented, echo-chambered clusters. Centrality metrics highlight the disproportionate role of key influencers, particularly @realDonaldTrump, in shaping visibility and directing discourse. These results contribute to research on alternative platforms, political communication, and online network behavior, demonstrating how infrastructure and community dynamics together reinforce ideological boundaries and limit cross-cutting engagement.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Database of Indian Social Media Influencers on Twitter,"Databases of highly networked individuals have been indispensable in studying narratives and influence on social media. To support studies on Twitter in India, we present a systematically categorised database of accounts of influence on Twitter in India, identified and annotated through an iterative process of friends, networks, and self-described profile information, verified manually. We built an initial set of accounts based on the friend network of a seed set of accounts based on real-world renown in various fields, and then snowballed ""friends of friends"" multiple times, and rank ordered individuals based on the number of in-group connections, and overall followers. We then manually classified identified accounts under the categories of entertainment, sports, business, government, institutions, journalism, civil society accounts that have independent standing outside of social media, as well as a category of ""digital first"" referring to accounts that derive their primary influence from online activity. Overall, we annotated 11580 unique accounts across all categories. The database is useful studying various questions related to the role of influencers in polarisation, misinformation, extreme speech, political discourse etc.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Predicting Users' Value Changes by the Friends' Influence from Social Media Usage,"Basic human values represent a set of values such as security, independence, success, kindness, and pleasure, which we deem important to our lives. Each of us holds different values with different degrees of significance. Existing studies show that values of a person can be identified from their social network usage. However, the value priority of a person may change over time due to different factors such as life experiences, influence, social structure and technology. Existing studies do not conduct any analysis regarding the change of users' value from the social influence, i.e., group persuasion, form the social media usage. In our research, first, we predict users' value score by the influence of friends from their social media usage. We propose a Bounded Confidence Model (BCM) based value dynamics model from 275 different ego networks in Facebook that predicts how social influence may persuade a person to change their value over time. Then, to predict better, we use particle swarm optimization based hyperparameter tuning technique. We observe that these optimized hyperparameters produce accurate future value score. We also run our approach with different machine learning based methods and find support vector regression (SVR) outperforms other regressor models. By using SVR with the best hyperparameters of BCM model, we find the lowest Mean Squared Error (MSE) score 0.00347.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Maximization in Social Networks: A Survey,"Online social networks have become an important platform for people to communicate, share knowledge and disseminate information. Given the widespread usage of social media, individuals' ideas, preferences and behavior are often influenced by their peers or friends in the social networks that they participate in. Since the last decade, influence maximization (IM) problem has been extensively adopted to model the diffusion of innovations and ideas. The purpose of IM is to select a set of k seed nodes who can influence the most individuals in the network.   In this survey, we present a systematical study over the researches and future directions with respect to IM problem. We review the information diffusion models and analyze a variety of algorithms for the classic IM algorithms. We propose a taxonomy for potential readers to understand the key techniques and challenges. We also organize the milestone works in time order such that the readers of this survey can experience the research roadmap in this field. Moreover, we also categorize other application-oriented IM studies and correspondingly study each of them. What's more, we list a series of open questions as the future directions for IM-related researches, where a potential reader of this survey can easily observe what should be done next in this field.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Interplay between social influence and competitive strategical games in multiplex networks,"We present a model that takes into account the coupling between evolutionary game dynamics and social influence. Importantly, social influence and game dynamics take place in different domains, which we model as different layers of a multiplex network. We show that the coupling between these dynamical processes can lead to cooperation in scenarios where the pure game dynamics predicts defection. In addition, we show that the structure of the network layers and the relation between them can further increase cooperation. Remarkably, if the layers are related in a certain way, the system can reach a polarized metastable state.These findings could explain the prevalence of polarization observed in many social dilemmas.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Data Driven Modeling Social Media Influence using Differential Equations,"Individuals modify their opinions towards a topic based on their social interactions. Opinion evolution models conceptualize the change of opinion as a uni-dimensional continuum, and the effect of influence is built by the group size, the network structures, or the relations among opinions within the group. However, how to model the personal opinion evolution process under the effect of the online social influence as a function remains unclear. Here, we show that the uni-dimensional continuous user opinions can be represented by compressed high-dimensional word embeddings, and its evolution can be accurately modelled by an ordinary differential equation (ODE) that reflects the social network influencer interactions. Our three major contributions are: (1) introduce a data-driven pipeline representing the personal evolution of opinions with a time kernel, (2) based on previous psychology models, we model the opinion evolution process as a function of online social influence using an ordinary differential equation, and (3) applied Our opinion evolution model to the real-time Twitter data. We perform our analysis on 87 active users with corresponding influencers on the COVID-19 topic from 2020 to 2022. The regression results demonstrate that 99% of the variation in the quantified opinions can be explained by the way we model the connected opinions from their influencers. Our research on the COVID-19 topic and for the account analysed shows that social media users primarily shift their opinion based on influencers they follow (e.g., model explains for 99% variation) and self-evolution of opinion over a long time scale is limited.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Misinformation Regulation in the Presence of Competition between Social Media Platforms (Extended Version),"Social media platforms have diverse content moderation policies, with many prominent actors hesitant to impose strict regulations. A key reason for this reluctance could be the competitive advantage that comes with lax regulation. A popular platform that starts enforcing content moderation rules may fear that it could lose users to less-regulated alternative platforms. Moreover, if users continue harmful activities on other platforms, regulation ends up being futile. This article examines the competitive aspect of content moderation by considering the motivations of all involved players (platformer, news source, and social media users), identifying the regulation policies sustained in equilibrium, and evaluating the information quality available on each platform. Applied to simple yet relevant social networks such as stochastic block models, our model reveals the conditions for a popular platform to enforce strict regulation without losing users. Effectiveness of regulation depends on the diffusive property of news posts, friend interaction qualities in social media, the sizes and cohesiveness of communities, and how much sympathizers appreciate surprising news from influencers.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Early Detection of Mental Health Issues Using Social Media Posts,"The increasing prevalence of mental health disorders, such as depression, anxiety, and bipolar disorder, calls for immediate need in developing tools for early detection and intervention. Social media platforms, like Reddit, represent a rich source of user-generated content, reflecting emotional and behavioral patterns. In this work, we propose a multi-modal deep learning framework that integrates linguistic and temporal features for early detection of mental health crises. Our approach is based on the method that utilizes a BiLSTM network both for text and temporal feature analysis, modeling sequential dependencies in a different manner, capturing contextual patterns quite well. This work includes a cross-modal attention approach that allows fusion of such outputs into context-aware classification of mental health conditions. The model was then trained and evaluated on a dataset of labeled Reddit posts preprocessed using text preprocessing, scaling of temporal features, and encoding of labels. Experimental results indicate that the proposed architecture performs better compared to traditional models with a validation accuracy of 74.55% and F1-Score of 0.7376. This study presents the importance of multi-modal learning for mental health detection and provides a baseline for further improvements by using more advanced attention mechanisms and other data modalities.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Beyond network centrality: Individual-level behavioral traits for predicting information superspreaders in social media,"Understanding the heterogeneous role of individuals in large-scale information spreading is essential to manage online behavior as well as its potential offline consequences. To this end, most existing studies from diverse research domains focus on the disproportionate role played by highly-connected ``hub"" individuals. However, we demonstrate here that information superspreaders in online social media are best understood and predicted by simultaneously considering two individual-level behavioral traits: influence and susceptibility. Specifically, we derive a nonlinear network-based algorithm to quantify individuals' influence and susceptibility from multiple spreading event data. By applying the algorithm to large-scale data from Twitter and Weibo, we demonstrate that individuals' estimated influence and susceptibility scores enable predictions of future superspreaders above and beyond network centrality, and reveal new insights on the network position of the superspreaders.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
SocialRec: User Activity Based Post Weighted Dynamic Personalized Post Recommendation System in Social Media,"User activities can influence their subsequent interactions with a post, generating interest in the user. Typically, users interact with posts from friends by commenting and using reaction emojis, reflecting their level of interest on social media such as Facebook, Twitter, and Reddit. Our objective is to analyze user history over time, including their posts and engagement on various topics. Additionally, we take into account the user's profile, seeking connections between their activities and social media platforms. By integrating user history, engagement, and persona, we aim to assess recommendation scores based on relevant item sharing by Hit Rate (HR) and the quality of the ranking system by Normalized Discounted Cumulative Gain (NDCG), where we achieve the highest for NeuMF 0.80 and 0.6 respectively. Our hybrid approach solves the cold-start problem when there is a new user, for new items cold-start problem will never occur, as we consider the post category values. To improve the performance of the model during cold-start we introduce collaborative filtering by looking for similar users and ranking the users based on the highest similarity scores.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social media in the Global South: A Network Dataset of the Malian Twittersphere,"With the expansion of mobile communications infrastructure, social media usage in the Global South is surging. Compared to the Global North, populations of the Global South have had less prior experience with social media from stationary computers and wired Internet. Many countries are experiencing violent conflicts that have a profound effect on their societies. As a result, social networks develop under different conditions than elsewhere, and our goal is to provide data for studying this phenomenon. In this dataset paper, we present a data collection of a national Twittersphere in a West African country of conflict. While not the largest social network in terms of users, Twitter is an important platform where people engage in public discussion. The focus is on Mali, a country beset by conflict since 2012 that has recently had a relatively precarious media ecology. The dataset consists of tweets and Twitter users in Mali and was collected in June 2022, when the Malian conflict became more violent internally both towards external and international actors. In a preliminary analysis, we assume that the conflictual context influences how people access social media and, therefore, the shape of the Twittersphere and its characteristics. The aim of this paper is to primarily invite researchers from various disciplines including complex networks and social sciences scholars to explore the data at hand further. We collected the dataset using a scraping strategy of the follower network and the identification of characteristics of a Malian Twitter user. The given snapshot of the Malian Twitter follower network contains around seven million accounts, of which 56,000 are clearly identifiable as Malian. In addition, we present the tweets. The dataset is available at: https://osf.io/mj2qt/","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Study on Herd Behavior Using Sentiment Analysis in Online Social Network,"Social media platforms are thriving nowadays, so a huge volume of data is produced. As it includes brief and clear statements, millions of people post their thoughts on microblogging sites every day. This paper represents and analyze the capacity of diverse strategies to volumetric, delicate, and social networks to predict critical opinions from online social networking sites. In the exploration of certain searching for relevant, the thoughts of people play a crucial role. Social media becomes a good outlet since the last decades to share the opinions globally. Sentiment analysis as well as opinion mining is a tool that is used to extract the opinions or thoughts of the common public. An occurrence in one place, be it economic, political, or social, may trigger large-scale chain public reaction across many other sites in an increasingly interconnected world. This study demonstrates the evaluation of sentiment analysis techniques using social media contents and creating the association between subjectivity with herd behavior and clustering coefficient as well as tries to predict the election result (2021 election in West Bengal). This is an implementation of sentiment analysis targeted at estimating the results of an upcoming election by assessing the public's opinion across social media. This paper also has a short discussion section on the usefulness of the idea in other fields.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding Illicit Drug Use Behaviors by Mining Social Media,"Drug use by people is on the rise and is of great interest to public health agencies and law enforcement agencies. As found by the National Survey on Drug Use and Health, 20 million Americans aged 12 years or older consumed illicit drugs in the past few 30 days. Given their ubiquity in everyday life, drug abuse related studies have received much and constant attention. However, most of the existing studies rely on surveys. Surveys present a fair number of problems because of their nature. Surveys on sensitive topics such as illicit drug use may not be answered truthfully by the people taking them. Selecting a representative sample to survey is another major challenge. In this paper, we explore the possibility of using big data from social media in order to understand illicit drug use behaviors. Instagram posts are collected using drug related terms by analyzing the hashtags supplied with each post. A large and dynamic dictionary of frequent illicit drug related slang is used to find these posts. These posts are studied to find common drug consumption behaviors with regard to time of day and week. Furthermore, by studying the accounts followed by the users of drug related posts, we hope to discover common interests shared by drug users.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Dynamic Multi-Relational Chinese Restaurant Process for Analyzing Influences on Users in Social Media,"We study the problem of analyzing influence of various factors affecting individual messages posted in social media. The problem is challenging because of various types of influences propagating through the social media network that act simultaneously on any user. Additionally, the topic composition of the influencing factors and the susceptibility of users to these influences evolve over time. This problem has not studied before, and off-the-shelf models are unsuitable for this purpose. To capture the complex interplay of these various factors, we propose a new non-parametric model called the Dynamic Multi-Relational Chinese Restaurant Process. This accounts for the user network for data generation and also allows the parameters to evolve over time. Designing inference algorithms for this model suited for large scale social-media data is another challenge. To this end, we propose a scalable and multi-threaded inference algorithm based on online Gibbs Sampling. Extensive evaluations on large-scale Twitter and Facebook data show that the extracted topics when applied to authorship and commenting prediction outperform state-of-the-art baselines. More importantly, our model produces valuable insights on topic trends and user personality trends, beyond the capability of existing approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Real-Time Influence Maximization on Dynamic Social Streams,"Influence maximization (IM), which selects a set of $k$ users (called seeds) to maximize the influence spread over a social network, is a fundamental problem in a wide range of applications such as viral marketing and network monitoring. Existing IM solutions fail to consider the highly dynamic nature of social influence, which results in either poor seed qualities or long processing time when the network evolves. To address this problem, we define a novel IM query named Stream Influence Maximization (SIM) on social streams. Technically, SIM adopts the sliding window model and maintains a set of $k$ seeds with the largest influence value over the most recent social actions. Next, we propose the Influential Checkpoints (IC) framework to facilitate continuous SIM query processing. The IC framework creates a checkpoint for each window slide and ensures an $\varepsilon$-approximate solution. To improve its efficiency, we further devise a Sparse Influential Checkpoints (SIC) framework which selectively keeps $O(\frac{\log{N}})$ checkpoints for a sliding window of size $N$ and maintains an $\frac{\varepsilon(1-)}{2}$-approximate solution. Experimental results on both real-world and synthetic datasets confirm the effectiveness and efficiency of our proposed frameworks against the state-of-the-art IM approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding the Political Ideology of Legislators from Social Media Images,"In this paper, we seek to understand how politicians use images to express ideological rhetoric through Facebook images posted by members of the U.S. House and Senate. In the era of social media, politics has become saturated with imagery, a potent and emotionally salient form of political rhetoric which has been used by politicians and political organizations to influence public sentiment and voting behavior for well over a century. To date, however, little is known about how images are used as political rhetoric. Using deep learning techniques to automatically predict Republican or Democratic party affiliation solely from the Facebook photographs of the members of the 114th U.S. Congress, we demonstrate that predicted class probabilities from our model function as an accurate proxy of the political ideology of images along a left-right (liberal-conservative) dimension. After controlling for the gender and race of politicians, our method achieves an accuracy of 59.28% from single photographs and 82.35% when aggregating scores from multiple photographs (up to 150) of the same person. To better understand image content distinguishing liberal from conservative images, we also perform in-depth content analyses of the photographs. Our findings suggest that conservatives tend to use more images supporting status quo political institutions and hierarchy maintenance, featuring individuals from dominant social groups, and displaying greater happiness than liberals.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning,"Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25%. Codes for this paper are released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
StartupBR: Higher Education's Influence on Social Networks and Entrepreneurship in Brazil,"Developing and middle-income countries increasingly empha-size higher education and entrepreneurship in their long-term develop-ment strategy. Our work focuses on the influence of higher education institutions (HEIs) on startup ecosystems in Brazil, an emerging economy. First, we describe regional variability in entrepreneurial network characteristics. Then we examine the influence of elite HEIs in economic hubs on entrepreneur networks. Second, we investigate the influence ofthe academic trajectories of startup founders, including their courses of study and HEIs of origin, on the fundraising capacity of startups. Given the growing capability of social media databases such as Crunchbase and LinkedIn to provide startup and individual-level data, we draw on computational methods to mine data for social network analysis. We find that HEI quality and the maturity of the ecosystem influence startup success. Our network analysis illustrates that elite HEIs have powerful influences on local entrepreneur ecosystems. Surprisingly, while the most nationally prestigious HEIs in the South and Southeast have the longest geographical reach, their network influence still remains local.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Media Impact on Website Ranking,"Internet is fast becoming critically important to commerce, industry and individuals. Search Engine (SE) is the most vital component for communication network and also used for discover information for users or people. Search engine optimization (SEO) is the process that is mostly used to increasing traffic from free, organic or natural listings on search engines and also helps to increase website ranking. It includes techniques like link building, directory submission, classified submission etc. but SMO, on the other hand, is the process of promoting your website on social media platforms. It includes techniques like RSS feeds, social news and bookmarking sites, video and blogging sites, as well as social networking sites, such as Facebook, Twitter, Google+, Tumblr, Pinterest, Instagram etc.Social media optimization is becoming increasingly important for search engine optimization, as search engines are increasingly utilizing the recommendations of users of social networks to rank pages in the search engine result pages. Since it is more difficult to tip the influence the search engines in this way. Social Media Optimization (SMO) may also use to generate traffic on a website, promote your business at the center of social marketing place and increase ranking.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding the Predictive Power of Computational Mechanics and Echo State Networks in Social Media,"There is a large amount of interest in understanding users of social media in order to predict their behavior in this space. Despite this interest, user predictability in social media is not well-understood. To examine this question, we consider a network of fifteen thousand users on Twitter over a seven week period. We apply two contrasting modeling paradigms: computational mechanics and echo state networks. Both methods attempt to model the behavior of users on the basis of their past behavior. We demonstrate that the behavior of users on Twitter can be well-modeled as processes with self-feedback. We find that the two modeling approaches perform very similarly for most users, but that they differ in performance on a small subset of the users. By exploring the properties of these performance-differentiated users, we highlight the challenges faced in applying predictive models to dynamic social data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Reciprocal versus Parasocial Relationships in Online Social Networks,"Many online social networks are fundamentally directed, i.e., they consist of both reciprocal edges (i.e., edges that have already been linked back) and parasocial edges (i.e., edges that haven't been linked back). Thus, understanding the structures and evolutions of reciprocal edges and parasocial ones, exploring the factors that influence parasocial edges to become reciprocal ones, and predicting whether a parasocial edge will turn into a reciprocal one are basic research problems.   However, there have been few systematic studies about such problems. In this paper, we bridge this gap using a novel large-scale Google+ dataset crawled by ourselves as well as one publicly available social network dataset. First, we compare the structures and evolutions of reciprocal edges and those of parasocial edges. For instance, we find that reciprocal edges are more likely to connect users with similar degrees while parasocial edges are more likely to link ordinary users (e.g., users with low degrees) and popular users (e.g., celebrities). However, the impacts of reciprocal edges linking ordinary and popular users on the network structures increase slowly as the social networks evolve. Second, we observe that factors including user behaviors, node attributes, and edge attributes all have significant impacts on the formation of reciprocal edges. Third, in contrast to previous studies that treat reciprocal edge prediction as either a supervised or a semi-supervised learning problem, we identify that reciprocal edge prediction is better modeled as an outlier detection problem. Finally, we perform extensive evaluations with the two datasets, and we show that our proposal outperforms previous reciprocal edge prediction approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Unveiling Political Influence Through Social Media: Network and Causal Dynamics in the 2022 French Presidential Election,"During the 2022 French presidential election, we collected daily Twitter messages on key topics posted by political candidates and their close networks. Using a data-driven approach, we analyze interactions among political parties, identifying central topics that shape the landscape of political debate. Moving beyond traditional correlation analyses, we apply a causal inference technique: Convergent Cross Mapping, to uncover directional influences among political communities, revealing how some parties are more likely to initiate changes in activity while others tend to respond. This approach allows us to distinguish true influence from mere correlation, highlighting asymmetric relationships and hidden dynamics within the social media political network. Our findings demonstrate how specific issues, such as health and foreign policy, act as catalysts for cross-party influence, particularly during critical election phases. These insights provide a novel framework for understanding political discourse dynamics and have practical implications for campaign strategists and media analysts seeking to monitor and respond to shifts in political influence in real time.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Co-evolution Model of Network Structure and User Behavior in Online Social Networks: The Case of Network-Driven Content Generation,"With the rapid growth of online social network sites (SNS), it has become imperative for platform owners and online marketers to investigate what drives content production on these platforms. However, previous research has found it difficult to statistically model these factors from observational data due to the inability to separately assess the effects of network formation and network influence. In this paper, we adopt and enhance an actor-oriented continuous-time model to jointly estimate the co-evolution of the users' social network structure and their content production behavior using a Markov Chain Monte Carlo (MCMC)- based simulation approach. Specifically, we offer a method to analyze non-stationary and continuous behavior with network effects in the presence of observable and unobservable covariates, similar to what is observed in social media ecosystems. Leveraging a unique dataset from a large social network site, we apply our model to data on university students across six months to find that: 1) users tend to connect with others that have similar posting behavior, 2) however, after doing so, users tend to diverge in posting behavior, and 3) peer influences are sensitive to the strength of the posting behavior. Further, our method provides researchers and practitioners with a statistically rigorous approach to analyze network effects in observational data. These results provide insights and recommendations for SNS platforms to sustain an active and viable community.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement,"Understanding true influence in social media requires distinguishing correlation from causation--particularly when analyzing misinformation spread. While existing approaches focus on exposure metrics and network structures, they often fail to capture the causal mechanisms by which external temporal signals trigger engagement. We introduce a novel joint treatment-outcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects. Our approach adapts causal inference techniques from healthcare to estimate Average Treatment Effects (ATE) within the sequential nature of social media interactions, tackling challenges from external confounding signals. Through our experiments on real-world misinformation and disinformation datasets, we show that our models outperform existing benchmarks by 15--22% in predicting engagement across diverse counterfactual scenarios, including exposure adjustment, timing shifts, and varied intervention durations. Case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation, the expert-based empirical influence.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Media and User Privacy,"Online users generate tremendous amounts of data. To better serve users, it is required to share the user-related data among researchers, advertisers and application developers. Publishing such data would raise more concerns on user privacy. To encourage data sharing and mitigate user privacy concerns, a number of anonymization and de-anonymization algorithms have been developed to help protect privacy of users. This paper reviews my doctoral research on online users privacy specifically in social media. In particular, I propose a new adversarial attack specialized for social media data. I further provide a principled way to assess effectiveness of anonymizing different aspects of social media data. My work sheds light on new privacy risks in social media data due to innate heterogeneity of user-generated data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Role of Follow Networks and Twitter's Content Recommender on Partisan Skew and Rumor Exposure during the 2022 U.S. Midterm Election,"Social media platforms shape users' experiences through the algorithmic systems they deploy. In this study, we examine to what extent Twitter's content recommender, in conjunction with a user's social network, impacts the topic, political skew, and reliability of information served on the platform during a high-stakes election. We utilize automated accounts to document Twitter's algorithmically curated and reverse chronological timelines throughout the U.S. 2022 midterm election. We find that the algorithmic timeline measurably influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. Critically, these impacts are mediated by the partisan makeup of one's personal social network, which often exerts greater influence than the algorithm alone. We find that the algorithmic feed decreases the proportion of election content shown to left-leaning accounts, and that it skews content toward right-leaning sources when compared to the reverse chronological feed. We additionally find evidence that the algorithmic system increases the prevalence of election-related rumors for right-leaning accounts, and has mixed effects on the prevalence of low-quality information sources. Our work provides insight into the outcomes of Twitter's complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Fabricating Holiness: Characterizing Religious Misinformation Circulators on Arabic Social Media,"Misinformation is a growing concern in a decade involving critical global events. While social media regulation is mainly dedicated towards the detection and prevention of fake news and political misinformation, there is limited research about religious misinformation which has only been addressed through qualitative approaches. In this work, we study the spread of fabricated quotes (Hadith) that are claimed to belong to Prophet Muhammad (the prophet of Islam) as a case study demonstrating one of the most common religious misinformation forms on Arabic social media. We attempt through quantitative methods to understand the characteristics of social media users who interact with fabricated Hadith. We spotted users who frequently circulate fabricated Hadith and others who frequently debunk it to understand the main differences between the two groups. We used Logistic Regression to automatically predict their behaviors and analyzed its weights to gain insights about the characteristics and interests of each group. We find that both fabricated Hadith circulators and debunkers have generally a lot of ties to religious accounts. However, circulators are identified by many accounts that follow the Shia branch of Islam, Sunni Islamic public figures from the gulf countries, and many Sunni non-professional pages posting Islamic content. On the other hand, debunkers are identified by following academic Islamic scholars from multiple countries and by having more intellectual non-religious interests like charity, politics, and activism.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Under the Conditions of Non-Agenda Ownership: Social Media Users in the 2019 Ukrainian Presidential Elections Campaign,"Owing to its history and challenging circumstances, social networks community in Ukraine is a very interesting polygon for the study of communications in the constantly changing environment, especially in the political discourse. This unique environment requires three dimensions to ascertain the political position of its participant. But 2019 presidential elections made this object even more spectacular. The winner of elections comedian Volodymyr Zelenskyi reached 73% of votes without any issue ownership, with empty agenda, and this influenced the electoral content of social networks and their authors behavior. We saw, that the issue ownership by other candidates succeeds in making their issues more salient in social networks. But the new phenomena, the non-agenda ownership, overcome any ideological influence, especially under the conditions of punishment mechanism applied to old politicians. Analyzing social media content and users behavior in the period between two rounds of elections, we found considerable overlaps between this campaign and the 2016 Trump campaign. We approved the widespread of filter bubbles, negative campaign messages, fake news and conspiracy theories. Active and powerful core of Ukrainian Facebook that was responsible for the Revolution of dignity now became less significant and even turns into the huge filter bubble of active people. We also proved that manipulations and fake news in the environment of private groups may be as much powerful as in a case of classical communication based around the opinion leaders.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social media battle for attention: opinion dynamics on competing networks,"In the age of information abundance, attention is a coveted resource. Social media platforms vigorously compete for users' engagement, influencing the evolution of their opinions on a variety of topics. With recommendation algorithms often accused of creating ""filter bubbles"", where like-minded individuals interact predominantly with one another, it's crucial to understand the consequences of this unregulated attention market. To address this, we present a model of opinion dynamics on a multiplex network. Each layer of the network represents a distinct social media platform, each with its unique characteristics. Users, as nodes in this network, share their opinions across platforms and decide how much time to allocate in each platform depending on its perceived quality. Our model reveals two key findings. i) When examining two platforms - one with a neutral recommendation algorithm and another with a homophily-based algorithm - we uncover that even if users spend the majority of their time on the neutral platform, opinion polarization can persist. ii) By allowing users to dynamically allocate their social energy across platforms in accordance to their homophilic preferences, a further segregation of individuals emerges. While network fragmentation is usually associated with ""echo chambers"", the emergent multi-platform segregation leads to an increase in users' satisfaction without the undesired increase in polarization. These results underscore the significance of acknowledging how individuals gather information from a multitude of sources. Furthermore, they emphasize that policy interventions on a single social media platform may yield limited impact.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs,"The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election,"The development of social media platforms has revolutionized the speed and manner in which information is disseminated, leading to both beneficial and detrimental effects on society. While these platforms facilitate rapid communication, they also accelerate the spread of rumors and extremist speech, impacting public perception and behavior significantly. This issue is particularly pronounced during election periods, where the influence of social media on election outcomes has become a matter of global concern. With the unprecedented number of elections in 2024, against this backdrop, the election ecosystem has encountered unprecedented challenges. This study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks. We have meticulously collected a dataset from PolitiFact and Twitter, focusing on politically relevant rumors. Our approach involves semantic analysis using a fine-tuned BERT model to vectorize text content and construct a directed graph where tweets and comments are nodes, and interactions are edges. The core of our method is a graph neural network, SAGEWithEdgeAttention, which extends the GraphSAGE model by incorporating first-order differences as edge attributes and applying an attention mechanism to enhance feature aggregation. This innovative approach allows for the fine-grained analysis of the complex social network structure, improving rumor detection accuracy. The study concludes that our method significantly outperforms traditional content analysis and time-based models, offering a theoretically sound and practically efficient solution.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Hot Streaks on Social Media,"Measuring the impact and success of human performance is common in various disciplines, including art, science, and sports. Quantifying impact also plays a key role on social media, where impact is usually defined as the reach of a user's content as captured by metrics such as the number of views, likes, retweets, or shares. In this paper, we study entire careers of Twitter users to understand properties of impact. We show that user impact tends to have certain characteristics: First, impact is clustered in time, such that the most impactful tweets of a user appear close to each other. Second, users commonly have 'hot streaks' of impact, i.e., extended periods of high-impact tweets. Third, impact tends to gradually build up before, and fall off after, a user's most impactful tweet. We attempt to explain these characteristics using various properties measured on social media, including the user's network, content, activity, and experience, and find that changes in impact are associated with significant changes in these properties. Our findings open interesting avenues for future research on virality and influence on social media.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Influence of Social Networks on Human Society,This report gives a brief overview of the origin of social networks and their most popular manifestation in the modern era - the Online Social Networks (OSNs) or social media. It further discusses the positive and negative implications of OSNs on human society. The coupling of Data Science and social media (social media mining) is then put forward as a powerful tool to overcome the current challenges and pave the path for futuristic advancements,"cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Identifying the Community Roles of Social Capitalists in the Twitter Network,"In the context of Twitter, social capitalists are specific users trying to increase their number of followers and interactions by any means. These users are not healthy for the Twitter network since they flaw notions of influence and visibility. Indeed, it has recently been observed that they are real and active users that can help malicious users such as spammers gaining influence. Studying their behavior and understanding their position in Twitter is thus of important interest. A recent work provided an efficient way to detect social capitalists using two simple topological measures. Based on this detection method, we study how social capitalists are distributed over Twitter's friend-to-follower network. We are especially interested in analyzing how they are organized, and how their links spread across the network. Answering these questions allows to know whether the social capitalism methods increase the actual visibility on the service. To that aim, we study the position of social capitalists on Twitter w.r.t. the community structure of the network. We base our work on the concept of community role of a node, which describes its position in a network depending on its connectivity at the community level. The topological measures originally defined to characterize these roles consider only some aspects of community-related connectivity and rely on a set of empirically fixed thresholds. We first show the limitations of such measures and then extend and generalize them by considering new aspects of the community-related connectivity. Moreover, we use an unsupervised approach to distinguish the roles, in order to provide more flexibility relatively to the studied system. We then apply our method to the case of social capitalists and show that they are highly visible on Twitter, due to the specific roles they occupy.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Community-Based Efficient Algorithms for User-Driven Competitive Influence Maximization in Social Networks,"Nowadays, people in the modern world communicate with their friends, relatives, and colleagues through the internet. Persons/nodes and communication/edges among them form a network. Social media networks are a type of network where people share their views with the community. There are several models that capture human behavior, such as a reaction to the information received from friends or relatives. The two fundamental models of information diffusion widely discussed in the social networks are the Independent Cascade Model and the Linear Threshold Model. Liu et al. [1] propose a variant of the linear threshold model in their paper title User-driven competitive influence Maximization(UDCIM) in social networks. Authors try to simulate human behavior where they do not make a decision immediately after being influenced, but take a pause for a while, and then they make a final decision. They propose the heuristic algorithms and prove the approximation factor under community constraints( The seed vertices belong to an identical community). Even finding the community is itself an NP-hard problem. In this article, we extend the existing work with algorithms and LP-formation of the problem. We also implement and test the LP-formulated equations on small datasets by using the Gurobi Solver [2]. We furthermore propose one heuristic and one genetic algorithm. The extensive experimentation is carried out on medium to large datasets, and the outcomes of both algorithms are plotted in the results and discussion section.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Randomized experiments to detect and estimate social influence in networks,"Estimation of social influence in networks can be substantially biased in observational studies due to homophily and network correlation in exposure to exogenous events. Randomized experiments, in which the researcher intervenes in the social system and uses randomization to determine how to do so, provide a methodology for credibly estimating of causal effects of social behaviors. In addition to addressing questions central to the social sciences, these estimates can form the basis for effective marketing and public policy.   In this review, we discuss the design space of experiments to measure social influence through combinations of interventions and randomizations. We define an experiment as combination of (1) a target population of individuals connected by an observed interaction network, (2) a set of treatments whereby the researcher will intervene in the social system, (3) a randomization strategy which maps individuals or edges to treatments, and (4) a measurement of an outcome of interest after treatment has been assigned. We review experiments that demonstrate potential experimental designs and we evaluate their advantages and tradeoffs for answering different types of causal questions about social influence. We show how randomization also provides a basis for statistical inference when analyzing these experiments.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Layered social influence promotes multiculturality in the Axelrod model,"Why is our society multicultural? Based on the two mechanisms of homophily and social influence, the classical model for the dissemination of cultures proposed by Axelrod predicts the existence of a fragmented regime where different cultures can coexist in a social network. However, in such model the multicultural regime is achievable only when a high number of cultural traits is present, and is not robust against cultural drift, i.e. the spontaneous mutations of agents' traits. In real systems, social influence is inherently organised in layers, meaning that individuals tend to diversify their connections according to the topic on which they interact. In this work we show that the observed persistence of multiculturality in real-world social systems is a natural consequence of the layered organisation of social influence. We find that the critical number of cultural traits that separates the monocultural and the multicultural regimes depends on the redundancy of pairwise connections across layers. Surprisingly, for low values of structural redundancy the system is always in a multicultural state, independently on the number of traits, and is robust to the presence of cultural drift. Moreover, we show that layered social influence allows the coexistence of different levels of consensus on different topics. The insight obtained from simulations on synthetic graphs are confirmed by the analysis of two real-world social networks, where the multicultural regime persists even for a very small number of cultural traits, suggesting that the layered organisation of social interactions might indeed be at the heart of multicultural societies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Debate on Online Social Networks at the Time of COVID-19: An Italian Case Study,"The COVID-19 pandemic is not only having a heavy impact on healthcare but also changing people's habits and the society we live in. Countries such as Italy have enforced a total lockdown lasting several months, with most of the population forced to remain at home. During this time, online social networks, more than ever, have represented an alternative solution for social life, allowing users to interact and debate with each other. Hence, it is of paramount importance to understand the changing use of social networks brought about by the pandemic. In this paper, we analyze how the interaction patterns around popular influencers in Italy changed during the first six months of 2020, within Instagram and Facebook social networks. We collected a large dataset for this group of public figures, including more than 54 million comments on over 140 thousand posts for these months. We analyze and compare engagement on the posts of these influencers and provide quantitative figures for aggregated user activity. We further show the changes in the patterns of usage before and during the lockdown, which demonstrated a growth of activity and sizable daily and weekly variations. We also analyze the user sentiment through the psycholinguistic properties of comments, and the results testified the rapid boom and disappearance of topics related to the pandemic. To support further analyses, we release the anonymized dataset.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding Information Spreading in Social Media during Hurricane Sandy: User Activity and Network Properties,"Many people use social media to seek information during disasters while lacking access to traditional information sources. In this study, we analyze Twitter data to understand information spreading activities of social media users during hurricane Sandy. We create multiple subgraphs of Twitter users based on activity levels and analyze network properties of the subgraphs. We observe that user information sharing activity follows a power-law distribution suggesting the existence of few highly active nodes in disseminating information and many other nodes being less active. We also observe close enough connected components and isolates at all levels of activity, and networks become less transitive, but more assortative for larger subgraphs. We also analyze the association between user activities and characteristics that may influence user behavior to spread information during a crisis. Users become more active in spreading information if they are centrally placed in the network, less eccentric, and have higher degrees. Our analysis provides insights on how to exploit user characteristics and network properties to spread information or limit the spreading of misinformation during a crisis event.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Can Generative Agent-Based Modeling Replicate the Friendship Paradox in Social Media Simulations?,"Generative Agent-Based Modeling (GABM) is an emerging simulation paradigm that combines the reasoning abilities of Large Language Models with traditional Agent-Based Modeling to replicate complex social behaviors, including interactions on social media. While prior work has focused on localized phenomena such as opinion formation and information spread, its potential to capture global network dynamics remains underexplored. This paper addresses this gap by analyzing GABM-based social media simulations through the lens of the Friendship Paradox (FP), a counterintuitive phenomenon where individuals, on average, have fewer friends than their friends. We propose a GABM framework for social media simulations, featuring generative agents that emulate real users with distinct personalities and interests. Using Twitter datasets on the US 2020 Election and the QAnon conspiracy, we show that the FP emerges naturally in GABM simulations. Consistent with real-world observations, the simulations unveil a hierarchical structure, where agents preferentially connect with others displaying higher activity or influence. Additionally, we find that infrequent connections primarily drive the FP, reflecting patterns in real networks. These findings validate GABM as a robust tool for modeling global social media phenomena and highlight its potential for advancing social science by enabling nuanced analysis of user behavior.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
StopHC: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms,"The mental health of social media users has started more and more to be put at risk by harmful, hateful, and offensive content. In this paper, we propose \textsc{StopHC}, a harmful content detection and mitigation architecture for social media platforms. Our aim with \textsc{StopHC} is to create more secure online environments. Our solution contains two modules, one that employs deep neural network architecture for harmful content detection, and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content. The efficacy of our solution is demonstrated by experiments conducted on two real-world datasets.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
An Exploration of Geo-temporal Characteristics of Users' Reactions on Social Media During the Pandemic,"During the outbreak of the COVID-19 pandemic, social networks become the preeminent medium for communication, social discussion, and entertainment. Social network users are regularly expressing their opinions about the impacts of the coronavirus pandemic. Therefore, social networks serve as a reliable source for studying the topics, emotions, and attitudes of users that are discussed during the pandemic. In this paper, we investigate the reactions and attitudes of people towards topics raised on social media platforms. We collected data of two large-scale COVID-19 datasets from Twitter and Instagram for six and three months, respectively. The paper analyzes the reaction of social network users on different aspects including sentiment analysis, topics detection, emotions, and geo-temporal characteristics of our dataset. We show that the dominant sentiment reactions on social media are neutral while the most discussed topics by social network users are about health issues. The paper examines the countries that attracted more posts and reactions from people, as well as the distribution of health-related topics discussed in the most mentioned countries. We shed light on the temporal shift of topics over countries. Our results show that posts from the top-mentioned countries influence and attract more reaction worldwide than posts from other parts of the world.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Conductance and Social Capital: Modeling and Empirically Measuring Online Social Influence,"Social influence pervades our everyday lives and lays the foundation for complex social phenomena. In a crisis like the COVID-19 pandemic, social influence can determine whether life-saving information is adopted. Existing literature studying online social influence suffers from several drawbacks. First, a disconnect appears between psychology approaches, which are generally performed and tested in controlled lab experiments, and the quantitative methods, which are usually data-driven and rely on network and event analysis. The former are slow, expensive to deploy, and typically do not generalize well to topical issues (such as an ongoing pandemic); the latter often oversimplify the complexities of social influence and ignore psychosocial literature. This work bridges this gap and presents three contributions towards modeling and empirically quantifying online influence. The first contribution is a data-driven Generalized Influence Model that incorporates two novel psychosocial-inspired mechanisms: the conductance of the diffusion network and the social capital distribution. The second contribution is a framework to empirically rank users' social influence using a human-in-the-loop active learning method combined with crowdsourced pairwise influence comparisons. We build a human-labeled ground truth, calibrate our generalized influence model and perform a large-scale evaluation of influence. We find that our generalized model outperforms the current state-of-the-art approaches and corrects the inherent biases introduced by the widely used follower count. As the third contribution, we apply the influence model to discussions around COVID-19. We quantify users' influence, and we tabulate it against their professions. We find that the executives, media, and military are more influential than pandemic-related experts such as life scientists and healthcare professionals.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Is radicalization reinforced by social media censorship?,"Radicalized beliefs, such as those tied to QAnon, Russiagate, and other political conspiracy theories, can lead some individuals and groups to engage in violent behavior, as evidenced in recent months. Understanding the mechanisms by which such beliefs are accepted, spread, and intensified is critical for any attempt to mitigate radicalization and avoid increased political polarization. This article presents and agent-based model of a social media network that enables investigation of the effects of censorship on the amount of dissenting information to which agents become exposed and the certainty of their radicalized views. The model explores two forms of censorship: 1) decentralized censorship-in which individuals can choose to break an online social network tie (unfriend or unfollow) with another individual who transmits conflicting beliefs and 2) centralized censorship-in which a single authority can ban an individual from the social media network for spreading a certain type of belief. This model suggests that both forms of censorship increase certainty in radicalized views by decreasing the amount of dissent to which an agent is exposed, but centralized ""banning"" of individuals has the strongest effect on radicalization.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Dataset of Coordinated Cryptocurrency-Related Social Media Campaigns,"The rise in adoption of cryptoassets has brought many new and inexperienced investors in the cryptocurrency space. These investors can be disproportionally influenced by information they receive online, and particularly from social media. This paper presents a dataset of crypto-related bounty events and the users that participate in them. These events coordinate social media campaigns to create artificial ""hype"" around a crypto project in order to influence the price of its token. The dataset consists of information about 15.8K cross-media bounty events, 185K participants, 10M forum comments and 82M social media URLs collected from the Bounties(Altcoins) subforum of the BitcoinTalk online forum from May 2014 to December 2022. We describe the data collection and the data processing methods employed and we present a basic characterization of the dataset. Furthermore, we discuss potential research opportunities afforded by the dataset across many disciplines and we highlight potential novel insights into how the cryptocurrency industry operates and how it interacts with its audience.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Detecting Ideal Instagram Influencer Using Social Network Analysis,"Social Media is a key aspect of modern society where people share their thoughts, views, feelings and sentiments. Over the last few years, the inflation in popularity of social media has resulted in a monumental increase in data. Users use this medium to express their thoughts, feelings, and opinions on a wide variety of subjects, including politics and celebrities. Social Media has thus evolved into a lucrative platform for companies to expand their scope and improve their prospects. The paper focuses on social network analysis (SNA) for a real-world online marketing strategy. The study contributes by comparing various centrality measures to identify the most central nodes in the network and uses a linear threshold model to understand the spreading behaviour of individual users. In conclusion, the paper correlates different centrality measures and spreading behaviour to identify the most influential user in the network","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Gender Asymmetries in Reality and Fiction: The Bechdel Test of Social Media,"The subjective nature of gender inequality motivates the analysis and comparison of data from real and fictional human interaction. We present a computational extension of the Bechdel test: A popular tool to assess if a movie contains a male gender bias, by looking for two female characters who discuss about something besides a man. We provide the tools to quantify Bechdel scores for both genders, and we measure them in movie scripts and large datasets of dialogues between users of MySpace and Twitter. Comparing movies and users of social media, we find that movies and Twitter conversations have a consistent male bias, which does not appear when analyzing MySpace. Furthermore, the narrative of Twitter is closer to the movies that do not pass the Bechdel test than to those that pass it.   We link the properties of movies and the users that share trailers of those movies. Our analysis reveals some particularities of movies that pass the Bechdel test: Their trailers are less popular, female users are more likely to share them than male users, and users that share them tend to interact less with male users. Based on our datasets, we define gender independence measurements to analyze the gender biases of a society, as manifested through digital traces of online behavior. Using the profile information of Twitter users, we find larger gender independence for urban users in comparison to rural ones. Additionally, the asymmetry between genders is larger for parents and lower for students. Gender asymmetry varies across US states, increasing with higher average income and latitude. This points to the relation between gender inequality and social, economical, and cultural factors of a society, and how gender roles exist in both fictional narratives and public online dialogues.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Media Use is Predictable from App Sequences: Using LSTM and Transformer Neural Networks to Model Habitual Behavior,"The present paper introduces a novel approach to studying social media habits through predictive modeling of sequential smartphone user behaviors. While much of the literature on media and technology habits has relied on self-report questionnaires and simple behavioral frequency measures, we examine an important yet understudied aspect of media and technology habits: their embeddedness in repetitive behavioral sequences. Leveraging Long Short-Term Memory (LSTM) and transformer neural networks, we show that (i) social media use is predictable at the within and between-person level and that (ii) there are robust individual differences in the predictability of social media use. We examine the performance of several modeling approaches, including (i) global models trained on the pooled data from all participants, (ii) idiographic person-specific models, and (iii) global models fine-tuned on person-specific data. Neither person-specific modeling nor fine-tuning on person-specific data substantially outperformed the global models, indicating that the global models were able to represent a variety of idiosyncratic behavioral patterns. Additionally, our analyses reveal that the person-level predictability of social media use is not substantially related to the frequency of smartphone use in general or the frequency of social media use, indicating that our approach captures an aspect of habits that is distinct from behavioral frequency. Implications for habit modeling and theoretical development are discussed.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Regret-Aware Framework for Effective Social Media Advertising,"Social Media Advertisement has emerged as an effective approach for promoting the brands of a commercial house. Hence, many of them have started using this medium to maximize the influence among the users and create a customer base. In recent times, several companies have emerged as Influence Provider who provides views of advertisement content depending on the budget provided by the commercial house. In this process, the influence provider tries to exploit the information diffusion phenomenon of a social network, and a limited number of highly influential users are chosen and activated initially. Due to diffusion phenomenon, the hope is that the advertisement content will reach a large number of people. Now, consider that a group of advertisers is approaching an influence provider with their respective budget and influence demand. Now, for any advertiser, if the influence provider provides more or less influence, it will be a loss for the influence provider. It is an important problem from the point of view of influence provider, as it is important to allocate the seed nodes to the advertisers so that the loss is minimized. In this paper, we study this problem, which we formally referred to as Regret Minimization in Social Media Advertisement Problem. We propose a noble regret model that captures the aggregated loss encountered by the influence provider while allocating the seed nodes. We have shown that this problem is a computationally hard problem to solve. We have proposed three efficient heuristic solutions to solve our problem, analyzed to understand their time and space requirements. They have been implemented with real world social network datasets, and several experiments have been conducted and compared to many baseline methods.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Estimating Emotion Contagion on Social Media via Localized Diffusion in Dynamic Graphs,"We present a computational approach for estimating emotion contagion on social media networks. Built on a foundation of psychology literature, our approach estimates the degree to which the perceivers' emotional states (positive or negative) start to match those of the expressors, based on the latter's content. We use a combination of deep learning and social network analysis to model emotion contagion as a diffusion process in dynamic social network graphs, taking into consideration key aspects like causality, homophily, and interference. We evaluate our approach on user behavior data obtained from a popular social media platform for sharing short videos. We analyze the behavior of 48 users over a span of 8 weeks (over 200k audio-visual short posts analyzed) and estimate how contagious the users with whom they engage with are on social media. As per the theory of diffusion, we account for the videos a user watches during this time (inflow) and the daily engagements; liking, sharing, downloading or creating new videos (outflow) to estimate contagion. To validate our approach and analysis, we obtain human feedback on these 48 social media platform users with an online study by collecting responses of about 150 participants. We report users who interact with more number of creators on the platform are 12% less prone to contagion, and those who consume more content of `negative' sentiment are 23% more prone to contagion. We will publicly release our code upon acceptance.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Bulimia Nervosa in the Digital Age: The Role of Social Media,"Globalization has fundamentally reshaped societal dynamics, influencing how individuals interact and perceive themselves and others. One significant consequence is the evolving landscape of eating disorders such as bulimia nervosa (BN), which are increasingly driven not just by internal psychological factors but by broader sociocultural and digital contexts. While mathematical modeling has provided valuable insights, traditional frameworks often fall short in capturing the nuanced roles of social contagion, digital media, and adaptive behavior. This review synthesizes two decades of quantitative modeling efforts, including compartmental, stochastic, and delay-based approaches. We spotlight foundational work that conceptualizes BN as a socially transmissible condition and identify critical gaps, especially regarding the intensifying impact of social media. Drawing on behavioral epidemiology and the adaptive behavior framework by Fenichel et al., we advocate for a new generation of models that incorporate feedback mechanisms, content-driven influence functions, and dynamic network effects. This work outlines a roadmap for developing more realistic, data-informed models that can guide effective public health interventions in the digital era.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Effects of Enterprise Social Media on Communication Networks,"Enterprise social media platforms (ESMPs) are web-based platforms with standard social media functionality, e.g., communicating with others, posting links and files, liking content, etc., yet all users are part of the same company. The first contribution of this work is the use of a difference-in-differences analysis of $99$ companies to measure the causal impact of ESMPs on companies' communication networks across the full spectrum of communication technologies used within companies: email, instant messaging, and ESMPs. Adoption caused companies' communication networks to grow denser and more well-connected by adding new, novel ties that often, but not exclusively, involve communication from one to many employees. Importantly, some new ties also bridge otherwise separate parts of the corporate communication network. The second contribution of this work, utilizing data on Microsoft's own communication network, is understanding how these communication technologies connect people across the corporate hierarchy. Compared to email and instant messaging, ESMPs excel at connecting nodes distant in the corporate hierarchy both vertically (between leaders and employees) and horizontally (between employees in similar roles but different sectors). Also, influence in ESMPs is more `democratic' than elsewhere, with high-influence nodes well-distributed across the corporate hierarchy. Overall, our results suggest that ESMPs boost information flow within companies and increase employees' attention to what is happening outside their immediate working group, above and beyond email and instant messaging.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Temporal influence over the Last.fm social network,"Several recent results show the influence of social contacts to spread certain properties over the network, but others question the methodology of these experiments by proposing that the measured effects may be due to homophily or a shared environment. In this paper we justify the existence of the social influence by considering the temporal behavior of Last.fm users. In order to clearly distinguish between friends sharing the same interest, especially since Last.fm recommends friends based on similarity of taste, we separated the timeless effect of similar taste from the temporal impulses of immediately listening to the same artist after a friend. We measured strong increase of listening to a completely new artist in a few hours period after a friend compared to non-friends representing a simple trend or external influence. In our experiment to eliminate network independent elements of taste, we improved collaborative filtering and trend based methods by blending with simple time aware recommendations based on the influence of friends. Our experiments are carried over the two-year ""scrobble"" history of 70,000 Last.fm users.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Critical Impact of Social Networks Infodemic on Defeating Coronavirus COVID-19 Pandemic: Twitter-Based Study and Research Directions,"News creation and consumption has been changing since the advent of social media. An estimated 2.95 billion people in 2019 used social media worldwide. The widespread of the Coronavirus COVID-19 resulted with a tsunami of social media. Most platforms were used to transmit relevant news, guidelines and precautions to people. According to WHO, uncontrolled conspiracy theories and propaganda are spreading faster than the COVID-19 pandemic itself, creating an infodemic and thus causing psychological panic, misleading medical advises, and economic disruption. Accordingly, discussions have been initiated with the objective of moderating all COVID-19 communications, except those initiated from trusted sources such as the WHO and authorized governmental entities. This paper presents a large-scale study based on data mined from Twitter. Extensive analysis has been performed on approximately one million COVID-19 related tweets collected over a period of two months. Furthermore, the profiles of 288,000 users were analyzed including unique users profiles, meta-data and tweets context. The study noted various interesting conclusions including the critical impact of the (1) exploitation of the COVID-19 crisis to redirect readers to irrelevant topics and (2) widespread of unauthentic medical precautions and information. Further data analysis revealed the importance of using social networks in a global pandemic crisis by relying on credible users with variety of occupations, content developers and influencers in specific fields. In this context, several insights and findings have been provided while elaborating computing and non-computing implications and research directions for potential solutions and social networks management strategies during crisis periods.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Maximization in Social Networks using Discretized Harris Hawks Optimization Algorithm and Neighbour Scout Strategy,"Influence Maximization (IM) is the task of determining k optimal influential nodes in a social network to maximize the influence spread using a propagation model. IM is a prominent problem for viral marketing, and helps significantly in social media advertising. However, developing effective algorithms with minimal time complexity for real-world social networks still remains a challenge. While traditional heuristic approaches have been applied for IM, they often result in minimal performance gains over the computationally expensive Greedy-based and Reverse Influence Sampling-based approaches. In this paper, we propose the discretization of the nature-inspired Harris Hawks Optimisation meta-heuristic algorithm using community structures for optimal selection of seed nodes for influence spread. In addition to Harris Hawks intelligence, we employ a neighbour scout strategy algorithm to avoid blindness and enhance the searching ability of the hawks. Further, we use a candidate nodes-based random population initialization approach, and these candidate nodes aid in accelerating the convergence process for the entire populace. We evaluate the efficacy of our proposed DHHO approach on six social networks using the Independent Cascade model for information diffusion. We observe that DHHO is comparable or better than competing meta-heuristic approaches for Influence Maximization across five metrics, and performs noticeably better than competing heuristic approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Election Polls on Social Media: Prevalence, Biases, and Voter Fraud Beliefs","Social media platforms allow users to create polls to gather public opinion on diverse topics. However, we know little about what such polls are used for and how reliable they are, especially in significant contexts like elections. Focusing on the 2020 presidential elections in the U.S., this study shows that outcomes of election polls on Twitter deviate from election results despite their prevalence. Leveraging demographic inference and statistical analysis, we find that Twitter polls are disproportionately authored by older males and exhibit a large bias towards candidate Donald Trump relative to representative mainstream polls. We investigate potential sources of biased outcomes from the point of view of inauthentic, automated, and counter-normative behavior. Using social media experiments and interviews with poll authors, we identify inconsistencies between public vote counts and those privately visible to poll authors, with the gap potentially attributable to purchased votes. We also find that Twitter accounts participating in election polls are more likely to be bots, and election poll outcomes tend to be more biased, before the election day than after. Finally, we identify instances of polls spreading voter fraud conspiracy theories and estimate that a couple thousand of such polls were posted in 2020. The study discusses the implications of biased election polls in the context of transparency and accountability of social media platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding the Hoarding Behaviors during the COVID-19 Pandemic using Large Scale Social Media Data,"The COVID-19 pandemic has affected people's lives around the world on an unprecedented scale. We intend to investigate hoarding behaviors in response to the pandemic using large-scale social media data. First, we collect hoarding-related tweets shortly after the outbreak of the coronavirus. Next, we analyze the hoarding and anti-hoarding patterns of over 42,000 unique Twitter users in the United States from March 1 to April 30, 2020, and dissect the hoarding-related tweets by age, gender, and geographic location. We find the percentage of females in both hoarding and anti-hoarding groups is higher than that of the general Twitter user population. Furthermore, using topic modeling, we investigate the opinions expressed towards the hoarding behavior by categorizing these topics according to demographic and geographic groups. We also calculate the anxiety scores for the hoarding and anti-hoarding related tweets using a lexical approach. By comparing their anxiety scores with the baseline Twitter anxiety score, we reveal further insights. The LIWC anxiety mean for the hoarding-related tweets is significantly higher than the baseline Twitter anxiety mean. Interestingly, beer has the highest calculated anxiety score compared to other hoarded items mentioned in the tweets.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Unsupervised detection of coordinated fake-follower campaigns on social media,"Automated social media accounts, known as bots, are increasingly recognized as key tools for manipulative online activities. These activities can stem from coordination among several accounts and these automated campaigns can manipulate social network structure by following other accounts, amplifying their content, and posting messages to spam online discourse. In this study, we present a novel unsupervised detection method designed to target a specific category of malicious accounts designed to manipulate user metrics such as online popularity. Our framework identifies anomalous following patterns among all the followers of a social media account. Through the analysis of a large number of accounts on the Twitter platform (rebranded as Twitter after the acquisition of Elon Musk), we demonstrate that irregular following patterns are prevalent and are indicative of automated fake accounts. Notably, we find that these detected groups of anomalous followers exhibit consistent behavior across multiple accounts. This observation, combined with the computational efficiency of our proposed approach, makes it a valuable tool for investigating large-scale coordinated manipulation campaigns on social media platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Can We Fix Social Media? Testing Prosocial Interventions using Generative Social Simulation,"Social media platforms have been widely linked to societal harms, including rising polarization and the erosion of constructive debate. Can these problems be mitigated through prosocial interventions? We address this question using a novel method - generative social simulation - that embeds Large Language Models within Agent-Based Models to create socially rich synthetic platforms. We create a minimal platform where agents can post, repost, and follow others. We find that the resulting following-networks reproduce three well-documented dysfunctions: (1) partisan echo chambers; (2) concentrated influence among a small elite; and (3) the amplification of polarized voices - creating a 'social media prism' that distorts political discourse. We test six proposed interventions, from chronological feeds to bridging algorithms, finding only modest improvements - and in some cases, worsened outcomes. These results suggest that core dysfunctions may be rooted in the feedback between reactive engagement and network growth, raising the possibility that meaningful reform will require rethinking the foundational dynamics of platform architecture.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Going viral: Optimizing Discount Allocation in Social Networks for Influence Maximization,"In this paper, we investigate the discount allocation problem in social networks. It has been reported that 40\% of consumers will share an email offer with their friend and 28\% of consumers will share deals via social media platforms. What does this mean for a business? Essentially discounts should not just be treated as short term solutions to attract individual customer, instead, allocating discounts to a small fraction of users (called seed users) may trigger a large cascade in a social network. This motivates us to study the influence maximization discount allocation problem: given a social network and budget, we need to decide to which initial set users should offer the discounts, and how much should the discounts be worth. Our goal is to maximize the number of customers who finally adopt the target product. We investigate this problem under both non-adaptive and adaptive settings. In the first setting, we have to commit the set of seed users and corresponding discounts all at once in advance. In the latter case, the decision process is performed in a sequential manner, and each seed user that is picked provides the feedback on the discount, or, in other words, reveals whether or not she will adopt the discount. We propose a simple greedy policy with an approximation ratio of $\frac{1}{2}(1 - 1/e)$ in non-adaptive setting. For the significantly more complex adaptive setting, we propose an adaptive greedy policy with bounded approximation ratio in terms of expected utility.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?,"This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Operations in Social Networks,"An important part of online activities are intended to control the public opinion and behavior, being considered currently a global threat. This article identifies and conceptualizes seven online strategies employed in social media influence operations. These procedures are quantified through the analysis of 80 incidents of foreign information manipulation and interference (FIMI), estimating their real-world usage and combination. Finally, we suggest future directions for research on influence operations.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Socially-Informed Content Analysis of Online Human Behavior,"The explosive growth of social media has not only revolutionized communication but also brought challenges such as political polarization, misinformation, hate speech, and echo chambers. This dissertation employs computational social science techniques to investigate these issues, understand the social dynamics driving negative online behaviors, and propose data-driven solutions for healthier digital interactions. I begin by introducing a scalable social network representation learning method that integrates user-generated content with social connections to create unified user embeddings, enabling accurate prediction and visualization of user attributes, communities, and behavioral propensities. Using this tool, I explore three interrelated problems: 1) COVID-19 discourse on Twitter, revealing polarization and asymmetric political echo chambers; 2) online hate speech, suggesting the pursuit of social approval motivates toxic behavior; and 3) moral underpinnings of COVID-19 discussions, uncovering patterns of moral homophily and echo chambers, while also indicating moral diversity and plurality can improve message reach and acceptance across ideological divides. These findings contribute to the advancement of computational social science and provide a foundation for understanding human behavior through the lens of social interactions and network homophily.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Online Social Network Data-Driven Early Detection on Short-Form Video Addiction,"Short-form video (SFV) has become a globally popular form of entertainment in recent years, appearing on major social media platforms. However, current research indicate that short video addiction can lead to numerous negative effects on both physical and psychological health, such as decreased attention span and reduced motivation to learn. Additionally, Short-form Video Addiction (SFVA) has been linked to other issues such as a lack of psychological support in real life, family or academic pressure, and social anxiety. Currently, the detection of SFVA typically occurs only after users experience negative consequences. Therefore, we aim to construct a short video addiction dataset based on social network behavior and design an early detection framework for SFVA. Previous mental health detection research on online social media has mostly focused on detecting depression and suicidal tendency. In this study, we propose the first early detection framework for SFVA EarlySD. We first introduce large language models (LLMs) to address the common issues of sparsity and missing data in graph datasets. Meanwhile, we categorize social network behavior data into different modalities and design a heterogeneous social network structure as the primary basis for detecting SFVA. We conduct a series of quantitative analysis on short video addicts using our self-constructed dataset, and perform extensive experiments to validate the effectiveness of our method EarlySD, using social data and heterogeneous social graphs in the detection of short video addiction.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploring Text Virality in Social Networks,"This paper aims to shed some light on the concept of virality - especially in social networks - and to provide new insights on its structure. We argue that: (a) virality is a phenomenon strictly connected to the nature of the content being spread, rather than to the influencers who spread it, (b) virality is a phenomenon with many facets, i.e. under this generic term several different effects of persuasive communication are comprised and they only partially overlap. To give ground to our claims, we provide initial experiments in a machine learning framework to show how various aspects of virality can be independently predicted according to content features.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Network Fusion and Mining: A Survey,"Looking from a global perspective, the landscape of online social networks is highly fragmented. A large number of online social networks have appeared, which can provide users with various types of services. Generally, the information available in these online social networks is of diverse categories, which can be represented as heterogeneous social networks (HSN) formally. Meanwhile, in such an age of online social media, users usually participate in multiple online social networks simultaneously to enjoy more social networks services, who can act as bridges connecting different networks together. So multiple HSNs not only represent information in single network, but also fuse information from multiple networks.   Formally, the online social networks sharing common users are named as the aligned social networks, and these shared users who act like anchors aligning the networks are called the anchor users. The heterogeneous information generated by users' social activities in the multiple aligned social networks provides social network practitioners and researchers with the opportunities to study individual user's social behaviors across multiple social platforms simultaneously. This paper presents a comprehensive survey about the latest research works on multiple aligned HSNs studies based on the broad learning setting, which covers 5 major research tasks, i.e., network alignment, link prediction, community detection, information diffusion and network embedding respectively.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Meta Path-based Approach for Rumor Detection on Social Media,"The prominent role of social media in people's daily lives has made them more inclined to receive news through social networks than traditional sources. This shift in public behavior has opened doors for some to diffuse fake news on social media; and subsequently cause negative economic, political, and social consequences as well as distrust among the public.   There are many proposed methods to solve the rumor detection problem, most of which do not take full advantage of the heterogeneous nature of news propagation networks. With this intention, we considered a previously proposed architecture as our baseline and performed the idea of structural feature extraction from the heterogeneous rumor propagation over its architecture using the concept of meta path-based embeddings. We named our model Meta Path-based Global Local Attention Network (MGLAN). Extensive experimental analysis on three state-of-the-art datasets has demonstrated that MGLAN outperforms other models by capturing node-level discrimination to different node types.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Information Transfer in Social Media,"Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior, characterizing patterns of information diffusion, and identifying influential individuals. In this paper we suggest a measure of causal relationships between nodes based on the information-theoretic notion of transfer entropy, or information transfer. This theoretically grounded measure is based on dynamic information, captures fine-grain notions of influence, and admits a natural, predictive interpretation. Causal networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics. We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures. In addition to altering our notion of who is influential, transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Fundamental structures of dynamic social networks,"Social systems are in a constant state of flux with dynamics spanning from minute-by-minute changes to patterns present on the timescale of years. Accurate models of social dynamics are important for understanding spreading of influence or diseases, formation of friendships, and the productivity of teams. While there has been much progress on understanding complex networks over the past decade, little is known about the regularities governing the micro-dynamics of social networks. Here we explore the dynamic social network of a densely-connected population of approximately 1000 individuals and their interactions in the network of real-world person-to-person proximity measured via Bluetooth, as well as their telecommunication networks, online social media contacts, geo-location, and demographic data. These high-resolution data allow us to observe social groups directly, rendering community detection unnecessary. Starting from 5-minute time slices we uncover dynamic social structures expressed on multiple timescales. On the hourly timescale, we find that gatherings are fluid, with members coming and going, but organized via a stable core of individuals. Each core represents a social context. Cores exhibit a pattern of recurring meetings across weeks and months, each with varying degrees of regularity. Taken together, these findings provide a powerful simplification of the social network, where cores represent fundamental structures expressed with strong temporal and spatial regularity. Using this framework, we explore the complex interplay between social and geospatial behavior, documenting how the formation of cores are preceded by coordination behavior in the communication networks, and demonstrating that social behavior can be predicted with high precision.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities,"Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
On the Impact of Social Media Recommendations on Opinion Consensus,"We consider a discrete opinion formation problem in a setting where agents are influenced by both information diffused by their social relations and from recommendations received directly from the social media manager. We study how the ""strength"" of the influence of the social media and the homophily ratio affect the probability of the agents of reaching a consensus and how these factors can determine the type of consensus reached. In a simple 2-symmetric block model we prove that agents converge either to a consensus or to a persistent disagreement. In particular, we show that when the homophily ratio is large, the social media has a very low capacity of determining the outcome of the opinion dynamics. On the other hand, when the homophily ratio is low, the social media influence can have an important role on the dynamics, either by making harder to reach a consensus or inducing it on extreme opinions. Finally, in order to extend our analysis to more general and realistic settings we give some experimental evidences that our results still hold on general networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Unpacking Discourses on Childbirth and Parenthood in Popular Social Media Platforms Across China, Japan, and South Korea","Social media use has been shown to be associated with low fertility desires. However, we know little about the discourses surrounding childbirth and parenthood that people consume online. We analyze 219,127 comments on 668 short videos related to reproduction and parenthood from Douyin and Tiktok in China, South Korea, and Japan, a region famous for its extremely low fertility level, to examine the topics and sentiment expressed online. BERTopic model is used to assist thematic analysis, and a large language model QWen is applied to label sentiment. We find that comments focus on childrearing costs in all countries, utility of children, particularly in Japan and South Korea, and individualism, primarily in China. Comments from Douyin exhibit the strongest anti-natalist sentiments, while the Japanese and Korean comments are more neutral. Short video characteristics, such as their stances or account type, significantly influence the responses, alongside regional socioeconomic indicators, including GDP, urbanization, and population sex ratio. This work provides one of the first comprehensive analyses of online discourses on family formation via popular algorithm-fed video sharing platforms in regions experiencing low fertility rates, making a valuable contribution to our understanding of the spread of family values online.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Blocking Adversarial Influence in Social Networks,"While social networks are widely used as a media for information diffusion, attackers can also strategically employ analytical tools, such as influence maximization, to maximize the spread of adversarial content through the networks. We investigate the problem of limiting the diffusion of negative information by blocking nodes and edges in the network. We formulate the interaction between the defender and the attacker as a Stackelberg game where the defender first chooses a set of nodes to block and then the attacker selects a set of seeds to spread negative information from. This yields an extremely complex bi-level optimization problem, particularly since even the standard influence measures are difficult to compute. Our approach is to approximate the attacker's problem as the maximum node domination problem. To solve this problem, we first develop a method based on integer programming combined with constraint generation. Next, to improve scalability, we develop an approximate solution method that represents the attacker's problem as an integer program, and then combines relaxation with duality to yield an upper bound on the defender's objective that can be computed using mixed integer linear programming. Finally, we propose an even more scalable heuristic method that prunes nodes from the consideration set based on their degree. Extensive experiments demonstrate the efficacy of our approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks,"Social media has been developing rapidly in public due to its nature of spreading new information, which leads to rumors being circulated. Meanwhile, detecting rumors from such massive information in social media is becoming an arduous challenge. Therefore, some deep learning methods are applied to discover rumors through the way they spread, such as Recursive Neural Network (RvNN) and so on. However, these deep learning methods only take into account the patterns of deep propagation but ignore the structures of wide dispersion in rumor detection. Actually, propagation and dispersion are two crucial characteristics of rumors. In this paper, we propose a novel bi-directional graph model, named Bi-Directional Graph Convolutional Networks (Bi-GCN), to explore both characteristics by operating on both top-down and bottom-up propagation of rumors. It leverages a GCN with a top-down directed graph of rumor spreading to learn the patterns of rumor propagation, and a GCN with an opposite directed graph of rumor diffusion to capture the structures of rumor dispersion. Moreover, the information from the source post is involved in each layer of GCN to enhance the influences from the roots of rumors. Encouraging empirical results on several benchmarks confirm the superiority of the proposed method over the state-of-the-art approaches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Networks through the Prism of Cognition,"Human relations are driven by social events-people interact, exchange information, share knowledge and emotions, and gather news from mass media. These events leave traces in human memory, the strength of which depends on cognitive factors such as emotions or attention span. Each trace continuously weakens over time unless another related event activity strengthens it. Here, we introduce a novel cognition-driven social network (CogSNet) model that accounts for cognitive aspects of social perception. The model explicitly represents each social interaction as a trace in human memory with its corresponding dynamics. The strength of the trace is the only measure of the influence that the interactions had on a person. For validation, we apply our model to NetSense data on social interactions among university students. The results show that CogSNet significantly improves the quality of modeling of human interactions in social networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Decision dynamics in complex networks subject to mass media and social contact transmission mechanisms,"The dynamics of decisions in complex networks is studied within a Markov process framework using numerical simulations combined with mathematical insight into the process mechanisms. A mathematical discrete-time model is derived based on a set of basic assumptions on the convincing mechanisms associated to two opinions. The model is analyzed with respect to multiplicity of critical points, illustrating in this way the main behavior to be expected in the network. Particular interest is focussed on the effect of social network and exogenous mass media-based influences on the decision behavior. A set of numerical simulation results is provided illustrating how these mechanisms impact the final decision results. The analysis reveals (i) the presence of fixed-point multiplicity (with a maximum of four different fixed points), multistability, and sensitivity with respect to process parameters, and (ii) that mass media have a strong impact on the decision behavior.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Who creates trends in online social media: The crowd or opinion leaders?,"Trends in online social media always reflect the collective attention of a vast number of individuals across the network. For example, Internet slang words can be ubiquitous because of social memes and online contagions in an extremely short period. From Weibo, a Twitter-like service in China, we find that the adoption of popular Internet slang words experiences two peaks in its temporal evolution, in which the former is relatively much lower than the latter. This interesting phenomenon in fact provides a decent window to disclose essential factors that drive the massive diffusion underlying trends in online social media. Specifically, the in-depth comparison between diffusions represented by different peaks suggests that more attention from the crowd at early stage of the propagation produces large-scale coverage, while the dominant participation of opinion leaders at the early stage just leads to popularity of small scope. Our results quantificationally challenge the conventional hypothesis of influentials. And the implications of these novel findings for marketing practice and influence maximization in social networks are also discussed.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
How social networks influence human behavior: An integrated latent space approach for differential social influence,"How social networks influence human behavior has been an interesting topic in applied research. Existing methods often utilized scale-level behavioral data to estimate the influence of a social network on human behavior. This study proposes a novel approach to studying social influence that utilizes item-level behavioral measures. Under the latent space modeling framework, we integrate the two interaction maps for respondents' social network data and item-level behavior measures. The interaction map visualizes the association between the latent homophily of the respondents and their behaviors measured at the item level in a low-dimensional latent space, revealing the potential, differential social influence effects across specific behaviors measured at the item level. We also measure overall social influence as the impact of the interaction map configuration contributed by the social network data on the behavior data. The performance and properties of the proposed approach are evaluated via simulation studies. We apply the proposed model to an empirical dataset to demonstrate how the students' friendship network influences their participation in school activities.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Homophily Principle in Social Network Analysis,"In recent years, social media has become a ubiquitous and integral part of social networking. One of the major attentions made by social researchers is the tendency of like-minded people to interact with one another in social groups, a concept which is known as Homophily. The study of homophily can provide eminent insights into the flow of information and behaviors within a society and this has been extremely useful in analyzing the formations of online communities. In this paper, we review and survey the effect of homophily in social networks and summarize the state of art methods that has been proposed in the past years to identify and measure the effect of homophily in multiple types of social networks and we conclude with a critical discussion of open challenges and directions for future research.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Networks Analysis to Retrieve Critical Comments on Online Platforms,"Social networks are rich source of data to analyze user habits in all aspects of life. User's behavior is decisive component of a health system in various countries. Promoting good behavior can improve the public health significantly. In this work, we develop a new model for social network analysis by using text analysis approach. We define each user reaction to global pandemic with analyzing his online behavior. Clustering a group of online users with similar habits, help to find how virus spread in different societies. Promoting the healthy life style in the high risk online users of social media have significant effect on public health and reducing the effect of global pandemic. In this work, we introduce a new approach to clustering habits based on user activities on social media in the time of pandemic and recommend a machine learning model to promote health in the online platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours,"Disinformation campaigns on social media, involving coordinated activities from malicious accounts towards manipulating public opinion, have become increasingly prevalent. Existing approaches to detect coordinated accounts either make very strict assumptions about coordinated behaviours, or require part of the malicious accounts in the coordinated group to be revealed in order to detect the rest. To address these drawbacks, we propose a generative model, AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group Estimation) which jointly models account activities and hidden group behaviours based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to capture inherent characteristics of coordination which is, accounts that coordinate must strongly influence each other's activities, and collectively appear anomalous from normal accounts. To address the challenges of optimizing the proposed model, we provide a bilevel optimization algorithm with theoretical guarantee on convergence. We verified the effectiveness of the proposed method and training algorithm on real-world social network data collected from Twitter related to coordinated campaigns from Russia's Internet Research Agency targeting the 2016 U.S. Presidential Elections, and to identify coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned model, we find that the average influence between coordinated account pairs is the highest.On COVID-19, we found coordinated group spreading anti-vaccination, anti-masks conspiracies that suggest the pandemic is a hoax and political scam.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Stochastic Models Predict User Behavior in Social Media,"User response to contributed content in online social media depends on many factors. These include how the site lays out new content, how frequently the user visits the site, how many friends the user follows, how active these friends are, as well as how interesting or useful the content is to the user. We present a stochastic modeling framework that relates a user's behavior to details of the site's user interface and user activity and describe a procedure for estimating model parameters from available data. We apply the model to study discussions of controversial topics on Twitter, specifically, to predict how followers of an advocate for a topic respond to the advocate's posts. We show that a model of user behavior that explicitly accounts for a user transitioning through a series of states before responding to an advocate's post better predicts response than models that fail to take these states into account. We demonstrate other benefits of stochastic models, such as their ability to identify users who are highly interested in advocate's posts.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Analyzing mob dynamics in social media networks using epidemiology model,"Epidemiological models, traditionally used to study disease spread, can effectively analyze mob behavior on social media by treating ideas, sentiments, or behaviors as ``contagions"" that propagate through user networks. In this research, we introduced a mathematical model to analyze social behavior related to COVID-19 spread by examining Twitter activity from April 2020 to June 2020. Our analysis focused on key terms such as ``lockdown"" and ``quarantine"" to track public sentiment and engagement trends during the pandemic. The threshold number $\Re_{0}$ is derived, and the stability of the steady states is established. Numerical simulations and sensitivity analysis of applicable parameters are carried out. The results show that negative sentiment on Twitter has less influence on COVID-19 spread compared to positive sentiment. However, the effect of negative sentiment on the spread of COVID-19 remains remarkably strong. Moreover, we use the Caputo operator with different parameter values to study the impact of social media platforms on the transmission of COVID-19 diseases.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
KIBS Innovative Entrepreneurship Networks on Social Media,"The analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature, especially in the context of Knowledge Intensive Business Services (KIBS). Therefore, this paper focuses on bridging this gap by applying text mining and sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media. Finally, we present and analyze the results of our quantitative analysis of 23.483 posts based on eleven Spanish and Italian consultancy KIBS Twitter Usernames and Keywords using data interpretation techniques such as clustering and topic modeling. This paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in business-to-business (B2B) companies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Image memorability predicts social media virality and externally-associated commenting,"Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images were consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional content. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found semantic distinctiveness was key to both memorability and virality. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
An agent-based model for emotion contagion and competition in online social media,"Recent studies suggest that human emotions diffuse in not only real-world communities but also online social media. More and more mechanisms beyond emotion contagion are revealed, including emotion correlations which indicate their influence and the coupling of emotion diffusion and network structure such as tie strength. Besides, different emotions might even compete in shaping the public opinion. However, a comprehensive model that considers up-to-date findings to replicate the patterns of emotion contagion in online social media is still missing. In this paper, to bridge this vital gap, we propose an agent-based emotion contagion model which combines features of emotion influence and tie strength preference in the dissemination process. The simulation results indicate that anger-dominated users have higher vitality than joy-dominated ones, and anger prefers weaker ties than joy in diffusion, which could make it easier to spread between online groups. Moreover, anger's high influence makes it competitive and easily to dominate the community, especially when negative public events occur. It is also surprisingly revealed that as the ratio of anger approaches joy with a gap less than 10%, angry tweets and users will eventually dominate the online social media and arrives the collective outrage in the cyber space. The critical gap disclosed here can be indeed warning signals at early stages for outrage controlling in online social media. All the parameters of the presented model can be easily estimated from the empirical observations and their values from historical data could help reproduce the emotion contagion of different circumstances. Our model would shed lights on the study of multiple issues like forecasting of emotion contagion in terms of computer simulations.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The impact of Twitter on political influence on the choice of a running mate: Social Network Analysis and Semantic Analysis -- A Review,"In this new era of social media, social networks are becoming increasingly important sources of user-generated content on the internet. These kinds of information resources, which include a lot of people's feelings, opinions, feedback, and reviews, are very useful for big businesses, markets, politics, journalism, and many other fields. Politics is one of the most talked-about and popular topics on social media networks right now. Many politicians use micro-blogging services like Twitter because they have a large number of followers and supporters on those networks. Politicians, political parties, political organizations, and foundations use social media networks to communicate with citizens ahead of time. Today, social media is used by hundreds of thousands of political groups and politicians. On these social media networks, every politician and political party has millions of followers, and politicians find new and innovative ways to urge individuals to participate in politics. Furthermore, social media assists politicians in various decision-making processes by providing recommendations, such as developing policies and strategies based on previous experiences, recommending and selecting suitable candidates for a particular constituency, recommending a suitable person for a particular position in the party, and launching a political campaign based on citizen sentiments on various issues and controversies, among other things. This research is a review on the use of social network analysis (SNA) and semantic analysis (SA) on the Twitter platform to study the supporters networks of political leaders because it can help in decision-making when predicting their political futures.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"The Digital Architectures of Social Media: Comparing Political Campaigning on Facebook, Twitter, Instagram, and Snapchat in the 2016 U.S. Election","The present study argues that political communication on social media is mediated by a platform's digital architecture, defined as the technical protocols that enable, constrain, and shape user behavior in a virtual space. A framework for understanding digital architectures is introduced, and four platforms (Facebook, Twitter, Instagram, and Snapchat) are compared along the typology. Using the 2016 US election as a case, interviews with three Republican digital strategists are combined with social media data to qualify the studyies theoretical claim that a platform's network structure, functionality, algorithmic filtering, and datafication model affect political campaign strategy on social media.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Conditions for viral influence spreading through multiplex correlated social networks,"A fundamental problem in network science is to predict how certain individuals are able to initiate new networks to spring up ""new ideas"". Frequently, these changes in trends are triggered by a few innovators who rapidly impose their ideas through ""viral"" influence spreading producing cascades of followers fragmenting an old network to create a new one. Typical examples include the raise of scientific ideas or abrupt changes in social media, like the raise of Facebook.com to the detriment of Myspace.com. How this process arises in practice has not been conclusively demonstrated. Here, we show that a condition for sustaining a viral spreading process is the existence of a multiplex correlated graph with hidden ""influence links"". Analytical solutions predict percolation phase transitions, either abrupt or continuous, where networks are disintegrated through viral cascades of followers as in empirical data. Our modeling predicts the strict conditions to sustain a large viral spreading via a scaling form of the local correlation function between multilayers, which we also confirm empirically. Ultimately, the theory predicts the conditions for viral cascading in a large class of multiplex networks ranging from social to financial systems and markets.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Dynamics of Ideological Biases of Social Media Users,"Humanity for centuries has perfected skills of interpersonal interactions and evolved patterns that enable people to detect lies and deceiving behavior of others in face-to-face settings. Unprecedented growth of people's access to mobile phones and social media raises an important question: How does this new technology influence people's interactions and support the use of traditional patterns? In this article, we answer this question for homophily-driven patterns in social media. In our previous studies, we found that, on a university campus, changes in student opinions were driven by the desire to hold popular opinions. Here, we demonstrate that the evolution of online platform-wide opinion groups is driven by the same desire. We focus on two social media: Twitter and Parler, on which we tracked the political biases of their users. On Parler, an initially stable group of Right-biased users evolved into a permanent Right-leaning echo chamber dominating weaker, transient groups of members with opposing political biases. In contrast, on Twitter, the initial presence of two large opposing bias groups led to the evolution of a bimodal bias distribution, with a high degree of polarization. We capture the movement of users from the initial to final bias groups during the tracking period. We also show that user choices are influenced by side-effects of homophily. Users entering the platform attempt to find a sufficiently large group whose members hold political biases within the range sufficiently close to their own. If successful, they stabilize their biases and become permanent members of the group. Otherwise, they leave the platform. We believe that the dynamics of users' behavior uncovered in this article create a foundation for technical solutions supporting social groups on social media and socially aware networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Cognitive network science for understanding online social cognitions: A brief review,"Social media are digitalising massive amounts of users' cognitions in terms of timelines and emotional content. Such Big Data opens unprecedented opportunities for investigating cognitive phenomena like perception, personality and information diffusion but requires suitable interpretable frameworks. Since social media data come from users' minds, worthy candidates for this challenge are cognitive networks, models of cognition giving structure to mental conceptual associations. This work outlines how cognitive network science can open new, quantitative ways for understanding cognition through online media, like: (i) reconstructing how users semantically and emotionally frame events with contextual knowledge unavailable to machine learning, (ii) investigating conceptual salience/prominence through knowledge structure in social discourse; (iii) studying users' personality traits like openness-to-experience, curiosity, and creativity through language in posts; (iv) bridging cognitive/emotional content and social dynamics via multilayer networks comparing the mindsets of influencers and followers. These advancements combine cognitive-, network- and computer science to understand cognitive mechanisms in both digital and real-world settings but come with limitations concerning representativeness, individual variability and data integration. Such aspects are discussed along the ethical implications of manipulating socio-cognitive data. In the future, reading cognitions through networks and social media can expose cognitive biases amplified by online platforms and relevantly inform policy making, education and markets about massive, complex cognitive trends.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media,"Social media platforms serve as a significant medium for sharing personal emotions, daily activities, and various life events, ensuring individuals stay informed about the latest developments. From the initiation of an account, users progressively expand their circle of friends or followers, engaging actively by posting, commenting, and sharing content. Over time, user behavior on these platforms evolves, influenced by demographic attributes and the networks they form. In this study, we present a novel approach that leverages open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint embedding technique, to analyze and predict the evolution of user behavior on social media over their lifetime. Our experiments demonstrate the potential of these models to forecast future stages of a user's social evolution, including network changes, future connections, and shifts in user activities. Experimental results highlight the effectiveness of our approach, with GPT-2 achieving the lowest perplexity (8.21) in a Cross-modal configuration, outperforming RoBERTa (9.11) and BERT, and underscoring the importance of leveraging Cross-modal configurations for superior performance. This approach addresses critical challenges in social media, such as friend recommendations and activity predictions, offering insights into the trajectory of user behavior. By anticipating future interactions and activities, this research aims to provide early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Survey of Generative Methods for Social Media Analysis,"This survey draws a broad-stroke, panoramic picture of the State of the Art (SoTA) of the research in generative methods for the analysis of social media data. It fills a void, as the existing survey articles are either much narrower in their scope or are dated. We included two important aspects that currently gain importance in mining and modeling social media: dynamics and networks. Social dynamics are important for understanding the spreading of influence or diseases, formation of friendships, the productivity of teams, etc. Networks, on the other hand, may capture various complex relationships providing additional insight and identifying important patterns that would otherwise go unnoticed.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Unraveling Social Perceptions & Behaviors towards Migrants on Twitter,"We draw insights from the social psychology literature to identify two facets of Twitter deliberations about migrants, i.e., perceptions about migrants and behaviors towards mi-grants. Our theoretical anchoring helped us in identifying two prevailing perceptions (i.e., sympathy and antipathy) and two dominant behaviors (i.e., solidarity and animosity) of social media users towards migrants. We have employed unsuper-vised and supervised approaches to identify these perceptions and behaviors. In the domain of applied NLP, our study of-fers a nuanced understanding of migrant-related Twitter de-liberations. Our proposed transformer-based model, i.e., BERT + CNN, has reported an F1-score of 0.76 and outper-formed other models. Additionally, we argue that tweets con-veying antipathy or animosity can be broadly considered hate speech towards migrants, but they are not the same. Thus, our approach has fine-tuned the binary hate speech detection task by highlighting the granular differences between perceptual and behavioral aspects of hate speeches.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Data Mining on Social Interaction Networks,"Social media and social networks have already woven themselves into the very fabric of everyday life. This results in a dramatic increase of social data capturing various relations between the users and their associated artifacts, both in online networks and the real world using ubiquitous devices. In this work, we consider social interaction networks from a data mining perspective - also with a special focus on real-world face-to-face contact networks: We combine data mining and social network analysis techniques for examining the networks in order to improve our understanding of the data, the modeled behavior, and its underlying emergent processes. Furthermore, we adapt, extend and apply known predictive data mining algorithms on social interaction networks. Additionally, we present novel methods for descriptive data mining for uncovering and extracting relations and patterns for hypothesis generation and exploration, in order to provide characteristic information about the data and networks. The presented approaches and methods aim at extracting valuable knowledge for enhancing the understanding of the respective data, and for supporting the users of the respective systems. We consider data from several social systems, like the social bookmarking system BibSonomy, the social resource sharing system flickr, and ubiquitous social systems: Specifically, we focus on data from the social conference guidance system Conferator and the social group interaction system MyGroup. This work first gives a short introduction into social interaction networks, before we describe several analysis results in the context of online social networks and real-world face-to-face contact networks. Next, we present predictive data mining methods, i.e., for localization, recommendation and link prediction. After that, we present novel descriptive data mining methods for mining communities and patterns.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Temporal Mental Health Dynamics on Social Media,"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant-supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depression, supported by the literature. We propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decision-making.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Pornography consumption in Social Media,"The structure of a social network is fundamentally related to the interests of its members. People assort spontaneously based on the topics that are relevant to them, forming social groups that revolve around different subjects. Online social media are also favorable ecosystems for the formation of topical communities centered on matters that are not commonly taken up by the general public because of the embarrassment, discomfort, or shock they may cause. Those are communities that depict or discuss what are usually referred to as deviant behaviors, conducts that are commonly considered inappropriate because they are somehow violative of society's norms or moral standards that are shared among the majority of the members of society. Pornography consumption, drug use, excessive drinking, illegal hunting, eating disorders, or any self-harming or addictive practice are all examples of deviant behaviors.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Understanding Opinions Towards Climate Change on Social Media,"Social media platforms such as Twitter (now known as X) have revolutionized how the public engage with important societal and political topics. Recently, climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation. In this work, we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media. To this end, we extracted and analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006 to 2019. Then, we construct a temporal graph from the user-user mentions network and utilize the Louvain community detection algorithm to analyze the changes in community structure around Conference of the Parties on Climate Change~(COP) events. Next, we also apply tools from the Natural Language Processing literature to perform sentiment analysis and topic modeling on the tweets. Our work acts as a first step towards understanding the evolution of pro-climate change communities around COP events. Answering these questions helps us understand how to raise people's awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Finding Influential Users in Social Media Using Association Rule Learning,"Influential users play an important role in online social networks since users tend to have an impact on one other. Therefore, the proposed work analyzes users and their behavior in order to identify influential users and predict user participation. Normally, the success of a social media site is dependent on the activity level of the participating users. For both online social networking sites and individual users, it is of interest to find out if a topic will be interesting or not. In this article, we propose association learning to detect relationships between users. In order to verify the findings, several experiments were executed based on social network analysis, in which the most influential users identified from association rule learning were compared to the results from Degree Centrality and Page Rank Centrality. The results clearly indicate that it is possible to identify the most influential users using association rule learning. In addition, the results also indicate a lower execution time compared to state-of-the-art methods.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Media Information Operations,"The battlefield of information warfare has moved to online social networks, where influence campaigns operate at unprecedented speed and scale. As with any strategic domain, success requires understanding the terrain, modeling adversaries, and executing interventions. This tutorial introduces a formal optimization framework for social media information operations (IO), where the objective is to shape opinions through targeted actions. This framework is parameterized by quantities such as network structure, user opinions, and activity levels - all of which must be estimated or inferred from data. We discuss analytic tools that support this process, including centrality measures for identifying influential users, clustering algorithms for detecting community structure, and sentiment analysis for gauging public opinion. These tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment. With the landscape mapped, we highlight threats such as coordinated bot networks, extremist recruitment, and viral misinformation. Countermeasures range from content-level interventions to mathematically optimized influence strategies. Finally, the emergence of generative AI transforms both offense and defense, democratizing persuasive capabilities while enabling scalable defenses. This shift calls for algorithmic innovation, policy reform, and ethical vigilance to protect the integrity of our digital public sphere.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mining Online Social Data for Detecting Social Network Mental Disorders,"An increasing number of social network mental disorders (SNMDs), such as Cyber-Relationship Addiction, Information Overload, and Net Compulsion, have been recently noted. Symptoms of these mental disorders are usually observed passively today, resulting in delayed clinical intervention. In this paper, we argue that mining online social behavior provides an opportunity to actively identify SNMDs at an early stage. It is challenging to detect SNMDs because the mental factors considered in standard diagnostic criteria (questionnaire) cannot be observed from online social activity logs. Our approach, new and innovative to the practice of SNMD detection, does not rely on self-revealing of those mental factors via questionnaires. Instead, we propose a machine learning framework, namely, Social Network Mental Disorder Detection (SNMDD), that exploits features extracted from social network data to accurately identify potential cases of SNMDs. We also exploit multi-source learning in SNMDD and propose a new SNMD-based Tensor Model (STM) to improve the performance. Our framework is evaluated via a user study with 3126 online social network users. We conduct a feature analysis, and also apply SNMDD on large-scale datasets and analyze the characteristics of the three SNMD types. The results show that SNMDD is promising for identifying online social network users with potential SNMDs.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Balancing Information Exposure in Social Networks,"Social media has brought a revolution on how people are consuming news. Beyond the undoubtedly large number of advantages brought by social-media platforms, a point of criticism has been the creation of echo chambers and filter bubbles, caused by social homophily and algorithmic personalization.   In this paper we address the problem of balancing the information exposure in a social network. We assume that two opposing campaigns (or viewpoints) are present in the network, and that network nodes have different preferences towards these campaigns. Our goal is to find two sets of nodes to employ in the respective campaigns, so that the overall information exposure for the two campaigns is balanced. We formally define the problem, characterize its hardness, develop approximation algorithms, and present experimental evaluation results.   Our model is inspired by the literature on influence maximization, but we offer significant novelties. First, balance of information exposure is modeled by a symmetric difference function, which is neither monotone nor submodular, and thus, not amenable to existing approaches. Second, while previous papers consider a setting with selfish agents and provide bounds on best response strategies (i.e., move of the last player), we consider a setting with a centralized agent and provide bounds for a global objective function.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Network Inference from a Mixture of Diffusion Models for Fake News Mitigation,"The dissemination of fake news intended to deceive people, influence public opinion and manipulate social outcomes, has become a pressing problem on social media. Moreover, information sharing on social media facilitates diffusion of viral information cascades. In this work, we focus on understanding and leveraging diffusion dynamics of false and legitimate contents in order to facilitate network interventions for fake news mitigation. We analyze real-world Twitter datasets comprising fake and true news cascades, to understand differences in diffusion dynamics and user behaviours with regards to fake and true contents. Based on the analysis, we model the diffusion as a mixture of Independent Cascade models (MIC) with parameters $_T, _F$ over the social network graph; and derive unsupervised inference techniques for parameter estimation of the diffusion mixture model from observed, unlabeled cascades. Users influential in the propagation of true and fake contents are identified using the inferred diffusion dynamics. Characteristics of the identified influential users reveal positive correlation between influential users identified for fake news and their relative appearance in fake news cascades. Identified influential users tend to be related to topics of more viral information cascades than less viral ones; and identified fake news influential users have relatively fewer counts of direct followers, compared to the true news influential users. Intervention analysis on nodes and edges demonstrates capacity of the inferred diffusion dynamics in supporting network interventions for mitigation.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Temporal Analysis of Influence to Predict Users' Adoption in Online Social Networks,"Different measures have been proposed to predict whether individuals will adopt a new behavior in online social networks, given the influence produced by their neighbors. In this paper, we show one can achieve significant improvement over these standard measures, extending them to consider a pair of time constraints. These constraints provide a better proxy for social influence, showing a stronger correlation to the probability of influence as well as the ability to predict influence.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Community Shaping in the Digital Age: A Temporal Fusion Framework for Analyzing Discourse Fragmentation in Online Social Networks,"This research presents a framework for analyzing the dynamics of online communities in social media platforms, utilizing a temporal fusion of text and network data. By combining text classification and dynamic social network analysis, we uncover mechanisms driving community formation and evolution, revealing the influence of real-world events. We introduced fourteen key elements based on social science theories to evaluate social media dynamics, validating our framework through a case study of Twitter data during major U.S. events in 2020. Our analysis centers on discrimination discourse, identifying sexism, racism, xenophobia, ableism, homophobia, and religious intolerance as main fragments. Results demonstrate rapid community emergence and dissolution cycles representative of discourse fragments. We reveal how real-world circumstances impact discourse dominance and how social media contributes to echo chamber formation and societal polarization. Our comprehensive approach provides insights into discourse fragmentation, opinion dynamics, and structural aspects of online communities, offering a methodology for understanding the complex interplay between online interactions and societal trends.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Who is driving the conversation? Analysing the nodality of British MPs and journalists on social media,"With the rise of social media, political conversations now take place in more diffuse environments. In this context, it is not always clear why some actors, more than others, have greater influence on how discussions are shaped. To investigate the factors behind such influence, we build on nodality, a concept in political science which describes the capacity of an actor to exchange information within discourse networks. This concept goes beyond traditional network metrics that describe the position of an actor in the network to include exogenous drivers of influence (e.g. factors relating to organisational hierarchies). We study online discourse on Twitter (now X) in the UK to measure the relative nodality of two sets of policy actors - Members of Parliament (MPs) and accredited journalists - on four policy topics. We find that influence on the platform is driven by two key factors: (i) active nodality, derived from the actor's level of topic-related engagement, and (ii) inherent nodality, which is independent of the platform discourse and reflects the actor's institutional position. These findings significantly further our understanding of the origins of influence on social media platforms and suggest in which contexts influence is transferable across topics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media,"Social media enables the rapid spread of many kinds of information, from memes to social movements. However, little is known about how information crosses linguistic boundaries. We apply causal inference techniques on the European Twitter network to quantify multilingual users' structural role and communication influence in cross-lingual information exchange. Overall, multilinguals play an essential role; posting in multiple languages increases betweenness centrality by 13%, and having a multilingual network neighbor increases monolinguals' odds of sharing domains and hashtags from another language 16-fold and 4-fold, respectively. We further show that multilinguals have a greater impact on diffusing information less accessible to their monolingual compatriots, such as information from far-away countries and content about regional politics, nascent social movements, and job opportunities. By highlighting information exchange across borders, this work sheds light on a crucial component of how information and ideas spread around the world.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Combined Centrality Measures for an Improved Characterization of Influence Spread in Social Networks,"Influence Maximization (IM) aims at finding the most influential users in a social network, i. e., users who maximize the spread of an opinion within a certain propagation model. Previous work investigated the correlation between influence spread and nodal centrality measures to bypass more expensive IM simulations. The results were promising but incomplete, since these studies investigated the performance (i. e., the ability to identify influential users) of centrality measures only in restricted settings, e. g., in undirected/unweighted networks and/or within a propagation model less common for IM. In this paper, we first show that good results within the Susceptible- Infected-Removed (SIR) propagation model for unweighted and undirected networks do not necessarily transfer to directed or weighted networks under the popular Independent Cascade (IC) propagation model. Then, we identify a set of centrality measures with good performance for weighted and directed networks within the IC model. Our main contribution is a new way to combine the centrality measures in a closed formula to yield even better results. Additionally, we also extend gravitational centrality (GC) with the proposed combined centrality measures. Our experiments on 50 real-world data sets show that our proposed centrality measures outperform well-known centrality measures and the state-of-the art GC measure significantly. social networks, influence maximization, centrality measures, IC propagation model, influential spreaders","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Polarization in Decentralized Online Social Networks,"Centralized social media platforms are currently experiencing a shift in user engagement, drawing attention to alternative paradigms like Decentralized Online Social Networks (DOSNs). The rising popularity of DOSNs finds its root in the accessibility of open-source software, enabling anyone to create a new instance (i.e., server) and participate in a decentralized network known as Fediverse. Despite this growing momentum, there has been a lack of studies addressing the effect of positive and negative interactions among instances within DOSNs. This work aims to fill this gap by presenting a preliminary examination of instances' polarization in DOSNs, focusing on Mastodon -- the most widely recognized decentralized social media platform, boasting over 10M users and nearly 20K instances to date. Our results suggest that polarization in the Fediverse emerges in unique ways, influenced by the desire to foster a federated environment between instances, also facilitating the isolation of instances that may pose potential risks to the Fediverse.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Is Twitter Enough? Investigating Situational Awareness in Social and Print Media during the Second COVID-19 Wave in India,"The pandemic required efficient allocation of public resources and transforming existing ways of societal functions. To manage any crisis, governments and public health researchers exploit the information available to them in order to make informed decisions, also defined as situational awareness. Gathering situational awareness using social media has been functional to manage epidemics. Previous research focused on using discussions during periods of epidemic crises on social media platforms like Twitter, Reddit, or Facebook and developing NLP techniques to filter out relevant discussions from a huge corpus of messages and posts. Social media usage varies with internet penetration and other socioeconomic factors, which might induce disparity in analyzing discussions across different geographies. However, print media is a ubiquitous information source, irrespective of geography. Further, topics discussed in news articles are already newsworthy, while on social media newsworthiness is a product of techno-social processes. Developing this fundamental difference, we study Twitter data during the second wave in India focused on six high-population cities with varied macroeconomic factors. Through a mixture of qualitative and quantitative methods, we further analyze two Indian newspapers during the same period and compare topics from both Twitter and the newspapers to evaluate situational awareness around the second phase of COVID on each of these platforms. We conclude that factors like internet penetration and GDP in a specific city influence the discourse surrounding situational updates on social media. Thus, augmenting information from newspapers with information extracted from social media would provide a more comprehensive perspective in resource deficit cities.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Contagion dynamics of extremist propaganda in social networks,"Recent terrorist attacks carried out on behalf of ISIS on American and European soil by lone wolf attackers or sleeper cells remind us of the importance of understanding the dynamics of radicalization mediated by social media communication channels. In this paper, we shed light on the social media activity of a group of twenty-five thousand users whose association with ISIS online radical propaganda has been manually verified. By using a computational tool known as dynamical activity-connectivity maps, based on network and temporal activity patterns, we investigate the dynamics of social influence within ISIS supporters. We finally quantify the effectiveness of ISIS propaganda by determining the adoption of extremist content in the general population and draw a parallel between radical propaganda and epidemics spreading, highlighting that information broadcasters and influential ISIS supporters generate highly-infectious cascades of information contagion. Our findings will help generate effective countermeasures to combat the group and other forms of online extremism.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Unraveling the Dynamics of Television Debates and Social Media Engagement: Insights from an Indian News Show,"The relationship between television shows and social media has become increasingly intertwined in recent years. Social media platforms, particularly Twitter, have emerged as significant sources of public opinion and discourse on topics discussed in television shows. In India, news debates leverage the popularity of social media to promote hashtags and engage users in discussions and debates on a daily basis.   This paper focuses on the analysis of one of India's most prominent and widely-watched TV news debate shows: ""Arnab Goswami-The Debate"". The study examines the content of the show by analyzing the hashtags used to promote it and the social media data corresponding to these hashtags. The findings reveal that the show exhibits a strong bias towards the ruling Bharatiya Janata Party (BJP), with over 60% of the debates featuring either pro-BJP or anti-opposition content. Social media support for the show primarily comes from BJP supporters. Notably, BJP leaders and influencers play a significant role in promoting the show on social media, leveraging their existing networks and resources to artificially trend specific hashtags. Furthermore, the study uncovers a reciprocal flow of information between the TV show and social media. We find evidence that the show's choice of topics is linked to social media posts made by party workers, suggesting a dynamic interplay between traditional media and online platforms.   By exploring the complex interaction between television debates and social media support, this study contributes to a deeper understanding of the evolving relationship between these two domains in the digital age. The findings hold implications for media researchers and practitioners, offering insights into the ways in which social media can influence traditional media and vice versa.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Simulating Rumor Spreading in Social Networks using LLM Agents,"With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Predicting Group Evolution in the Social Network,"Groups - social communities are important components of entire societies, analysed by means of the social network concept. Their immanent feature is continuous evolution over time. If we know how groups in the social network has evolved we can use this information and try to predict the next step in the given group evolution. In the paper, a new aproach for group evolution prediction is presented and examined. Experimental studies on four evolving social networks revealed that (i) the prediction based on the simple input features may be very accurate, (ii) some classifiers are more precise than the others and (iii) parameters of the group evolution extracion method significantly influence the prediction quality.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Examining Untempered Social Media: Analyzing Cascades of Polarized Conversations,"Online social media, periodically serves as a platform for cascading polarizing topics of conversation. The inherent community structure present in online social networks (homophily) and the advent of fringe outlets like Gab have created online ""echo chambers"" that amplify the effects of polarization, which fuels detrimental behavior. Recently, in October 2018, Gab made headlines when it was revealed that Robert Bowers, the individual behind the Pittsburgh Synagogue massacre, was an active member of this social media site and used it to express his anti-Semitic views and discuss conspiracy theories. Thus to address the need of automated data-driven analyses of such fringe outlets, this research proposes novel methods to discover topics that are prevalent in Gab and how they cascade within the network. Specifically, using approximately 34 million posts, and 3.7 million cascading conversation threads with close to 300k users; we demonstrate that there are essentially five cascading patterns that manifest in Gab and the most ""viral"" ones begin with an echo-chamber pattern and grow out to the entire network. Also, we empirically show, through two models viz. Susceptible-Infected and Bass, how the cascades structurally evolve from one of the five patterns to the other based on the topic of the conversation with upto 84% accuracy.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Belief Dynamics in Social Networks: A Fluid-Based Analysis,"The advent and proliferation of social media have led to the development of mathematical models describing the evolution of beliefs/opinions in an ecosystem composed of socially interacting users. The goal is to gain insights into collective dominant social beliefs and into the impact of different components of the system, such as users' interactions, while being able to predict users' opinions. Following this thread, in this paper we consider a fairly general dynamical model of social interactions, which captures all the main features exhibited by a social system. For such model, by embracing a mean-field approach, we derive a diffusion differential equation that represents asymptotic belief dynamics, as the number of users grows large. We then analyze the steady-state behavior as well as the time dependent (transient) behavior of the system. In particular, for the steady-state distribution, we obtain simple closed-form expressions for a relevant class of systems, while we propose efficient semi-analytical techniques in the most general cases. At last, we develop an efficient semi-analytical method to analyze the dynamics of the users' belief over time, which can be applied to a remarkably large class of systems.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Synthetic Social Media Influence Experimentation via an Agentic Reinforcement Learning Large Language Model Bot,"Understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information. Traditional influence analysis is typically divided into qualitative assessments of personal attributes (e.g., psychology of influence) and quantitative evaluations of influence power mechanisms (e.g., social network analysis). One challenge faced by researchers is the ethics of real-world experimentation and the lack of social influence data. In this study, we provide a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to test topic-specific influence mechanisms ethically. Our framework contains agents that generate posts, form opinions on specific topics, and socially follow/unfollow each other based on the outcome of discussions. This simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge. Using our own framework, we design an opinion leader that utilizes Reinforcement Learning (RL) to adapt its linguistic interaction with the community to maximize its influence and followers over time. Our current findings reveal that constraining the action space and incorporating self-observation are key factors for achieving stable and consistent opinion leader generation for topic-specific influence. This demonstrates the simulation framework's capacity to create agents that can adapt to complex and unpredictable social dynamics. The work is important in an age of increasing online influence on social attitudes and emerging technologies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Morality in the mundane: Categorizing moral reasoning in real-life social situations,"Moral reasoning reflects how people acquire and apply moral rules in particular situations. With increasingly social interactions happening online, social media data provides an unprecedented opportunity to assess in-the-wild moral reasoning. We investigate the commonsense aspects of morality in ordinary matters empirically. To this end, we examine data from a Reddit subcommunity (i.e., a subreddit) where an author may describe their behavior in a situation to seek comments about whether that behavior was appropriate. Other users comment to provide judgments and reasoning. We focus on the novel problem of understanding the moral reasoning implicit in user comments about the propriety of an author's behavior. Especially, we explore associations between the common elements of the indicated reasoning and the extractable social factors. Our results suggest the reasoning depends on the author's gender and the topic of a post, such as when expressing anger emotion and using sensible words (e.g., f-ck, hell, and damn) in work-related situations. Moreover, we find that the commonly expressed semantics also depends on commenters' interests.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis,"Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Predicting Opioid Relapse Using Social Media Data,"Opioid addiction is a severe public health threat in the U.S, causing massive deaths and many social problems. Accurate relapse prediction is of practical importance for recovering patients since relapse prediction promotes timely relapse preventions that help patients stay clean. In this paper, we introduce a Generative Adversarial Networks (GAN) model to predict the addiction relapses based on sentiment images and social influences. Experimental results on real social media data from Reddit.com demonstrate that the GAN model delivers a better performance than comparable alternative techniques. The sentiment images generated by the model show that relapse is closely connected with two emotions `joy' and `negative'. This work is one of the first attempts to predict relapses using massive social media data and generative adversarial nets. The proposed method, combined with knowledge of social media mining, has the potential to revolutionize the practice of opioid addiction prevention and treatment.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploring the Role of Randomization on Belief Rigidity in Online Social Networks,"People often stick to their existing beliefs, ignoring contradicting evidence or only interacting with those who reinforce their views. Social media platforms often facilitate such tendencies of homophily and echo-chambers as they promote highly personalized content to maximize user engagement. However, increased belief rigidity can negatively affect real-world policy decisions such as leading to climate change inaction and increased vaccine hesitancy. To understand and effectively tackle belief rigidity on online social networks, designing and evaluating various intervention strategies is crucial, and increasing randomization in the network can be considered one such intervention. In this paper, we empirically quantify the effects of a randomized social network structure on belief rigidity, specifically examining the potential benefits of introducing randomness into the network. We show that individuals' beliefs are positively influenced by peer opinions, regardless of whether those opinions are similar to or differ from their own by passively sensing belief rigidity through our experimental framework. Moreover, people incorporate a slightly higher variety of different peers (based on their opinions) into their networks when the recommendation algorithm provides them with diverse content, compared to when it provides them with similar content. Our results indicate that in some cases, there might be benefits to randomization, providing empirical evidence that a more randomized network could be a feasible way of helping people get out of their echo-chambers. Our findings have broader implications in computing and platform design of social media, and can help combat overly rigid beliefs in online social networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
COVID-19 and social media: Beyond polarization,"The COVID-19 pandemic brought upon a massive wave of disinformation, exacerbating polarization in the increasingly divided landscape of online discourse. In this context, popular social media users play a major role, as they have the ability to broadcast messages to large audiences and influence public opinion. In this paper, we make use of openly available data to study the behavior of popular users discussing the pandemic on Twitter. We tackle the issue from a network perspective, considering users as nodes and following relationships as directed edges. The resulting network structure is modeled by embedding the actors in a latent social space, where users closer to one another have a higher probability of following each other. The results suggest the existence of two distinct communities, which can be interpreted as ""generally pro"" and ""generally against"" vaccine mandates, corroborating existing evidence on the pervasiveness of echo chambers on the platform. By focusing on a number of notable users, such as politicians, activists, and news outlets, we further show that the two groups are not entirely homogeneous, and that not just the two poles are represented. To the contrary, the latent space captures an entire spectrum of beliefs between the two extremes, demonstrating that polarization, while present, is not the only driver of the network, and that more moderate, ""central"" users are key players in the discussion.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploring the social influence of Kaggle virtual community on the M5 competition,"One of the most significant differences of M5 over previous forecasting competitions is that it was held on Kaggle, an online platform of data scientists and machine learning practitioners. Kaggle provides a gathering place, or virtual community, for web users who are interested in the M5 competition. Users can share code, models, features, loss functions, etc. through online notebooks and discussion forums. This paper aims to study the social influence of virtual community on user behaviors in the M5 competition. We first research the content of the M5 virtual community by topic modeling and trend analysis. Further, we perform social media analysis to identify the potential relationship network of the virtual community. We study the roles and characteristics of some key participants that promote the diffusion of information within the M5 virtual community. Overall, this study provides in-depth insights into the mechanism of the virtual community's influence on the participants and has potential implications for future online competitions.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Balancing spreads of influence in a social network,"The personalization of our news consumption on social media has a tendency to reinforce our pre-existing beliefs instead of balancing our opinions. This finding is a concern for the health of our democracies which rely on an access to information providing diverse viewpoints. To tackle this issue from a computational perspective, Garimella et al. (NIPS'17) modeled the spread of these viewpoints, also called campaigns, using the well-known independent cascade model and studied an optimization problem that aims at balancing information exposure in a social network when two opposing campaigns propagate in the network. The objective in their $NP$-hard optimization problem is to maximize the number of people that are exposed to either both or none of the viewpoints. For two different settings, one corresponding to a model where campaigns spread in a correlated manner, and a second one, where the two campaigns spread in a heterogeneous manner, they provide constant ratio approximation algorithms. In this paper, we investigate a more general formulation of this problem. That is, we assume that $$ different campaigns propagate in a social network and we aim to maximize the number of people that are exposed to either $$ or none of the campaigns, where $\ge\ge2$. We provide dedicated approximation algorithms for both the correlated and heterogeneous settings. Interestingly, for the heterogeneous setting with $\ge 3$, we give a reduction leading to several approximation hardness results. Maybe most importantly, we obtain that the problem cannot be approximated within a factor of $n^{-g(n)}$ for any $g(n)=o(1)$ assuming Gap-ETH, denoting with $n$ the number of nodes in the social network. For $\ge 4$, there is no $n^{-}$-approximation algorithm if a certain class of one-way functions exists, where $> 0$ is a given constant which depends on $$.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Scalable Framework for Spatiotemporal Analysis of Location-based Social Media Data,"In the past several years, social media (e.g., Twitter and Facebook) has been experiencing a spectacular rise and popularity, and becoming a ubiquitous discourse for content sharing and social networking. With the widespread of mobile devices and location-based services, social media typically allows users to share whereabouts of daily activities (e.g., check-ins and taking photos), and thus strengthens the roles of social media as a proxy to understand human behaviors and complex social dynamics in geographic spaces. Unlike conventional spatiotemporal data, this new modality of data is dynamic, massive, and typically represented in stream of unstructured media (e.g., texts and photos), which pose fundamental representation, modeling and computational challenges to conventional spatiotemporal analysis and geographic information science. In this paper, we describe a scalable computational framework to harness massive location-based social media data for efficient and systematic spatiotemporal data analysis. Within this framework, the concept of space-time trajectories (or paths) is applied to represent activity profiles of social media users. A hierarchical spatiotemporal data model, namely a spatiotemporal data cube model, is developed based on collections of space-time trajectories to represent the collective dynamics of social media users across aggregation boundaries at multiple spatiotemporal scales. The framework is implemented based upon a public data stream of Twitter feeds posted on the continent of North America. To demonstrate the advantages and performance of this framework, an interactive flow mapping interface (including both single-source and multiple-source flow mapping) is developed to allow real-time, and interactive visual exploration of movement dynamics in massive location-based social media at multiple scales.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
On the Behaviour of Deviant Communities in Online Social Networks,"On-line social networks are complex ensembles of inter-linked communities that interact on different topics. Some communities are characterized by what are usually referred to as deviant behaviors, conducts that are commonly considered inappropriate with respect to the society's norms or moral standards. Eating disorders, drug use, and adult content consumption are just a few examples. We refer to such communities as deviant networks. It is commonly believed that such deviant networks are niche, isolated social groups, whose activity is well separated from the mainstream social-media life. According to this assumption, research studies have mostly considered them in isolation. In this work we focused on adult content consumption networks, which are present in many on-line social media and in the Web in general. We found that few small and densely connected communities are responsible for most of the content production. Differently from previous work, we studied how such communities interact with the whole social network. We found that the produced content flows to the rest of the network mostly directly or through bridge-communities, reaching at least 450 times more users. We also show that a large fraction of the users can be inadvertently exposed to such content through indirect content resharing. We also discuss a demographic analysis of the producers and consumers networks. Finally, we show that it is easily possible to identify a few core users to radically uproot the diffusion process. We aim at setting the basis to study deviant communities in context.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features,"In recent years, with the expansion of the Internet and attractive social media infrastructures, people prefer to follow the news through these media. Despite the many advantages of these media in the news field, the lack of any control and verification mechanism has led to the spread of fake news, as one of the most important threats to democracy, economy, journalism and freedom of expression. Designing and using automatic methods to detect fake news on social media has become a significant challenge. In this paper, we examine the publishers' role in detecting fake news on social media. We also suggest a high accurate multi-modal framework, namely FR-Detect, using user-related and content-related features with early detection capability. For this purpose, two new user-related features, namely Activity Credibility and Influence, have been introduced for publishers. Furthermore, a sentence-level convolutional neural network is provided to combine these features with latent textual content features properly. Experimental results have shown that the publishers' features can improve the performance of content-based models by up to 13% and 29% in accuracy and F1-score, respectively.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Multi-Winner Contests for Strategic Diffusion in Social Networks,"Strategic diffusion encourages participants to take active roles in promoting stakeholders' agendas by rewarding successful referrals. As social media continues to transform the way people communicate, strategic diffusion has become a powerful tool for stakeholders to influence people's decisions or behaviors for desired objectives. Existing reward mechanisms for strategic diffusion are usually either vulnerable to false-name attacks or not individually rational for participants that have made successful referrals. Here, we introduce a novel multi-winner contests (MWC) mechanism for strategic diffusion in social networks. The MWC mechanism satisfies several desirable properties, including false-name-proofness, individual rationality, budget constraint, monotonicity, and subgraph constraint. Numerical experiments on four real-world social network datasets demonstrate that stakeholders can significantly boost participants' aggregated efforts with proper design of competitions. Our work sheds light on how to design manipulation-resistant mechanisms with appropriate contests.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Blocking Maximization in Social Networks under the Competitive Linear Threshold Model Technical Report,"In many real-world situations, different and often opposite opinions, innovations, or products are competing with one another for their social influence in a networked society. In this paper, we study competitive influence propagation in social networks under the competitive linear threshold (CLT) model, an extension to the classic linear threshold model. Under the CLT model, we focus on the problem that one entity tries to block the influence propagation of its competing entity as much as possible by strategically selecting a number of seed nodes that could initiate its own influence propagation. We call this problem the influence blocking maximization (IBM) problem. We prove that the objective function of IBM in the CLT model is submodular, and thus a greedy algorithm could achieve 1-1/e approximation ratio. However, the greedy algorithm requires Monte-Carlo simulations of competitive influence propagation, which makes the algorithm not efficient. We design an efficient algorithm CLDAG, which utilizes the properties of the CLT model, to address this issue. We conduct extensive simulations of CLDAG, the greedy algorithm, and other baseline algorithms on real-world and synthetic datasets. Our results show that CLDAG is able to provide best accuracy in par with the greedy algorithm and often better than other algorithms, while it is two orders of magnitude faster than the greedy algorithm.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Searching for superspreaders of information in real-world social media,"A number of predictors have been suggested to detect the most influential spreaders of information in online social media across various domains such as Twitter or Facebook. In particular, degree, PageRank, k-core and other centralities have been adopted to rank the spreading capability of users in information dissemination media. So far, validation of the proposed predictors has been done by simulating the spreading dynamics rather than following real information flow in social networks. Consequently, only model-dependent contradictory results have been achieved so far for the best predictor. Here, we address this issue directly. We search for influential spreaders by following the real spreading dynamics in a wide range of networks. We find that the widely-used degree and PageRank fail in ranking users' influence. We find that the best spreaders are consistently located in the k-core across dissimilar social platforms such as Twitter, Facebook, Livejournal and scientific publishing in the American Physical Society. Furthermore, when the complete global network structure is unavailable, we find that the sum of the nearest neighbors' degree is a reliable local proxy for user's influence. Our analysis provides practical instructions for optimal design of strategies for ""viral"" information dissemination in relevant applications.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
SoMeR: Multi-View User Representation Learning for Social Media,"Social media user representation learning aims to capture user preferences, interests, and behaviors in low-dimensional vector representations. These representations are critical to a range of social problems, including predicting user behaviors and detecting inauthentic accounts. However, existing methods are either designed for commercial applications, or rely on specific features like text contents, activity patterns, or platform metadata, failing to holistically model user behavior across different modalities. To address these limitations, we propose SoMeR, a Social Media user Representation learning framework that incorporates temporal activities, text contents, profile information, and network interactions to learn comprehensive user portraits. SoMeR encodes user post streams as sequences of time-stamped textual features, uses transformers to embed this along with profile data, and jointly trains with link prediction and contrastive learning objectives to capture user similarity. We demonstrate SoMeR's versatility through three applications: 1) Identifying information operation driver accounts, 2) Measuring online polarization after major events, and 3) Predicting future user participation in Reddit hate communities. SoMeR provides new solutions to better understand user behavior in the socio-political domains, enabling more informed decisions and interventions.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploratory Analysis of a Social Media Network in Sri Lanka during the COVID-19 Virus Outbreak,"During the COVID-19 pandemic, multiple aspects of human life were subjected to unprecedented changes, globally. In Sri Lanka, a developing country located in South Asia, it was possible to observe a range of events that arose due to the influence of the COVID-19 virus outbreak. Thus, the people of Sri Lanka used Social Media to voice their opinions regarding such events and those involved in them, enabling the ideal avenue to explore the social perception. However, the outcome of such actions was at certain times detrimental. This study was conducted as an attempt to identify the reasons for such instances as well as to identify the behaviours of the Sri Lankan populace during such a crisis event. To support this study, observations, as well as data of related posts from a sample of 50 sources, were manually collected from the most popular social media platform in Sri Lanka, Facebook. The posts considered spanned until approximately a month after the initial major virus outbreak in the country and contained content that even vaguely related to the virus. Utilising such data, various forms of analyses such as topic significance and topic co-occurrences were conducted. The findings highlight, while there can be social detrimental ideas shared, the majority of the posts point constructive and positive thoughts suggesting the successful influence from the cultural and social values Sri Lanka society promotes throughout.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach,"With the surge of domestic tourism in India and the influence of social media on young tourists, this paper aims to address the research question on how ""social return"" - responses received on social media sharing - of recent trip details can influence decision-making for short-term future travels. The paper develops a multi-model framework to build a predictive machine learning model that establishes a relationship between a traveler's social return, various social media usage, trip-related factors, and her future trip-planning behavior. The primary data was collected via a survey from Indian tourists. After data cleaning, the imbalance in the data was addressed using a robust oversampling method, and the reliability of the predictive model was ensured by applying a Monte Carlo cross-validation technique. The results suggest at least 75% overall accuracy in predicting the influence of social return on changing the future trip plan. Moreover, the model fit results provide crucial practical implications for the domestic tourism sector in India with future research directions concerning social media, destination marketing, smart tourism, heritage tourism, etc.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Klout Score: Measuring Influence Across Multiple Social Networks,"In this work, we present the Klout Score, an influence scoring system that assigns scores to 750 million users across 9 different social networks on a daily basis. We propose a hierarchical framework for generating an influence score for each user, by incorporating information for the user from multiple networks and communities. Over 3600 features that capture signals of influential interactions are aggregated across multiple dimensions for each user. The features are scalably generated by processing over 45 billion interactions from social networks every day, as well as by incorporating factors that indicate real world influence. Supervised models trained from labeled data determine the weights for features, and the final Klout Score is obtained by hierarchically combining communities and networks. We validate the correctness of the score by showing that users with higher scores are able to spread information more effectively in a network. Finally, we use several comparisons to other ranking systems to show that highly influential and recognizable users across different domains have high Klout scores.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Using Gaussian Mixtures to Model Evolving Multi-Modal Beliefs Across Social Media,"We use Gaussian mixtures to model formation and evolution of multi-modal beliefs and opinion uncertainty across social networks. In this model, opinions evolve by Bayesian belief update when incorporating exogenous factors (signals from outside sources, e.g., news articles) and by non-Bayesian mixing dynamics when incorporating endogenous factors (interactions across social media). The modeling enables capturing the richness of behavior observed in multi-modal opinion dynamics while maintaining interpretability and simplicity of scalar models. We present preliminary results on opinion formation and uncertainty to investigate the effect of stubborn individuals (as social influencers). This leads to a notion of centrality based on the ease with which an individual can disrupt the flow of information across the social network.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
More or Less? Predict the Social Influence of Malicious URLs on Social Media,"Users of Online Social Networks (OSNs) interact with each other more than ever. In the context of a public discussion group, people receive, read, and write comments in response to articles and postings. In the absence of access control mechanisms, OSNs are a great environment for attackers to influence others, from spreading phishing URLs, to posting fake news. Moreover, OSN user behavior can be predicted by social science concepts which include conformity and the bandwagon effect. In this paper, we show how social recommendation systems affect the occurrence of malicious URLs on Facebook. We exploit temporal features to build a prediction framework, having greater than 75% accuracy, to predict whether the following group users' behavior will increase or not. Included in this work, we demarcate classes of URLs, including those malicious URLs classified as creating critical damage, as well as those of a lesser nature which only inflict light damage such as aggressive commercial advertisements and spam content. It is our hope that the data and analyses in this paper provide a better understanding of OSN user reactions to different categories of malicious URLs, thereby providing a way to mitigate the influence of these malicious URL attacks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Agent-Based Modelling Meets Generative AI in Social Network Simulations,"Agent-Based Modelling (ABM) has emerged as an essential tool for simulating social networks, encompassing diverse phenomena such as information dissemination, influence dynamics, and community formation. However, manually configuring varied agent interactions and information flow dynamics poses challenges, often resulting in oversimplified models that lack real-world generalizability. Integrating modern Large Language Models (LLMs) with ABM presents a promising avenue to address these challenges and enhance simulation fidelity, leveraging LLMs' human-like capabilities in sensing, reasoning, and behavior. In this paper, we propose a novel framework utilizing LLM-empowered agents to simulate social network users based on their interests and personality traits. The framework allows for customizable agent interactions resembling various social network platforms, including mechanisms for content resharing and personalized recommendations. We validate our framework using a comprehensive Twitter dataset from the 2020 US election, demonstrating that LLM-agents accurately replicate real users' behaviors, including linguistic patterns and political inclinations. These agents form homogeneous ideological clusters and retain the main themes of their community. Notably, preference-based recommendations significantly influence agent behavior, promoting increased engagement, network homophily and the formation of echo chambers. Overall, our findings underscore the potential of LLM-agents in advancing social media simulations and unraveling intricate online dynamics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
How well can machine learning predict demographics of social media users?,"The wide use of social media sites and other digital technologies have resulted in an unprecedented availability of digital data that are being used to study human behavior across research domains. Although unsolicited opinions and sentiments are available on these platforms, demographic details are usually missing. Demographic information is pertinent in fields such as demography and public health, where significant differences can exist across sex, racial and socioeconomic groups. In an attempt to address this shortcoming, a number of academic studies have proposed methods for inferring the demographics of social media users using details such as names, usernames, and network characteristics. Gender is the easiest trait to accurately infer, with measures of accuracy higher than 90 percent in some studies. Race, ethnicity and age tend to be more challenging to predict for a variety of reasons including the novelty of social media to certain age groups and a lack of significant deviations in user details across racial and ethnic groups. Although the endeavor to predict user demographics is plagued with ethical questions regarding privacy and data ownership, knowing the demographics in a data sample can aid in addressing issues of bias and population representation, so that existing societal inequalities are not exacerbated.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
What is Tumblr: A Statistical Overview and Comparison,"Tumblr, as one of the most popular microblogging platforms, has gained momentum recently. It is reported to have 166.4 millions of users and 73.4 billions of posts by January 2014. While many articles about Tumblr have been published in major press, there is not much scholar work so far. In this paper, we provide some pioneer analysis on Tumblr from a variety of aspects. We study the social network structure among Tumblr users, analyze its user generated content, and describe reblogging patterns to analyze its user behavior. We aim to provide a comprehensive statistical overview of Tumblr and compare it with other popular social services, including blogosphere, Twitter and Facebook, in answering a couple of key questions: What is Tumblr? How is Tumblr different from other social media networks? In short, we find Tumblr has more rich content than other microblogging platforms, and it contains hybrid characteristics of social networking, traditional blogosphere, and social media. This work serves as an early snapshot of Tumblr that later work can leverage.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Applying Machine Learning for characterizing social networks Agent-based models,"Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Identifying Crisis Response Communities in Online Social Networks for Compound Disasters: The Case of Hurricane Laura and Covid-19,"Online social networks allow different agencies and the public to interact and share the underlying risks and protective actions during major disasters. This study revealed such crisis communication patterns during hurricane Laura compounded by the COVID-19 pandemic. Laura was one of the strongest (Category 4) hurricanes on record to make landfall in Cameron, Louisiana. Using the Application Programming Interface (API), this study utilizes large-scale social media data obtained from Twitter through the recently released academic track that provides complete and unbiased observations. The data captured publicly available tweets shared by active Twitter users from the vulnerable areas threatened by Laura. Online social networks were based on user influence feature ( mentions or tags) that allows notifying other users while posting a tweet. Using network science theories and advanced community detection algorithms, the study split these networks into twenty-one components of various sizes, the largest of which contained eight well-defined communities. Several natural language processing techniques (i.e., word clouds, bigrams, topic modeling) were applied to the tweets shared by the users in these communities to observe their risk-taking or risk-averse behavior during a major compounding crisis. Social media accounts of local news media, radio, universities, and popular sports pages were among those who involved heavily and interacted closely with local residents. In contrast, emergency management and planning units in the area engaged less with the public. The findings of this study provide novel insights into the design of efficient social media communication guidelines to respond better in future disasters.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Deep Unified Multimodal Embeddings for Understanding both Content and Users in Social Media Networks,"There has been an explosion of multimodal content generated on social media networks in the last few years, which has necessitated a deeper understanding of social media content and user behavior. We present a novel content-independent content-user-reaction model for social multimedia content analysis. Compared to prior works that generally tackle semantic content understanding and user behavior modeling in isolation, we propose a generalized solution to these problems within a unified framework. We embed users, images and text drawn from open social media in a common multimodal geometric space, using a novel loss function designed to cope with distant and disparate modalities, and thereby enable seamless three-way retrieval. Our model not only outperforms unimodal embedding based methods on cross-modal retrieval tasks but also shows improvements stemming from jointly solving the two tasks on Twitter data. We also show that the user embeddings learned within our joint multimodal embedding model are better at predicting user interests compared to those learned with unimodal content on Instagram data. Our framework thus goes beyond the prior practice of using explicit leader-follower link information to establish affiliations by extracting implicit content-centric affiliations from isolated users. We provide qualitative results to show that the user clusters emerging from learned embeddings have consistent semantics and the ability of our model to discover fine-grained semantics from noisy and unstructured data. Our work reveals that social multimodal content is inherently multimodal and possesses a consistent structure because in social networks meaning is created through interactions between users and content.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Influence in the Concurrent Diffusion of Information and Behaviors in Online Social Networks,"The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integrating text mining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Dynamics in online social networks,"An increasing number of today's social interactions occurs using online social media as communication channels. Some online social networks have become extremely popular in the last decade. They differ among themselves in the character of the service they provide to online users. For instance, Facebook can be seen mainly as a platform for keeping in touch with close friends and relatives, Twitter is used to propagate and receive news, LinkedIn facilitates the maintenance of professional contacts, Flickr gathers amateurs and professionals of photography, etc. Albeit different, all these online platforms share an ingredient that pervades all their applications. There exists an underlying social network that allows their users to keep in touch with each other and helps to engage them in common activities or interactions leading to a better fulfillment of the service's purposes. This is the reason why these platforms share a good number of functionalities, e.g., personal communication channels, broadcasted status updates, easy one-step information sharing, news feeds exposing broadcasted content, etc. As a result, online social networks are an interesting field to study an online social behavior that seems to be generic among the different online services. Since at the bottom of these services lays a network of declared relations and the basic interactions in these platforms tend to be pairwise, a natural methodology for studying these systems is provided by network science. In this chapter we describe some of the results of research studies on the structure, dynamics and social activity in online social networks. We present them in the interdisciplinary context of network science, sociological studies and computer science.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling echo chambers and polarization dynamics in social networks,"Echo chambers and opinion polarization recently quantified in several sociopolitical contexts and across different social media, raise concerns on their potential impact on the spread of misinformation and on openness of debates. Despite increasing efforts, the dynamics leading to the emergence of these phenomena stay unclear. We propose a model that introduces the dynamics of radicalization, as a reinforcing mechanism driving the evolution to extreme opinions from moderate initial conditions. Inspired by empirical findings on social interaction dynamics, we consider agents characterized by heterogeneous activities and homophily. We show that the transition between a global consensus and emerging radicalized states is mostly governed by social influence and by the controversialness of the topic discussed. Compared with empirical data of polarized debates on Twitter, the model qualitatively reproduces the observed relation between users' engagement and opinions, as well as opinion segregation in the interaction network. Our findings shed light on the mechanisms that may lie at the core of the emergence of echo chambers and polarization in social media.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media","Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation. This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable. Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation. To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework. Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection. We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features). By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy. To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler. We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior. Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
How Social Media Big Data Can Improve Suicide Prevention,"In the light of increasing clues on social media impact on self-harm and suicide risks, there is still no evidence on who are and how factually engaged in suicide-related online behaviors. This study reports new findings of high-performance supercomputing investigation of publicly accessible big data sourced from one of the world-largest social networking site. Three-month supercomputer searching resulted in 570,156 young adult users who consumed suicide-related information on social media. Most of them were 21-24 year olds with higher share of females (58%) of predominantly younger age. Every eight user was alarmingly engrossed with up to 15 suicide-related online groups. Evidently, suicide groups on social media are highly underrated public health issue that might weaken the prevention efforts. Suicide prevention strategies that target social media users must be implemented extensively. While major gap in functional understanding of technologies relevance for use in public mental health still exists, current findings act for better understanding digital technologies utility for translational advance and offer relevant evidence-based framework for improving suicide prevention in general population.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Deciphering Unilateral Communication Patterns in Directed Temporal Networks: Network Role Distribution Approach,"In the vast expanse of online communication, identifying unilateral preference patterns can be pivotal in understanding and mitigating risks such as predatory behavior. This paper presents a comprehensive approach to dissect and visualize such patterns in social networks. Through the lens of a directed network model, we simulate a scenario where a predominant cluster 'A' disperses information unilaterally towards a much larger, but passive, cluster 'B', while being overseen by a vigilant cluster 'C', restricted by an information blocking cluster 'D', and countered by an alerting cluster 'E'. Incorporated into this study is a simulation framework that models the flow of information across a directed network comprising various clusters with distinct roles and communication behaviors. The simulation employs a dynamic system where clusters 'A' through 'E' interact over a series of time steps, with each cluster's activity shaped by both intrinsic message-generation rules and external media influences. By tracking the accumulated media influence on each cluster, we gain a nuanced understanding of the long-term effects of media on communication patterns. The results provide a window into the cyclical nature of influence and the propagation of information, with potential applications in detecting and mitigating unilateral communication patterns that could signal harmful activities such as online predation.   This study, therefore, presents a comprehensive approach that combines network theory, simulation modeling, and dynamic media influence analysis to explore and understand the complexities of unilateral preference communication within social networks.This paper is partially an attempt to utilize ""Generative AI"" and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling of User Portrait Through Social Media,"Nowadays, massive useful data of user information and social behavior have been accumulated on the Internet, providing a possibility of profiling user's personality traits online. In this paper, we propose a psychological modeling method based on computational linguistic features to profile Big Five personality traits of users on Sina Weibo (a Twitter-like microblogging service in China) and their correlations with user's social behaviors. To the best of our knowledge, this is the first research on investigating the potential relationship between profile information, social-network behaviors and personality traits of users on Sina Weibo. Our results demonstrate an effective modeling approach to understanding demographic and psychological portraits of users on social media without customer disruption, which is useful for commercial incorporations to provide better personalized products and services.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Continuous Influence-based Community Partition for Social Networks,"Community partition is of great importance in social networks because of the rapid increasing network scale, data and applications. We consider the community partition problem under LT model in social networks, which is a combinatorial optimization problem that divides the social network to disjoint $m$ communities. Our goal is to maximize the sum of influence propagation through maximizing it within each community. As the influence propagation function of community partition problem is supermodular under LT model, we use the method of Lov{$\acute{a}$}sz Extension to relax the target influence function and transfer our goal to maximize the relaxed function over a matroid polytope. Next, we propose a continuous greedy algorithm using the properties of the relaxed function to solve our problem, which needs to be discretized in concrete implementation. Then, random rounding technique is used to convert the fractional solution to integer solution. We present a theoretical analysis with $1-1/e$ approximation ratio for the proposed algorithms. Extensive experiments are conducted to evaluate the performance of the proposed continuous greedy algorithms on real-world online social networks datasets and the results demonstrate that continuous community partition method can improve influence spread and accuracy of the community partition effectively.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
#Bigbirds Never Die: Understanding Social Dynamics of Emergent Hashtag,"We examine the growth, survival, and context of 256 novel hashtags during the 2012 U.S. presidential debates. Our analysis reveals the trajectories of hashtag use fall into two distinct classes: ""winners"" that emerge more quickly and are sustained for longer periods of time than other ""also-rans"" hashtags. We propose a ""conversational vibrancy"" framework to capture dynamics of hashtags based on their topicality, interactivity, diversity, and prominence. Statistical analyses of the growth and persistence of hashtags reveal novel relationships between features of this framework and the relative success of hashtags. Specifically, retweets always contribute to faster hashtag adoption, replies extend the life of ""winners"" while having no effect on ""also-rans."" This is the first study on the lifecycle of hashtag adoption and use in response to purely exogenous shocks. We draw on theories of uses and gratification, organizational ecology, and language evolution to discuss these findings and their implications for understanding social influence and collective action in social media more generally.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Individual Fairness for Social Media Influencers,"Nowadays, many social media platforms are centered around content creators (CC). On these platforms, the tie formation process depends on two factors: (a) the exposure of users to CCs (decided by, e.g., a recommender system), and (b) the following decision-making process of users. Recent research studies underlined the importance of content quality by showing that under exploratory recommendation strategies, the network eventually converges to a state where the higher the quality of the CC, the higher their expected number of followers. In this paper, we extend prior work by (a) looking beyond averages to assess the fairness of the process and (b) investigating the importance of exploratory recommendations for achieving fair outcomes. Using an analytical approach, we show that non-exploratory recommendations converge fast but usually lead to unfair outcomes. Moreover, even with exploration, we are only guaranteed fair outcomes for the highest (and lowest) quality CCs.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Characterising User Content on a Multi-lingual Social Network,"Social media has been on the vanguard of political information diffusion in the 21st century. Most studies that look into disinformation, political influence and fake-news focus on mainstream social media platforms. This has inevitably made English an important factor in our current understanding of political activity on social media. As a result, there has only been a limited number of studies into a large portion of the world, including the largest, multilingual and multi-cultural democracy: India. In this paper we present our characterisation of a multilingual social network in India called ShareChat. We collect an exhaustive dataset across 72 weeks before and during the Indian general elections of 2019, across 14 languages. We investigate the cross lingual dynamics by clustering visually similar images together, and exploring how they move across language barriers. We find that Telugu, Malayalam, Tamil and Kannada languages tend to be dominant in soliciting political images (often referred to as memes), and posts from Hindi have the largest cross-lingual diffusion across ShareChat (as well as images containing text in English). In the case of images containing text that cross language barriers, we see that language translation is used to widen the accessibility. That said, we find cases where the same image is associated with very different text (and therefore meanings). This initial characterisation paves the way for more advanced pipelines to understand the dynamics of fake and political content in a multi-lingual and non-textual setting.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Hiding Individuals and Communities in a Social Network,"The Internet and social media have fueled enormous interest in social network analysis. New tools continue to be developed and used to analyse our personal connections, with particular emphasis on detecting communities or identifying key individuals in a social network. This raises privacy concerns that are likely to exacerbate in the future. With this in mind, we ask the question: Can individuals or groups actively manage their connections to evade social network analysis tools?   By addressing this question, the general public may better protect their privacy, oppressed activist groups may better conceal their existence, and security agencies may better understand how terrorists escape detection. We first study how an individual can evade ""network centrality"" analysis without compromising his or her influence within the network. We prove that an optimal solution to this problem is hard to compute. Despite this hardness, we demonstrate that even a simple heuristic, whereby attention is restricted to the individual's immediate neighbourhood, can be surprisingly effective in practice. For instance, it could disguise Mohamed Atta's leading position within the WTC terrorist network, and that is by rewiring a strikingly-small number of connections. Next, we study how a community can increase the likelihood of being overlooked by community-detection algorithms. We propose a measure of concealment, expressing how well a community is hidden, and use it to demonstrate the effectiveness of a simple heuristic, whereby members of the community either ""unfriend"" certain other members, or ""befriend"" some non-members, in a coordinated effort to camouflage their community.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Communities and Hierarchical Structures in Dynamic Social Networks: Analysis and Visualization,Detection of community structures in social networks has attracted lots of attention in the domain of sociology and behavioral sciences. Social networks also exhibit dynamic nature as these networks change continuously with the passage of time. Social networks might also present a hierarchical structure led by individuals that play important roles in a society such as Managers and Decision Makers. Detection and Visualization of these networks changing over time is a challenging problem where communities change as a function of events taking place in the society and the role people play in it.   In this paper we address these issues by presenting a system to analyze dynamic social networks. The proposed system is based on dynamic graph discretization and graph clustering. The system allows detection of major structural changes taking place in social communities over time and reveals hierarchies by identifying influential people in a social networks. We use two different data sets for the empirical evaluation and observe that our system helps to discover interesting facts about the social and hierarchical structures present in these social networks.,"cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Capacity Constrained Influence Maximization in Social Networks,"Influence maximization (IM) aims to identify a small number of influential individuals to maximize the information spread and finds applications in various fields. It was first introduced in the context of viral marketing, where a company pays a few influencers to promote the product. However, apart from the cost factor, the capacity of individuals to consume content poses challenges for implementing IM in real-world scenarios. For example, players on online gaming platforms can only interact with a limited number of friends. In addition, we observe that in these scenarios, (i) the initial adopters of promotion are likely to be the friends of influencers rather than the influencers themselves, and (ii) existing IM solutions produce sub-par results with high computational demands. Motivated by these observations, we propose a new IM variant called capacity constrained influence maximization (CIM), which aims to select a limited number of influential friends for each initial adopter such that the promotion can reach more users. To solve CIM effectively, we design two greedy algorithms, MG-Greedy and RR-Greedy, ensuring the $1/2$-approximation ratio. To improve the efficiency, we devise the scalable implementation named RR-OPIM+ with $(1/2-)$-approximation and near-linear running time. We extensively evaluate the performance of 9 approaches on 6 real-world networks, and our solutions outperform all competitors in terms of result quality and running time. Additionally, we deploy RR-OPIM+ to online game scenarios, which improves the baseline considerably.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Quantifying Mental Health from Social Media with Neural User Embeddings,"Mental illnesses adversely affect a significant proportion of the population worldwide. However, the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of mental health conditions are often years out of date. Automated approaches to supplement these survey methods with broad, aggregated information derived from social media content provides a potential means for near real-time estimates at scale. These may, in turn, provide grist for supporting, evaluating and iteratively improving upon public health programs and interventions.   We propose a novel model for automated mental health status quantification that incorporates user embeddings. This builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories. Such embeddings capture latent characteristics of individuals (e.g., political leanings) and encode a soft notion of homophily. In this paper, we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses. To this end, we estimated user embeddings for a set of users known to be affected by depression and post-traumatic stress disorder (PTSD), and for a set of demographically matched `control' users. We then evaluated these embeddings with respect to: (i) their ability to capture homophilic relations with respect to mental health status; and (ii) the performance of downstream mental health prediction models based on these features. Our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions, and are predictive of mental health.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Age of Social Sensing,"Online social media, such as Twitter and Instagram, democratized information broadcast, allowing anyone to share information about themselves and their surroundings at an unprecedented scale. The large volume of information thus posted on these media offer a new lens into the physical world through the eyes of the social network. The exploitation of this lens to inspect aspects of world state has recently been termed social sensing. The power of manipulating reality via the use (or intentional misuse) of social media opened concerns with issues ranging from radicalization by terror propaganda to potential manipulation of elections in mature democracies. Many important challenges and open research questions arise in this emerging field that aims to better understand how information can be extracted from the medium and what properties characterize the extracted information and the world it represents. Addressing the above challenges requires multi-disciplinary research at the intersection of computer science and social sciences that combines cyber-physical computing, sociology, sensor networks, social networks, cognition, data mining, estimation theory, data fusion, information theory, linguistics, machine learning, behavioral economics, and possibly others. This paper surveys important directions in social sensing, identifies current research challenges, and outlines avenues for future research.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Y Social: an LLM-powered Social Media Digital Twin,"In this paper we introduce Y, a new-generation digital twin designed to replicate an online social media platform. Digital twins are virtual replicas of physical systems that allow for advanced analyses and experimentation. In the case of social media, a digital twin such as Y provides a powerful tool for researchers to simulate and understand complex online interactions. {\tt Y} leverages state-of-the-art Large Language Models (LLMs) to replicate sophisticated agent behaviors, enabling accurate simulations of user interactions, content dissemination, and network dynamics. By integrating these aspects, Y offers valuable insights into user engagement, information spread, and the impact of platform policies. Moreover, the integration of LLMs allows Y to generate nuanced textual content and predict user responses, facilitating the study of emergent phenomena in online environments.   To better characterize the proposed digital twin, in this paper we describe the rationale behind its implementation, provide examples of the analyses that can be performed on the data it enables to be generated, and discuss its relevance for multidisciplinary research.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling,"Human personality traits are the key drivers behind our decision-making, influencing our life path on a daily basis. Inference of personality traits, such as Myers-Briggs Personality Type, as well as an understanding of dependencies between personality traits and users' behavior on various social media platforms is of crucial importance to modern research and industry applications. The emergence of diverse and cross-purpose social media avenues makes it possible to perform user personality profiling automatically and efficiently based on data represented across multiple data modalities. However, the research efforts on personality profiling from multi-source multi-modal social media data are relatively sparse, and the level of impact of different social network data on machine learning performance has yet to be comprehensively evaluated. Furthermore, there is not such dataset in the research community to benchmark. This study is one of the first attempts towards bridging such an important research gap. Specifically, in this work, we infer the Myers-Briggs Personality Type indicators, by applying a novel multi-view fusion framework, called ""PERS"" and comparing the performance results not just across data modalities but also with respect to different social network data sources. Our experimental results demonstrate the PERS's ability to learn from multi-view data for personality profiling by efficiently leveraging on the significantly different data arriving from diverse social multimedia sources. We have also found that the selection of a machine learning approach is of crucial importance when choosing social network data sources and that people tend to reveal multiple facets of their personality in different social media avenues. Our released social multimedia dataset facilitates future research on this direction.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mental Health Coping Stories on Social Media: A Causal-Inference Study of Papageno Effect,"The Papageno effect concerns how media can play a positive role in preventing and mitigating suicidal ideation and behaviors. With the increasing ubiquity and widespread use of social media, individuals often express and share lived experiences and struggles with mental health. However, there is a gap in our understanding about the existence and effectiveness of the Papageno effect in social media, which we study in this paper. In particular, we adopt a causal-inference framework to examine the impact of exposure to mental health coping stories on individuals on Twitter. We obtain a Twitter dataset with $\sim$2M posts by $\sim$10K individuals. We consider engaging with coping stories as the Treatment intervention, and adopt a stratified propensity score approach to find matched cohorts of Treatment and Control individuals. We measure the psychosocial shifts in affective, behavioral, and cognitive outcomes in longitudinal Twitter data before and after engaging with the coping stories. Our findings reveal that, engaging with coping stories leads to decreased stress and depression, and improved expressive writing, diversity, and interactivity. Our work discusses the practical and platform design implications in supporting mental wellbeing.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Predicting the Role of Political Trolls in Social Media,"We investigate the political roles of ""Internet trolls"" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role ---left, news feed, right--- by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e.,~embeddings, for the trolls. Experiments on the ""IRA Russian Troll"" dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Discovering Coordinated Processes From Social Online Networks,"The rapid growth of social media presents a unique opportunity to study coordinated agent behavior in an unfiltered environment. Online processes often exhibit complex structures that reflect the nature of the user behavior, whether it is authentic and genuine, or part of a coordinated effort by malicious agents to spread misinformation and disinformation. Detection of AI-generated content can be extremely challenging due to the high quality of large language model-generated text. Therefore, approaches that use metadata like post timings are required to effectively detect coordinated AI-driven campaigns. Existing work that models the spread of information online is limited in its ability to represent different control flows that occur within the network in practice. Process mining offers techniques for the discovery of process models with different routing constructs and are yet to be applied to social networks. We propose to leverage process mining methods for the discovery of AI and human agent behavior within social networks. Applying process mining techniques to real-world Twitter (now X) event data, we demonstrate how the structural and behavioral properties of discovered process models can reveal coordinated AI and human behaviors online.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Framework to Support the Trust Process in News and Social Media,"Current society is heavily influenced by the spread of online information, containing all sorts of claims, commonly found in news stories, tweets, and social media postings. Depending on the user, they may be considered ""true"" or ""false"", according to the agent's trust on the claim. In this paper, we discuss the concept of content trust and trust process, and propose a framework to describe the trust process, which can support various possible models of content trust.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Visualizing Communication on Social Media: Making Big Data Accessible,"The broad adoption of the web as a communication medium has made it possible to study social behavior at a new scale. With social media networks such as Twitter, we can collect large data sets of online discourse. Social science researchers and journalists, however, may not have tools available to make sense of large amounts of data or of the structure of large social networks. In this paper, we describe our recent extensions to Truthy, a system for collecting and analyzing political discourse on Twitter. We introduce several new analytical perspectives on online discourse with the goal of facilitating collaboration between individuals in the computational and social sciences. The design decisions described in this article are motivated by real-world use cases developed in collaboration with colleagues at the Indiana University School of Journalism.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Bandit algorithms for real-time data capture on large social medias,"We study the problem of real time data capture on social media. Due to the different limitations imposed by those media, but also to the very large amount of information, it is impossible to collect all the data produced by social networks such as Twitter. Therefore, to be able to gather enough relevant information related to a predefined need, it is necessary to focus on a subset of the information sources. In this work, we focus on user-centered data capture and consider each account of a social network as a source that can be listened to at each iteration of a data capture process, in order to collect the corresponding produced contents. This process, whose aim is to maximize the quality of the information gathered, is constrained by the number of users that can be monitored simultaneously. The problem of selecting a subset of accounts to listen to over time is a sequential decision problem under constraints, which we formalize as a bandit problem with multiple selections. Therefore, we propose several bandit models to identify the most relevant users in real time. First, we study of the case of the stochastic bandit, in which each user corresponds to a stationary distribution. Then, we introduce two contextual bandit models, one stationary and the other non stationary, in which the utility of each user can be estimated by assuming some underlying structure in the reward space. The first approach introduces the notion of profile, which corresponds to the average behavior of a user. The second approach takes into account the activity of a user in order to predict his future behavior. Finally, we are interested in models that are able to tackle complex temporal dependencies between users, with the use of a latent space within which the information transits from one iteration to the other. Each of the proposed approaches is validated on both artificial and real datasets.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
How Visibility and Divided Attention Constrain Social Contagion,"How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that %retweeting behavior is constrained by a few simple principles. the ""principle of least effort"" combined with limited attention plays a dominant role in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter stream. Moreover, our measurements quantify how a user's limited attention is divided among incoming tweets, providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet. Our study indicates that the finite ability to process incoming information constrains social contagion, and we conclude that rapid decay of visibility is the primary barrier to information propagation online.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Generation and Influence of Eccentric Ideas on Social Networks,"Studying extreme ideas in routine choices and discussions is of utmost importance to understand the increasing polarization in society. In this study, we focus on understanding the generation and influence of extreme ideas in routine conversations which we label ""eccentric"" ideas. The eccentricity of any idea is defined as the deviation of that idea from the norm of the social neighborhood. We collected and analyzed data from two completely different sources: public social media and online experiments in a controlled environment. We compared the popularity of ideas against their eccentricity to understand individuals' fascination towards eccentricity. We found that more eccentric ideas have a higher probability of getting a greater number of ""likes"". Additionally, we demonstrate that the social neighborhood of an individual conceals eccentricity changes in one's own opinions and facilitates generation of eccentric ideas at a collective level.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Network Analysis for Social Neuroscientists,"Although social neuroscience is concerned with understanding how the brain interacts with its social environment, prevailing research in the field has primarily considered the human brain in isolation, deprived of its rich social context. Emerging work in social neuroscience that leverages tools from network analysis has begun to pursue this issue, advancing knowledge of how the human brain influences and is influenced by the structures of its social environment. In this paper, we provide an overview of key theory and methods in network analysis (especially for social systems) as an introduction for social neuroscientists who are interested in relating individual cognition to the structures of an individual's social environments. We also highlight some exciting new work as examples of how to productively use these tools to investigate questions of relevance to social neuroscientists. We include tutorials to help with practical implementation of the concepts that we discuss. We conclude by highlighting a broad range of exciting research opportunities for social neuroscientists who are interested in using network analysis to study social systems.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media,"Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Dissecting a Social Botnet: Growth, Content and Influence in Twitter","Social botnets have become an important phenomenon on social media. There are many ways in which social bots can disrupt or influence online discourse, such as, spam hashtags, scam twitter users, and astroturfing. In this paper we considered one specific social botnet in Twitter to understand how it grows over time, how the content of tweets by the social botnet differ from regular users in the same dataset, and lastly, how the social botnet may have influenced the relevant discussions. Our analysis is based on a qualitative coding for approximately 3000 tweets in Arabic and English from the Syrian social bot that was active for 35 weeks on Twitter before it was shutdown. We find that the growth, behavior and content of this particular botnet did not specifically align with common conceptions of botnets. Further we identify interesting aspects of the botnet that distinguish it from regular users.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Network Prebunking Problem: Optimizing Prebunking Targets to Suppress the Spread of Misinformation in Social Networks,"As a countermeasure against misinformation that undermines the healthy use of social media, a preventive intervention known as \textit{prebunking} has recently attracted attention in the field of psychology. Prebunking aims to strengthen individuals' cognitive resistance to misinformation by presenting weakened doses of misinformation or by teaching common manipulation techniques before they encounter actual misinformation. Despite the growing body of evidence supporting its effectiveness in reducing susceptibility to misinformation at the individual level, an important open question remains: how best to identify the optimal targets for prebunking interventions to mitigate the spread of misinformation in a social network. To address this issue, we formulate a combinatorial optimization problem, called the \textit{network prebunking problem}, which aims to select optimal prebunking targets that minimizes the spread of misinformation in a social network under limited intervention budgets. We show that the problem is NP-hard and that its objective function is monotone and submodular, which provides a theoretical foundation for approximation guarantees of greedy algorithms. However, since the greedy algorithm is computationally expensive and does not scale to large networks, we propose an efficient approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence (MIA) approach, which restricts influence propagation around each node to a local directed tree rooted at that node. Through numerical experiments using real-world social network datasets, we demonstrate that MIA-NPP effectively suppresses the spread of misinformation under both fully observed and uncertain model parameter settings.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Blue Start: A large-scale pairwise and higher-order social network dataset,"Large-scale networks have been instrumental in shaping the way that we think about how individuals interact with one another, developing key insights in mathematical epidemiology, computational social science, and biology. However, many of the underlying social systems through which diseases spread, information disseminates, and individuals interact are inherently mediated through groups of arbitrary size, known as higher-order interactions. There is a gap between higher-order dynamics of group formation and fragmentation, contagion spread, and social influence and the data necessary to validate these higher-order mechanisms. Similarly, few datasets bridge the gap between these pairwise and higher-order network data. Because of its open API, the Bluesky social media platform provides a laboratory for observing social ties at scale. In addition to pairwise following relationships, unlike many other social networks, Bluesky features user-curated lists known as ""starter packs"" as a mechanism for social network growth. We introduce ""A Blue Start"", a large-scale network dataset comprising 26.7M users and their 1.6B pairwise following relationships and 301.3K groups representing starter packs. This dataset will be an essential resource for the study of higher-order network science.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic Social Networks,"Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SleepNet, a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights the improvement provided by incorporating social networks in the model. Additionally, we conduct robustness analysis to demonstrate the system's performance in real-life conditions. The outcomes affirm the stability of SleepNet against perturbations in input data. Further analyses emphasize the significance of network topology in prediction performance revealing that users with higher eigenvalue centrality are more vulnerable to data perturbations.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"White or Blue, the Whale gets its Vengeance: A Social Media Analysis of the Blue Whale Challenge","The Blue Whale Challenge is a series of self-harm causing tasks that are propagated via online social media under the disguise of a ""game."" The list of tasks must be completed in a duration of 50 days and they cause both physical and mental harm to the player. The final task is to commit suicide. The game is supposed to be administered by people called ""curators"" who incite others to cause self-mutilation and commit suicide. The curators and potential players are known to contact each other on social networking websites and the conversations between them are suspected to take place mainly via direct messages which are difficult to track. Though, in order to find curators, the players make public posts containing certain hashtags/keywords to catch their attention. Even though a lot of these social networks have moderated posts talking about the game, yet some posts manage to pass their filters. Our research focuses on (1) understanding the social media spread of the challenge, (2) spotting the behaviour of the people taking interest in Blue Whale challenge and, (3) analysing demographics of the users who may be involved in playing the game.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Assessing Vaccination Sentiments with Online Social Media: Implications for Infectious Disease Dynamics and Control,"There is great interest in the dynamics of health behaviors in social networks and how they affect collective public health outcomes, but measuring population health behaviors over time and space requires substantial resources. Here, we use publicly available data from 101,853 users of online social media collected over a time period of almost six months to measure the spatio-temporal sentiment towards a new vaccine. We validated our approach by identifying a strong correlation between sentiments expressed online and CDC- estimated vaccination rates by region. Analysis of the network of opinionated users showed that information flows more often between users who share the same sentiments - and less often between users who do not share the same sentiments - than expected by chance alone. We also found that most communities are dominated by either positive or negative sentiments towards the novel vaccine. Simulations of infectious disease transmission show that if clusters of negative vaccine sentiments lead to clusters of unprotected individuals, the likelihood of disease outbreaks are greatly increased. Online social media provide unprecedented access to data allowing for inexpensive and efficient tools to identify target areas for intervention efforts and to evaluate their effectiveness.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
How to Network in Online Social Networks,"In this paper, we consider how to maximize users' influence in Online Social Networks (OSNs) by exploiting social relationships only. Our first contribution is to extend to OSNs the model of Kempe et al. [1] on the propagation of information in a social network and to show that a greedy algorithm is a good approximation of the optimal algorithm that is NP-hard. However, the greedy algorithm requires global knowledge, which is hardly practical. Our second contribution is to show on simulations on the full Twitter social graph that simple and practical strategies perform close to the greedy algorithm.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Analyzing Social Book Reading Behavior on Goodreads and how it predicts Amazon Best Sellers,"A book's success/popularity depends on various parameters - extrinsic and intrinsic. In this paper, we study how the book reading characteristics might influence the popularity of a book. Towards this objective, we perform a cross-platform study of Goodreads entities and attempt to establish the connection between various Goodreads entities and the popular books (""Amazon best sellers""). We analyze the collective reading behavior on Goodreads platform and quantify various characteristic features of the Goodreads entities to identify differences between these Amazon best sellers (ABS) and the other non-best selling books. We then develop a prediction model using the characteristic features to predict if a book shall become a best seller after one month (15 days) since its publication. On a balanced set, we are able to achieve a very high average accuracy of 88.72% (85.66%) for the prediction where the other competitive class contains books which are randomly selected from the Goodreads dataset. Our method primarily based on features derived from user posts and genre related characteristic properties achieves an improvement of 16.4% over the traditional popularity factors (ratings, reviews) based baseline methods. We also evaluate our model with two more competitive set of books a) that are both highly rated and have received a large number of reviews (but are not best sellers) (HRHR) and b) Goodreads Choice Awards Nominated books which are non-best sellers (GCAN). We are able to achieve quite good results with very high average accuracy of 87.1% and as well a high ROC for ABS vs GCAN. For ABS vs HRHR, our model yields a high average accuracy of 86.22%.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Uncovering Social Network Activity Using Joint User and Topic Interaction,"The emergence of online social platforms, such as social networks and social media, has drastically affected the way people apprehend the information flows to which they are exposed. In such platforms, various information cascades spreading among users is the main force creating complex dynamics of opinion formation, each user being characterized by their own behavior adoption mechanism. Moreover, the spread of multiple pieces of information or beliefs in a networked population is rarely uncorrelated. In this paper, we introduce the Mixture of Interacting Cascades (MIC), a model of marked multidimensional Hawkes processes with the capacity to model jointly non-trivial interaction between cascades and users. We emphasize on the interplay between information cascades and user activity, and use a mixture of temporal point processes to build a coupled user/cascade point process model. Experiments on synthetic and real data highlight the benefits of this approach and demonstrate that MIC achieves superior performance to existing methods in modeling the spread of information cascades. Finally, we demonstrate how MIC can provide, through its learned parameters, insightful bi-layered visualizations of real social network activity data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media,"Mindset reconstruction maps how individuals structure and perceive knowledge, a map unfolded here by investigating language and its cognitive reflection in the human mind, i.e. the mental lexicon. Textual forma mentis networks (TFMN) are glass boxes introduced for extracting, representing and understanding mindsets' structure, in Latin ""forma mentis"", from textual data. Combining network science, psycholinguistics and Big Data, TFMNs successfully identified relevant concepts, without supervision, in benchmark texts. Once validated, TFMNs were applied to the case study of the gender gap in science, which was strongly linked to distorted mindsets by recent studies. Focusing over social media perception and online discourse, this work analysed 10,000 relevant tweets. ""Gender"" and ""gap"" elicited a mostly positive perception, with a trustful/joyous emotional profile and semantic associates that: celebrated successful female scientists, related gender gap to wage differences, and hoped for a future resolution. The perception of ""woman"" highlighted discussion about sexual harassment and stereotype threat (a form of implicit cognitive bias) relative to women in science ""sacrificing personal skills for success"". The reconstructed perception of ""man"" highlighted social users' awareness of the myth of male superiority in science. No anger was detected around ""person"", suggesting that gap-focused discourse got less tense around genderless terms. No stereotypical perception of ""scientist"" was identified online, differently from real-world surveys. The overall analysis identified the online discourse as promoting a mostly stereotype-free, positive/trustful perception of gender disparity, aware of implicit/explicit biases and projected to closing the gap. TFMNs opened new ways for investigating perceptions in different groups, offering detailed data-informed grounding for policy making.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events,"Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence Pathway Discovery on Social Media,"This paper addresses influence pathway discovery, a key emerging problem in today's online media. We propose a discovery algorithm that leverages recently published work on unsupervised interpretable ideological embedding, a mapping of ideological beliefs (done in a self-supervised fashion) into interpretable low-dimensional spaces. Computing the ideological embedding at scale allows one to analyze correlations between the ideological positions of leaders, influencers, news portals, or population segments, deriving potential influence pathways. The work is motivated by the importance of social media as the preeminent means for global interactions and collaborations on today's Internet, as well as their frequent (mis-)use to wield influence that targets social beliefs and attitudes of selected populations. Tools that enable the understanding and mapping of influence propagation through population segments on social media are therefore increasingly important. In this paper, influence is measured by the perceived ideological shift over time that is correlated with influencers' activity. Correlated shifts in ideological embeddings indicate changes, such as swings/switching (among competing ideologies), polarization (depletion of neutral ideological positions), escalation/radicalization (shifts to more extreme versions of the ideology), or unification/cooldown (shifts towards more neutral stances). Case-studies are presented to explore selected influence pathways (i) in a recent French election, (ii) during political discussions in the Philippines, and (iii) for some Russian messaging during the Russia/Ukraine conflict.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Network-Aware Approach for Searching As-You-Type in Social Media (Extended Version),"We present in this paper a novel approach for as-you-type top-$k$ keyword search over social media. We adopt a natural ""network-aware"" interpretation for information relevance, by which information produced by users who are closer to the seeker is considered more relevant. In practice, this query model poses new challenges for effectiveness and efficiency in online search, even when a complete query is given as input in one keystroke. This is mainly because it requires a joint exploration of the social space and classic IR indexes such as inverted lists. We describe a memory-efficient and incremental prefix-based retrieval algorithm, which also exhibits an anytime behavior, allowing to output the most likely answer within any chosen running-time limit. We evaluate it through extensive experiments for several applications and search scenarios, including searching for posts in micro-blogging (Twitter and Tumblr), as well as searching for businesses based on reviews in Yelp. They show that our solution is effective in answering real-time as-you-type searches over social media.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Quantified Self Meets Social Media: Sharing of Weight Updates on Twitter,"An increasing number of people use wearables and other smart devices to quantify various health conditions, ranging from sleep patterns, to body weight, to heart rates. Of these Quantified Selfs many choose to openly share their data via online social networks such as Twitter and Facebook. In this study, we use data for users who have chosen to connect their smart scales to Twitter, providing both a reliable time series of their body weight, as well as insights into their social surroundings and general online behavior. Concretely, we look at which social media features are predictive of physical status, such as body weight at the individual level, and activity patterns at the population level. We show that it is possible to predict an individual's weight using their online social behaviors, such as their self-description and tweets. Weekly and monthly patterns of quantified-self behaviors are also discovered. These findings could contribute to building models to monitor public health and to have more customized personal training interventions.   While there are many studies using either quantified self or social media data in isolation, this is one of the few that combines the two data sources and, to the best of our knowledge, the only one that uses public data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Predicting Adoption Probabilities in Social Networks,"In a social network, adoption probability refers to the probability that a social entity will adopt a product, service, or opinion in the foreseeable future. Such probabilities are central to fundamental issues in social network analysis, including the influence maximization problem. In practice, adoption probabilities have significant implications for applications ranging from social network-based target marketing to political campaigns; yet, predicting adoption probabilities has not received sufficient research attention. Building on relevant social network theories, we identify and operationalize key factors that affect adoption decisions: social influence, structural equivalence, entity similarity, and confounding factors. We then develop the locally-weighted expectation-maximization method for Nave Bayesian learning to predict adoption probabilities on the basis of these factors. The principal challenge addressed in this study is how to predict adoption probabilities in the presence of confounding factors that are generally unobserved. Using data from two large-scale social networks, we demonstrate the effectiveness of the proposed method. The empirical results also suggest that cascade methods primarily using social influence to predict adoption probabilities offer limited predictive power, and that confounding factors are critical to adoption probability predictions.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Weakening the Inner Strength: Spotting Core Collusive Users in YouTube Blackmarket Network,"Social reputation (e.g., likes, comments, shares, etc.) on YouTube is the primary tenet to popularize channels/videos. However, the organic way to improve social reputation is tedious, which often provokes content creators to seek the services of online blackmarkets for rapidly inflating content reputation. Such blackmarkets act underneath a thriving collusive ecosystem comprising core users and compromised accounts (together known as collusive users). Core users form the backbone of blackmarkets; thus, spotting and suspending them may help in destabilizing the entire collusive network. Although a few studies focused on collusive user detection on Twitter, Facebook, and YouTube, none of them differentiate between core users and compromised accounts.   We are the first to present a rigorous analysis of core users in YouTube blackmarkets. To this end, we collect a new dataset of collusive YouTube users. We study the core-periphery structure of the underlying collusive commenting network (CCN). We examine the topology of CCN to explore the behavioral dynamics of core and compromised users. We then introduce KORSE, a novel graph-based method to automatically detect core users based only on the topological structure of CCN. KORSE performs a weighted k-core decomposition using our proposed metric, called Weighted Internal Core Collusive Index (WICCI). However, KORSE is infeasible to adopt in practice as it requires complete interactions among collusive users to construct CCN. We, therefore, propose NURSE, a deep fusion framework that only leverages user timelines (without considering the underlying CCN) to detect core blackmarket users. Experimental results show that NURSE is quite close to KORSE in detecting core users and outperforms nine baselines.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Discovering Opioid Use Patterns from Social Media for Relapse Prevention,"The United States is currently experiencing an unprecedented opioid crisis, and opioid overdose has become a leading cause of injury and death. Effective opioid addiction recovery calls for not only medical treatments, but also behavioral interventions for impacted individuals. In this paper, we study communication and behavior patterns of patients with opioid use disorder (OUD) from social media, intending to demonstrate how existing information from common activities, such as online social networking, might lead to better prediction, evaluation, and ultimately prevention of relapses. Through a multi-disciplinary and advanced novel analytic perspective, we characterize opioid addiction behavior patterns by analyzing opioid groups from Reddit.com - including modeling online discussion topics, analyzing text co-occurrence and correlations, and identifying emotional states of people with OUD. These quantitative analyses are of practical importance and demonstrate innovative ways to use information from online social media, to create technology that can assist in relapse prevention.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Negative Ties Highlight Hidden Extremes in Social Media Polarization,"Human interactions in the online world comprise a combination of positive and negative exchanges. These diverse interactions can be captured using signed network representations, where edges take positive or negative weights to indicate the sentiment of the interaction between individuals. Signed networks offer valuable insights into online political polarization by capturing antagonistic interactions and ideological divides on social media platforms. This study analyzes polarization on Meneame, a Spanish social media platform that facilitates engagement with news stories through comments and voting. Using a dual-method approach, Signed Hamiltonian Eigenvector Embedding for Proximity (SHEEP) for signed networks and Correspondence Analysis (CA) for unsigned networks, we investigate how including negative ties enhances the understanding of structural polarization levels across different conversation topics on the platform. While the unsigned Meneame network effectively delineates ideological communities, only by incorporating negative ties can we identify ideologically extreme users who engage in antagonistic behaviors: without them, the most extreme users remain indistinguishable from their less confrontational ideological peers.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Tracking the evolution of crisis processes and mental health on social media during the COVID-19 pandemic,"The COVID-19 pandemic has affected all aspects of society, not only bringing health hazards, but also posing challenges to public order, governments and mental health. Moreover, it is the first one in history in which people from around the world uses social media to massively express their thoughts and concerns. This study aims at examining the stages of crisis response and recovery as a sociological problem by operationalizing a well-known model of crisis stages in terms of a psycho-linguistic analysis. Based on a large collection of Twitter data spanning from March to August 2020 in Argentina, we present a thematic analysis on the differences in language used in social media posts, and look at indicators that reveal the different stages of a crisis and the country response thereof. The analysis was combined with a study of the temporal prevalence of mental health conversations across the time span. Beyond the Argentinian case-study, the proposed approach and analyses can be applied to any public large-scale data. This approach can provide insights for the design of public health politics oriented to monitor and eventually intervene during the different stages of a crisis, and thus improve the adverse mental health effects on the population.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Network-based Control of Epidemic via Flattening the Infection Curve: High-Clustered vs. Low-Clustered Social Networks,"Recent studies in network science and control have shown a meaningful relationship between the epidemic processes (e.g., COVID-19 spread) and some network properties. This paper studies how such network properties, namely clustering coefficient and centrality measures (or node influence metrics), affect the spread of viruses and the growth of epidemics over scale-free networks. The results can be used to target individuals (the nodes in the network) to \textit{flatten the infection curve}. This so-called flattening of the infection curve is to reduce the health service costs and burden to the authorities/governments. Our Monte-Carlo simulation results show that clustered networks are, in general, easier to flatten the infection curve, i.e., with the same connectivity and the same number of isolated individuals they result in more flattened curves. Moreover, distance-based centrality measures, which target the nodes based on their average network distance to other nodes (and not the node degrees), are better choices for targeting individuals for isolation/vaccination.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks,"Understanding and predicting the popularity of online items is an important open problem in social media analysis. Considerable progress has been made recently in data-driven predictions, and in linking popularity to external promotions. However, the existing methods typically focus on a single source of external influence, whereas for many types of online content such as YouTube videos or news articles, attention is driven by multiple heterogeneous sources simultaneously - e.g. microblogs or traditional media coverage. Here, we propose RNN-MAS, a recurrent neural network for modeling asynchronous streams. It is a sequence generator that connects multiple streams of different granularity via joint inference. We show RNN-MAS not only to outperform the current state-of-the-art Youtube popularity prediction system by 17%, but also to capture complex dynamics, such as seasonal trends of unseen influence. We define two new metrics: promotion score quantifies the gain in popularity from one unit of promotion for a Youtube video; the loudness level captures the effects of a particular user tweeting about the video. We use the loudness level to compare the effects of a video being promoted by a single highly-followed user (in the top 1% most followed users) against being promoted by a group of mid-followed users. We find that results depend on the type of content being promoted: superusers are more successful in promoting Howto and Gaming videos, whereas the cohort of regular users are more influential for Activism videos. This work provides more accurate and explainable popularity predictions, as well as computational tools for content producers and marketers to allocate resources for promotion campaigns.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The power of dynamic social networks to predict individuals' mental health,"Precision medicine has received attention both in and outside the clinic. We focus on the latter, by exploiting the relationship between individuals' social interactions and their mental health to develop a predictive model of one's likelihood to be depressed or anxious from rich dynamic social network data. To our knowledge, we are the first to do this. Existing studies differ from our work in at least one aspect: they do not model social interaction data as a network; they do so but analyze static network data; they examine ""correlation"" between social networks and health but without developing a predictive model; or they study other individual traits but not mental health. In a systematic and comprehensive evaluation, we show that our predictive model that uses dynamic social network data is superior to its static network as well as non-network equivalents when run on the same data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Knowledge workers collaborative learning behavior modeling in an organizational social network,"Computations related to learning processes within an organizational social network area require some network model preparation and specific algorithms in order to implement human behaviors in simulated environments. The proposals in this research model of collaborative learning in an organizational social network are based on knowledge resource distribution through the establishment of a knowledge flow. The nodes, which represent knowledge workers, contain information about workers social and cognitive abilities. Moreover, the workers are described by their set of competences, their skill level, and the collaborative learning behavior that can be detected through knowledge flow analysis. The proposed approach assumes that an increase in workers competence is a result of collaborative learning. In other words, collaborative learning can be analyzed as a process of knowledge flow that is being broadcast in a network. In order to create a more effective organizational social network for co-learning, the authors found the best strategies for knowledge facilitator, knowledge collector, and expert roles allocation. Special attention is paid to the process of knowledge flow in the community of practice. Acceleration within the community of practice happens when knowledge flows more effectively between community members. The presented procedure makes it possible to add new ties to the community of practice in order to influence community members competences. Both the proposed allocation and acceleration approaches were confirmed through simulations.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Virality Prediction and Community Structure in Social Networks,"How does network structure affect diffusion? Recent studies suggest that the answer depends on the type of contagion. Complex contagions, unlike infectious diseases (simple contagions), are affected by social reinforcement and homophily. Hence, the spread within highly clustered communities is enhanced, while diffusion across communities is hampered. A common hypothesis is that memes and behaviors are complex contagions. We show that, while most memes indeed behave like complex contagions, a few viral memes spread across many communities, like diseases. We demonstrate that the future popularity of a meme can be predicted by quantifying its early spreading pattern in terms of community concentration. The more communities a meme permeates, the more viral it is. We present a practical method to translate data about community structure into predictive knowledge about what information will spread widely. This connection may lead to significant advances in computational social science, social media analytics, and marketing applications.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
System for Filtering Messages on Social Media Content,"The social networking era has left us with little privacy. The details of the social network users are published on Social Networking sites. Vulnerability has reached new heights due to the overpowering effects of social networking. The sites like Facebook, Twitter are having a huge set of users who publish their files, comments, messages in other users walls. These messages and comments could be of any nature. Even friends could post a comment that would harm a persons integrity. Thus there has to be a system which will monitor the messages and comments that are posted on the walls. If the messages are found to be neutral (does not have any harmful content), then it can be published. If the messages are found to have non-neutral content in them, then these messages would be blocked by the social network manager. The messages that are non-neutral would be of sexual, offensive, hatred, pun intended nature. Thus the social network manager can classify content as neutral and non-neutral and notify the user if there seems to be messages of non-neutral behavior.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Heterogeneous Susceptibilities in Social Influence Models,"Network autocorrelation models are widely used to evaluate the impact of social influence on some variable of interest. This is a large class of models that parsimoniously accounts for how one's neighbors influence one's own behaviors or opinions by incorporating the network adjacency matrix into the joint distribution of the data. These models assume homogeneous susceptibility to social influence, however, which may be a strong assumption in many contexts. This paper proposes a hierarchical model that allows the influence parameter to be a function of individual attributes and/or of local network topological features. We derive an approximation of the posterior distribution in a general framework that is applicable to the Durbin, network effects, network disturbances, or network moving average autocorrelation models. The proposed approach can also be applied to investigating determinants of social influence in the context of egocentric network data. We apply our method to a data set collected via mobile phones in which we determine the effect of social influence on physical activity levels, as well as classroom data in which we investigate peer influence on student defiance. With this last data set, we also investigate the performance of the proposed egocentric network model.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
TIES: Temporal Interaction Embeddings For Enhancing Social Media Integrity At Facebook,"Since its inception, Facebook has become an integral part of the online social community. People rely on Facebook to make connections with others and build communities. As a result, it is paramount to protect the integrity of such a rapidly growing network in a fast and scalable manner. In this paper, we present our efforts to protect various social media entities at Facebook from people who try to abuse our platform. We present a novel Temporal Interaction EmbeddingS (TIES) model that is designed to capture rogue social interactions and flag them for further suitable actions. TIES is a supervised, deep learning, production ready model at Facebook-scale networks. Prior works on integrity problems are mostly focused on capturing either only static or certain dynamic features of social entities. In contrast, TIES can capture both these variant behaviors in a unified model owing to the recent strides made in the domains of graph embedding and deep sequential pattern learning. To show the real-world impact of TIES, we present a few applications especially for preventing spread of misinformation, fake account detection, and reducing ads payment risks in order to enhance the platform's integrity.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Influence of the Dynamic Social Network Timeframe Type and Size on the Group Evolution Discovery,"New technologies allow to store vast amount of data about users interaction. From those data the social network can be created. Additionally, because usually also time and dates of this activities are stored, the dynamic of such network can be analysed by splitting it into many timeframes representing the state of the network during specific period of time. One of the most interesting issue is group evolution over time. To track group evolution the GED method can be used. However, choice of the timeframe type and length might have great influence on the method results. Therefore, in this paper, the influence of timeframe type as well as timeframe length on the GED method results is extensively analysed.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Attacking Fake News Detectors via Manipulating News Social Engagement,"Social media is one of the main sources for news consumption, especially among the younger generation. With the increasing popularity of news consumption on various social media platforms, there has been a surge of misinformation which includes false information or unfounded claims. As various text- and social context-based fake news detectors are proposed to detect misinformation on social media, recent works start to focus on the vulnerabilities of fake news detectors. In this paper, we present the first adversarial attack framework against Graph Neural Network (GNN)-based fake news detectors to probe their robustness. Specifically, we leverage a multi-agent reinforcement learning (MARL) framework to simulate the adversarial behavior of fraudsters on social media. Research has shown that in real-world settings, fraudsters coordinate with each other to share different news in order to evade the detection of fake news detectors. Therefore, we modeled our MARL framework as a Markov Game with bot, cyborg, and crowd worker agents, which have their own distinctive cost, budget, and influence. We then use deep Q-learning to search for the optimal policy that maximizes the rewards. Extensive experimental results on two real-world fake news propagation datasets demonstrate that our proposed framework can effectively sabotage the GNN-based fake news detector performance. We hope this paper can provide insights for future research on fake news detection.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A community role approach to assess social capitalists visibility in the Twitter network,"In the context of Twitter, social capitalists are specific users trying to increase their number of followers and interactions by any means. These users are not healthy for the service, because they are either spammers or real users flawing the notions of influence and visibility. Studying their behavior and understanding their position in Twit-ter is thus of important interest. It is also necessary to analyze how these methods effectively affect user visibility. Based on a recently proposed method allowing to identify social capitalists, we tackle both points by studying how they are organized, and how their links spread across the Twitter follower-followee network. To that aim, we consider their position in the network w.r.t. its community structure. We use the concept of community role of a node, which describes its position in a network depending on its connectiv-ity at the community level. However, the topological measures originally defined to characterize these roles consider only certain aspects of the community-related connectivity, and rely on a set of empirically fixed thresholds. We first show the limitations of these measures, before extending and generalizing them. Moreover, we use an unsupervised approach to identify the roles, in order to provide more flexibility relatively to the studied system. We then apply our method to the case of social capitalists and show they are highly visible on Twitter, due to the specific roles they hold.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Cost Function Learning in Memorized Social Networks with Cognitive Behavioral Asymmetry,"This paper investigates the cost function learning in social information networks, wherein the influence of humans' memory on information consumption is explicitly taken into account. We first propose a model for social information-diffusion dynamics with a focus on systematic modeling of asymmetric cognitive bias, represented by confirmation bias and novelty bias. Building on the proposed social model, we then propose the M$^{3}$IRL: a model and maximum-entropy based inverse reinforcement learning framework for learning the cost functions of target individuals in the memorized social networks. Compared with the existing Bayesian IRL, maximum entropy IRL, relative entropy IRL and maximum causal entropy IRL, the characteristics of M$^{3}$IRL are significantly different here: no dependency on the Markov Decision Process principle, the need of only a single finite-time trajectory sample, and bounded decision variables. Finally, the effectiveness of the proposed social information-diffusion model and the M$^{3}$IRL algorithm are validated by the online social media data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
BlueTempNet: A Temporal Multi-network Dataset of Social Interactions in Bluesky Social,"Decentralized social media platforms like Bluesky Social (Bluesky) have made it possible to publicly disclose some user behaviors with millisecond-level precision. Embracing Bluesky's principles of open-source and open-data, we present the first collection of the temporal dynamics of user-driven social interactions. BlueTempNet integrates multiple types of networks into a single multi-network, including user-to-user interactions (following and blocking users) and user-to-community interactions (creating and joining communities). Communities are user-formed groups in custom Feeds, where users subscribe to posts aligned with their interests. Following Bluesky's public data policy, we collect existing Bluesky Feeds, including the users who liked and generated these Feeds, and provide tools to gather users' social interactions within a date range. This data-collection strategy captures past user behaviors and supports the future data collection of user behavior.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Mining social media data for biomedical signals and health-related behavior,"Social media data has been increasingly used to study biomedical and health-related phenomena. From cohort level discussions of a condition to planetary level analyses of sentiment, social media has provided scientists with unprecedented amounts of data to study human behavior and response associated with a variety of health conditions and medical treatments. Here we review recent work in mining social media for biomedical, epidemiological, and social phenomena information relevant to the multilevel complexity of human health. We pay particular attention to topics where social media data analysis has shown the most progress, including pharmacovigilance, sentiment analysis especially for mental health, and other areas. We also discuss a variety of innovative uses of social media data for health-related applications and important limitations in social media data access and use.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
User Identity Linkage on Social Networks: A Review of Modern Techniques and Applications,"In an Online Social Network (OSN), users can create a unique public persona by crafting a user identity that may encompass profile details, content, and network-related information. As a result, a relevant task of interest is related to the ability to link identities across different OSNs. Linking users across social networks can have multiple implications in several contexts both at the individual level and at the group level. At the individual level, the main interest in linking the same identity across social networks is to enable a better knowledge of each user. At the group level, linking user identities through different OSNs helps in predicting user behaviors, network dynamics, information diffusion, and migration phenomena across social media. The process of tying together user accounts on different OSNs is challenging and has attracted more and more research attention in the last fifteen years. The purpose of this work is to provide a comprehensive review of recent studies (from 2016 to the present) on User Identity Linkage (UIL) methods across online social networks. This review aims to offer guidance for other researchers in the field by outlining the main problem formulations, the different feature extraction strategies, algorithms, machine learning models, datasets, and evaluation metrics proposed by researchers working in this area. The proposed overview takes a pragmatic perspective to highlight the concrete possibilities for accomplishing this task depending on the type of available data.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Prediction of Cyberbullying Incidents on the Instagram Social Network,"Cyberbullying is a growing problem affecting more than half of all American teens. The main goal of this paper is to investigate fundamentally new approaches to understand and automatically detect and predict incidents of cyberbullying in Instagram, a media-based mobile social network. In this work, we have collected a sample data set consisting of Instagram images and their associated comments. We then designed a labeling study and employed human contributors at the crowd-sourced CrowdFlower website to label these media sessions for cyberbullying. A detailed analysis of the labeled data is then presented, including a study of relationships between cyberbullying and a host of features such as cyberaggression, profanity, social graph features, temporal commenting behavior, linguistic content, and image content. Using the labeled data, we further design and evaluate the performance of classifiers to automatically detect and pre- dict incidents of cyberbullying and cyberaggression.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Low Government Performance and Uncivil Political Posts on Social Media: Evidence from the COVID-19 Crisis in the US,"Political expression through social media has already taken root as a form of political participation. Meanwhile, democracy seems to be facing an epidemic of incivility on social media platforms. With this background, online political incivility has recently become a growing concern in the field of political communication studies. However, it is less clear how a government's performance is linked with people's uncivil political expression on social media; investigating the existence of performance evaluation behavior through social media expression seems to be important, as it is a new form of non-institutionalized political participation. To fill this gap in the literature, the present study hypothesizes that when government performance worsens, people become frustrated and send uncivil messages to the government via social media. To test this hypothesis, the present study collected over 8 million posts on X/Twitter directed at US state governors and classified them as uncivil or not, using a neural network-based machine learning method, and examined the impact of worsening state-level COVID-19 cases on the number of uncivil posts directed at state governors. The results of the statistical analyses showed that increases in state-level COVID-19 cases led to a significantly higher number of uncivil posts against state governors. Finally, the present study discusses the implications of the findings from two perspectives: non-institutionalized political participation and the importance of elections in democracies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Politics and Propaganda on Social Media: How Twitter and Meta Moderate State-Linked Information Operations,"Why do Social Media Corporations (SMCs) engage in state-linked information operations? Social media can significantly influence the global political landscape, allowing governments and other political entities to engage in concerted information operations, shaping or manipulating domestic and foreign political agendas. In response to state-linked political manipulation tactics on social media, Twitter and Meta carried out take-down operations against propaganda networks, accusing them of interfering foreign elections, organizing disinformation campaigns, manipulating political debates and many other issues. This research investigates the two SMCs' policy orientation to explain which factors can affect these two companies' reaction against state-linked information operations. We find that good governance indicators such as democracy are significant elements of SMCs' country-focus. This article also examines whether Meta and Twitter's attention to political regime characteristics is influenced by international political alignments. This research illuminates recent trends in SMCs' take-down operations and illuminating interplay between geopolitics and domestic regime characteristics.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Community Detection for Heterogeneous Multiple Social Networks,"The community plays a crucial role in understanding user behavior and network characteristics in social networks. Some users can use multiple social networks at once for a variety of objectives. These users are called overlapping users who bridge different social networks. Detecting communities across multiple social networks is vital for interaction mining, information diffusion, and behavior migration analysis among networks. This paper presents a community detection method based on nonnegative matrix tri-factorization for multiple heterogeneous social networks, which formulates a common consensus matrix to represent the global fused community. Specifically, the proposed method involves creating adjacency matrices based on network structure and content similarity, followed by alignment matrices which distinguish overlapping users in different social networks. With the generated alignment matrices, the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks. The effectiveness of the proposed method is evaluated with new metrics on Twitter, Instagram, and Tumblr datasets. The results of the experiments demonstrate its superior performance in terms of community quality and community fusion.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Which friends are more popular than you? Contact strength and the friendship paradox in social networks,"The friendship paradox states that in a social network, egos tend to have lower degree than their alters, or, ""your friends have more friends than you do"". Most research has focused on the friendship paradox and its implications for information transmission, but treating the network as static and unweighted. Yet, people can dedicate only a finite fraction of their attention budget to each social interaction: a high-degree individual may have less time to dedicate to individual social links, forcing them to modulate the quantities of contact made to their different social ties. Here we study the friendship paradox in the context of differing contact volumes between egos and alters, finding a connection between contact volume and the strength of the friendship paradox. The most frequently contacted alters exhibit a less pronounced friendship paradox compared with the ego, whereas less-frequently contacted alters are more likely to be high degree and give rise to the paradox. We argue therefore for a more nuanced version of the friendship paradox: ""your closest friends have slightly more friends than you do"", and in certain networks even: ""your best friend has no more friends than you do"". We demonstrate that this relationship is robust, holding in both a social media and a mobile phone dataset. These results have implications for information transfer and influence in social networks, which we explore using a simple dynamical model.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Towards Detecting Compromised Accounts on Social Networks,"Compromising social network accounts has become a profitable course of action for cybercriminals. By hijacking control of a popular media or business account, attackers can distribute their malicious messages or disseminate fake information to a large user base. The impacts of these incidents range from a tarnished reputation to multi-billion dollar monetary losses on financial markets. In our previous work, we demonstrated how we can detect large-scale compromises (i.e., so-called campaigns) of regular online social network users. In this work, we show how we can use similar techniques to identify compromises of individual high-profile accounts. High-profile accounts frequently have one characteristic that makes this detection reliable -- they show consistent behavior over time. We show that our system, were it deployed, would have been able to detect and prevent three real-world attacks against popular companies and news agencies. Furthermore, our system, in contrast to popular media, would not have fallen for a staged compromise instigated by a US restaurant chain for publicity reasons.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Influence of Social Media Writing on Online Search Behavior for Seasonal Events: The Sociophysics Approach,"Using seasonal topics as the study subject, in this study, we focus on the timing gap between social media writing and online search behavior. To conduct our analysis, we used the mathematical model of search behavior, comprising the sociophysics approach. The seasonal topics selected were St.Valentine's Day, Halloween and New Year countdown. We also picked up the event like Christmas and Halloween. We analyzed the influence of blogs and Twitter on search behavior and found a deviation of interest in terms of timing. We also analyzed Japanese seasonal event of eating Eho-maki in February 3 and eels at the day of the ox in midsummer.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Analyzing the Engagement of Social Relationships During Life Event Shocks in Social Media,"Individuals experiencing unexpected distressing events, shocks, often rely on their social network for support. While prior work has shown how social networks respond to shocks, these studies usually treat all ties equally, despite differences in the support provided by different social relationships. Here, we conduct a computational analysis on Twitter that examines how responses to online shocks differ by the relationship type of a user dyad. We introduce a new dataset of over 13K instances of individuals' self-reporting shock events on Twitter and construct networks of relationship-labeled dyadic interactions around these events. By examining behaviors across 110K replies to shocked users in a pseudo-causal analysis, we demonstrate relationship-specific patterns in response levels and topic shifts. We also show that while well-established social dimensions of closeness such as tie strength and structural embeddedness contribute to shock responsiveness, the degree of impact is highly dependent on relationship and shock types. Our findings indicate that social relationships contain highly distinctive characteristics in network interactions and that relationship-specific behaviors in online shock responses are unique from those of offline settings.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Machine Learning Techniques for Brand-Influencer Matchmaking on the Instagram Social Network,"The social media revolution has changed the way that brands interact with consumers. Instead of spending their advertising budget on interstate billboards, more and more companies are choosing to partner with so-called Internet ""influencers"" --- individuals who have gained a loyal following on online platforms for the high quality of the content they post. Unfortunately, it's not always easy for small brands to find the right influencer: someone who aligns with their corporate image and has not yet grown in popularity to the point of unaffordability. In this paper we sought to develop a system for brand-influencer matchmaking, harnessing the power and flexibility of modern machine learning techniques. The result is an algorithm that can predict the most fruitful brand-influencer partnerships based on the similarity of the content they post.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Online learning for Social Spammer Detection on Twitter,"Social networking services like Twitter have been playing an import role in people's daily life since it supports new ways of communicating effectively and sharing information. The advantages of these social network services enable them rapidly growing. However, the rise of social network services is leading to the increase of unwanted, disruptive information from spammers, malware discriminators, and other content polluters. Negative effects of social spammers do not only annoy users, but also lead to financial loss and privacy issues. There are two main challenges of spammer detection on Twitter. Firstly, the data of social network scale with a huge volume of streaming social data. Secondly, spammers continually change their spamming strategy such as changing content patterns or trying to gain social influence, disguise themselves as far as possible. With those challenges, it is hard to directly apply traditional batch learning methods to quickly adapt newly spamming pattern in the high-volume and real-time social media data. We need an anti-spammer system to be able to adjust the learning model when getting a label feedback. Moreover, the data on social media may be unbounded. Then, the system must allow update efficiency model in both computation and memory requirements. Online learning is an ideal solution for this problem. These methods incrementally adapt the learning model with every single feedback and adjust to the changing patterns of spammers overtime. Our experiments demonstrate that an anti-spam system based on online learning approach is efficient in fast changing of spammers comparing with batch learning methods. We also attempt to find the optimal online learning method and study the effectiveness of various feature sets on these online learning methods.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Bots for Online Public Health Interventions,"According to the Center for Disease Control and Prevention, in the United States hundreds of thousands initiate smoking each year, and millions live with smoking-related dis- eases. Many tobacco users discuss their habits and preferences on social media. This work conceptualizes a framework for targeted health interventions to inform tobacco users about the consequences of tobacco use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that leverages machine learning to identify users posting pro-tobacco tweets and select individualized interventions to address their interest in tobacco use. We searched the Twitter feed for tobacco-related keywords and phrases, and trained a convolutional neural network using over 4,000 tweets dichotomously manually labeled as either pro- tobacco or not pro-tobacco. This model achieves a 90% recall rate on the training set and 74% on test data. Users posting pro- tobacco tweets are matched with former smokers with similar interests who posted anti-tobacco tweets. Algorithmic matching, based on the power of peer influence, allows for the systematic delivery of personalized interventions based on real anti-tobacco tweets from former smokers. Experimental evaluation suggests that our system would perform well if deployed. This research offers opportunities for public health researchers to increase health awareness at scale. Future work entails deploying the fully operational Notobot system in a controlled experiment within a public health campaign.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Online Actions with Offline Impact: How Online Social Networks Influence Online and Offline User Behavior,"Many of today's most widely used computing applications utilize social networking features and allow users to connect, follow each other, share content, and comment on others' posts. However, despite the widespread adoption of these features, there is little understanding of the consequences that social networking has on user retention, engagement, and online as well as offline behavior. Here, we study how social networks influence user behavior in a physical activity tracking application. We analyze 791 million online and offline actions of 6 million users over the course of 5 years, and show that social networking leads to a significant increase in users' online as well as offline activities. Specifically, we establish a causal effect of how social networks influence user behavior. We show that the creation of new social connections increases user online in-application activity by 30%, user retention by 17%, and user offline real-world physical activity by 7% (about 400 steps per day). By exploiting a natural experiment we distinguish the effect of social influence of new social connections from the simultaneous increase in user's motivation to use the app and take more steps. We show that social influence accounts for 55% of the observed changes in user behavior, while the remaining 45% can be explained by the user's increased motivation to use the app. Further, we show that subsequent, individual edge formations in the social network lead to significant increases in daily steps. These effects diminish with each additional edge and vary based on edge attributes and user demographics. Finally, we utilize these insights to develop a model that accurately predicts which users will be most influenced by the creation of new social network connections.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Controlling Elections through Social Influence,"Election control considers the problem of an adversary who attempts to tamper with a voting process, in order to either ensure that their favored candidate wins (constructive control) or another candidate loses (destructive control). As online social networks have become significant sources of information for potential voters, a new tool in an attacker's arsenal is to effect control by harnessing social influence, for example, by spreading fake news and other forms of misinformation through online social media.   We consider the computational problem of election control via social influence, studying the conditions under which finding good adversarial strategies is computationally feasible. We consider two objectives for the adversary in both the constructive and destructive control settings: probability and margin of victory (POV and MOV, respectively). We present several strong negative results, showing, for example, that the problem of maximizing POV is inapproximable for any constant factor. On the other hand, we present approximation algorithms which provide somewhat weaker approximation guarantees, such as bicriteria approximations for the POV objective and constant-factor approximations for MOV. Finally, we present mixed integer programming formulations for these problems. Experimental results show that our approximation algorithms often find near-optimal control strategies, indicating that election control through social influence is a salient threat to election integrity.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Quantification of the propagation of rumors on social media,"The propagation of a rumor (unverified information) on a social network is subject to several factors mainly related to the content of this information and especially to the behaviors (profiles) of the actors on this network that retransmit. This state of affairs may vary this propagation as the case may be, and this is what we call the depth of the rumor. This project is tackling this problem. From a real case of the spread of a rumor on Twitter, this contribution proposes an academic approach to quantify the depth of a rumor on social networks and this, for use and interpretation, by specialists concerned by the nature of this information and its auditor.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Time-Critical Influence Maximization in Social Networks with Time-Delayed Diffusion Process,"Influence maximization is a problem of finding a small set of highly influential users, also known as seeds, in a social network such that the spread of influence under certain propagation models is maximized. In this paper, we consider time-critical influence maximization, in which one wants to maximize influence spread within a given deadline. Since timing is considered in the optimization, we also extend the Independent Cascade (IC) model and the Linear Threshold (LT) model to incorporate the time delay aspect of influence diffusion among individuals in social networks. We show that time-critical influence maximization under the time-delayed IC and LT models maintains desired properties such as submodularity, which allows a greedy approximation algorithm to achieve an approximation ratio of $1-1/e$. To overcome the inefficiency of the greedy algorithm, we design two heuristic algorithms: the first one is based on a dynamic programming procedure that computes exact influence in tree structures and directed acyclic subgraphs, while the second one converts the problem to one in the original models and then applies existing fast heuristic algorithms to it. Our simulation results demonstrate that our algorithms achieve the same level of influence spread as the greedy algorithm while running a few orders of magnitude faster, and they also outperform existing fast heuristics that disregard the deadline constraint and delays in diffusion.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"Preserving privacy enables ""co-existence equilibrium"" of competitive diffusion in social networks","With the advent of social media, different companies often promote competing products simultaneously for word-of-mouth diffusion and adoption by users in social networks. For such scenarios of competitive diffusion, prior studies show that the weaker product will soon become extinct (i.e., ""winner takes all""). It is intriguing to observe that in practice, however, competing products, such as iPhone and Android phone, often coexist in the market. This discrepancy may result from many factors such as the phenomenon that a user in the real world may not spread its use of a product due to dissatisfaction of the product or privacy protection. In this paper, we incorporate users' privacy for spreading behavior into competitive diffusion of two products and develop a problem formulation for privacy-aware competitive diffusion. Then, we prove that privacy-preserving mechanisms can enable a ""coexistence equilibrium"" (i.e., two competing products coexist in the equilibrium) in competitive diffusion over social networks. In addition to the rigorous analysis, we also demonstrate our results with experiments over real network topologies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Uncovering Flaming Events on News Media in Social Media,"Social networking sites (SNSs) facilitate the sharing of ideas and information through different types of feedback including publishing posts, leaving comments and other type of reactions. However, some comments or feedback on SNSs are inconsiderate and offensive, and sometimes this type of feedback has a very negative effect on a target user. The phenomenon known as flaming goes hand-in-hand with this type of posting that can trigger almost instantly on SNSs. Most popular users such as celebrities, politicians and news media are the major victims of the flaming behaviors and so detecting these types of events will be useful and appreciated. Flaming event can be monitored and identified by analyzing negative comments received on a post. Thus, our main objective of this study is to identify a way to detect flaming events in SNS using a sentiment prediction method. We use a deep Neural Network (NN) model that can identity sentiments of variable length sentences and classifies the sentiment of SNSs content (both comments and posts) to discover flaming events. Our deep NN model uses Word2Vec and FastText word embedding methods as its training to explore which method is the most appropriate. The labeled dataset for training the deep NN is generated using an enhanced lexicon based approach. Our deep NN model classifies the sentiment of a sentence into five classes: Very Positive, Positive, Neutral, Negative and Very Negative. To detect flaming incidents, we focus only on the comments classified into the Negative and Very Negative classes. As a use-case, we try to explore the flaming phenomena in the news media domain and therefore we focused on news items posted by three popular news media on Facebook (BBCNews, CNN and FoxNews) to train and test the model.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"""It Listens Better Than My Therapist"": Exploring Social Media Discourse on LLMs as Mental Health Tool","The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Endorsements on Social Media: An Empirical Study of Affiliate Marketing Disclosures on YouTube and Pinterest,"Online advertisements that masquerade as non-advertising content pose numerous risks to users. Such hidden advertisements appear on social media platforms when content creators or ""influencers"" endorse products and brands in their content. While the Federal Trade Commission (FTC) requires content creators to disclose their endorsements in order to prevent deception and harm to users, we do not know whether and how content creators comply with the FTC's guidelines. In this paper, we studied disclosures within affiliate marketing, an endorsement-based advertising strategy used by social media content creators. We examined whether content creators follow the FTC's disclosure guidelines, how they word the disclosures, and whether these disclosures help users identify affiliate marketing content as advertisements. To do so, we first measured the prevalence of and identified the types of disclosures in over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a user study with 1,791 participants to test the efficacy of these disclosures. Our findings reveal that only about 10% of affiliate marketing content on both platforms contains any disclosures at all. Further, users fail to understand shorter, non-explanatory disclosures. Based on our findings, we make various design and policy suggestions to help improve advertising disclosure practices on social media platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"The effects of network structure, competition and memory time on social spreading phenomena","Online social media have greatly affected the way in which we communicate with each other. However, little is known about what are the fundamental mechanisms driving dynamical information flow in online social systems. Here, we introduce a generative model for online sharing behavior that is analytically tractable and which can reproduce several characteristics of empirical micro-blogging data on hashtag usage, such as (time-dependent) heavy-tailed distributions of meme popularity. The presented framework constitutes a null model for social spreading phenomena which, in contrast to purely empirical studies or simulation-based models, clearly distinguishes the roles of two distinct factors affecting meme popularity: the memory time of users and the connectivity structure of the social network.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Studying User Footprints in Different Online Social Networks,"With the growing popularity and usage of online social media services, people now have accounts (some times several) on multiple and diverse services like Facebook, LinkedIn, Twitter and YouTube. Publicly available information can be used to create a digital footprint of any user using these social media services. Generating such digital footprints can be very useful for personalization, profile management, detecting malicious behavior of users. A very important application of analyzing users' online digital footprints is to protect users from potential privacy and security risks arising from the huge publicly available user information. We extracted information about user identities on different social networks through Social Graph API, FriendFeed, and Profilactic; we collated our own dataset to create the digital footprints of the users. We used username, display name, description, location, profile image, and number of connections to generate the digital footprints of the user. We applied context specific techniques (e.g. Jaro Winkler similarity, Wordnet based ontologies) to measure the similarity of the user profiles on different social networks. We specifically focused on Twitter and LinkedIn. In this paper, we present the analysis and results from applying automated classifiers for disambiguating profiles belonging to the same user from different social networks. UserID and Name were found to be the most discriminative features for disambiguating user profiles. Using the most promising set of features and similarity metrics, we achieved accuracy, precision and recall of 98%, 99%, and 96%, respectively.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Join the Chat: How Curiosity Sparks Participation in Telegram Groups,"This study delves into the mechanisms that spark user curiosity driving active engagement within public Telegram groups. By analyzing approximately 6 million messages from 29,196 users across 409 groups, we identify and quantify the key factors that stimulate users to actively participate (i.e., send messages) in group discussions. These factors include social influence, novelty, complexity, uncertainty, and conflict, all measured through metrics derived from message sequences and user participation over time. After clustering the messages, we apply explainability techniques to assign meaningful labels to the clusters. This approach uncovers macro categories representing distinct curiosity stimulation profiles, each characterized by a unique combination of various stimuli. Social influence from peers and influencers drives engagement for some users, while for others, rare media types or a diverse range of senders and media sparks curiosity. Analyzing patterns, we found that user curiosity stimuli are mostly stable, but, as the time between the initial message increases, curiosity occasionally shifts. A graph-based analysis of influence networks reveals that users motivated by direct social influence tend to occupy more peripheral positions, while those who are not stimulated by any specific factors are often more central, potentially acting as initiators and conversation catalysts. These findings contribute to understanding information dissemination and spread processes on social media networks, potentially contributing to more effective communication strategies.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Community Specific Temporal Topic Discovery from Social Media,"Studying temporal dynamics of topics in social media is very useful to understand online user behaviors. Most of the existing work on this subject usually monitors the global trends, ignoring variation among communities. Since users from different communities tend to have varying tastes and interests, capturing community-level temporal change can improve the understanding and management of social content. Additionally, it can further facilitate the applications such as community discovery, temporal prediction and online marketing. However, this kind of extraction becomes challenging due to the intricate interactions between community and topic, and intractable computational complexity.   In this paper, we take a unified solution towards the community-level topic dynamic extraction. A probabilistic model, CosTot (Community Specific Topics-over-Time) is proposed to uncover the hidden topics and communities, as well as capture community-specific temporal dynamics. Specifically, CosTot considers text, time, and network information simultaneously, and well discovers the interactions between community and topic over time. We then discuss the approximate inference implementation to enable scalable computation of model parameters, especially for large social data. Based on this, the application layer support for multi-scale temporal analysis and community exploration is also investigated.   We conduct extensive experimental studies on a large real microblog dataset, and demonstrate the superiority of proposed model on tasks of time stamp prediction, link prediction and topic perplexity.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Modeling Information Diffusion in Online Social Networks with Partial Differential Equations,"Online social networks such as Twitter and Facebook have gained tremendous popularity for information exchange. The availability of unprecedented amounts of digital data has accelerated research on information diffusion in online social networks. However, the mechanism of information spreading in online social networks remains elusive due to the complexity of social interactions and rapid change of online social networks. Much of prior work on information diffusion over online social networks has based on empirical and statistical approaches. The majority of dynamical models arising from information diffusion over online social networks involve ordinary differential equations which only depend on time. In a number of recent papers, the authors propose to use partial differential equations(PDEs) to characterize temporal and spatial patterns of information diffusion over online social networks. Built on intuitive cyber-distances such as friendship hops in online social networks, the reaction-diffusion equations take into account influences from various external out-of-network sources, such as the mainstream media, and provide a new analytic framework to study the interplay of structural and topical influences on information diffusion over online social networks. In this survey, we discuss a number of PDE-based models that are validated with real datasets collected from popular online social networks such as Digg and Twitter. Some new developments including the conservation law of information flow in online social networks and information propagation speeds based on traveling wave solutions are presented to solidify the foundation of the PDE models and highlight the new opportunities and challenges for mathematicians as well as computer scientists and researchers in online social networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Keep Your Friends Close and Your Facebook Friends Closer: A Multiplex Network Approach to the Analysis of Offline and Online Social Ties,"Social media allow for an unprecedented amount of interaction between people online. A fundamental aspect of human social behavior, however, is the tendency of people to associate themselves with like-minded individuals, forming homogeneous social circles both online and offline. In this work, we apply a new model that allows us to distinguish between social ties of varying strength, and to observe evidence of homophily with regards to politics, music, health, residential sector & year in college, within the online and offline social network of 74 college students. We present a multiplex network approach to social tie strength, here applied to mobile communication data - calls, text messages, and co-location, allowing us to dimensionally identify relationships by considering the number of communication channels utilized between students. We find that strong social ties are characterized by maximal use of communication channels, while weak ties by minimal use. We are able to identify 75% of close friendships, 90% of weaker ties, and 90% of Facebook friendships as compared to reported ground truth. We then show that stronger ties exhibit greater profile similarity than weaker ones. Apart from high homogeneity in social circles with respect to political and health aspects, we observe strong homophily driven by music, residential sector and year in college. Despite Facebook friendship being highly dependent on residence and year, exposure to less homogeneous content can be found in the online rather than the offline social circles of students, most notably in political and music aspects.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Social Bow Tie,"Understanding tie strength in social networks, and the factors that influence it, have received much attention in a myriad of disciplines for decades. Several models incorporating indicators of tie strength have been proposed and used to quantify relationships in social networks, and a standard set of structural network metrics have been applied to predominantly online social media sites to predict tie strength. Here, we introduce the concept of the ""social bow tie"" framework, a small subgraph of the network that consists of a collection of nodes and ties that surround a tie of interest, forming a topological structure that resembles a bow tie. We also define several intuitive and interpretable metrics that quantify properties of the bow tie. We use random forests and regression models to predict categorical and continuous measures of tie strength from different properties of the bow tie, including nodal attributes. We also investigate what aspects of the bow tie are most predictive of tie strength in two distinct social networks: a collection of 75 rural villages in India and a nationwide call network of European mobile phone users. Our results indicate several of the bow tie metrics are highly predictive of tie strength, and we find the more the social circles of two individuals overlap, the stronger their tie, consistent with previous findings. However, we also find that the more tightly-knit their non-overlapping social circles, the weaker the tie. This new finding complements our current understanding of what drives the strength of ties in social networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
A Heterogeneous Multimodal Graph Learning Framework for Recognizing User Emotions in Social Networks,"The rapid expansion of social media platforms has provided unprecedented access to massive amounts of multimodal user-generated content. Comprehending user emotions can provide valuable insights for improving communication and understanding of human behaviors. Despite significant advancements in Affective Computing, the diverse factors influencing user emotions in social networks remain relatively understudied. Moreover, there is a notable lack of deep learning-based methods for predicting user emotions in social networks, which could be addressed by leveraging the extensive multimodal data available. This work presents a novel formulation of personalized emotion prediction in social networks based on heterogeneous graph learning. Building upon this formulation, we design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that utilizes deep learning-based features for user emotion recognition. Additionally, we include a dynamic context fusion module in HMG-Emo that is capable of adaptively integrating the different modalities in social media data. Through extensive experiments, we demonstrate the effectiveness of HMG-Emo and verify the superiority of adopting a graph neural network-based approach, which outperforms existing baselines that use rich hand-crafted features. To the best of our knowledge, HMG-Emo is the first multimodal and deep-learning-based approach to predict personalized emotions within online social networks. Our work highlights the significance of exploiting advanced deep learning techniques for less-explored problems in Affective Computing.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
The Complexity of Social Media Response: Statistical Evidence For One-Dimensional Engagement Signal in Twitter,"Many years after online social networks exceeded our collective attention, social influence is still built on attention capital. Quality is not a prerequisite for viral spreading, yet large diffusion cascades remain the hallmark of a social influencer. Consequently, our exposure to low-quality content and questionable influence is expected to increase. Since the conception of influence maximization frameworks, multiple content performance metrics became available, albeit raising the complexity of influence analysis. In this paper, we examine and consolidate a diverse set of content engagement metrics. The correlations discovered lead us to propose a new, more holistic, one-dimensional engagement signal. We then show it is more predictable than any individual influence predictors previously investigated. Our proposed model achieves strong engagement ranking performance and is the first to explain half of the variance with features available early. We share the detailed numerical workflow to compute the new compound engagement signal. The model is immediately applicable to social media monitoring, influencer identification, campaign engagement forecasting, and curating user feeds.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Cluster size entropy in the Axelrod model of social influence: small-world networks and mass media,"We study the Axelrod's cultural adaptation model using the concept of cluster size entropy, $S_{c}$ that gives information on the variability of the cultural cluster size present in the system. Using networks of different topologies, from regular to random, we find that the critical point of the well-known nonequilibrium monocultural-multicultural (order-disorder) transition of the Axelrod model is unambiguously given by the maximum of the $S_{c}(q)$ distributions. The width of the cluster entropy distributions can be used to qualitatively determine whether the transition is first- or second-order. By scaling the cluster entropy distributions we were able to obtain a relationship between the critical cultural trait $q_c$ and the number $F$ of cultural features in regular networks. We also analyze the effect of the mass media (external field) on social systems within the Axelrod model in a square network. We find a new partially ordered phase whose largest cultural cluster is not aligned with the external field, in contrast with a recent suggestion that this type of phase cannot be formed in regular networks. We draw a new $q-B$ phase diagram for the Axelrod model in regular networks.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Detecting Cyberbullying and Cyberaggression in Social Media,"Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics. More than half of young social media users worldwide have been exposed to such prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions, with negative consequences such as embarrassment, depression, isolation from other community members, which embed the risk to lead to even more critical consequences, such as suicide attempts.   In this work, we take the first concrete steps to understand the characteristics of abusive behavior in Twitter, one of today's largest social media platforms. We analyze 1.2 million users and 2.1 million tweets, comparing users participating in discussions around seemingly normal topics like the NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering text, user, and network-based attributes. Using various state-of-the-art machine learning algorithms, we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status of Twitter user accounts marked as abusive by our methodology, and study the performance of potential mechanisms that can be used by Twitter to suspend users in the future.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation,"In the era of rapid development of social media, social recommendation systems as hybrid recommendation systems have been widely applied. Existing methods capture interest similarity between users to filter out interest-irrelevant relations in social networks that inevitably decrease recommendation accuracy, however, limited research has a focus on the mutual influence of semantic information between the social network and the user-item interaction network for further improving social recommendation. To address these issues, we introduce a social \underline{r}ecommendation model with ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly propose to construct a social tensor in order to smooth the training process of the model. Then, a graph convolutional network and a tensor convolutional network are employed to capture user's item preference and social preference, respectively. Considering the different semantic information in the user-item interaction network and the social network, a bi-semantic coordination loss is proposed to model the mutual influence of semantic information. To alleviate the interference of interest-irrelevant relations on multi-semantic modeling, we further use Bayesian posterior probability to mine potential social relations to replace social noise. Finally, the sliding window mechanism is utilized to update the social tensor as the input for the next iteration. Extensive experiments on three real datasets show Burger has a superior performance compared with the state-of-the-art models.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
QT2S: A System for Monitoring Road Traffic via Fine Grounding of Tweets,"Social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. The geographical dimension of such user behavior and events has recently caught a lot of attention in several domains: mobility, humanitarian, or infrastructural. While resolving the location of a user can be straightforward, depending on the affordances of their device and/or of the application they are using, in most cases, locating a user demands a larger effort, such as exploiting textual features. On Twitter for instance, only 2% of all tweets are geo-referenced. In this paper, we present a system for zoomed-in grounding (below city level) for short messages (e.g., tweets). The system combines different natural language processing and machine learning techniques to increase the number of geo-grounded tweets, which is essential to many applications such as disaster response and real-time traffic monitoring.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
TweetBoost: Influence of Social Media on NFT Valuation,"NFT or Non-Fungible Token is a token that certifies a digital asset to be unique. A wide range of assets including, digital art, music, tweets, memes, are being sold as NFTs. NFT-related content has been widely shared on social media sites such as Twitter. We aim to understand the dominant factors that influence NFT asset valuation. Towards this objective, we create a first-of-its-kind dataset linking Twitter and OpenSea (the largest NFT marketplace) to capture social media profiles and linked NFT assets. Our dataset contains 245,159 tweets posted by 17,155 unique users, directly linking 62,997 NFT assets on OpenSea worth 19 Million USD. We have made the dataset public. We analyze the growth of NFTs, characterize the Twitter users promoting NFT assets, and gauge the impact of Twitter features on the virality of an NFT. Further, we investigate the effectiveness of different social media and NFT platform features by experimenting with multiple machine learning and deep learning models to predict an asset's value. Our results show that social media features improve the accuracy by 6% over baseline models that use only NFT platform features. Among social media features, count of user membership lists, number of likes and retweets are important features.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis,"The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and largely stimulated online public discourse. We demonstrate that as the pandemic progressed, public sentiment shared on social networks was shaped by risk perceptions, political ideologies and health-protective behaviours shared by Persons in the Public Eye, often in a negative light.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Winning the Social Media Influence Battle: Uncertainty-Aware Opinions to Understand and Spread True Information via Competitive Influence Maximization,"Competitive Influence Maximization (CIM) involves entities competing to maximize influence in online social networks (OSNs). Current Deep Reinforcement Learning (DRL) methods in CIM rely on simplistic binary opinion models (i.e., an opinion is represented by either 0 or 1) and often overlook the complexity of users' behavioral characteristics and their prior knowledge. We propose a novel DRL-based framework that enhances CIM analysis by integrating Subjective Logic (SL) to accommodate uncertain opinions, users' behaviors, and their preferences. This approach targets the mitigation of false information by effectively propagating true information. By modeling two competitive agents, one spreading true information and the other spreading false information, we capture the strategic interplay essential to CIM. Our framework utilizes an uncertainty-based opinion model (UOM) to assess the impact on information quality in OSNs, emphasizing the importance of user behavior alongside network topology in selecting influential seed nodes. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, achieving faster and more influential results (i.e., outperforming over 20%) under realistic network conditions. Moreover, our method shows robust performance in partially observable networks, effectively doubling the performance when users are predisposed to disbelieve true information.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Social Media Influence Operations,"Social media platforms enable largely unrestricted many-to-many communication. In times of crisis, they offer a space for collective sense-making and gave rise to new social phenomena (e.g. open-source investigations). However, they also serve as a tool for threat actors to conduct cyber-enabled social influence operations (CeSIOs) in order to shape public opinion and interfere in decision-making processes. CeSIOs rely on the employment of sock puppet accounts to engage authentic users in online communication, exert influence, and subvert online discourse. Large Language Models (LLMs) may further enhance the deceptive properties of sock puppet accounts. Recent LLMs are able to generate targeted and persuasive text which is for the most part indistinguishable from human-written content -- ideal features for covert influence. This article reviews recent developments at the intersection of LLMs and influence operations, summarizes LLMs' salience, and explores the potential impact of LLM-instrumented sock puppet accounts for CeSIOs. Finally, mitigation measures for the near future are highlighted.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
"On the Social Influence in Human Behavior: Physical, Homophily, and Social Communities","Understanding the forces governing human behavior and social dynamics is a challenging problem. Individuals' decisions and actions are affected by interlaced factors, such as physical location, homophily, and social ties. In this paper, we propose to examine the role that distinct communities, linked to these factors, play as sources of social influence. The ego network is typically used in the social influence analysis. Our hypothesis is that individuals are embedded in communities not only related to their direct social relationships, but that involve different and complex forces. We analyze physical, homophily, and social communities to evaluate their relation with subjects' behavior. We prove that social influence is correlated with these communities, and each one of them is (differently) significant for individuals. We define community-based features, which reflect the subject involvement in these groups, and we use them with a supervised learning algorithm to predict subject participation in social events. Results indicate that both communities and ego network are relevant sources of social influence, confirming that the ego network alone is not sufficient to explain this phenomenon. Moreover, we classify users according to the degree of social influence they experienced with respect to their groups, recognizing classes of behavioral phenotypes. To our knowledge, this is the first work that proves the existence of phenotypes related to the social influence phenomenon.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
On the Aggression Diffusion Modeling and Minimization in Online Social Networks,"Aggression in online social networks has been studied mostly from the perspective of machine learning which detects such behavior in a static context. However, the way aggression diffuses in the network has received little attention as it embeds modeling challenges. In fact, modeling how aggression propagates from one user to another, is an important research topic since it can enable effective aggression monitoring, especially in media platforms which up to now apply simplistic user blocking techniques. In this paper, we address aggression propagation modeling and minimization on Twitter, since it is a popular microblogging platform at which aggression had several onsets. We propose various methods building on two well-known diffusion models, Independent Cascade (IC) and Linear Threshold (LT), to study the aggression evolution in the social network. We experimentally investigate how well each method can model aggression propagation using real Twitter data, while varying parameters, such as seed users selection, graph edge weighting, users' activation timing, etc. It is found that the best performing strategies are the ones to select seed users with a degree-based approach, weigh user edges based on their social circles' overlaps, and activate users according to their aggression levels. We further employ the best performing models to predict which ordinary real users could become aggressive (and vice versa) in the future, and achieve up to AUC=0.89 in this prediction task. Finally, we investigate aggression minimization by launching competitive cascades to ""inform"" and ""heal"" aggressors. We show that IC and LT models can be used in aggression minimization, providing less intrusive alternatives to the blocking techniques currently employed by popular online social network platforms.","cat:cs.SI AND (""social media"" OR ""social network"") AND (behavior OR influence OR mental)",0
Bio-inspired Synthetic Ivory as a Sustainable Material for Piano Keys,"Natural ivory is no longer readily or legally available, as it is obtained primarily from elephant tusks, which now enjoy international protection. Ivory, however, is the best material known for piano keys. We present a hydroxylapatite-gelatin biocomposite that is chemically identical to natural ivory but with functional properties optimized to replace it. As this biocomposite is fabricated from abundant materials in an environmentally friendly process and is furthermore biodegradable, it is a sustainable solution for piano keys with the ideal functional properties of natural ivory.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
WaterWise: Co-optimizing Carbon- and Water-Footprint Toward Environmentally Sustainable Cloud Computing,"The carbon and water footprint of large-scale computing systems poses serious environmental sustainability risks. In this study, we discover that, unfortunately, carbon and water sustainability are at odds with each other - and, optimizing one alone hurts the other. Toward that goal, we introduce, WaterWise, a novel job scheduler for parallel workloads that intelligently co-optimizes carbon and water footprint to improve the sustainability of geographically distributed data centers.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Integrating AI's Carbon Footprint into Risk Management Frameworks: Strategies and Tools for Sustainable Compliance in Banking Sector,"This paper examines the integration of AI's carbon footprint into the risk management frameworks (RMFs) of the banking sector, emphasising its importance in aligning with sustainability goals and regulatory requirements. As AI becomes increasingly central to banking operations, its energy-intensive processes contribute significantly to carbon emissions, posing environmental, regulatory, and reputational risks. Regulatory frameworks such as the EU AI Act, Corporate Sustainability Reporting Directive (CSRD), Corporate Sustainability Due Diligence Directive (CSDDD), and the Prudential Regulation Authority's SS1/23 are driving banks to incorporate environmental considerations into their AI model governance. Recent advancements in AI research, like the Open Mixture-of-Experts (OLMoE) framework and the Agentic RAG framework, offer more efficient and dynamic AI models, reducing their carbon footprint without compromising performance. Using these technological examples, the paper outlines a structured approach for banks to identify, assess, and mitigate AI's carbon footprint within their RMFs, including adopting energy-efficient models, utilising green cloud computing, and implementing lifecycle management.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
New Era of Artificial Intelligence in Education: Towards a Sustainable Multifaceted Revolution,"The recent high performance of ChatGPT on several standardized academic tests has thrust the topic of artificial intelligence (AI) into the mainstream conversation about the future of education. As deep learning is poised to shift the teaching paradigm, it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of AI-driven technologies at schools and universities. This research aims to investigate the potential impact of AI on education through review and analysis of the existing literature across three major axes: applications, advantages, and challenges. Our review focuses on the use of artificial intelligence in collaborative teacher--student learning, intelligent tutoring systems, automated assessment, and personalized learning. We also report on the potential negative aspects, ethical issues, and possible future routes for AI implementation in education. Ultimately, we find that the only way forward is to embrace the new technology, while implementing guardrails to prevent its abuse.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Aware Data Center Management: A Sustainable Approach to Reducing Carbon Footprint,"The rapid expansion of cloud computing and data center infrastructure has led to significant energy consumption, posing environmental challenges due to the growing carbon footprint. This research explores energy-aware management strategies aimed at creating sustainable data center operations. By integrating advanced energy-efficient technologies and optimizing resource utilization, this study proposes a framework to minimize power usage while maintaining high performance. Key elements include dynamic workload allocation, renewable energy integration, and intelligent cooling systems, all of which contribute to reducing overall energy consumption. The study also examines the impact of these strategies on operational costs and performance efficiency, demonstrating how sustainable practices can be both environmentally and economically beneficial. Through simulations and case studies, the research offers practical insights into reducing carbon emissions in data centers, supporting the transition towards greener cloud infrastructure. The findings highlight the potential for scalable, energy-aware data center designs that significantly lower environmental impact while ensuring optimal functionality, contributing to the global effort of mitigating climate change.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Toward Sustainable Generative AI: A Scoping Review of Carbon Footprint and Environmental Impacts Across Training and Inference Stages,"Generative AI is spreading rapidly, creating significant social and economic value while also raising concerns about its high energy use and environmental sustainability. While prior studies have predominantly focused on the energy-intensive nature of the training phase, the cumulative environmental footprint generated during large-scale service operations, particularly in the inference phase, has received comparatively less attention. To bridge this gap this study conducts a scoping review of methodologies and research trends in AI carbon footprint assessment. We analyze the classification and standardization status of existing AI carbon measurement tools and methodologies, and comparatively examine the environmental impacts arising from both training and inference stages. In addition, we identify how multidimensional factors such as model size, prompt complexity, serving environments, and system boundary definitions shape the resulting carbon footprint. Our review reveals critical limitations in current AI carbon accounting practices, including methodological inconsistencies, technology-specific biases, and insufficient attention to end-to-end system perspectives. Building on these insights, we propose future research and governance directions: (1) establishing standardized and transparent universal measurement protocols, (2) designing dynamic evaluation frameworks that incorporate user behavior, (3) developing life-cycle monitoring systems that encompass embodied emissions, and (4) advancing multidimensional sustainability assessment framework that balance model performance with environmental efficiency. This paper provides a foundation for interdisciplinary dialogue aimed at building a sustainable AI ecosystem and offers a baseline guideline for researchers seeking to understand the environmental implications of AI across technical, social, and operational dimensions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Recommender Systems for Social Good: The Role of Accountability and Sustainability,"This work examines the role of recommender systems in promoting sustainability, social responsibility, and accountability, with a focus on alignment with the United Nations Sustainable Development Goals (SDGs). As recommender systems become increasingly integrated into daily interactions, they must go beyond personalization to support responsible consumption, reduce environmental impact, and foster social good. We explore strategies to mitigate the carbon footprint of recommendation models, ensure fairness, and implement accountability mechanisms. By adopting these approaches, recommender systems can contribute to sustainable and socially beneficial outcomes, aligning technological advancements with the SDGs focused on environmental sustainability and social well-being.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations,"The environmental impact of Large Language Models (LLMs) is rising significantly, with inference now accounting for more than half of their total lifecycle carbon emissions. However, existing simulation frameworks, which are increasingly used to determine efficient LLM deployments, lack any concept of power and, therefore, cannot accurately estimate inference-related emissions. We present a simulation framework to assess the energy and carbon implications of LLM inference under varying deployment setups. First, we extend a high-fidelity LLM inference simulator with a GPU power model that estimates power consumption based on utilization metrics, enabling analysis across configurations like batch size, sequence length, and model parallelism. Second, we integrate simulation outputs into an energy system co-simulation environment to quantify carbon emissions under specific grid conditions and explore the potential of carbon-aware scheduling. Through scenario-based analysis, our framework reveals how inference parameters affect energy demand and carbon footprint, demonstrates a renewable offset potential of up to 69.2% in an illustrative deployment case, and provides a foundation for future carbon-aware inference infrastructure design.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
An Energy and Carbon Footprint Analysis of Distributed and Federated Learning,"Classical and centralized Artificial Intelligence (AI) methods require moving data from producers (sensors, machines) to energy hungry data centers, raising environmental concerns due to computational and communication resource demands, while violating privacy. Emerging alternatives to mitigate such high energy costs propose to efficiently distribute, or federate, the learning tasks across devices, which are typically low-power. This paper proposes a novel framework for the analysis of energy and carbon footprints in distributed and federated learning (FL). The proposed framework quantifies both the energy footprints and the carbon equivalent emissions for vanilla FL methods and consensus-based fully decentralized approaches. We discuss optimal bounds and operational points that support green FL designs and underpin their sustainability assessment. Two case studies from emerging 5G industry verticals are analyzed: these quantify the environmental footprints of continual and reinforcement learning setups, where the training process is repeated periodically for continuous improvements. For all cases, sustainability of distributed learning relies on the fulfillment of specific requirements on communication efficiency and learner population size. Energy and test accuracy should be also traded off considering the model and the data footprints for the targeted industrial applications.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Sustainable SecureML: Quantifying Carbon Footprint of Adversarial Machine Learning,"The widespread adoption of machine learning (ML) across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions. This issue becomes more pressing in adversarial ML, which focuses on enhancing model security against different network-based attacks. Implementing defenses in ML systems often necessitates additional computational resources and network security measures, exacerbating their environmental impacts. In this paper, we pioneer the first investigation into adversarial ML's carbon footprint, providing empirical evidence connecting greater model robustness to higher emissions. Addressing the critical need to quantify this trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This novel metric, inspired by economic elasticity principles, captures the sensitivity of carbon emissions to changes in adversarial robustness. We demonstrate the RCTI through an experiment involving evasion attacks, analyzing the interplay between robustness against attacks, performance, and carbon emissions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time,"As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost. The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions. DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
ECO-CHIP: Estimation of Carbon Footprint of Chiplet-based Architectures for Sustainable VLSI,"Decades of progress in energy-efficient and low-power design have successfully reduced the operational carbon footprint in the semiconductor industry. However, this has led to an increase in embodied emissions, encompassing carbon emissions arising from design, manufacturing, packaging, and other infrastructural activities. While existing research has developed tools to analyze embodied carbon at the computer architecture level for traditional monolithic systems, these tools do not apply to near-mainstream heterogeneous integration (HI) technologies. HI systems offer significant potential for sustainable computing by minimizing carbon emissions through two key strategies: ``reducing"" computation by reusing pre-designed chiplet IP blocks and adopting hierarchical approaches to system design. The reuse of chiplets across multiple designs, even spanning multiple generations of integrated circuits (ICs), can substantially reduce embodied carbon emissions throughout the operational lifespan. This paper introduces a carbon analysis tool specifically designed to assess the potential of HI systems in facilitating greener VLSI system design and manufacturing approaches. The tool takes into account scaling, chiplet and packaging yields, design complexity, and even carbon overheads associated with advanced packaging techniques employed in heterogeneous systems. Experimental results demonstrate that HI can achieve a reduction of embodied carbon emissions up to 70\% compared to traditional large monolithic systems. These findings suggest that HI can pave the way for sustainable computing practices, contributing to a more environmentally conscious semiconductor industry.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Cultural context shapes the carbon footprints of recipes,"Food systems are responsible for a third of global anthropogenic greenhouse gas emissions central to global warming and climate change. Increasing awareness of the environmental impact of food-centric emissions has led to the carbon footprint quantification of food products. However, food consumption is dictated by traditional dishes, the cultural capsules that encode traditional protocols for culinary preparations. Carbon footprint estimation of recipes will provide actionable insights into the environmental sustainability of culturally influenced patterns in recipe compositions. By integrating the carbon footprint data of food products with a gold-standard repository of recipe compositions, we show that the ingredient constitution dictates the carbon load of recipes. Beyond the prevalent focus on individual food products, our analysis quantifies the carbon footprint of recipes within the cultural contexts that shape culinary protocols. While emphasizing the widely understood harms of animal-sourced ingredients, this article presents a nuanced perspective on the environmental impact of culturally influenced dietary practices. Along with the grasp of taste and nutrition correlates, such an understanding can help design palatable and environmentally sustainable recipes. Systematic compilation of fine-grained carbon footprint data is the way forward to address the challenge of sustainably feeding an anticipated population of 10 billion.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Strategies for Cooperative Multi-Channel MAC Protocols,"Distributed Information SHaring (DISH) is a new cooperative approach to designing multi-channel MAC protocols. It aids nodes in their decision making processes by compensating for their missing information via information sharing through other neighboring nodes. This approach was recently shown to significantly boost the throughput of multi-channel MAC protocols. However, a critical issue for ad hoc communication devices, i.e., energy efficiency, has yet to be addressed. In this paper, we address this issue by developing simple solutions which (1) reduce the energy consumption (2) without compromising the throughput performance, and meanwhile (3) maximize cost efficiency. We propose two energy-efficient strategies: in-situ energy conscious DISH which uses existing nodes only, and altruistic DISH which needs additional nodes called altruists. We compare five protocols with respect to the strategies and identify altruistic DISH to be the right choice in general: it (1) conserves 40-80% of energy, (2) maintains the throughput advantage gained from the DISH approach, and (3) more than doubles the cost efficiency compared to protocols without applying the strategy. On the other hand, our study shows that in-situ energy conscious DISH is suitable only in certain limited scenarios.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for Effective Carbon-Aware Scheduling,"The rapid increase in computing demand and its corresponding energy consumption have focused attention on computing's impact on the climate and sustainability. Prior work proposes metrics that quantify computing's carbon footprint across several lifecycle phases, including its supply chain, operation, and end-of-life. Industry uses these metrics to optimize the carbon footprint of manufacturing hardware and running computing applications. Unfortunately, prior work on optimizing datacenters' carbon footprint often succumbs to the \emph{sunk cost fallacy} by considering embodied carbon emissions (a sunk cost) when making operational decisions (i.e., job scheduling and placement), which leads to operational decisions that do not always reduce the total carbon footprint.   In this paper, we evaluate carbon-aware job scheduling and placement on a given set of servers for a number of carbon accounting metrics. Our analysis reveals state-of-the-art carbon accounting metrics that include embodied carbon emissions when making operational decisions can actually increase the total carbon footprint of executing a set of jobs. We study the factors that affect the added carbon cost of such suboptimal decision-making. We then use a real-world case study from a datacenter to demonstrate how the sunk carbon fallacy manifests itself in practice. Finally, we discuss the implications of our findings in better guiding effective carbon-aware scheduling in on-premise and cloud datacenters.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability Assessment of Future Accelerators,"The Large Particle Physics Laboratory Directors Group (LDG) established the Working Group on the Sustainability Assessment of Future Accelerators in 2024 with the mandate to develop guidelines and a list of key parameters for the assessment of the sustainability of future accelerators in particle physics. While focused on accelerator projects, much of the work will also be relevant to other current and future Research Infrastructures. The development and continuous update of such a framework aim to enable a coherent communication amongst scientists and adequately convey the information to a broader set of stakeholders.   This document outlines the major findings and recommendations from the LDG Sustainability WG report - a summary of current best practices recommended to be adopted by new Research Infrastructures. The full report will be available in June 2025 at: https://ldg.web.cern.ch/working-groups/sustainability-assessment-of-accelerators. Not all of sustainability topics are addressed at the same level. The assessment process is complex, largely under development and a homogeneous evaluation of all the aspects deserves a strategy to be pursued over time.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Stochastic Carbon Footprint Tracing Methods in Power Systems,"As the penetration of distributed energy resources (DER) and renewable energy sources (RES) increases, carbon footprint tracking requires more granular analysis results. Existing carbon footprint tracking methods focus on deterministic steady-state analysis where the high uncertainties of RES cannot be considered. Considering the deficiency of the existing deterministic method, this paper proposes two stochastic carbon footprint tracking methods to cope with the impact of RES uncertainty on load-side carbon footprint tracing. The first method introduces probabilistic analysis in the framework of carbon emissions flow (CEF) to provide a global reference for the spatial characteristic of the power system component carbon intensity distribution. Considering that the CEF network expands with the increasing penetration of DERs, the second method can effectively improve the computational efficiency over the first method while ensuring the computational accuracy on the large power systems. These proposed models are tested and compared in a synthetic 1004-bus test system in the case study to demonstrate the performance of the two proposed methods","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Toward Sustainable HPC: Carbon Footprint Estimation and Environmental Implications of HPC Systems,"The rapid growth in demand for HPC systems has led to a rise in carbon footprint, which requires urgent intervention. In this work, we present a comprehensive analysis of the carbon footprint of high-performance computing (HPC) systems, considering the carbon footprint during both the hardware manufacturing and system operational stages. Our work employs HPC hardware component carbon footprint modeling, regional carbon intensity analysis, and experimental characterization of the system life cycle to highlight the importance of quantifying the carbon footprint of HPC systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
EcoLearn: Optimizing the Carbon Footprint of Federated Learning,"Federated Learning (FL) distributes machine learning (ML) training across edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span hundreds of devices and is thus resource- and energy-intensive, it has a significant carbon footprint. Importantly, since energy's carbon-intensity differs substantially (by up to 60$\times$) across locations, training on the same device using the same amount of energy, but at different locations, can incur widely different carbon emissions. While prior work has focused on improving FL's resource- and energy-efficiency by optimizing time-to-accuracy, it implicitly assumes all energy has the same carbon intensity and thus does not optimize carbon efficiency, i.e., work done per unit of carbon emitted.   To address the problem, we design EcoLearn, which minimizes FL's carbon footprint without significantly affecting model accuracy or training time. EcoLearn achieves a favorable tradeoff by integrating carbon awareness into multiple aspects of FL training, including i) selecting clients with high data utility and low carbon, ii) provisioning more clients during the initial training rounds, and iii) mitigating stragglers by dynamically adjusting client over-provisioning based on carbon. We implement EcoLearn and its carbon-aware FL training policies in the Flower framework and show that it reduces the carbon footprint of training (by up to $10.8$$\times$) while maintaining model accuracy and training time (within $\sim$$1$\%) compared to state-of-the-art approaches.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Power-Efficient Image Storage: Leveraging Super Resolution Generative Adversarial Network for Sustainable Compression and Reduced Carbon Footprint,"In recent years, large-scale adoption of cloud storage solutions has revolutionized the way we think about digital data storage. However, the exponential increase in data volume, especially images, has raised environmental concerns regarding power and resource consumption, as well as the rising digital carbon footprint emissions. The aim of this research is to propose a methodology for cloud-based image storage by integrating image compression technology with SuperResolution Generative Adversarial Networks (SRGAN). Rather than storing images in their original format directly on the cloud, our approach involves initially reducing the image size through compression and downsizing techniques before storage. Upon request, these compressed images will be retrieved and processed by SRGAN to generate images. The efficacy of the proposed method is evaluated in terms of PSNR and SSIM metrics. Additionally, a mathematical analysis is given to calculate power consumption and carbon footprint assesment. The proposed data compression technique provides a significant solution to achieve a reasonable trade off between environmental sustainability and industrial efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Bitcoin's future carbon footprint,"The carbon footprint of Bitcoin has drawn wide attention, but Bitcoin's long-term impact on the climate remains uncertain. Here we present a framework to overcome uncertainties in previous estimates and project Bitcoin's electricity consumption and carbon footprint in the long term. If we assume Bitcoin's market capitalization grows in line with the one of gold, we find that the annual electricity consumption of Bitcoin may increase from 60 to 400 TWh between 2020 and 2100. The future carbon footprint of Bitcoin strongly depends on the decarbonization pathway of the electricity sector. If the electricity sector achieves carbon neutrality by 2050, Bitcoin's carbon footprint has peaked already. However, in the business-as-usual scenario, emissions sum up to 2 gigatons until 2100, an amount comparable to 7% of global emissions in 2019. The Bitcoin price spike at the end of 2020 shows, however, that progressive development of market capitalization could yield an electricity consumption of more than 100 TWh already in 2021, and lead to cumulative emissions of over 5 gigatons by 2100. Therefore, we also discuss policy instruments to reduce Bitcoin's future carbon footprint.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis,"Environmental sustainability, particularly in relation to climate change, is a key concern for consumers, producers, and policymakers. The carbon footprint, based on greenhouse gas emissions, is a standard metric for quantifying the contribution to climate change of activities and is often assessed using life cycle assessment (LCA). However, conducting LCA is complex due to opaque and global supply chains, as well as fragmented data. This paper presents a methodology that combines advances in LCA and publicly available databases with knowledge-augmented AI techniques, including retrieval-augmented generation, to estimate cradle-to-gate carbon footprints of food products. We introduce a chatbot interface that allows users to interactively explore the carbon impact of composite meals and relate the results to familiar activities. A live web demonstration showcases our proof-of-concept system with arbitrary food items and follow-up questions, highlighting both the potential and limitations - such as database uncertainties and AI misinterpretations - of delivering LCA insights in an accessible format.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of astronomical observatories,"The carbon footprint of astronomical research is an increasingly topical issue. From a comparison of existing literature, we infer an annual per capita carbon footprint of several tens of tonnes of CO$_2$ equivalents for an average person working in astronomy. Astronomical observatories contribute significantly to the carbon footprint of astronomy, and we examine the related sources of greenhouse gas emissions as well as lever arms for their reduction. Comparison with other scientific domains illustrates that astronomy is not the only field that needs to accomplish significant carbon footprint reductions of their research facilities. We show that limiting global warming to 1.5C or 2C implies greenhouse gas emission reductions that can only be reached by a systemic change of astronomical research activities, and we argue that a new narrative for doing astronomical research is needed if we want to keep our planet habitable.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Estimate of the carbon footprint of astronomical research infrastructures,"The carbon footprint of astronomical research is an increasingly topical issue with first estimates of research institute and national community footprints having recently been published. As these assessments have typically excluded the contribution of astronomical research infrastructures, we complement these studies by providing an estimate of the contribution of astronomical space missions and ground-based observatories using greenhouse gas emission factors that relates cost and payload mass to carbon footprint. We find that worldwide active astronomical research infrastructures currently have a carbon footprint of 20.3$\pm$3.3 MtCO$_2$ equivalent (CO$_2$e) and an annual emission of 1,169$\pm$249 ktCO$_2$e yr$^{-1}$ corresponding to a footprint of 36.6$\pm$14.0 tCO$_2$e per year per astronomer. Compared with contributions from other aspects of astronomy research activity, our results suggest that research infrastructures make the single largest contribution to the carbon footprint of an astronomer. We discuss the limitations and uncertainties of our method and explore measures that can bring greenhouse gas emissions from astronomical research infrastructures towards a sustainable level.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Footprint Accounting Driven by Large Language Models and Retrieval-augmented Generation,"Carbon footprint accounting is crucial for quantifying greenhouse gas emissions and achieving carbon neutrality.The dynamic nature of processes, accounting rules, carbon-related policies, and energy supply structures necessitates real-time updates of CFA. Traditional life cycle assessment methods rely heavily on human expertise, making near-real-time updates challenging. This paper introduces a novel approach integrating large language models (LLMs) with retrieval-augmented generation technology to enhance the real-time, professional, and economical aspects of carbon footprint information retrieval and analysis. By leveraging LLMs' logical and language understanding abilities and RAG's efficient retrieval capabilities, the proposed method LLMs-RAG-CFA can retrieve more relevant professional information to assist LLMs, enhancing the model's generative abilities. This method offers broad professional coverage, efficient real-time carbon footprint information acquisition and accounting, and cost-effective automation without frequent LLMs' parameter updates. Experimental results across five industries(primary aluminum, lithium battery, photovoltaic, new energy vehicles, and transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional methods and other LLMs, achieving higher information retrieval rates and significantly lower information deviations and carbon footprint accounting deviations. The economically viable design utilizes RAG technology to balance real-time updates with cost-effectiveness, providing an efficient, reliable, and cost-saving solution for real-time carbon emission management, thereby enhancing environmental sustainability practices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability: Delivering Agility's Promise,"Sustainability is a promise by agile development, as it is part of both the Agile Alliance's and the Scrum Alliance's vision. Thus far, not much has been delivered on this promise. This paper explores the Agile Manifesto and points out how agility could contribute to sustainability in its three dimensions - social, economic, and environmental. Additionally, this paper provides some sample cases of companies focusing on both sustainability (partially or holistically) and agile development.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Computational Fluid Dynamics: its Carbon Footprint and Role in Carbon Emission Reduction,"Turbulent flow physics regulates the aerodynamic properties of lifting surfaces, the thermodynamic efficiency of vapor power systems, and exchanges of natural and anthropogenic quantities between the atmosphere and ocean, to name just a few applications. The dynamics of turbulent flows are described via numerical integration of the non-linear Navier-Stokes equation -- a procedure known as computational fluid dynamics (CFD). At the dawn of scientific computing in the late 1950s, it would be many decades before terms such as ``carbon footprint'' or ``sustainability'' entered the lexicon, and longer still before these themes attained national priority throughout advanced economies. This paper introduces a framework designed to calculate the carbon footprint of CFD and its contribution to carbon emission reduction strategies. We will distinguish between ""hero"" and ""routine"" calculations, noting that the carbon footprint of hero calculations is largely determined by the energy source mix utilized. We will also review CFD of flows where turbulence effects are modeled, thus reducing the degrees of freedom. Estimates of the carbon footprint are presented for such fully- and partially-resolved simulations as functions of turbulence activity and calculation year, demonstrating a reduction in carbon emissions by two to five orders of magnitude at practical conditions. Beyond analyzing CO2 emissions, we quantify the benefits of applying CFD towards overall carbon emission reduction. The community's effort to avoid redundant calculations via turbulence databases merits particular attention, with estimates indicating that a single database could potentially reduce CO2 emissions by approximately O(1) million metric tons. Additionally, implementing CFD in the fluids industry has markedly decreased dependence on wind tunnel testing, which is anticipated to lead to CO2 emission reduction.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Quantum Computing: Opportunities and Challenges of Benchmarking Carbon in the Quantum Computing Lifecycle,"While researchers in both industry and academia are racing to build Quantum Computing (QC) platforms with viable performance and functionality, the environmental impacts of this endeavor, such as its carbon footprint, e-waste generation, mineral use, and water and energy consumption, remain largely unknown. A similar oversight occurred during the semiconductor revolution and continues to have disastrous consequences for the health of our planet. As we build the quantum computing stack from the ground up, it is crucial to comprehensively assess it through an environmental sustainability lens for its entire life-cycle: production, use, and disposal. In this paper, we highlight the need and challenges in establishing a QC sustainability benchmark that enables researchers to make informed architectural design decisions and celebrate the potential quantum environmental advantage. We propose a carbon-aware quantum computing (CQC) framework that provides the foundational methodology and open research questions for calculating the total life-cycle carbon footprint of a QC platform. Our call to action to the research community is the establishment of a new research direction known as, sustainable quantum computing that promotes both quantum computing for sustainability-oriented applications and the sustainability of quantum computing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study,"The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain. Furthermore, the study uncovers correlations between carbon emissions and various attributes such as model size, dataset size, and ML application domains. These results highlight the need for software measurements to improve energy reporting practices and promote carbon-efficient model development within the Hugging Face community. In response to this issue, two classifications are proposed: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. The aim of these classification proposals is to foster transparency and sustainable model development within the ML community.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of astronomical research infrastructures,"We estimate the carbon footprint of astronomical research infrastructures, including space telescopes and probes and ground-based observatories. Our analysis suggests annual greenhouse gas emissions of $1.2\pm0.2$ MtCO$_2$e yr$^{-1}$ due to construction and operation of the world-fleet of astronomical observatories, corresponding to a carbon footprint of 36.6$\pm$14.0 tCO$_2$e per year and average astronomer. We show that decarbonising astronomical facilities is compromised by the continuous deployment of new facilities, suggesting that a significant reduction in the deployment pace of new facilities is needed to reduce the carbon footprint of astronomy. We propose measures that would bring astronomical activities more in line with the imperative to reduce the carbon footprint of all human activities.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Footprint Evaluation of Code Generation through LLM as a Service,"Due to increased computing use, data centers consume and emit a lot of energy and carbon. These contributions are expected to rise as big data analytics, digitization, and large AI models grow and become major components of daily working routines. To reduce the environmental impact of software development, green (sustainable) coding and claims that AI models can improve energy efficiency have grown in popularity. Furthermore, in the automotive industry, where software increasingly governs vehicle performance, safety, and user experience, the principles of green coding and AI-driven efficiency could significantly contribute to reducing the sector's environmental footprint. We present an overview of green coding and metrics to measure AI model sustainability awareness. This study introduces LLM as a service and uses a generative commercial AI language model, GitHub Copilot, to auto-generate code. Using sustainability metrics to quantify these AI models' sustainability awareness, we define the code's embodied and operational carbon.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of distributed cloud storage,"The ICT (Information Communication Technologies) ecosystem is estimated to be responsible, as of today, for 10% of the total worldwide energy demand - equivalent to the combined energy production of Germany and Japan. Cloud storage, mainly operated through large and densely-packed data centers, constitutes a non-negligible part of it. However, since the cloud is a fast-inflating market and the energy-efficiency of data centers is mostly an insensitive issue for the collectivity, its carbon footprint shows no signs of slowing down. In this paper, we analyze a novel paradigm for cloud storage (implemented by Cubbit, http://cubbit.io), in which data are stored and distributed over a network of p2p-interacting ARM-based single-board devices. We compare Cubbit's distributed cloud to the traditional centralized solution in terms of environmental footprint and energy efficiency. We demonstrate that, compared to the centralized cloud, the distributed architecture of Cubbit has a carbon footprint reduced of a 77% factor for data storage and of a 50% factor for data transfers. These results provide an example of how a radical paradigm shift in a large-reach technology can benefit both the final consumer as well as our society as a whole.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training,"Deep learning has experienced significant growth in recent years, resulting in increased energy consumption and carbon emission from the use of GPUs for training deep neural networks (DNNs). Answering the call for sustainability, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations. Moreover, postponing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Furthermore, in order to proactively adapt to shifting carbon intensity, we propose a lightweight machine learning algorithm that predicts the carbon intensity of the upcoming time frame. Our solution, Chase, reduces the total carbon footprint of training ResNet-50 on ImageNet by 13.6% while only increasing training time by 2.5%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Aware Quality Adaptation for Energy-Intensive Services,"The energy demand of modern cloud services, particularly those related to generative AI, is increasing at an unprecedented pace. To date, carbon-aware computing strategies have primarily focused on batch process scheduling or geo-distributed load balancing. However, such approaches are not applicable to services that require constant availability at specific locations due to latency, privacy, data, or infrastructure constraints.   In this paper, we explore how the carbon footprint of energy-intensive services can be reduced by adjusting the fraction of requests served by different service quality tiers. We show that adapting this quality of responses with respect to grid carbon intensity can lead to additional carbon savings beyond resource and energy efficiency and introduce a forecast-based multi-horizon optimization that reaches close-to-optimal carbon savings.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Sustainable Low Carbon Emission Mini Data Centres,"Mini data centres have become increasingly prevalent in diverse organizations in recent years. They can be easily deployed at large scale, with high resilience. They are also cost-effective and provide highsecurity protection. On the other hand, IT technologies have resulted in the development of ever more energy-efficient servers, leading to the periodic replacement of older-generation servers in mini data centres. However, the disposal of older servers has resulted in electronic waste that further aggravates the already critical e-waste problem. Furthermore, despite the shift towards more energy-efficient servers, many mini data centres still rely heavily on high-carbon energy sources. This contributes to data centres' overall carbon footprint. All these issues are concerns for sustainability. In order to address this sustainability issue, this paper proposes an approach to extend the lifespan of older-generation servers in mini data centres. This is made possible thanks to a novel solar-powered computing technology, named Genesis, that compensates for the energy overhead generated by older servers. As a result, electronic waste can be reduced while improving system sustainability by reusing functional server hardware. Moreover, Genesis does not require server cooling, which reduces energy and water requirements. Analytical reasoning is applied to compare the efficiency of typical conventional mini data centre designs against alternative Genesis-based designs, in terms of energy, carbon emissions and exploitation costs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of IRAP,"We present an assessment of the greenhouse gases emissions of the Institute for Research in Astrophysics and Planetology (IRAP), located in Toulouse (France). It was performed following the established ""Bilan Carbone"" methodology, over a large scope compared to similar previous studies, including in particular the contribution from the purchase of goods and services as well as IRAP's use of external research infrastructures, such as ground-based observatories and space-borne facilities. The carbon footprint of the institute for the reference year 2019 is 7400 +/- 900 tCO2e. If we exclude the contribution from external research infrastructures to focus on a restricted perimeter over which the institute has some operational control, IRAP's emissions in 2019 amounted to 3300 +/- 400 tCO2e. Over the restricted perimeter, the contribution from purchasing goods and services is dominant, about 40% of the total, slightly exceeding the contribution from professional travel including hotel stays, which accounts for 38%. Local infrastructures make a smaller contribution to IRAP's carbon footprint, about 25% over the restricted perimeter. We note that this repartition may be specific to IRAP, since the energy used to produce the electricity and heating has a relatively low carbon footprint. Over the full perimeter, the large share from the use of ground-based observatories and space-borne facilities and the fact that the majority of IRAP purchases are related to instrument development indicate that research infrastructures represent the most significant challenge for reducing the carbon footprint of research at our institute. With ~260 staff members employed, our results imply that performing research in astronomy and astrophysics at IRAP according to the standards of 2019 produces average GHG emissions of 28 tCO2e/yr per person involved in that activity (Abridged).","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
AI-Enhanced Decision-Making for Sustainable Supply Chains: Reducing Carbon Footprints in the USA,"Organizations increasingly need to reassess their supply chain strategies in the rapidly modernizing world towards sustainability. This is particularly true in the United States, where supply chains are very extensive and consume a large number of resources. This research paper discusses how AI can support decision-making for sustainable supply chains with a special focus on carbon footprints. These AI technologies, including machine learning, predictive analytics, and optimization algorithms, will enable companies to be more efficient, reduce emissions, and display regulatory and consumer demands for sustainability, among other aspects. The paper reviews challenges and opportunities regarding implementing AI-driven solutions to promote sustainable supply chain practices in the USA.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Modeling the Carbon Footprint of HPC: The Top 500 and EasyC,"Climate change is a critical concern for HPC systems, but GHG protocol carbon-emission accounting methodologies are difficult for a single system, and effectively infeasible for a collection of systems. As a result, there is no HPC-wide carbon reporting, and even the largest HPC sites do not do GHG protocol reporting.   We assess the carbon footprint of HPC, focusing on the Top 500 systems. The key challenge lies in modeling the carbon footprint with limited data availability.   With the disclosed top500 website data, and using a new tool, EasyC, we were able to model the operational carbon of 391 HPC systems and the embodied carbon of 283 HPC systems. We further show how this coverage can be enhanced by exploiting additional public information. With improved coverage, then interpolation is used to produce the first carbon footprint estimates of the Top 500 HPC systems. They are 1.4 million MT CO2e operational carbon (1 Year) and 1.9 million MT CO2e embodied carbon. We also project how the Top 500's carbon footprint will increase through 2030.   A key enabler is the EasyC tool which models carbon footprint with only a few data metrics. We explore availability of data and enhancement, showing that coverage can be increased to 98% of Top 500 systems for operational and 80.8% of the systems for embodied emissions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Core Hours and Carbon Credits: Incentivizing Sustainability in HPC,"Realizing a shared responsibility between providers and consumers is critical to manage the sustainability of HPC. However, while cost may motivate efficiency improvements by infrastructure operators, broader progress is impeded by a lack of user incentives. We conduct a survey of HPC users that reveals fewer than 30 percent are aware of their energy consumption, and that energy efficiency is among users' lowest priority concerns. One explanation is that existing pricing models may encourage users to prioritize performance over energy efficiency. We propose two transparent multi-resource pricing schemes, Energy- and Carbon-Based Accounting, that seek to change this paradigm by incentivizing more efficient user behavior. These two schemes charge for computations based on their energy consumption or carbon footprint, respectively, rewarding users who leverage efficient hardware and software. We evaluate these two pricing schemes via simulation, in a prototype, and a user study.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Minimizing Carbon Footprint for Timely E-Truck Transportation: Hardness and Approximation Algorithm,"Carbon footprint optimization (CFO) is important for sustainable heavy-duty e-truck transportation. We consider the CFO problem for timely transportation of e-trucks, where the truck travels from an origin to a destination across a national highway network subject to a deadline. The goal is to minimize the carbon footprint by orchestrating path planning, speed planning, and intermediary charging planning. We first show that it is NP-hard even just to find a feasible CFO solution. We then develop a $(1+_F, 1+_)$ bi-criteria approximation algorithm that achieves a carbon footprint within a ratio of $(1+_F)$ to the minimum with no deadline violation and at most a ratio of $(1+_)$ battery capacity violation (for any positive $_F$ and $_$). Its time complexity is polynomial in the size of the highway network, $1/_F$, and $1/_$. Such algorithmic results are among the best possible unless P=NP. Simulation results based on real-world traces show that our scheme reduces up to 11\% carbon footprint as compared to baseline alternatives considering only energy consumption but not carbon footprint.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Banking; Evaluation of the European Business Models,"Sustainable business models also offer banks competitive advantages such as increasing brand reputation and cost reduction. However, no framework is presented to evaluate the sustainability of banking business models. To bridge this theoretical gap, the current study using A Delphi-Analytic Hierarchy Process method, firstly, developed a sustainable business model to evaluate the sustainability of the business model of banks. In the second step, the sustainability performance of sixteen banks from eight European countries including Norway, the UK, Poland, Hungary, Germany, France, Spain, and Italy, assessed. The proposed business model components of this study were ranked in terms of their impact on achieving sustainability goals. Consequently, the proposed model components of this study, based on their impact on sustainability, are respectively value proposition, core competencies, financial aspects, business processes, target customers, resources, technology, customer interface, and partner network. The results of the comparison of the banks studied by each country disclosed that the sustainability of the Norwegian and German banks business models is higher than in other counties. The studied banks of Hungary and Spain came in second, the banks of the UK, Poland, and France ranked third, and finally, the Italian banks ranked fourth in the sustainability of their business models.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
An open-source tool to assess the carbon footprint of research,"Research institutions are bound to contribute to greenhouse gas emission (GHG) reduction efforts for several reasons. First, part of the scientific community's research deals with climate change issues. Second, scientists contribute to students' education: they must be consistent and role models. Third the literature on the carbon footprint of researchers points to the high level of some individual footprints. In a quest for consistency and role models, scientists, teams of scientists or universities have started to quantify their carbon footprints and debate on reduction options. Indeed, measuring the carbon footprint of research activities requires tools designed to tackle its specific features. In this paper, we present an open-source web application, GES 1point5, developed by an interdisciplinary team of scientists from several research labs in France. GES 1point5 is specifically designed to estimate the carbon footprint of research activities in France. It operates at the scale of research labs, i.e. laboratoires, which are the social structures around which research is organized in France and the smallest decision making entities in the French research system. The application allows French research labs to compute their own carbon footprint along a standardized, open protocol. The data collected in a rapidly growing network of labs will be used as part of the Labos 1point5 project to estimate France's research carbon footprint. At the time of submitting this manuscript, 89 research labs had engaged with GES 1point5 to estimate their greenhouse gas emissions. We expect that an international adoption of GES 1point5 (adapted to fit domestic specifics) could contribute to establishing a global understanding of the drivers of the research carbon footprint worldwide and the levers to decrease it.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development,"Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A first look into the carbon footprint of federated learning,"Despite impressive results, deep learning-based technologies also raise severe privacy and environmental concerns induced by the training procedure often conducted in data centers. In response, alternatives to centralized training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and social groups advocating for privacy protection. \textit{However, the potential environmental impact related to FL remains unclear and unexplored. This paper offers the first-ever systematic study of the carbon footprint of FL.} First, we propose a rigorous model to quantify the carbon footprint, hence facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. Our findings show that, depending on the configuration, FL can emit up to two order of magnitude more carbon than centralized machine learning. However, in certain settings, it can be comparable to centralized learning due to the reduced energy consumption of embedded devices. We performed extensive experiments across different types of datasets, settings and various deep learning models with FL. Finally, we highlight and connect the reported results to the future challenges and trends in FL to reduce its environmental impact, including algorithms efficiency, hardware capabilities, and stronger industry transparency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability Transitions and Bending the Curve of Biodiversity Collapse in the Amazon Forest,"This paper undertakes an analysis of deforestation in the Amazon area using a pathways-based approach to sustainability. We ground the analysis primarily in the sustainability transitions literature but also draw a bridge with socio-ecological concepts which helps us to understand the nature of transitions in this context. The concept of a deforestation system is developed by examining the interplay of infrastructure, technologies, narratives, and institutions. Drawing on a literature review and an in-depth case study of Puerto Maldonado in Madre de Dios, Peru, the paper identifies three pathways for addressing deforestation: optimisation, natural capital, and regenerative change. We suggest that while the optimisation pathway provides partial solutions through mitigation and compensation strategies, it often reinforces extractivist logics. The study also underscores the limitations of natural capital frameworks, which tend to rely on centralised governance and market-based instruments while lacking broader social engagement. In contrast, our findings emphasise the potential of regenerative strategies rooted in local agency, community-led experimentation, and context-sensitive institutional arrangements. The paper contributes to ongoing debates on biodiversity governance by illustrating how the spatial and long-term dynamics of deforestation interact, and why inclusive, territorially grounded pathways are crucial for bending the curve of biodiversity loss.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Open and Linked Data Model for Carbon Footprint Scenarios,"Carbon footprint quantification is key to well-informed decision making over carbon reduction potential, both for individuals and for companies. Many carbon footprint case studies for products and services have been circulated recently. Due to the complex relationships within each scenario, however, the underlying assumptions often are difficult to understand. Also, re-using and adapting a scenario to local or individual circumstances is not a straightforward task. To overcome these challenges, we propose an open and linked data model for carbon footprint scenarios which improves data quality and transparency by design. We demonstrate the implementation of our idea with a web-based data interpreter prototype.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,"The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \url{https://github.com/SotaroKaneda/MLCarbon}.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis,"The increasing energy consumption and carbon footprint of deep learning (DL) due to growing compute requirements has become a cause of concern. In this work, we focus on the carbon footprint of developing DL models for medical image analysis (MIA), where volumetric images of high spatial resolution are handled. In this study, we present and compare the features of four tools from literature to quantify the carbon footprint of DL. Using one of these tools we estimate the carbon footprint of medical image segmentation pipelines. We choose nnU-net as the proxy for a medical image segmentation pipeline and experiment on three common datasets. With our work we hope to inform on the increasing energy costs incurred by MIA. We discuss simple strategies to cut-down the environmental impact that can make model selection and training processes more efficient.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of large astronomy meetings,"The annual meeting of the European Astronomical Society took place in Lyon, France, in 2019, but in 2020 it was held online only due the COVID-19 pandemic. The carbon footprint of the virtual meeting was roughly 3,000 times smaller than the face-to-face one, providing encouragement for more ecologically minded conferencing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference","This paper introduces an infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models in commercial datacenters. The framework combines public API performance data with company-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes model-level energy, water, and carbon metrics. Results show the most energy-intensive models exceed 29 Wh per long prompt, over 65 times the most efficient systems. Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35{,}000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset. These findings highlight a growing paradox: as AI becomes cheaper and faster, global adoption drives disproportionate resource consumption. Our methodology offers a standardized, empirically grounded basis for sustainability benchmarking and accountability in AI deployment.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models,"Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Estimating The Carbon Footprint Of Digital Agriculture Deployment: A Parametric Bottom-Up Modelling Approach,"Digitalization appears as a lever to enhance agriculture sustainability. However, existing works on digital agriculture's own sustainability remain scarce, disregarding the environmental effects of deploying digital devices on a large-scale. We propose a bottom-up method to estimate the carbon footprint of digital agriculture scenarios considering deployment of devices over a diversity of farm sizes. It is applied to two use-cases and demonstrates that digital agriculture encompasses a diversity of devices with heterogeneous carbon footprints and that more complex devices yield higher footprints not always compensated by better performances or scaling gains. By emphasizing the necessity of considering the multiplicity of devices, and the territorial distribution of farm sizes when modelling digital agriculture deployments, this study highlights the need for further exploration of the first-order effects of digital technologies in agriculture.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Framework for Energy and Carbon Footprint Analysis of Distributed and Federated Edge Learning,"Recent advances in distributed learning raise environmental concerns due to the large energy needed to train and move data to/from data centers. Novel paradigms, such as federated learning (FL), are suitable for decentralized model training across devices or silos that simultaneously act as both data producers and learners. Unlike centralized learning (CL) techniques, relying on big-data fusion and analytics located in energy hungry data centers, in FL scenarios devices collaboratively train their models without sharing their private data. This article breaks down and analyzes the main factors that influence the environmental footprint of FL policies compared with classical CL/Big-Data algorithms running in data centers. The proposed analytical framework takes into account both learning and communication energy costs, as well as the carbon equivalent emissions; in addition, it models both vanilla and decentralized FL policies driven by consensus. The framework is evaluated in an industrial setting assuming a real-world robotized workplace. Results show that FL allows remarkable end-to-end energy savings (30%-40%) for wireless systems characterized by low bit/Joule efficiency (50 kbit/Joule or lower). Consensus-driven FL does not require the parameter server and further reduces emissions in mesh networks (200 kbit/Joule). On the other hand, all FL policies are slower to converge when local data are unevenly distributed (often 2x slower than CL). Energy footprint and learning loss can be traded off to optimize efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework,"Deep neural network models are used today in various applications of artificial intelligence, the strengthening of which, in the face of adversarial attacks is of particular importance. An appropriate solution to adversarial attacks is adversarial training, which reaches a trade-off between robustness and generalization. This paper introduces a novel framework (Layer Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks. LSA can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis. The LSA framework identifies a list of Most Vulnerable Layers (MVL list) of the given network. The relative error, as a comparison measure, is used to evaluate representation sustainability of each layer against adversarial inputs. The proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layer-wise regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches. The proposed idea performs well theoretically and experimentally for state-of-the-art multilayer perceptron and convolutional neural network architectures. Compared with the AT-LR and its corresponding base adversarial training, the classification accuracy of more significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and CIFAR-10 benchmark datasets, respectively. The LSA framework is available and published at https://github.com/khalooei/LSA.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Software Sustainability: A Design Case for Achieving Sustainable Pension Services in Developing Country,"The need for efficient and sustainable software to improve business and achieve goals cannot be over-emphasized. Sustainable digital services and product delivery cannot be achieved without embracing sustainable software design practices. Despite the current research progress on software sustainability, most software development practitioners in developing countries are unclear about what constitutes software sustainability and often lack the proper understanding of how to implement it in their specific industry domain. Research efforts from software engineering focused on promoting software sustainability awareness in developed countries, and fewer efforts have been channeled to studying the same awareness in developing countries. This has affected the level of awareness about sustainable software design practices in most developing countries. This research investigates the awareness of software sustainability in the Nigerian pension industry and its challenges among practitioners. The software development practitioners were engaged and interviewed. We offered ways to mitigate the identified challenges and promote the awareness of software sustainability in the pension industry. Our findings further show that, with the right sustainability knowledge, the software practitioners in the pension industry have the potential to support their organization's sustainable culture and improve the efficiency of product design and service delivery.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Ichnos: A Carbon Footprint Estimator for Scientific Workflows,"Scientific workflows facilitate the automation of data analysis, and are used to process increasing amounts of data. Therefore, they tend to be resource-intensive and long-running, leading to significant energy consumption and carbon emissions. With ever-increasing emissions from the ICT sector, it is crucial to quantify and understand the carbon footprint of scientific workflows. However, existing tooling requires significant effort from users - such as setting up power monitoring before executing workloads, or translating monitored metrics into the carbon footprints post-execution. In this paper, we introduce a system to estimate the carbon footprint of Nextflow scientific workflows that enables post-hoc estimation based on existing workflow traces, power models for computational resources utilised, and carbon intensity data aligned with the execution time. We discuss our automated power modelling approach, and compare it with commonly used estimation methodologies. Furthermore, we exemplify several potential use cases and evaluate our energy consumption estimation approach, finding its estimation error to be between 3.9-10.3%, outperforming both baseline methodologies.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation,"The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Profiling the carbon footprint of performance bugs,"Much debate nowadays is devoted to the impacts of modern information and communication technology on global carbon emissions. Green information and communication technology is a paradigm creating a sustainable and environmentally friendly computing field that tries to minimize the adverse effects on the environment. Green information and communication technology are under constant development nowadays. Thus, in this paper, we undertake the problem of performance bugs that, until recently, have never been studied so profoundly. We assume that inappropriate software implementations can have a crucial influence on global carbon emissions. Here, we classify those performance bugs and develop inappropriate implementations of four programs written in C++. To mitigate these simulated performance bugs, measuring software and hardware methods that can estimate the increased carbon footprint properly were proposed.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Under the hood of a carbon footprint calculator,We explain the mathematical theory of the Input-Output method for carbon footprints computations.,"all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
From Silicon Shield to Carbon Lock-in ? The Environmental Footprint of Electronic Components Manufacturing in Taiwan (2015-2020),"Taiwan plans to rapidly increase its industrial production capacity of electronic components while concurrently setting policies for its ecological transition. Given that the island is responsible for the manufacturing of a significant part of worldwide electronics components, the sustainability of the Taiwanese electronics industry is therefore of critical interest. In this paper, we survey the environmental footprint of 16 Taiwanese electronic components manufacturers (ECM) using corporate sustainability responsibility reports (CSR). Based on data from 2015 to 2020, this study finds out that our sample of 16 manufacturers increased its greenhouse gases (GHG) emissions by 7.5\% per year, its final energy and electricity consumption by 8.8\% and 8.9\%, and the water usage by 6.1\%. We show that the volume of manufactured electronic components and the environmental footprints compiled in this study are strongly correlated, which suggests that relative efficiency gains are not sufficient to curb the environmental footprint at the national scale. Given the critical nature of electronics industry for Taiwan's geopolitics and economics, the observed increase of energy consumption and the slow renewable energy roll-out, these industrial activities could create a carbon lock-in, blocking the Taiwanese government from achieving its carbon reduction goals and its sustainability policies. Besides, the European Union, the USA or even China aim at developing an industrial ecosystem targeting sub-10nm CMOS technology nodes similar to Taiwan. This study thus provides important insights regarding the environmental implications associated with such a technology roadmap. All data and calculation models used in this study are provided as supplementary material.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Next-Generation Sustainable Wireless Systems: Energy Efficiency Meets Environmental Impact,"Aligning with the global mandates pushing towards advanced technologies with reduced resource consumption and environmental impacts, the sustainability of wireless networks becomes a significant concern in 6G systems. To address this concern, a native integration of sustainability into the operations of next-generation networks through novel designs and metrics is necessary. Nevertheless, existing wireless sustainability efforts remain limited to energy-efficient network designs which fail to capture the environmental impact of such systems. In this paper, a novel sustainability metric is proposed that captures emissions per bit, providing a rigorous measure of the environmental footprint associated with energy consumption in 6G networks. This metric also captures how energy, computing, and communication resource parameters influence the reduction of emissions per bit. Then, the problem of allocating the energy, computing and communication resources is posed as a multi-objective (MO) optimization problem. To solve the resulting non-convex problem, our framework leverages MO reinforcement learning (MORL) to maximize the novel sustainability metric alongside minimizing energy consumption and average delays in successfully delivering the data, all while adhering to constraints on energy resource capacity. The proposed MORL methodology computes a global policy that achieves a Pareto-optimal tradeoff among multiple objectives, thereby balancing environmental sustainability with network performance. Simulation results show that the proposed approach reduces the average emissions per bit by around 26% compared to state-of-the-art methods that do not explicitly integrate carbon emissions into their control objectives.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Efficient 3D DNN Acceleration: Optimizing Performance and Sustainability,"As Deep Neural Networks (DNNs) continue to drive advancements in artificial intelligence, the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes. 3D integration improves performance but introduces sustainability challenges, making carbon-aware optimization essential. In this work, we propose a carbon-efficient design methodology for 3D DNN accelerators, leveraging approximate computing and genetic algorithm-based design space exploration to optimize Carbon Delay Product (CDP). By integrating area-efficient approximate multipliers into Multiply-Accumulate (MAC) units, our approach effectively reduces silicon area and fabrication overhead while maintaining high computational accuracy. Experimental evaluations across three technology nodes (45nm, 14nm, and 7nm) show that our method reduces embodied carbon by up to 30% with negligible accuracy drop.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A User Study of Perceived Carbon Footprint,"We propose a statistical model to understand people's perception of their carbon footprint. Driven by the observation that few people think of CO2 impact in absolute terms, we design a system to probe people's perception from simple pairwise comparisons of the relative carbon footprint of their actions. The formulation of the model enables us to take an active-learning approach to selecting the pairs of actions that are maximally informative about the model parameters. We define a set of 18 actions and collect a dataset of 2183 comparisons from 176 users on a university campus. The early results reveal promising directions to improve climate communication and enhance climate mitigation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning,"To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end tool for precise carbon footprint estimation in IoT-enabled DL, with deviations as low as 5\% for operational and 3.23\% for embodied carbon footprints compared to actual measurements across various DL models. Additionally, practical applications of \carb~are showcased through multiple user case studies.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Chasing Carbon: The Elusive Environmental Footprint of Computing,"Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Multi-Modal Transportation and Routing focusing on Costs and Carbon Emissions Reduction,"Transportation plays a critical role in supply chain networks, directly impacting cost efficiency, delivery reliability, and environmental sustainability. This study provides an enhanced optimization model for transportation planning, emphasizing environmental sustainability and cost-efficiency. An Integer Linear Programming (ILP) model was developed to minimize the total transportation costs by considering organizational and third-party vehicles' operational and rental costs while incorporating constraints on carbon emissions. The model incorporates multi-modal transportation routing and emission caps to select the optimized number of organizational and rental vehicles of different modes in each route to ensure adherence to sustainability goals. Key innovations include adding carbon emission constraints and optimizing route selection to reduce overall emissions. The model was implemented using the Gurobi solver, and numerical analysis reveals a trade-off between cost minimization and carbon footprint reduction. The results indicate that adopting tight environmental policies increases the costs by around 8% on average while more than 95% of the vehicles utilized will be rented. These insights provide actionable guidance for industries aiming to enhance both economic performance and environmental responsibility.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Carbon Footprint-Aware Recommender Systems for Greener Item Recommendation,"The commodity and widespread use of online shopping are having an unprecedented impact on climate, with emission figures from key actors that are easily comparable to those of a large-scale metropolis. Despite online shopping being fueled by recommender systems (RecSys) algorithms, the role and potential of the latter in promoting more sustainable choices is little studied. One of the main reasons for this could be attributed to the lack of a dataset containing carbon footprint emissions for the items. While building such a dataset is a rather challenging task, its presence is pivotal for opening the doors to novel perspectives, evaluations, and methods for RecSys research. In this paper, we target this bottleneck and study the environmental role of RecSys algorithms. First, we mine a dataset that includes carbon footprint emissions for its items. Then, we benchmark conventional RecSys algorithms in terms of accuracy and sustainability as two faces of the same coin. We find that RecSys algorithms optimized for accuracy overlook greenness and that longer recommendation lists are greener but less accurate. Then, we show that a simple reranking approach that accounts for the item's carbon footprint can establish a better trade-off between accuracy and greenness. This reranking approach is modular, ready to use, and can be applied to any RecSys algorithm without the need to alter the underlying mechanisms or retrain models. Our results show that a small sacrifice of accuracy can lead to significant improvements of recommendation greenness across all algorithms and list lengths. Arguably, this accuracy-greenness trade-off could even be seen as an enhancement of user satisfaction, particularly for purpose-driven users who prioritize the environmental impact of their choices. We anticipate this work will serve as the starting point for studying RecSys for more sustainable recommendations.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters,"In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"The Carbon Cost of Conversation, Sustainability in the Age of Language Models","Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Historical Evolution of Global Inequality in Carbon Emissions and Footprints versus Redistributive Scenarios,"Ambitious scenarios of carbon emission redistribution for mitigating climate change in line with the Paris Agreement and reaching the sustainable development goal of eradicating poverty have been proposed recently. They imply a strong reduction in carbon footprint inequality by 2030 that effectively halves the Gini coefficient to about 0.25. This paper examines feasibility of these scenarios by analyzing the historical evolution of both weighted international inequality in CO2 emissions attributed territorially and global inequality in carbon footprints attributed to end consumers. For the latter, a new dataset is constructed that is more comprehensive than existing ones. In both cases, we find a decreasing trend in global inequality, partially attributed to the move of China from the lower to the middle part of the distribution, with footprints more unequal than territorial emissions. These results show that realization of the redistributive scenarios would require an unprecedented reduction in global inequality far below historical levels. Moreover, the territorial emissions data, available for more recent years up to 2017, show a saturation of the decreasing Gini coefficient at a level of 0.5. This observation confirms an earlier prediction based on maximal entropy reasoning that the Lorenz curve converges to the exponential distribution. This saturation further undermines feasibility of the redistributive scenarios, which are also hindered by structural tendencies that reinforce carbon footprint inequality under global capitalism. One way out of this conundrum is a fast decarbonization of the global energy supply in order to decrease global carbon emissions without relying crucially on carbon inequality reduction.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustaining Research Software via Research Software Engineers and Professional Associations,"Research software is a class of software developed to support research. Today a wealth of such software is created daily in universities, government, and commercial research enterprises worldwide. The sustainability of this software faces particular challenges due, at least in part, to the type of people who develop it. These Research Software Engineers (RSEs) face challenges in developing and sustaining software that differ from those faced by the developers of traditional software. As a result, professional associations have begun to provide support, advocacy, and resources for RSEs. These benefits are critical to sustaining RSEs, especially in environments where their contributions are often undervalued and not rewarded. This paper focuses on how professional associations, such as the United States Research Software Engineer Association (US-RSE), can provide this.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The United Nations Sustainable Development Goals in Systems Engineering: Eliciting sustainability requirements,"This paper discusses a PhD research project testing the hypothesis that using the United Nations Sustainable Development Goals(SDG) as explicit inputs to drive the Software Requirements Engineering process will result in requirements with improved sustainability benefits. The research has adopted the Design Science Research Method (DSRM) [21] to test a process named SDG Assessment for Requirements Elicitation (SDGARE). Three DSRM cycles are being used to test the hypothesis in safety-critical, highprecision, software-intensive systems in aerospace and healthcare. Initial results from the first two DSRM cycles support the hypothesis. However, these cycles are in a plan-driven (waterfall) development context and future research agenda would be a similar application in an Agile development context.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model","Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences,"Throughout its lifecycle, a large language model (LLM) generates a substantially larger carbon footprint during inference than training. LLM inference requests vary in batch size, prompt length, and token generation number, while cloud providers employ different GPU types and quantities to meet diverse service-level objectives for accuracy and latency. It is crucial for both users and cloud providers to have a tool that quickly and accurately estimates the carbon impact of LLM inferences based on a combination of inference request and hardware configurations before execution. Estimating the carbon footprint of LLM inferences is more complex than training due to lower and highly variable model FLOPS utilization, rendering previous equation-based models inaccurate. Additionally, existing machine learning (ML) prediction methods either lack accuracy or demand extensive training data, as they inadequately handle the distinct prefill and decode phases, overlook hardware-specific features, and inefficiently sample uncommon inference configurations. We introduce \coo, a graph neural network (GNN)-based model that greatly improves the accuracy of LLM inference carbon footprint predictions compared to previous methods.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Embedding Sustainability in Complex Projects: A Pedagogic Practice Simulation Approach,"Sustainability is focussed on avoiding the long-term depletion of natural resources. Under the terms of a government plan to tackle climate change, a driver for improved sustainability is the cut of greenhouse gas emissions in the UK to almost zero by 2050. With this type of change, new themes are continuously being developed which drive complex projects, such as the development of new power generation methods, which encompass challenging lead times and demanding requirements. Consideration of the implementation of strategies and key concepts, which may engender sustainability within complex projects therefore presents an opportunity for further critical debate, review, and application through a project management lens. Sustainability incorporation in project management has been documented in academic literature, with this emerging field providing new challenges. For example, project management education can provide a holistic base for the inculcation of sustainability factors to a range of industries, including complex projects. Likewise, practitioner interest and approaches to sustainability in project management are being driven by the recently Chartered Association for Project Management (APM). Whilst this body makes a significant contribution to the UK economy across many sectors, it also addresses ongoing sustainability challenges. Therefore, by drawing on research and practitioner developments, the authors argue that by connecting with the next generation through practice simulation approaches, and embedding sustainability issues within project management tools and methods, improved focus on sustainability in complex project management may be achieved.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Dirty Bits in Low-Earth Orbit: The Carbon Footprint of Launching Computers,"Low-Earth Orbit (LEO) satellites are increasingly proposed for communication and in-orbit computing, achieving low-latency global services. However, their sustainability remains largely unexamined. This paper investigates the carbon footprint of computing in space, focusing on lifecycle emissions from launch over orbital operation to re-entry. We present ESpaS, a lightweight tool for estimating carbon intensities across CPU usage, memory, and networking in orbital vs. terrestrial settings. Three worked examples compare (i) launch technologies (state-of-the-art rocket vs. potential next generation) and (ii) operational emissions of data center workloads in orbit and on the ground. Results show that, even under optimistic assumptions, in-orbit systems incur significantly higher carbon costs - up to an order of magnitude more than terrestrial equivalents - primarily due to embodied emissions from launch and re-entry. Our findings advocate for carbon-aware design principles and regulatory oversight in developing sustainable digital infrastructure in orbit.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances,"'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Green Algorithms: Quantifying the carbon footprint of computation,"Climate change is profoundly affecting nearly all aspects of life on earth, including human societies, economies and health. Various human activities are responsible for significant greenhouse gas emissions, including data centres and other sources of large-scale computation. Although many important scientific milestones have been achieved thanks to the development of high-performance computing, the resultant environmental impact has been underappreciated. In this paper, we present a methodological framework to estimate the carbon footprint of any computational task in a standardised and reliable way, based on the processing time, type of computing cores, memory available and the efficiency and location of the computing facility. Metrics to interpret and contextualise greenhouse gas emissions are defined, including the equivalent distance travelled by car or plane as well as the number of tree-months necessary for carbon sequestration. We develop a freely available online tool, Green Algorithms (www.green-algorithms.org), which enables a user to estimate and report the carbon footprint of their computation. The Green Algorithms tool easily integrates with computational processes as it requires minimal information and does not interfere with existing code, while also accounting for a broad range of CPUs, GPUs, cloud computing, local servers and desktop computers. Finally, by applying Green Algorithms, we quantify the greenhouse gas emissions of algorithms used for particle physics simulations, weather forecasts and natural language processing. Taken together, this study develops a simple generalisable framework and freely available tool to quantify the carbon footprint of nearly any computation. Combined with a series of recommendations to minimise unnecessary CO2 emissions, we hope to raise awareness and facilitate greener computation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems,"High-performance computing (HPC) systems are becoming increasingly water-intensive due to their reliance on water-based cooling and the energy used in power generation. However, the water footprint of HPC remains relatively underexplored-especially in contrast to the growing focus on carbon emissions. In this paper, we present ThirstyFLOPS - a comprehensive water footprint analysis framework for HPC systems. Our approach incorporates region-specific metrics, including Water Usage Effectiveness, Power Usage Effectiveness, and Energy Water Factor, to quantify water consumption using real-world data. Using four representative HPC systems - Marconi, Fugaku, Polaris, and Frontier - as examples, we provide implications for HPC system planning and management. We explore the impact of regional water scarcity and nuclear-based energy strategies on HPC sustainability. Our findings aim to advance the development of water-aware, environmentally responsible computing infrastructures.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Analysis of the carbon footprint of HPC,"The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale. Unfortunately, this growth also comes with ever-increasing energy costs, leading to a high carbon footprint. This paper investigates the evolution of high performance systems in terms of carbon emissions. A lot of studies focus on Top500 (and Green500) as the tip of an iceberg to identify trends in the domain in terms of computing performance. We propose here to go further in considering the whole span life of several large scale systems and to link the evolution with trajectory toward 2030. More precisely, we introduce the energy mix in the analysis of Top500 systems and we derive a predictive model for estimating the weight of HPC for the next 5 years.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Entity Linking using LLMs for Automated Product Carbon Footprint Estimation,"Growing concerns about climate change and sustainability are driving manufacturers to take significant steps toward reducing their carbon footprints. For these manufacturers, a first step towards this goal is to identify the environmental impact of the individual components of their products. We propose a system leveraging large language models (LLMs) to automatically map components from manufacturer Bills of Materials (BOMs) to Life Cycle Assessment (LCA) database entries by using LLMs to expand on available component information. Our approach reduces the need for manual data processing, paving the way for more accessible sustainability practices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Mapping Inequalities in Activity-based Carbon Footprints of Urban Dwellers using Fine-grained Human Trajectory Data,"Effective climate mitigation strategies in cities rely on understanding and mapping urban carbon footprints. One significant source of carbon is a product of lifestyle choices and travel behaviors of urban residents. Although previous research addressed consumption- and home-related footprints, activity-based footprints of urban dwellers have garnered less attention. This study relies on deidentified human trajectory data from 5 million devices to examine the activity-based carbon footprint in Harris County, Texas. Our analysis of the heterogeneity of footprints based on places visited and distance traveled reveals significant inequality: 10% of users account for 88% of visitation-based footprints and 71% of distance-traveled footprints. We also identify the influence of income on activity-based carbon footprint gap of users related to their travel behavior and lifestyle choices, with high-income users having larger footprints due to lifestyle choices, while low- to medium-income users' footprints are due to limited access. Our findings underscore the need for urban design adjustments to reduce carbon-intensive behaviors and to improve facility distribution. Our conclusions highlight the importance of addressing urban design parameters that shape carbon-intensive lifestyle choices and facility distribution, decisions which have implications for developing interventions to reduce carbon footprints caused by human activities.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability-Aware Cloud Computing Using Virtual Carbon Tax,"In this paper, a solution for sustainable cloud system is proposed and then implemented on a real testbed. The solution composes of optimization of a profit model and introduction of virtual carbon tax to limit environmental footprint of the cloud. The proposed multi-criteria optimizer of the cloud system suggests new optimum CPU frequencies for CPU-cores when the local grid energy mix or the cloud workload changes. The cloud system is implemented on a blade system, and proper middlewares are developed to interact with the blades. The experimental results show that it is possible to significantly decrease the targeted environmental footprint of the system and keep it profitable.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Does Crypto Kill? Relationship between Electricity Consumption Carbon Footprints and Bitcoin Transactions,"Cryptocurrencies are gaining more popularity due to their security, making counterfeits impossible. However, these digital currencies have been criticized for creating a large carbon footprint due to their algorithmic complexity and decentralized system design for proof of work and mining. We hypothesize that the carbon footprint of cryptocurrency transactions has a higher dependency on carbon-rich fuel sources than green or renewable fuel sources. We provide a machine learning framework to model such transactions and correlate them with the electricity generation patterns to estimate and analyze their carbon cost.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Online Learning of Temporal Dependencies for Sustainable Foraging Problem,"The sustainable foraging problem is a dynamic environment testbed for exploring the forms of agent cognition in dealing with social dilemmas in a multi-agent setting. The agents need to resist the temptation of individual rewards through foraging and choose the collective long-term goal of sustainability. We investigate methods of online learning in Neuro-Evolution and Deep Recurrent Q-Networks to enable agents to attempt the problem one-shot as is often required by wicked social problems. We further explore if learning temporal dependencies with Long Short-Term Memory may be able to aid the agents in developing sustainable foraging strategies in the long term. It was found that the integration of Long Short-Term Memory assisted agents in developing sustainable strategies for a single agent, however failed to assist agents in managing the social dilemma that arises in the multi-agent scenario.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Follow the Sun and Go with the Wind: Carbon Footprint Optimized Timely E-Truck Transportation,"We study the carbon footprint optimization (CFO) of a heavy-duty e-truck traveling from an origin to a destination across a national highway network subject to a hard deadline, by optimizing path planning, speed planning, and intermediary charging planning. Such a CFO problem is essential for carbon-friendly e-truck operations. However, it is notoriously challenging to solve due to (i) the hard deadline constraint, (ii) positive battery state-of-charge constraints, (iii) non-convex carbon footprint objective, and (iv) enormous geographical and temporal charging options with diverse carbon intensity. Indeed, we show that the CFO problem is NP-hard. As a key contribution, we show that under practical settings it is equivalent to finding a generalized restricted shortest path on a stage-expanded graph, which extends the original transportation graph to model charging options. Compared to alternative approaches, our formulation incurs low model complexity and reveals a problem structure useful for algorithm design. We exploit the insights to develop an efficient dual-subgradient algorithm that always converges. As another major contribution, we prove that (i) each iteration only incurs polynomial-time complexity, albeit it requires solving an integer charging planning problem optimally, and (ii) the algorithm generates optimal results if a condition is met and solutions with bounded optimality loss otherwise. Extensive simulations based on real-world traces show that our scheme reduces up to 28% carbon footprint compared to baseline alternatives. The results also demonstrate that e-truck reduces 56% carbon footprint than internal combustion engine trucks.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation,"Product sustainability reports provide valuable insights into the environmental impacts of a product and are often distributed in PDF format. These reports often include a combination of tables and text, which complicates their analysis. The lack of standardization and the variability in reporting formats further exacerbate the difficulty of extracting and interpreting relevant information from large volumes of documents. In this paper, we tackle the challenge of answering questions related to carbon footprints within sustainability reports available in PDF format. Unlike previous approaches, our focus is on addressing the difficulties posed by the unstructured and inconsistent nature of text extracted from PDF parsing. To facilitate this analysis, we introduce CarbonPDF-QA, an open-source dataset containing question-answer pairs for 1735 product report documents, along with human-annotated answers. Our analysis shows that GPT-4o struggles to answer questions with data inconsistencies. To address this limitation, we propose CarbonPDF, an LLM-based technique specifically designed to answer carbon footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama 3 with our training data. Our results show that our technique outperforms current state-of-the-art techniques, including question-answering (QA) systems finetuned on table and text data.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Optimizing Carbon Footprint in ICT through Swarm Intelligence with Algorithmic Complexity,"Global emissions from fossil fuel combustion and cement production were recorded in 2022, signaling a resurgence to pre-pandemic levels and providing an apodictic indication that emission peaks have not yet been achieved. Significant contributions to this upward trend are made by the Information and Communication Technology (ICT) industry due to its substantial energy consumption. This shows the need for further exploration of swarm intelligence applications to measure and optimize the carbon footprint within ICT. All causative factors are evaluated based on the quality of data collection; variations from each source are quantified; and an objective function related to carbon footprint in ICT energy management is optimized. Emphasis is placed on the asyndetic integration of data sources to construct a convex optimization problem. An apodictic necessity to prevent the erosion of accuracy in carbon footprint assessments is addressed. Complexity percentages ranged from 5.25% for the Bat Algorithm to 7.87% for Fast Bacterial Swarming, indicating significant fluctuations in resource intensity among algorithms. These findings suggest that we were able to quantify the environmental impact of various swarm algorithms.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Value-transforming financial, carbon and biodiversity footprint accounting","Transformative changes in our production and consumption habits are needed to halt biodiversity loss. Organizations are the way we humans have organized our everyday life, and much of our negative environmental impacts, also called carbon and biodiversity footprints, are caused by organizations. Here we explore how the accounts of any organization can be exploited to develop an integrated carbon and biodiversity footprint account. As a metric we utilize spatially explicit potential global loss of species across all ecosystem types and argue that it can be understood as the biodiversity equivalent. The utility of the biodiversity equivalent for biodiversity could be like what carbon dioxide equivalent is for climate. We provide a global country specific dataset that organizations, experts and researchers can use to assess consumption-based biodiversity footprints. We also argue that the current integration of financial and environmental accounting is superficial and provide a framework for a more robust financial value-transforming accounting model. To test the methodologies, we utilized a Finnish university as a living lab. Assigning an offsetting cost to the footprints significantly altered the financial value of the organization. We believe such value-transforming accounting is needed to draw the attention of senior executives and investors to the negative environmental impacts of their organizations.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference,"The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of ""generation directives"" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
On the Sustainability of Lightweight Cryptography Based on PUFs Implemented on NAND Flash Memories Using Programming Disturbances,"In this work, we examine the potential of Physical Unclonable Functions (PUFs) that have been implemented on NAND Flash memories using programming disturbances to act as sustainable primitives for the purposes of lightweight cryptography. In particular, we investigate the ability of such PUFs to tolerate temperature and voltage variations, and examine the current shortcomings of existing NAND-Flash-memory PUFs that are based on programming disturbances as well as how these could potentially be addressed in order to provide more robust and more sustainable security solutions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Semantic Segmentation Network for Urban-Scale Building Footprint Extraction Using RGB Satellite Imagery,"Urban areas consume over two-thirds of the world's energy and account for more than 70 percent of global CO2 emissions. As stated in IPCC's Global Warming of 1.5C report, achieving carbon neutrality by 2050 requires a clear understanding of urban geometry. High-quality building footprint generation from satellite images can accelerate this predictive process and empower municipal decision-making at scale. However, previous Deep Learning-based approaches face consequential issues such as scale invariance and defective footprints, partly due to ever-present class-wise imbalance. Additionally, most approaches require supplemental data such as point cloud data, building height information, and multi-band imagery - which has limited availability and are tedious to produce. In this paper, we propose a modified DeeplabV3+ module with a Dilated Res-Net backbone to generate masks of building footprints from three-channel RGB satellite imagery only. Furthermore, we introduce an F-Beta measure in our objective function to help the model account for skewed class distributions and prevent false-positive footprints. In addition to F-Beta, we incorporate an exponentially weighted boundary loss and use a cross-dataset training strategy to further increase the quality of predictions. As a result, we achieve state-of-the-art performances across three public benchmarks and demonstrate that our RGB-only method produces higher quality visual results and is agnostic to the scale, resolution, and urban density of satellite imagery.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models,"Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Sustainability, behavior patterns and crises","Sustainability has been defined as meeting the needs of the present without compromising the ability of future generations to meet their own needs. But what are the needs of the present? And are they met? From the poor performance of the 2030 Sustainable Development Goals (SDG), defined by the UN in 2015, not even the collective needs of the present seem to be met. How to expect not to compromise the needs of the future? Is the achievement of global world goals incompatible with the characteristic processes of human evolution, as some authors have recently suggested? Simple mathematical models cannot capture the whole breadth of human experience and destiny. But, on the other hand, one should not neglect whatever insights they may provide. And what these models teach us is how the behavior pattern ""Parochial cooperation - Conflict - Growth"" was reached and how this pattern, in addition to leading to several types of crises, is also on the way of the global governance needed to achieve the SDG's","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Robust Sustainability Assessment Methodology for Aircraft Parts: Application to a Fuselage Panel,"The paper presents a cradle-to-gate sustainability assessment methodology specifically designed to evaluate aircraft components in a robust and systematic manner. This methodology integrates multi-criteria decision-making (MCDM) analysis across ten criteria, categorized under environmental impact, cost, and performance. Environmental impact is analyzed through life cycle assessment and cost through life cycle costing, with both analyses facilitated by SimaPro software. Performance is measured in terms of component mass and specific stiffness. The robustness of this methodology is tested through various MCDM techniques, normalization approaches, and objective weighting methods. To demonstrate the methodology, the paper assesses the sustainability of a fuselage panel, comparing nine variants that differ in materials, joining techniques, and part thicknesses. All approaches consistently identify thermoplastic CFRP panels as the most sustainable option, with the geometric mean aggregation of weights providing balanced criteria consideration across environmental, cost, and performance aspects. The adaptability of this proposed methodology is illustrated, showing its applicability to any aircraft component with the requisite data. This structured approach offers critical insights to support sustainable decision-making in aircraft component design and procurement.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficiency and Sustainability in New Generation Cloud Computing: A Vision and Directions for Integrated Management of Data Centre Resources and Workloads,"Cloud computing has become a critical infrastructure for modern society, like electric power grids and roads. As the backbone of the modern economy, it offers subscription-based computing services anytime, anywhere, on a pay-as-you-go basis. Its use is growing exponentially with the continued development of new classes of applications driven by a huge number of emerging networked devices. However, the success of Cloud computing has created a new global energy challenge, as it comes at the cost of vast energy usage. Currently, data centres hosting Cloud services world-wide consume more energy than most countries. Globally, by 2025, they are projected to consume 20% of global electricity and emit up to 5.5% of the world's carbon emissions. In addition, a significant part of the energy consumed is transformed into heat which leads to operational problems, including a reduction in system reliability and the life expectancy of devices, and escalation in cooling requirements. Therefore, for future generations of Cloud computing to address the environmental and operational consequences of such significant energy usage, they must become energy-efficient and environmentally sustainable while continuing to deliver high-quality services.   In this paper, we propose a vision for learning-centric approach for the integrated management of new generation Cloud computing environments to reduce their energy consumption and carbon footprint while delivering service quality guarantees. In this paper, we identify the dimensions and key issues of integrated resource management and our envisioned approaches to address them. We present a conceptual architecture for energy-efficient new generation Clouds and early results on the integrated management of resources and workloads that evidence its potential benefits towards energy efficiency and sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations,"With the ever-growing adoption of artificial intelligence (AI), AI-based software and its negative impact on the environment are no longer negligible, and studying and mitigating this impact has become a critical area of research. However, it is currently unclear which role environmental sustainability plays during AI adoption in industry and how AI regulations influence Green AI practices and decision-making in industry. We therefore aim to investigate the Green AI perception and management of industry practitioners. To this end, we conducted a total of 11 interviews with participants from 10 different organizations that adopted AI-based software. The interviews explored three main themes: AI adoption, current efforts in mitigating the negative environmental impact of AI, and the influence of the EU AI Act and the Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that 9 of 11 participants prioritized business efficiency during AI adoption, with minimal consideration of environmental sustainability. Monitoring and mitigation of AI's environmental impact were very limited. Only one participant monitored negative environmental effects. Regarding applied mitigation practices, six participants reported no actions, with the others sporadically mentioning techniques like prompt engineering, relying on smaller models, or not overusing AI. Awareness and compliance with the EU AI Act are low, with only one participant reporting on its influence, while the CSRD drove sustainability reporting efforts primarily in larger companies. All in all, our findings reflect a lack of urgency and priority for sustainable AI among these companies. We suggest that current regulations are not very effective, which has implications for policymakers. Additionally, there is a need to raise industry awareness, but also to provide user-friendly techniques and tools for Green AI practices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The carbon footprint of proposed $\rm e^+e^-$ Higgs factories,"The energy consumption of any of the $\rm e^+e^-$ Higgs factory projects that can credibly operate immediately after the end of LHC, namely three linear colliders (CLIC, operating at $\sqrt{s}=380$GeV; and ILC and $\rm C^3$, operating at $\sqrt{s}=250$ GeV) and two circular colliders (CEPC and FCC-ee, operating at $\sqrt{s}=240$ GeV), will be everything but negligible. Future Higgs boson studies may therefore have a significant environmental impact. This note proposes to include the carbon footprint for a given physics performance as a top-level gauge for the design optimization and, eventually, the choice of the future facility. The projected footprints per Higgs boson produced, evaluated using the 2021 carbon emission of available electricity, are found to vary by a factor 100 depending on the considered Higgs factory project.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Intensity-Aware Adaptive Inference of DNNs,"DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day. Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods. We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint. The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Recursive Definition of Goodness of Space for Bridging the Concepts of Space and Place for Sustainability,"Conceived and developed by Christopher Alexander through his life's work: The Nature of Order, wholeness is defined as a mathematical structure of physical space in our surroundings. Yet, there was no mathematics, as Alexander admitted then, that was powerful enough to capture his notion of wholeness. Recently, a mathematical model of wholeness, together with its topological representation, has been developed that is capable of addressing not only why a space is good, but also how much goodness the space has. This paper develops a structural perspective on goodness of space - both large- and small-scale - in order to bridge two basic concepts of space and place through the very concept of wholeness. The wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view. A space is good, genuinely and objectively, if its adjacent spaces are good, the larger space to which it belongs is good, and what is contained in the space is also good. Eventually, goodness of space - sustainability of space - is considered a matter of fact rather than of opinion under the new view of space: space is neither lifeless nor neutral, but a living structure capable of being more living or less living, or more sustainable or less sustainable. Under the new view of space, geography or architecture will become part of complexity science, not only for understanding complexity, but also for making and remaking complex or living structures. Keywords: Scaling law, head/tail breaks, living structure, beauty, streets, cities","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Assessing the embodied carbon footprint of IoT edge devices with a bottom-up life-cycle approach,"In upcoming years, the number of Internet-of-Things (IoT) devices is expected to surge up to tens of billions of physical objects. However, while the IoT is often presented as a promising solution to tackle environmental challenges, the direct environmental impacts generated over the life cycle of the physical devices are usually overlooked. It is implicitly assumed that their environmental burden is negligible compared to the positive impacts they can generate. In this paper, we present a parametric framework based on hardware profiles to evaluate the cradle-to-gate carbon footprint of IoT edge devices. We exploit our framework in three ways. First, we apply it on four use cases to evaluate their respective production carbon footprint. Then, we show that the heterogeneity inherent to IoT edge devices must be considered as the production carbon footprint between simple and complex devices can vary by a factor of more than 150x. Finally, we estimate the absolute carbon footprint induced by the worldwide production of IoT edge devices through a macroscopic analysis over a 10-year period. Results range from 22 to 562 MtCO2-eq/year in 2027 depending on the deployment scenarios. However, the truncation error acknowledged for LCA bottom-up approaches usually lead to an undershoot of the environmental impacts. We compared the results of our use cases with the few reports available from Google and Apple, which suggest that our estimates could be revised upwards by a factor around 2x to compensate for the truncation error. Worst-case scenarios in 2027 would therefore reach more than 1000 MtCO2-eq/year. This truly stresses the necessity to consider environmental constraints when designing and deploying IoT edge devices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The HCI GenAI CO2ST Calculator: A Tool for Calculating the Carbon Footprint of Generative AI Use in Human-Computer Interaction Research,"Increased usage of generative AI (GenAI) in Human-Computer Interaction (HCI) research induces a climate impact from carbon emissions due to energy consumption of the hardware used to develop and run GenAI models and systems. The exact energy usage and and subsequent carbon emissions are difficult to estimate in HCI research because HCI researchers most often use cloud-based services where the hardware and its energy consumption are hidden from plain view. The HCI GenAI CO2ST Calculator is a tool designed specifically for the HCI research pipeline, to help researchers estimate the energy consumption and carbon footprint of using generative AI in their research, either a priori (allowing for mitigation strategies or experimental redesign) or post hoc (allowing for transparent documentation of carbon footprint in written reports of the research).","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
AI Governance and Ethics Framework for Sustainable AI and Sustainability,"AI is transforming the existing technology landscape at a rapid phase enabling data-informed decision making and autonomous decision making. Unlike any other technology, because of the decision-making ability of AI, ethics and governance became a key concern. There are many emerging AI risks for humanity, such as autonomous weapons, automation-spurred job loss, socio-economic inequality, bias caused by data and algorithms, privacy violations and deepfakes. Social diversity, equity and inclusion are considered key success factors of AI to mitigate risks, create values and drive social justice. Sustainability became a broad and complex topic entangled with AI. Many organizations (government, corporate, not-for-profits, charities and NGOs) have diversified strategies driving AI for business optimization and social-and-environmental justice. Partnerships and collaborations become important more than ever for equity and inclusion of diversified and distributed people, data and capabilities. Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority. These AI ethics and governance should be underpinned by human ethics.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Exploring the Relation between NPP-VIIRS Nighttime Lights and Carbon Footprint, Population Growth, and Energy Consumption in the UAE","Due to global warming and its detrimental effect, every country is responsible to join the global effort to reduce carbon emissions. In order to improve the mitigation plan of climate change, accurate es-timates of carbon emissions, population, and electricity consumption are critical. Carbon footprint is significantly linked to the socioeconomic development of the country which can be reflected in the city's infrastructure and urbanization. We may be able to estimate the carbon footprint, population growth, and electricity consumption of a city by observing the nighttime light reflecting its urbanization. This is more challenging in oil-producing countries where urbanization can be more complicated. In this study, we are therefore investigating the possibility of correlating the remotely sensed NPP-VIIRS Nighttime light (NTL) estimation with the aforementioned socioeconomic indicators. Daily NPP-VIIRS NTL were obtained for the period between 2012 to 2021 for the United Arab Emirates (UAE) which is one of the top oil producing countries. The socioeconomic indicators of the UAE, including the population, electricity consumption, and carbon dioxide emissions, have been obtained for the same period. The analysis of the correlation between the NTLs and the population indicates that there is a high correlation of more than 0.9. There is also a very good correlation of 0.7 between NTLs and carbon emissions and electricity consumption. However, these correlations differ from one city to another. For example, Dubai has shown the highest correlation between population and NTLs (R2 > 0.8). However, the correlation was the lowest in Al-Ain, a rural city (R2 < 0.4) with maximum electricity consumption of 1.1E04 GWh. These results demonstrate that NTLs can be considered as a promising proxy for carbon footprint and urbanization in oil-producing regions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Confronting the Carbon-footprint Challenge of Blockchain,"The distributed consensus mechanism is the backbone of the rapidly developing blockchain network. Blockchain platforms consume vast amounts of electricity based on the current consensus mechanism of Proof of Work. Here, we point out an advanced consensus mechanism named Proof of Stake that can eliminate the extensive energy consumption of the current PoW-based blockchain. We comprehensively elucidate the current and projected energy consumption and carbon footprint of the PoW and PoS based Bitcoin and Ethereum blockchain platforms.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models,"The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large reduction in modeling time from days to minutes.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Densify & Conquer: Densified, smaller base-stations can conquer the increasing carbon footprint problem in nextG wireless","Connectivity on-the-go has been one of the most impressive technological achievements in the 2010s decade. However, multiple studies show that this has come at an expense of increased carbon footprint, that also rivals the entire aviation sector's carbon footprint. The two major contributors of this increased footprint are (a) smartphone batteries which affect the embodied footprint and (b) base-stations that occupy ever-increasing energy footprint to provide the last mile wireless connectivity to smartphones. The root-cause of both these turn out to be the same, which is communicating over the last-mile lossy wireless medium. We show in this paper, titled DensQuer, how base-station densification, which is to replace a single larger base-station with multiple smaller ones, reduces the effect of the last-mile wireless, and in effect conquers both these adverse sources of increased carbon footprint. Backed by a open-source ray-tracing computation framework (Sionna), we show how a strategic densification strategy can minimize the number of required smaller base-stations to practically achievable numbers, which lead to about 3x power-savings in the base-station network. Also, DensQuer is able to also reduce the required deployment height of base-stations to as low as 15m, that makes the smaller cells easily deployable on trees/street poles instead of requiring a dedicated tower. Further, by utilizing newly introduced hardware power rails in Google Pixel 7a and above phones, we also show that this strategic densified network leads to reduction in mobile transmit power by 10-15 dB, leading to about 3x reduction in total cellular power consumption, and about 50% increase in smartphone battery life when it communicates data via the cellular network.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Public Sector Sustainable Energy Scheduler -- A Blockchain and IoT Integrated System,"In response to the European Commission's aim of cutting carbon emissions by 2050, there is a growing need for cutting-edge solutions to promote low-carbon energy consumption in public infrastructures. This paper introduces a Proof of Concept (PoC) that integrates the transparency and immutability of blockchain and the Internet of Things (IoT) to enhance energy efficiency in tangible government-held public assets, focusing on curbing carbon emissions. Our system design utilizes a forecasting and optimization framework, inscribing the scheduled operations of heat pumps on a public sector blockchain. Registering usage metrics on the blockchain facilitates the verification of energy conservation, allows transparency in public energy consumption, and augments public awareness of energy usage patterns. The system fine-tunes the operations of electric heat pumps, prioritizing their use during low-carbon emission periods in power systems occurring during high renewable energy generations. Adaptive temperature configuration and schedules enable energy management in public venues, but blockchains' processing power and latency may represent bottlenecks setting scalability limits. However, the proof-of-concept weakness and other barriers are surpassed by the public sector blockchain advantages, leading to future research and tech innovations to fully exploit the synergies of blockchain and IoT in harnessing sustainable, low-carbon energy in the public domain.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Development Towards Sustainability: How to judge past and proposed policies?,"The scientific data about the state of our planet, presented at the 2012 (Rio+20) summit, documented that today's human family lives even less sustainably than it did in 1992. The data indicate furthermore that the environmental impacts from our current economic activities are so large, that we are approaching situations where potentially controllable regional problems can easily lead to uncontrollable global disasters.   Assuming that (1) the majority of the human family, once adequately informed, wants to achieve a ""sustainable way of life"" and (2) that the ""development towards sustainability"" roadmap will be based on scientific principles, one must begin with unambiguous and quantifiable definitions of these goals. As will be demonstrated, the well known scientific method to define abstract and complex issues by their negation, satisfies these requirements. Following this new approach, it also becomes possible to decide if proposed and actual policies changes will make our way of life less unsustainable, and thus move us potentially into the direction of sustainability. Furthermore, if potentially dangerous tipping points are to be avoided, the transition roadmap must include some minimal speed requirements. Combining the negation method and the time evolution of that remaining natural capital in different domains, the transition speed for a ""development towards sustainability"" can be quantified at local, regional and global scales.   The presented ideas allow us to measure the rate of natural capital depletion and the rate of restoration that will be required if humanity is to avoid reaching a sustainable future by a collapse transition.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Radio Resource Management and Path Planning in Intelligent Transportation Systems via Reinforcement Learning for Environmental Sustainability,"Efficient and dynamic path planning has become an important topic for urban areas with larger density of connected vehicles (CV) which results in reduction of travel time and directly contributes to environmental sustainability through reducing energy consumption. CVs exploit the cellular wireless vehicle-to-everything (C-V2X) communication technology to disseminate the vehicle-to-infrastructure (V2I) messages to the Base-station (BS) to improve situation awareness on urban roads. In this paper, we investigate radio resource management (RRM) in such a framework to minimize the age of information (AoI) so as to enhance path planning results. We use the fact that V2I messages with lower AoI value result in less error in estimating the road capacity and more accurate path planning. Through simulations, we compare road travel times and volume over capacity (V/C) against different levels of AoI and demonstrate the promising performance of the proposed framework.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Joint User-RB Association and Power Allocation for Uplink Hybrid NOMA-OMA,"In this paper, energy efficient resource allocation is considered for an uplink hybrid system, where non-orthogonal multiple access (NOMA) is integrated into orthogonal multiple access (OMA). To ensure the quality of service for the users, a minimum rate requirement is pre-defined for each user. We formulate an energy efficiency (EE) maximization problem by jointly optimizing the user clustering, channel assignment and power allocation. To address this hard problem, a many-to-one bipartite graph is first constructed considering the users and resource blocks (RBs) as the two sets of nodes. Based on swap matching, a joint user-RB association and power allocation scheme is proposed, which converges within a limited number of iterations. Moreover, for the power allocation under a given user-RB association, we first derive the feasibility condition. If feasible, a low-complexity algorithm is proposed, which obtains optimal EE under any successive interference cancellation (SIC) order and an arbitrary number of users. In addition, for the special case of two users per cluster, analytical solutions are provided for the two SIC orders, respectively. These solutions shed light on how the power is allocated for each user to maximize the EE. Numerical results are presented, which show that the proposed joint user-RB association and power allocation algorithm outperforms other hybrid multiple access based and OMA-based schemes.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
EcoLife: Carbon-Aware Serverless Function Scheduling for Sustainable Computing,"This work introduces ECOLIFE, the first carbon-aware serverless function scheduler to co-optimize carbon footprint and performance. ECOLIFE builds on the key insight of intelligently exploiting multi-generation hardware to achieve high performance and lower carbon footprint. ECOLIFE designs multiple novel extensions to Particle Swarm Optimization (PSO) in the context of serverless execution environment to achieve high performance while effectively reducing the carbon footprint.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"High Resolution Modeling and Analysis of Cryptocurrency Mining's Impact on Power Grids: Carbon Footprint, Reliability, and Electricity Price","Blockchain technologies are considered one of the most disruptive innovations of the last decade, enabling secure decentralized trust-building. However, in recent years, with the rapid increase in the energy consumption of blockchain-based computations for cryptocurrency mining, there have been growing concerns about their sustainable operation in electric grids. This paper investigates the tri-factor impact of such large loads on carbon footprint, grid reliability, and electricity market price in the Texas grid. We release open-source high-resolution data to enable high-resolution modeling of influencing factors such as location and flexibility. We reveal that the per-megawatt-hour carbon footprint of cryptocurrency mining loads across locations can vary by as much as 50% of the crude system average estimate. We show that the flexibility of mining loads can significantly mitigate power shortages and market disruptions that can result from the deployment of mining loads. These findings suggest policymakers to facilitate the participation of large mining facilities in wholesale markets and require them to provide mandatory demand response.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Prospective pathways of green graphene-based lab-on-chip devices: the pursuit toward sustainability,"At present, analytical lab-on-chip devices find their usage in different facets of chemical analysis, biological analysis, point of care analysis, biosensors, etc. In addition, graphene has already established itself as an essential component of advanced lab-on-chip devices. Graphene-based lab-on-chip devices have achieved appreciable admiration because of their peerless performance in comparison to others. However, to accomplish a sustainable future a device must undergo Green-Screening to check its environmental compatibility. Thus, extensive research is carried out globally to make the graphene-based lab-on-chip green, though it is yet to be achieved. Nevertheless, as a ray of hope, there are few existing strategies that can be stitched together for feasible fabrication of environment-friendly green graphene-based analytical lab-on-chip, and those prospective pathways are reviewed in this paper.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Photonics for Sustainable Computing,"Photonic integrated circuits are finding use in a variety of applications including optical transceivers, LIDAR, bio-sensing, photonic quantum computing, and Machine Learning (ML). In particular, with the exponentially increasing sizes of ML models, photonics-based accelerators are getting special attention as a sustainable solution because they can perform ML inferences with multiple orders of magnitude higher energy efficiency than CMOS-based accelerators. However, recent studies have shown that hardware manufacturing and infrastructure contribute significantly to the carbon footprint of computing devices, even surpassing the emissions generated during their use. For example, the manufacturing process accounts for 74% of the total carbon emissions from Apple in 2019. This prompts us to ask -- if we consider both the embodied (manufacturing) and operational carbon cost of photonics, is it indeed a viable avenue for a sustainable future? So, in this paper, we build a carbon footprint model for photonic chips and investigate the sustainability of photonics-based accelerators by conducting a case study on ADEPT, a photonics-based accelerator for deep neural network inference. Our analysis shows that photonics can reduce both operational and embodied carbon footprints with its high energy efficiency and at least 4$\times$ less fabrication carbon cost per unit area than 28 nm CMOS.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large Language Model Services,"Recent advancements in large language models (LLMs) have led to their widespread adoption and large-scale deployment across various domains. However, their environmental impact, particularly during inference, has become a growing concern due to their substantial energy consumption and carbon footprint. Existing research has focused on inference computation alone, overlooking the analysis and optimization of carbon footprint in network-aided LLM service systems. To address this gap, we propose AOLO, a framework for analysis and optimization for low-carbon oriented wireless LLM services. AOLO introduces a comprehensive carbon footprint model that quantifies greenhouse gas emissions across the entire LLM service chain, including computational inference and wireless communication. Furthermore, we formulate an optimization problem aimed at minimizing the overall carbon footprint, which is solved through joint optimization of inference outputs and transmit power under quality-of-experience and system performance constraints. To achieve this joint optimization, we leverage the energy efficiency of spiking neural networks (SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented optimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL). Comprehensive simulations demonstrate that SDRL algorithm significantly reduces overall carbon footprint, achieving an 18.77% reduction compared to the benchmark soft actor-critic, highlighting its potential for enabling more sustainable LLM inference services.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Open Source and Sustainability: the Role of Universities,"One important goal in sustainability is making technologies available to the maximum possible number of individuals, and especially to those living in less developed areas (Goal 9 of SDG). However, the diffusion of technical knowledge is hindered by a number of factors, among which the Intellectual Property Rights (IPR) system plays a primary role. While opinions about the real effect of IPRs in stimulating and disseminating innovation differ, there is a growing number of authors arguing that a different approach may be more effective in promoting global development. The success of the Open Source (OS) model in the field of software has led analysts to speculate whether this paradigm can be extended to other fields. Key to this model are both free access to knowledge and the right to use other people's results.   Abstract After reviewing the main features of the OS model, we explore different areas where it can be profitably applied, such as hardware design and production; we finally discuss how academical institutions can (and should) help diffusing the OS philosophy and practice. Widespread use of OS software, fostering of research projects aimed to use and develop OS software and hardware, the use of open education tools, and a strong commitment to open access publishing are some of the discussed examples.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Green Recommender Systems: Understanding and Minimizing the Carbon Footprint of AI-Powered Personalization,"As global warming soars, the need to assess and reduce the environmental impact of recommender systems is becoming increasingly urgent. Despite this, the recommender systems community hardly understands, addresses, and evaluates the environmental impact of their work. In this study, we examine the environmental impact of recommender systems research by reproducing typical experimental pipelines. Based on our results, we provide guidelines for researchers and practitioners on how to minimize the environmental footprint of their work and implement green recommender systems - recommender systems designed to minimize their energy consumption and carbon footprint. Our analysis covers 79 papers from the 2013 and 2023 ACM RecSys conferences, comparing traditional ""good old-fashioned AI"" models with modern deep learning models. We designed and reproduced representative experimental pipelines for both years, measuring energy consumption using a hardware energy meter and converting it into CO2 equivalents. Our results show that papers utilizing deep learning models emit approximately 42 times more CO2 equivalents than papers using traditional models. On average, a single deep learning-based paper generates 2,909 kilograms of CO2 equivalents - more than the carbon emissions of a person flying from New York City to Melbourne or the amount of CO2 sequestered by one tree over 260 years. This work underscores the urgent need for the recommender systems and wider machine learning communities to adopt green AI principles, balancing algorithmic advancements and environmental responsibility to build a sustainable future with AI-powered personalization.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Scenarios of future annual carbon footprints of astronomical research infrastructures,"Research infrastructures have been identified as an important source of greenhouse gas emissions of astronomical research. Based on a comprehensive inventory of 1,211 ground-based observatories and space missions, we assessed the evolution of the number of astronomical facilities and their carbon footprint from 1945 to 2022. We found that space missions dominate greenhouse gas emissions in astronomy, showing an important peak at the end of the 1960ies, followed by a decrease that has turned again into a rise over the last decade. Extrapolating past trends, we predict that greenhouse gas emissions from astronomical facilities will experience no strong decline in the future, and may even rise substantially, unless research practices are changed. We demonstrate that a continuing growth in the number of operating astronomical facilities is not environmentally sustainable. These findings should motivate the astronomical community to reflect about the necessary evolutions that would put astronomical research on a sustainable path.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Latent Pollution Model: The Hidden Carbon Footprint in 3D Image Synthesis,"Contemporary developments in generative AI are rapidly transforming the field of medical AI. These developments have been predominantly driven by the availability of large datasets and high computing power, which have facilitated a significant increase in model capacity. Despite their considerable potential, these models demand substantially high power, leading to high carbon dioxide (CO2) emissions. Given the harm such models are causing to the environment, there has been little focus on the carbon footprints of such models. This study analyzes carbon emissions from 2D and 3D latent diffusion models (LDMs) during training and data generation phases, revealing a surprising finding: the synthesis of large images contributes most significantly to these emissions. We assess different scenarios including model sizes, image dimensions, distributed training, and data generation steps. Our findings reveal substantial carbon emissions from these models, with training 2D and 3D models comparable to driving a car for 10 km and 90 km, respectively. The process of data generation is even more significant, with CO2 emissions equivalent to driving 160 km for 2D models and driving for up to 3345 km for 3D synthesis. Additionally, we found that the location of the experiment can increase carbon emissions by up to 94 times, and even the time of year can influence emissions by up to 50%. These figures are alarming, considering they represent only a single training and data generation phase for each model. Our results emphasize the urgent need for developing environmentally sustainable strategies in generative AI.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Estimating the carbon footprint of the GRAND Project, a multi-decade astrophysics experiment","We present a pioneering estimate of the global yearly greenhouse gas emissions of a large-scale Astrophysics experiment over several decades: the Giant Array for Neutrino Detection (GRAND). The project aims at detecting ultra-high energy neutrinos with a 200,000 radio antenna array over 200,000\,km$^2$ as of the 2030s. With a fully transparent methodology based on open source data, we calculate the emissions related to three unavoidable sources: travel, digital technologies and hardware equipment. We find that these emission sources have a different impact depending on the stages of the experiment. Digital technologies and travel prevail for the small-scale prototyping phase (GRANDProto300), whereas hardware equipment (material production and transportation) and data transfer/storage largely outweigh the other emission sources in the large-scale phase (GRAND200k). In the mid-scale phase (GRAND10k), the three sources contribute equally. This study highlights the considerable carbon footprint of a large-scale astrophysics experiment, but also shows that there is room for improvement. We discuss various lines of actions that could be implemented. The GRAND project being still in its prototyping stage, our results provide guidance to the future collaborative practices and instrumental design in order to reduce its carbon footprint.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting,"Development of modern deep learning methods has been driven primarily by the push for improving model efficacy (accuracy metrics). This sole focus on efficacy has steered development of large-scale models that require massive resources, and results in considerable carbon footprint across the model life-cycle. In this work, we explore how physics inductive biases can offer useful trade-offs between model efficacy and model efficiency (compute, energy, and carbon). We study a variety of models for spatio-temporal forecasting, a task governed by physical laws and well-suited for exploring different levels of physics inductive bias. We show that embedding physics inductive biases into the model design can yield substantial efficiency gains while retaining or even improving efficacy for the tasks under consideration. In addition to using standard physics-informed spatio-temporal models, we demonstrate the usefulness of more recent models like flow matching as a general purpose method for spatio-temporal forecasting. Our experiments show that incorporating physics inductive biases offer a principled way to improve the efficiency and reduce the carbon footprint of machine learning models. We argue that model efficiency, along with model efficacy, should become a core consideration driving machine learning model development and deployment.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy and Carbon Considerations of Fine-Tuning BERT,"Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Building Energy Efficiency through Advanced Regression Models and Metaheuristic Techniques for Sustainable Management,"In the context of global sustainability, buildings are significant consumers of energy, emphasizing the necessity for innovative strategies to enhance efficiency and reduce environmental impact. This research leverages extensive raw data from building infrastructures to uncover energy consumption patterns and devise strategies for optimizing resource use. We investigate the factors influencing energy efficiency and cost reduction in buildings, utilizing Lasso Regression, Decision Tree, and Random Forest models for accurate energy use forecasting. Our study delves into the factors affecting energy utilization, focusing on primary fuel and electrical energy, and discusses the potential for substantial cost savings and environmental benefits. Significantly, we apply metaheuristic techniques to enhance the Decision Tree algorithm, resulting in improved predictive precision. This enables a more nuanced understanding of the characteristics of buildings with high and low energy efficiency potential. Our findings offer practical insights for reducing energy consumption and operational costs, contributing to the broader goals of sustainable development and cleaner production. By identifying key drivers of energy use in buildings, this study provides a valuable framework for policymakers and industry stakeholders to implement cleaner and more sustainable energy practices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink","Machine Learning (ML) workloads have rapidly grown in importance, but raised concerns about their carbon footprint. Four best practices can reduce ML training energy by up to 100x and CO2 emissions up to 1000x. By following best practices, overall ML energy use (across research, development, and production) held steady at <15% of Google's total energy use for the past three years. If the whole ML field were to adopt best practices, total carbon emissions from training would reduce. Hence, we recommend that ML papers include emissions explicitly to foster competition on more than just model quality. Estimates of emissions in papers that omitted them have been off 100x-100,000x, so publishing emissions has the added benefit of ensuring accurate accounting. Given the importance of climate change, we must get the numbers right to make certain that we work on its biggest challenges.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Zeoco: An insight into daily carbon footprint consumption,"Climate change, which is now considered one of the biggest threats to humanity, is also the reason behind various other environmental concerns. Continued negligence might lead us to an irreparably damaged environment. After the partial failure of the Paris Agreement, it is quite evident that we as individuals need to come together to bring about a change on a large scale to have a significant impact. This paper discusses our approach towards obtaining a realistic measure of the carbon footprint index being consumed by a user through day-to-day activities performed via a smart phone app and offering incentives in weekly and monthly leader board rankings along with a reward system. The app helps ease out decision makings on tasks like travel, shopping, electricity consumption, and gain a different and rather numerical perspective over the daily choices.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
An Analysis of Energy Consumption and Carbon Footprints of Cryptocurrencies and Possible Solutions,"There is an urgent need to control global warming caused by humans to achieve a sustainable future. $CO_2$ levels are rising steadily and while countries worldwide are actively moving toward the sustainability goals proposed during the Paris Agreement in 2015, we are still a long way to go from achieving a sustainable mode of global operation. The increased popularity of cryptocurrencies since the introduction of Bitcoin in 2009 has been accompanied by an increasing trend in greenhouse gas emissions and high electrical energy consumption. Popular energy tracking studies (e.g., Digiconomist and the Cambridge Bitcoin Energy Consumption Index (CBECI)) have estimated energy consumption ranges of 29.96 TWh to 135.12 TWh and 26.41 TWh to 176.98 TWh respectively for Bitcoin as of July 2021, which are equivalent to the energy consumption of countries such as Sweden and Thailand. The latest estimate by Digiconomist on carbon footprints shows a 64.18 Mt$CO_2$ emission by Bitcoin as of July 2021, close to the emissions by Greece and Oman. This review compiles estimates made by various studies from 2018 to 2021. We compare with the energy consumption and carbon footprints of these cryptocurrencies with countries around the world, and centralized transaction methods such as Visa. We identify the problems associated with cryptocurrencies, and propose solutions that can help reduce their energy usage and carbon footprints. Finally, we present case studies on cryptocurrency networks namely, Ethereum 2.0 and Pi Network, with a discussion on how they solve some of the challenges we have identified.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Power Allocation in Uplink mmWave Massive MIMO with NOMA,"In this paper, we study the energy efficiency (EE) maximization problem for an uplink millimeter wave massive multiple-input multiple-output system with non-orthogonal multiple access (NOMA). Multiple two-user clusters are formed according to their channel correlation and gain difference, and NOMA is applied within each cluster. Then, a hybrid analog-digital beamforming scheme is designed to lower the number of radio frequency chains at the base station (BS). On this basis, we formulate a power allocation (PA) problem to maximize the EE under users' quality of service requirements. An iterative algorithm is proposed to obtain the PA. Moreover, an enhanced NOMA scheme is also proposed, by exploiting the global information at the BS. Numerical results show that the proposed NOMA schemes achieve superior EE when compared with the conventional orthogonal multiple access scheme.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CarbonScaler: Leveraging Cloud Workload Elasticity for Optimizing Carbon-Efficiency,"Cloud platforms are increasing their emphasis on sustainability and reducing their operational carbon footprint. A common approach for reducing carbon emissions is to exploit the temporal flexibility inherent to many cloud workloads by executing them in periods with the greenest energy and suspending them at other times. Since such suspend-resume approaches can incur long delays in job completion times, we present a new approach that exploits the elasticity of batch workloads in the cloud to optimize their carbon emissions. Our approach is based on the notion of ""carbon scaling,"" similar to cloud autoscaling, where a job dynamically varies its server allocation based on fluctuations in the carbon cost of the grid's energy. We develop a greedy algorithm for minimizing a job's carbon emissions via carbon scaling that is based on the well-known problem of marginal resource allocation. We implement a CarbonScaler prototype in Kubernetes using its autoscaling capabilities and an analytic tool to guide the carbon-efficient deployment of batch applications in the cloud. We then evaluate CarbonScaler using real-world machine learning training and MPI jobs on a commercial cloud platform and show that it can yield i) 51% carbon savings over carbon-agnostic execution; ii) 37% over a state-of-the-art suspend-resume policy; and iii) 8% over the best static scaling policy.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness","The relentless growth of large-scale artificial intelligence (AI) has created unprecedented demand for computational power, straining the energy, bandwidth, and scaling limits of conventional electronic platforms. Electronic-photonic integrated circuits (EPICs) have emerged as a compelling platform for next-generation AI systems, offering inherent advantages in ultra-high bandwidth, low latency, and energy efficiency for computing and interconnection. Beyond performance, EPICs also hold unique promises for sustainability. Fabricated in relaxed process nodes with fewer metal layers and lower defect densities, photonic devices naturally reduce embodied carbon footprint (CFP) compared to advanced digital electronic integrated circuits, while delivering orders-of-magnitude higher computing performance and interconnect bandwidth. To further advance the sustainability of photonic AI systems, we explore how electronic-photonic design automation (EPDA) and cross-layer co-design methodologies can amplify these inherent benefits. We present how advanced EPDA tools enable more compact layout generation, reducing both chip area and metal layer usage. We will also demonstrate how cross-layer device-circuit-architecture co-design unlocks new sustainability gains for photonic hardware: ultra-compact photonic circuit designs that minimize chip area cost, reconfigurable hardware topology that adapts to evolving AI workloads, and intelligent resilience mechanisms that prolong lifetime by tolerating variations and faults. By uniting intrinsic photonic efficiency with EPDA- and co-design-driven gains in area efficiency, reconfigurability, and robustness, we outline a vision for lifelong-sustainable electronic-photonic AI systems. This perspective highlights how EPIC AI systems can simultaneously meet the performance demands of modern AI and the urgent imperative for sustainable computing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Superdirective Antenna Pairs for Energy-Efficient Terahertz Massive MIMO,"Terahertz (THz) communication is widely deemed the next frontier of wireless networks owing to the abundant spectrum resources in the THz band. Whilst THz signals suffer from severe propagation losses, a massive antenna array can be deployed at the base station (BS) to mitigate those losses through beamforming. Nevertheless, a very large number of antennas increases the BS's hardware complexity and power consumption, and hence it can lead to poor energy efficiency (EE). To surmount this fundamental problem, we propose a novel array design based on superdirectivity and nonuniform inter-element spacing. Specifically, we exploit the mutual coupling between closely spaced elements to form superdirective pairs. A unique property of them is that all require the same excitation amplitude, and thus can be driven by a single radio frequency chain akin to conventional phased arrays. Moreover, they facilitate multi-port impedance matching, which ensures maximum power transfer for any beamforming angle. After addressing the implementation issues of superdirectivity, we show that the number of BS antennas can be effectively reduced without sacrificing the achievable rate. Simulation results demonstrate that our design offers huge EE gains compared to uncoupled arrays with uniform spacing, and hence could be a radical solution for future THz systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Data Analytics for Improving Energy Efficiency in Short Sea Shipping,"To meet the urgent requirements for the climate change mitigation, several proactive measures of energy efficiency have been implemented in maritime industry. Many of these practices depend highly on the onboard data of vessel's operation and environmental conditions. In this paper, a high resolution onboard data from passenger vessels in short-sea shipping (SSS) have been collected and preprocessed. We first investigated the available data to deploy it effectively to model the physics of the vessel, and hence the vessel performance. Since in SSS, the weather measurements and forecasts might have not been in temporal and spatial resolutions that accurately representing the actual environmental conditions. Then, We proposed a data-driven modeling approach for vessel energy efficiency. This approach addresses the challenges of data representation and energy modeling by combining and aggregating data from multiple sources and seamlessly integrates explainable artificial intelligence (XAI) to attain clear insights about the energy efficiency for a vessel in SSS. After that, the developed model of energy efficiency has been utilized in developing a framework for optimizing the vessel voyage to minimize the fuel consumption and meeting the constraint of arrival time. Moreover, we developed a spatial clustering approach for labeling the vessel paths to detect the paths for vessels with operating routes of repeatable and semi-repeatable paths.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Empowering the Earth system by technology: Using thermodynamics of the Earth system to illustrate a possible sustainable future of the planet,"With the use of the appropriate technology, such as photovoltaics and seawater desalination, humans have the ability to sustainably increase their production of food and energy while minimising detrimental impacts on the Earth system.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Resource Allocation for IRS-Assisted Multi-Antenna Uplink Systems,"In this paper, we study the resource allocation for an intelligent reflecting surface (IRS)-assisted uplink system, where the base station is equipped with multiple antennas. We propose to jointly optimize the transmit power of the users, active beamforming at the base station, and passive beamforming at the IRS to maximize the overall system energy efficiency while maintaining users' minimum rate constraints. This problem belongs to a highly intractable non-convex bi-quadratic programming problem, for which an iterative solution based on block coordinate descent is proposed. Extensive simulations are conducted to demonstrate the effectiveness of the proposed scheme and the benefits of having more elements at the IRS.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Less Carbon Footprint in Edge Computing by Joint Task Offloading and Energy Sharing,"In sprite the state-of-the-art, significantly reducing carbon footprint (CF) in communications systems remains urgent. We address this challenge in the context of edge computing. The carbon intensity of electricity supply largely varies spatially as well as temporally. This, together with energy sharing via a battery management system (BMS), justifies the potential of CF-oriented task offloading, by redistributing the computational tasks in time and space. In this paper, we consider optimal task scheduling and offloading, as well as battery charging to minimize the total CF. We formulate this CF minimization problem as an integer linear programming model. However, we demonstrate that, via a graph-based reformulation, the problem can be cast as a minimum-cost flow problem. This finding reveals that global optimum can be admitted in polynomial time. Numerical results using real-world data show that optimization can reduce up to 83.3% of the total CF.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI,"Artificial intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning which have accelerated progress on many tasks thought to be out of reach of AI. These recent ML methods are often compute hungry, energy intensive, and result in significant green house gas emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts that go beyond the energy consumption driven carbon emissions. The primary solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the compute and energy efficiency with which ML systems operate. In this perspective, we argue that it is time to look beyond efficiency in order to make ML more environmentally sustainable. We present three high-level discrepancies between the many variables that influence the efficiency of ML and the environmental sustainability of ML. Firstly, we discuss how compute efficiency does not imply energy efficiency or carbon efficiency. Second, we present the unexpected effects of efficiency on operational emissions throughout the ML model life cycle. And, finally, we explore the broader environmental impacts that are not accounted by efficiency. These discrepancies show as to why efficiency alone is not enough to remedy the adverse environmental impacts of ML. Instead, we argue for systems thinking as the next step towards holistically improving the environmental sustainability of ML.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Green Mirage: Impact of Location- and Market-based Carbon Intensity Estimation on Carbon Optimization Efficacy,"In recent years, there has been an increased emphasis on reducing the carbon emissions from electricity consumption. Many organizations have set ambitious targets to reduce the carbon footprint of their operations as a part of their sustainability goals. The carbon footprint of any consumer of electricity is computed as the product of the total energy consumption and the carbon intensity of electricity. Third-party carbon information services provide information on carbon intensity across regions that consumers can leverage to modulate their energy consumption patterns to reduce their overall carbon footprint. In addition, to accelerate their decarbonization process, large electricity consumers increasingly acquire power purchase agreements (PPAs) from renewable power plants to obtain renewable energy credits that offset their ""brown"" energy consumption. There are primarily two methods for attributing carbon-free energy, or renewable energy credits, to electricity consumers: location-based and market-based. These two methods yield significantly different carbon intensity values for various consumers. As there is a lack of consensus which method to use for carbon-free attribution, a concurrent application of both approaches is observed in practice. In this paper, we show that such concurrent applications can cause discrepancies in the carbon savings reported by carbon optimization techniques. Our analysis across three state-of-the-art carbon optimization techniques shows possible overestimation of up to 55.1% in the carbon reductions reported by the consumers and even increased emissions for consumers in some cases. We also find that carbon optimization techniques make different decisions under the market-based method and location-based method, and the market-based method can yield up to 28.2% less carbon savings than those claimed by the location-based method for consumers without PPAs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Sustainable learning, cognitive gains, and improved attitudes in College Algebra flipped classrooms","The objective of this article is to investigate the effect of active-learning pedagogy on learners' academic achievement and their attitude toward mathematics using both quantitative and qualitative methods. We cultivated sustainable learning in mathematics education for college freshmen ($n = 55$) by exposing them to both the conventional teaching method (CTM) and flipped classroom pedagogy (FCP). By splitting them into control and experimental groups alternately ($n_1 = 24$, $n_2 = 31$) and by selecting the four most challenging topics in college algebra, we measured their cognitive gains quantitatively via a sequence of pre- and post-tests. Both groups improved academically over time across all these four topics with statistically very significant outcomes $(p < 0.001)$. Although they were not always statistically significant ($p > 0.05$) in some topics, the post-test results suggest that generally, the FCP trumps the CTM in cognitive gains, except for the first topic on factorization, where the opposite is true with a very statistically significant mean difference $(p < 0.001)$. By examining non-cognitive gains qualitatively, we analyzed the students' feedback on the FCP and their responses to a perception inventory. The finding suggests a favorable response toward the FCP with primary improvements in the attitudes toward mathematics and increased levels of cooperation among students. Since these students are so happy to have control of their own learning, they were more relaxed, motivated, confident, active, and responsible in learning under the FCP. We are confident that although this study is relatively small in scale, it will yield incremental and long-lasting effects not only for the learners themselves but also for other role-takers in education sectors who aspire in nurturing sustainable long-life learning and achieving sustainable development goals successfully.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Per Transistor (CPT): The Golden Formula for Green Computing Metrics,"As computing power advances, the environmental cost of semiconductor manufacturing and operation has become a critical concern. However, current sustainability metrics fail to quantify carbon emissions at the transistor level, the fundamental building block of modern processors. This paper introduces a Carbon Per Transistor (CPT) formula -- a novel approach and green implementation metric to measuring the CO$_2$ footprint of semiconductor chips from fabrication to end-of-life. By integrating emissions from silicon crystal growth, wafer production, chip manufacturing, and operational power dissipation, the CPT formula provides a scientifically rigorous benchmark for evaluating the sustainability of computing hardware. Using real-world data from Intel Core i9-13900K, AMD Ryzen 9 7950X, and Apple M1/M2/M3 processors, we reveal a startling insight-manufacturing emissions dominate, contributing 60-125 kg CO$_2$ per CPU, far exceeding operational emissions over a typical device lifespan. Notably, Apple's high-transistor-count M-series chips, despite their energy efficiency, exhibit a significantly larger carbon footprint than traditional processors due to extensive fabrication impact. This research establishes a critical reference point for green computing initiatives, enabling industry leaders and researchers to make data-driven decisions in reducing semiconductor-related emissions and get correct estimates for the green factor of the information technology process. The proposed formula paves the way for carbon-aware chip design, regulatory standards, and future innovations in sustainable computing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Green Networking: Exploiting Degrees of Freedom towards Energy-Efficient 5G Systems,"The carbon footprint concern in the development and deployment of 5G new radio systems has drawn the attention to several stakeholders. In this article, we analyze the critical power consuming component of all candidate 5G system architectures-the power amplifier (PA)-and propose PA-centric resource management solutions for green 5G communications. We discuss the impact of ongoing trends in cellular communications on sustainable green networking and analyze two communications architectures that allow exploiting the extra degrees-of-freedom (DoF) from multi-antenna and massive antenna deployments: small cells/distributed antenna network and massive MIMO. For small cell systems with a moderate number of antennas, we propose a peak to average power ratio-aware resource allocation scheme for joint orthogonal frequency and space division multiple access. For massive MIMO systems, we develop a highly parallel recurrent neural network for energy-efficient precoding. Simulation results for representative 5G deployment scenarios demonstrate an energy efficiency improvement of one order of magnitude or higher with respect to current state-of-the-art solutions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Efficiency Considerations for Popular AI Benchmarks,"Advances in artificial intelligence need to become more resource-aware and sustainable. This requires clear assessment and reporting of energy efficiency trade-offs, like sacrificing fast running time for higher predictive performance. While first methods for investigating efficiency have been proposed, we still lack comprehensive results for popular methods and data sets. In this work, we attempt to fill this information gap by providing empiric insights for popular AI benchmarks, with a total of 100 experiments. Our findings are evidence of how different data sets all have their own efficiency landscape, and show that methods can be more or less likely to act efficiently.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Balancing Innovation and Sustainability: Addressing the Environmental Impact of Bitcoin Mining,"This study explores the intersection of technological innovation and environmental sustainability in the context of Bitcoin mining. With Bitcoin's growing adoption, concerns surrounding the energy consumption and environmental impact of mining activities have intensified. The study examines the core process of Bitcoin mining, focusing on its energy-intensive proof-of-work mechanism, and provides a detailed analysis of its ecological footprint, especially in terms of carbon emissions and electronic waste. Various models estimate that Bitcoin's energy consumption rivals that of entire nations, highlighting serious sustainability concerns. To address these issues, the paper unearths potential technological innovations, such as energy-efficient mining hardware and the integration of renewable energy sources, as viable strategies to reduce environmental impact. Additionally, the study reviews current sustainability initiatives, including efforts to lower carbon footprints and manage electronic waste effectively. Regulatory developments and market-based approaches are also discussed as possible pathways to mitigate the environmental harm associated with Bitcoin mining. Ultimately, the paper advocates for a balanced approach that fosters technological innovation while promoting environmental responsibility, suggesting that, with appropriate policy and technological interventions, Bitcoin mining can evolve to be both innovative and sustainable.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Case for Sustainability and Environment Friendliness in Software Development and Architecture Decisions by Taking Energy-Efficient Design Decisions,"IT power usage is a significant concern. Data center energy consumption is estimated to account for 1% to 1.5% of all energy consumption worldwide. Hardware designers, data center designers, and other members of the IT community have been working to improve energy efficiency across many parts of the IT infrastructure; however, little attention has been paid to the energy efficiency of software components. Indeed, energy efficiency is currently not a common performance criteria for software. In this work, we attempt to quantify the potential for gains in energy efficiency in software, based on a set of examples drawn from common, everyday decisions made by software developers and enterprise architects. Our results show that there is potential for significant energy savings through energy-conscious choices at software development and selection time, making the software and IT artifact sustainable and environment friendly.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
COVID-19 and Digital Transformation -- Developing an Open Experimental Testbed for Sustainable and Innovative Environments (ETSIE) using Fuzzy Cognitive Maps,"This paper sketches a new approach using Fuzzy Cognitive Maps (FCMs) to operably map and simulate digital transformation in architecture and urban planning. Today these processes are poorly understood. Many current studies on digital transformation are only treating questions of economic efficiency. Sustainability and social impact only play a minor role. Decisive definitions, concepts and terms stay unclear. Therefore this paper develops an open experimental testbed for sustainable and innovative environments (ETSIE) for three different digital transformation scenarios using FCMs. A traditional growth-oriented scenario, a COVID-19 scenario and an innovative and sustainable COVID-19 scenario are modeled and tested. All three scenarios have the same number of components, connections and the same driver components. Only the initial state vectors are different and the internal correlations are weighted differently. This allows for comparing all three scenarios on an equal basis. The mental modeler software is used (Gray et al. 2013). This paper presents one of the first applications of FCMs in the context of digital transformation. It is shown, that the traditional growth-oriented scenario is structurally very similar to the current COVID-19 scenario. The current pandemic is able to accelerate digital transformation to a certain extent. But the pandemic does not guarantee for a distinct sustainable and innovative future development. Only by changing the initial state vectors and the weights of the connections an innovative and sustainable turnaround in a third scenario becomes possible.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Responder: Coordinating Demand Response for the Datacenter Fleet,"The increasing integration of renewable energy sources results in fluctuations in carbon intensity throughout the day. To mitigate their carbon footprint, datacenters can implement demand response (DR) by adjusting their load based on grid signals. However, this presents challenges for private datacenters with diverse workloads and services. One of the key challenges is efficiently and fairly allocating power curtailment across different workloads. In response to these challenges, we propose the Carbon Responder framework.   The Carbon Responder framework aims to reduce the carbon footprint of heterogeneous workloads in datacenters by modulating their power usage. Unlike previous studies, Carbon Responder considers both online and batch workloads with different service level objectives and develops accurate performance models to achieve performance-aware power allocation. The framework supports three alternative policies: Efficient DR, Fair and Centralized DR, and Fair and Decentralized DR. We evaluate Carbon Responder polices using production workload traces from a private hyperscale datacenter. Our experimental results demonstrate that the efficient Carbon Responder policy reduces the carbon footprint by around 2x as much compared to baseline approaches adapted from existing methods. The fair Carbon Responder policies distribute the performance penalties and carbon reduction responsibility fairly among workloads.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Software Sustainability & High Energy Physics,"New facilities of the 2020s, such as the High Luminosity Large Hadron Collider (HL-LHC), will be relevant through at least the 2030s. This means that their software efforts and those that are used to analyze their data need to consider sustainability to enable their adaptability to new challenges, longevity, and efficiency, over at least this period. This will help ensure that this software will be easier to develop and maintain, that it remains available in the future on new platforms, that it meets new needs, and that it is as reusable as possible. This report discusses a virtual half-day workshop on ""Software Sustainability and High Energy Physics"" that aimed 1) to bring together experts from HEP as well as those from outside to share their experiences and practices, and 2) to articulate a vision that helps the Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) to create a work plan to implement elements of software sustainability. Software sustainability practices could lead to new collaborations, including elements of HEP software being directly used outside the field, and, as has happened more frequently in recent years, to HEP developers contributing to software developed outside the field rather than reinventing it. A focus on and skills related to sustainable software will give HEP software developers an important skill that is essential to careers in the realm of software, inside or outside HEP. The report closes with recommendations to improve software sustainability in HEP, aimed at the HEP community via IRIS-HEP and the HEP Software Foundation (HSF).","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
How to Sustainably Monitor ML-Enabled Systems? Accuracy and Energy Efficiency Tradeoffs in Concept Drift Detection,"ML-enabled systems that are deployed in a production environment typically suffer from decaying model prediction quality through concept drift, i.e., a gradual change in the statistical characteristics of a certain real-world domain. To combat this, a simple solution is to periodically retrain ML models, which unfortunately can consume a lot of energy. One recommended tactic to improve energy efficiency is therefore to systematically monitor the level of concept drift and only retrain when it becomes unavoidable. Different methods are available to do this, but we know very little about their concrete impact on the tradeoff between accuracy and energy efficiency, as these methods also consume energy themselves.   To address this, we therefore conducted a controlled experiment to study the accuracy vs. energy efficiency tradeoff of seven common methods for concept drift detection. We used five synthetic datasets, each in a version with abrupt and one with gradual drift, and trained six different ML models as base classifiers. Based on a full factorial design, we tested 420 combinations (7 drift detectors * 5 datasets * 2 types of drift * 6 base classifiers) and compared energy consumption and drift detection accuracy.   Our results indicate that there are three types of detectors: a) detectors that sacrifice energy efficiency for detection accuracy (KSWIN), b) balanced detectors that consume low to medium energy with good accuracy (HDDM_W, ADWIN), and c) detectors that consume very little energy but are unusable in practice due to very poor accuracy (HDDM_A, PageHinkley, DDM, EDDM). By providing rich evidence for this energy efficiency tactic, our findings support ML practitioners in choosing the best suited method of concept drift detection for their ML-enabled systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework,"In this research paper, we propose a new type of energy-efficient Green AI architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems. We introduce a multi-layered framework and meta-architecture that integrates state-of-the-art machine learning algorithms, energy-conscious computational models, and optimization techniques to facilitate decision-making for resource reuse, waste reduction, and sustainable production.We tested the framework on real-world datasets from lithium-ion battery recycling and urban waste management systems, demonstrating its practical applicability. Notably, the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency. Quantitative optimization was based on mathematical models such as mixed-integer linear programming and lifecycle assessments. Moreover, AI algorithms improved classification accuracy on urban waste by 20 percent, while optimized logistics reduced transportation emissions by 30 percent. We present graphical analyses and visualizations of the developed framework, illustrating its impact on energy efficiency and sustainability as reflected in the simulation results. This paper combines the principles of Green AI with practical insights into how such architectural models contribute to circular economies, presenting a fully scalable and scientifically rooted solution aligned with applicable UN Sustainability Goals worldwide. These results open avenues for incorporating newly developed AI technologies into sustainable management strategies, potentially safeguarding local natural capital while advancing technological progress.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Bitcoin's Carbon Footprint Revisited: Proof of Work Mining for Renewable Energy Expansion,"Despite their potential in many respects, blockchain and distributed ledger technology (DLT) technology have been the target of criticism for the energy intensity of the proof-of-work (PoW) consensus algorithm in general and of Bitcoin mining in particular. However, mining is also believed to have the potential to drive net decarbonization and renewable penetration in the energy grid by providing ancillary and other services. In this paper, we systematize the state of the art in this regard. Although not completely absent from the literature, the extent to which flexible load response (FLR) through PoW mining may support grid decarbonization remains insufficiently studied and hence contested. We approach this research gap by systematizing both the strengths and the limitations of mining to provide FLR services for energy grids. We find that a net-decarbonizing effect led by renewable-based mining is indeed plausible.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Achieving Carbon Neutrality for I/O Devices,"Achieving carbon neutrality has become a critical goal in mitigating the environmental impacts of human activities, particularly in the face of global climate challenges. Input/Output (I/O) devices, such as keyboards, mice, displays, and printers, contribute significantly to greenhouse gas emissions through their manufacturing, operation, and disposal processes. In this paper, we explores sustainable strategies for achieving carbon neutrality in I/O devices, emphasizing the importance of environmentally conscious design and development. Through a comprehensive review of existing literature and best approaches, we introduces a framework to outline approaches for reducing the carbon footprint of I/O devices. The result underscore the necessity of integrating sustainability into the lifecycle of I/O devices to support global carbon neutrality goals and promote long-term environmental sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Intelligent Global Routing in Path-Aware Networks,"The growing energy consumption of Information and Communication Technology (ICT) has raised concerns about its environmental impact. However, the carbon efficiency of data transmission over the Internet has so far received little attention. This carbon efficiency can be enhanced effectively by sending traffic over carbon-efficient inter-domain paths. However, challenges in estimating and disseminating carbon intensity of inter-domain paths have prevented carbon-aware path selection from becoming a reality.   In this paper, we take advantage of path-aware network architectures to overcome these challenges. In particular, we design CIRo, a system for forecasting the carbon intensity of inter-domain paths and disseminating them across the Internet. We implement a proof of concept for CIRo on the codebase of the SCION path-aware Internet architecture and test it on the SCIONLab global research testbed. Further, we demonstrate the potential of CIRo for reducing the carbon footprint of endpoints and end domains through large-scale simulations. We show that CIRo can reduce the carbon intensity of communications by at least 47% for half of the domain pairs and the carbon footprint of Internet usage by at least 50% for 87% of end domains.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CASA: A Framework for SLO and Carbon-Aware Autoscaling and Scheduling in Serverless Cloud Computing,"Serverless computing is an emerging cloud computing paradigm that can reduce costs for cloud providers and their customers. However, serverless cloud platforms have stringent performance requirements (due to the need to execute short duration functions in a timely manner) and a growing carbon footprint. Traditional carbon-reducing techniques such as shutting down idle containers can reduce performance by increasing cold-start latencies of containers required in the future. This can cause higher violation rates of service level objectives (SLOs). Conversely, traditional latency-reduction approaches of prewarming containers or keeping them alive when not in use can improve performance but increase the associated carbon footprint of the serverless cluster platform. To strike a balance between sustainability and performance, in this paper, we propose a novel carbon- and SLO-aware framework called CASA to schedule and autoscale containers in a serverless cloud computing cluster. Experimental results indicate that CASA reduces the operational carbon footprint of a FaaS cluster by up to 2.6x while also reducing the SLO violation rate by up to 1.4x compared to the state-of-the-art.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Anti-patterns and the energy efficiency of Android applications,"The boom in mobile apps has changed the traditional landscape of software development by introducing new challenges due to the limited resources of mobile devices, e.g., memory, CPU, network bandwidth and battery. The energy consumption of mobile apps is nowadays a hot topic and researchers are actively investigating the role of coding practices on energy efficiency. Recent studies suggest that design quality can conflict with energy efficiency. Therefore, it is important to take into account energy efficiency when evolving the design of a mobile app. The research community has proposed approaches to detect and remove anti-patterns (i.e., poor solutions to design and implementation problems) in software systems but, to the best of our knowledge, none of these approaches have included anti-patterns that are specific to mobile apps and--or considered the energy efficiency of apps. In this paper, we fill this gap in the literature by analyzing the impact of eight type of anti-patterns on a testbed of 59 android apps extracted from F-Droid. First, we (1) analyze the impact of anti-patterns in mobile apps with respect to energy efficiency; then (2) we study the impact of different types of anti-patterns on energy efficiency. We found that then energy consumption of apps containing anti-patterns and not (refactored apps) is statistically different. Moreover, we find that the impact of refactoring anti-patterns can be positive (7 type of anti-patterns) or negative (2 type of anti-patterns). Therefore, developers should consider the impact on energy efficiency of refactoring when applying maintenance activities.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Cloud Computing: Foundations and Future Directions,"Major cloud providers such as Microsoft, Google, Facebook and Amazon rely heavily on datacenters to support the ever-increasing demand for their computational and application services. However, the financial and carbon footprint related costs of running such large infrastructure negatively impacts the sustainability of cloud services. Most of existing efforts primarily focus on minimizing the energy consumption of servers. In this paper, we devise a conceptual model and practical design guidelines for holistic management of all resources (including servers, networks, storage, cooling systems) to improve the energy efficiency and reduce carbon footprints in Cloud Data Centers (CDCs). Furthermore, we discuss the intertwined relationship between energy and reliability for sustainable cloud computing, where we highlight the associated research issues. Finally, we propose a set of future research directions in the field and setup grounds for further practical developments.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency,"Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfort, presents a significant challenge for traditional HVAC systems. These systems often fail to dynamically adjust to real-time changes in electricity market rates or individual comfort preferences, leading to increased energy costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL) Artificial Intelligence framework that optimizes HVAC performance by incorporating real-time user feedback and responding to fluctuating electricity prices. Unlike conventional systems that require predefined information about occupancy or comfort levels, our approach learns and adapts based on ongoing user input. By integrating the occupancy prediction model with reinforcement learning, the system improves operational efficiency and reduces energy costs in line with electricity market dynamics, thereby contributing to demand response initiatives. Through simulations, we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort. This feedback-driven approach ensures personalized comfort control without the need for predefined settings, offering a scalable solution that balances individual preferences with economic and environmental goals.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Neutralized Joint User Association and Base Station Switching for Green Cellular Networks,"Mitigating climate change and its impacts is one of the sustainable development goals (SDGs) required by United Nations for an urgent action. Increasing carbon emissions due to human activities is the root cause to climate change. Telecommunication networks that provide service connectivity to mobile users contribute great amount of carbon emissions by consuming lots of non-renewable energy sources. Beyond the improvement on energy efficiency, to reduce the carbon footprint, telecom operators are increasing their adoption of renewable energy (e.g., wind power). The high variability of renewable energy in time and location; however, creates difficulties for operators when utilizing renewables for the reduction of carbon emissions. In this paper, we consider a heterogeneous network consisted of one macro base station (MBS) and multiple small base stations (SBSs) where each base station (BS) is powered by both of renewable and non-renewable energy. Different from the prior works that target on the total power consumption, we propose a novel scheme to minimize the carbon footprint of networks by dynamically switching the ON/OFF modes of SBSs and adjusting the association between users and BSs to access renewables as much as possible. Our numerical analysis shows that the proposed scheme significantly reduces up to 86% of the nonrenewable energy consumption compared to two representative baselines.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
DinAR: Augmenting Reality for Sustainable Dining,"Sustainable food is among the many challenges associated with climate change. The resources required to grow or gather the food and the distance it travels to reach the consumer are two key factors of an ingredient's sustainability. Food that is grown locally and is currently ""in-season"" will have a lower carbon footprint, but when dining out these details unfortunately may not affect one's ordering preferences. We introduce DinAR as an immersive experience to make this information more accessible and to encourage better dining choices through friendly competition with a leaderboard of sustainability scores. Our study measures the effectiveness of immersive AR experiences on impacting consumer preferences towards sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Scaling indicator and planning plane: an indicator and a visual tool for exploring the relationship between urban form, energy efficiency and carbon emissions","Ecosystems and other naturally resilient systems exhibit allometric scaling in the distribution of sizes of their elements. In this paper we define an allometry inspired scaling indicator for cities that is a first step towards quantifying the resilience borne of a complex systems' hierarchical structural composition. The scaling indicator is calculated using large census datasets and is analogous to fractal dimension in spatial analysis. Lack of numerical rigor and the resulting variation in scaling indicators -inherent in the use of box counting mechanism for fractal dimension calculation for cities- has been one of the hindrances in the adoption of fractal dimension as an urban indicator of note. The intra-urban indicator of scaling in population density distribution developed here is calculated for 58 US cities using a methodology that produces replicable results, employing large census-block wise population datasets from the 2010 US Census 2010 and the 2007 US Economic Census. We show that rising disparity -as measured by the proposed indicator of population density distribution in census blocks in metropolitan statistical areas (using US Census 2010 data) adversely affects energy consumption efficiency and carbon emissions in cities and leads to a higher urban carbon footprint. We then define a planning plane as a visual and analytic tool for incorporation of scaling indicator analysis into policy and decision-making.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
System Support for Environmentally Sustainable Computing in Data Centers,"Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Energy-Sustainable IoT Connectivity: Vision, Technological Enablers, Challenges, and Future Directions","Technology solutions must effectively balance economic growth, social equity, and environmental integrity to achieve a sustainable society. Notably, although the Internet of Things (IoT) paradigm constitutes a key sustainability enabler, critical issues such as the increasing maintenance operations, energy consumption, and manufacturing/disposal of IoT devices have long-term negative economic, societal, and environmental impacts and must be efficiently addressed. This calls for self-sustainable IoT ecosystems requiring minimal external resources and intervention, effectively utilizing renewable energy sources, and recycling materials whenever possible, thus encompassing energy sustainability. In this work, we focus on energy-sustainable IoT during the operation phase, although our discussions sometimes extend to other sustainability aspects and IoT lifecycle phases. Specifically, we provide a fresh look at energy-sustainable IoT and identify energy provision, transfer, and energy efficiency as the three main energy-related processes whose harmonious coexistence pushes toward realizing self-sustainable IoT systems. Their main related technologies, recent advances, challenges, and research directions are also discussed. Moreover, we overview relevant performance metrics to assess the energy-sustainability potential of a certain technique, technology, device, or network and list some target values for the next generation of wireless systems. Overall, this paper offers insights that are valuable for advancing sustainability goals for present and future generations.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning,"The growing reliance on large-scale data centers to run resource-intensive workloads has significantly increased the global carbon footprint, underscoring the need for sustainable computing solutions. While container orchestration platforms like Kubernetes help optimize workload scheduling to reduce carbon emissions, existing methods often depend on centralized machine learning models that raise privacy concerns and struggle to generalize across diverse environments. In this paper, we propose a federated learning approach for energy consumption prediction that preserves data privacy by keeping sensitive operational data within individual enterprises. By extending the Kubernetes Efficient Power Level Exporter (Kepler), our framework trains XGBoost models collaboratively across distributed clients using Flower's FedXgbBagging aggregation using a bagging strategy, eliminating the need for centralized data sharing. Experimental results on the SPECPower benchmark dataset show that our FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a centralized baseline. This work addresses the unresolved trade-off between data privacy and energy prediction efficiency in prior systems such as Kepler and CASPER and offers enterprises a viable pathway toward sustainable cloud computing without compromising operational privacy.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Calculate the carbon footprint of your IT assets with EcoDiag, an EcoInfo service","You hear a lot about environmental issues, you may have initiated positive personal attitudes, you have a group will in your unit, and you are wondering about the environmental impact of digital technology in your professional environment...EcoDiag is here for you!The CNRS EcoInfo GDS has found that assessing the environmental impacts of digital technology is complex and can discourage the best will, which is why we offer you a simple and effective method to estimate the carbon footprint of your fleet through this new service based on our experience. We chose an indicator that everyone could understand: CO2e emissions (CO2e equivalent). Based on an inventory of the digital services used and the unit's computer equipment, our methodology and expertise will allow you to establish a global estimate of GHG emissions. This evaluation will be accompanied by recommendations and resources (sheets, guide, material) to integrate this work into a Hc{}res-type report or use them as part of a more global carbon assessment for your structure. The provision of advice will allow you to post a policy to reduce GHG emissions generated by digital technology in the following years!Meet us in front of the poster to discover and leave with our tool for assessing the environmental impacts of your unit's digital activities. You will have the tools and a recommendation guide to control and reduce these impacts in a continuous improvement process. Concrete examples from two units will illustrate our approach and demonstrate the ease of use of the EcoDiag service.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs,"Over the years, the chip industry has consistently developed high-performance processors to address the increasing demands across diverse applications. However, the rapid expansion of chip production has significantly increased carbon emissions, raising critical concerns about environmental sustainability. While researchers have previously modeled the carbon footprint (CFP) at both system and processor levels, a holistic analysis of sustainability trends encompassing the entire chip lifecycle remains lacking. This paper presents CarbonSet, a comprehensive dataset integrating sustainability and performance metrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and assess the design of next-generation processors. Leveraging this dataset, we conducted detailed analysis of flagship processors' sustainability trends over the last decade. This paper further highlights that modern processors are not yet sustainably designed, with total carbon emissions increasing more than 50$\times$ in the past three years due to the surging demand driven by the AI boom. Power efficiency remains a significant concern, while advanced process nodes pose new challenges requiring to effectively amortize the dramatically increased manufacturing carbon emissions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
On the Energy Consumption Forecasting of Data Centers Based on Weather Conditions: Remote Sensing and Machine Learning Approach,"The energy consumption of Data Centers (DCs) is a very important figure for the telecommunications operators, not only in terms of cost, but also in terms of operational reliability. A relation between the energy consumption and the weather conditions would indicate that weather forecast models could be used for predicting energy consumption of DCs. A reliable forecast would result in a more efficient management of the available energy and would make it easier to take advantage of the modern types of power-grid based on renewable energy resources. In this ,paper, we exploit the capabilities provided by the FIESTA-IoT platform in order to investigate the correlation between the weather conditions and the energy consumption in DCs. Then, by using multi-variable linear regression process, we model this correlation between the energy consumption and the dominant weather conditions parameters in order to effectively forecast the energy consumption based on the weather forecast. We have validated our results through live measurements from the RealDC testbed. Results from our proposed approach indicate that forecasting of energy consumption based on weather conditions could help not only DC operators in managing their cooling systems and power usage, but also electricity companies in optimizing their power distribution systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions,"In this paper, we explore the intersection of privacy, security, and environmental sustainability in cloud-based office solutions, focusing on quantifying user- and network-side energy use and associated carbon emissions. We hypothesise that privacy-focused services are typically more energy-efficient than those funded through data collection and advertising. To evaluate this, we propose a framework that systematically measures environmental costs based on energy usage and network data traffic during well-defined, automated usage scenarios. To test our hypothesis, we first analyse how underlying architectures and business models, such as monetisation through personalised advertising, contribute to the environmental footprint of these services. We then explore existing methodologies and tools for software environmental impact assessment. We apply our framework to three mainstream email services selected to reflect different privacy policies, from ad-supported tracking-intensive models to privacy-focused designs: Microsoft Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a self-hosted email solution, evaluated with and without end-to-end encryption. We show that the self-hosted solution, even with 14% of device energy and 15% of emissions overheads from PGP encryption, remains the most energy-efficient, saving up to 33% of emissions per session compared to Gmail. Among commercial providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per session compared to Outlook, whose emissions can be further reduced by 2% through ad-blocking.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Co-Production of Cement and Carbon Nanotubes with a Carbon Negative Footprint,"C2CNT (Carbon dioxide to carbon nanotube) cement plants have been introduced and analyzed which provide a significant economic incentive to eliminate the massive CO2 greenhouse gas emissions of current plants and serves as a template for carbon mitigation in other industrial manufacturing processes. Rather than regarding CO2 as a costly pollutant, this is accomplished by treating CO2 as a feedstock resource to generate valuable products (carbon nanotubes). The exhaust from partial and full oxy-fuel cement plant configurations are coupled to the inlet of a C2CNT chamber in which CO2 is transformed by electrolysis in a molten carbonate electrolyte at a steel cathode and a nickel anode. In this high yield 4e- per CO2 process, the CO2 is transformed into carbon nanotubes at the cathode, and pure oxygen at the anode that is looped back in improving the cement line energy efficiency and rate of production. A partial oxy-fuel process looping the pure oxygen back in through the plant calcinator has been compared to a full oxy-fuel process in which it is looped back in through the plant kiln. The second provides the advantage of easier retrofit, while the first maximizes efficiency by minimizing the volume of gas throughput (eliminating N2 from air). An upper limit to the electrical cost to drive C2CNT electrolysis is USD70 based on Texas wind power costs, but will be lower with fuel expenses when oxy-fuel plant energy improvements are taken account. The current value of a ton of carbon nanotubes is substantially in excess of a ton of cement. Hence a C2CNT cement plant consumes USD50 of electricity, emits no CO2, and produces USD100 of cement and USD60,000 of carbon nanotubes per ton of CO2 avoided. As the cement product ages, CO2 is spontaneously absorbed. This is a powerful economic incentive, rather than economic cost, to mitigate climate change through a carbon negative process.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers,"Training large-scale artificial intelligence (AI) models demands significant computational power and energy, leading to increased carbon footprint with potential environmental repercussions. This paper delves into the challenges of training AI models across geographically distributed (geo-distributed) data centers, emphasizing the balance between learning performance and carbon footprint. We consider Federated Learning (FL) as a solution, which prioritizes model parameter exchange over raw data, ensuring data privacy and compliance with local regulations. Given the variability in carbon intensity across regions, we propose a new framework called CAFE (short for Carbon-Aware Federated Learning) to optimize training within a fixed carbon footprint budget. Our approach incorporates coreset selection to assess learning performance, employs the Lyapunov drift-plus-penalty framework to address the unpredictability of future carbon intensity, and devises an efficient algorithm to address the combinatorial complexity of the data center selection. Through extensive simulations using real-world carbon intensity data, we demonstrate the efficacy of our algorithm, highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers,"The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environment's future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, the carbon footprint of TinyML systems is not negligible, necessitating that designers factor in environmental impact when formulating new devices. Finally, we outline research directions to enable further sustainable contributions of TinyML.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training","Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Time Series Vector Autoregression Prediction of the Ecological Footprint based on Energy Parameters,"Sustainability became the most important component of world development, as countries worldwide fight the battle against the climate change. To understand the effects of climate change, the ecological footprint, along with the biocapacity should be observed. The big part of the ecological footprint, the carbon footprint, is most directly associated with the energy, and specifically fuel sources. This paper develops a time series vector autoregression prediction model of the ecological footprint based on energy parameters. The objective of the paper is to forecast the EF based solely on energy parameters and determine the relationship between the energy and the EF. The dataset included global yearly observations of the variables for the period 1971-2014. Predictions were generated for every variable that was used in the model for the period 2015-2024. The results indicate that the ecological footprint of consumption will continue increasing, as well as the primary energy consumption from different sources. However, the energy consumption from coal sources is predicted to have a declining trend.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Survey on Task Scheduling in Carbon-Aware Container Orchestration,"The soaring energy demands of large-scale software ecosystems and cloud data centers, accelerated by the intensive training and deployment of large language models, have driven energy consumption and carbon footprint to unprecedented levels. In response, both industry and academia are increasing efforts to reduce the carbon emissions associated with cloud computing through more efficient task scheduling and infrastructure orchestration. In this work, we present a systematic review of various Kubernetes scheduling strategies, categorizing them into hardware-centric and software-centric, annotating each with its sustainability objectives, and grouping them according to the algorithms they use. We propose a comprehensive taxonomy for cloud task scheduling studies, with a particular focus on the environmental sustainability aspect. We analyze emerging research trends and open challenges, and our findings provide critical insight into the design of sustainable scheduling solutions for next-generation cloud computing systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Two-Stage Online Algorithm for EV Charging Station Energy Management and Carbon Trading,"The increasing electric vehicle (EV) adoption challenges the energy management of charging stations (CSs) due to the large number of EVs and the underlying uncertainties. Moreover, the carbon footprint of CSs is growing significantly due to the rising charging power demand. This makes it important for CSs to properly manage their energy usage and ensure their carbon footprint stay within their carbon emission quotas. This paper proposes a two-stage online algorithm for this purpose, considering the different time scales of energy management and carbon trading. In the first stage, the CS characterizes the real-time aggregate EV power flexibility, in terms of upper and lower bounds on the total charging power, by a Lyapunov optimization-based online algorithm. In the second stage, the CS co-optimizes energy management and carbon trading, with EV charging power chosen within the aggregate flexibility region provided by the first stage. A generalized battery model is proposed to capture the dynamic carbon footprint changes and carbon trading. A virtual carbon queue is designed to develop an online algorithm for the second stage, which can ensure the carbon footprint of CS be within its carbon emission quota and its total operation cost is nearly offline optimal. Case studies validate the effectiveness and advantages of the proposed algorithm.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Making AI Less ""Thirsty"": Uncovering and Addressing the Secret Water Footprint of AI Models","The growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny. Nonetheless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under the radar. For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More critically, the global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4-6 Denmark or half of the United Kingdom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond to the global water challenges, AI can, and also must, take social responsibility and lead by example by addressing its own water footprint. In this paper, we provide a principled methodology to estimate the water footprint of AI, and also discuss the unique spatial-temporal diversities of AI's runtime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to enable truly sustainable AI.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"SHIELD: Sustainable Hybrid Evolutionary Learning Framework for Carbon, Wastewater, and Energy-Aware Data Center Management","Today's cloud data centers are often distributed geographically to provide robust data services. But these geo-distributed data centers (GDDCs) have a significant associated environmental impact due to their increasing carbon emissions and water usage, which needs to be curtailed. Moreover, the energy costs of operating these data centers continue to rise. This paper proposes a novel framework to co-optimize carbon emissions, water footprint, and energy costs of GDDCs, using a hybrid workload management framework called SHIELD that integrates machine learning guided local search with a decomposition-based evolutionary algorithm. Our framework considers geographical factors and time-based differences in power generation/use, costs, and environmental impacts to intelligently manage workload distribution across GDDCs and data center operation. Experimental results show that SHIELD can realize 34.4x speedup and 2.1x improvement in Pareto Hypervolume while reducing the carbon footprint by up to 3.7x, water footprint by up to 1.8x, energy costs by up to 1.3x, and a cumulative improvement across all objectives (carbon, water, cost) of up to 4.8x compared to the state-of-the-art.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones,"Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable AI Regulation,"Current proposals for AI regulation, in the EU and beyond, aim to spur AI that is trustworthy (e.g., AI Act) and accountable (e.g., AI Liability) What is missing, however, is a robust regulatory discourse and roadmap to make AI, and technology more broadly, environmentally sustainable. This paper aims to take first steps to fill this gap. The ICT sector contributes up to 3.9 percent of global greenhouse gas (GHG) emissions-more than global air travel at 2.5 percent. The carbon footprint and water consumption of AI, especially large-scale generative models like GPT-4, raise significant sustainability concerns. The paper is the first to assess how current and proposed technology regulations, including EU environmental law, the General Data Protection Regulation (GDPR), and the AI Act, could be adjusted to better account for environmental sustainability. The GDPR, for instance, could be interpreted to limit certain individual rights like the right to erasure if these rights significantly conflict with broader sustainability goals. In a second step, the paper suggests a multi-faceted approach to achieve sustainable AI regulation. It advocates for transparency mechanisms, such as disclosing the GHG footprint of AI systems, as laid out in the proposed EU AI Act. However, sustainable AI regulation must go beyond mere transparency. The paper proposes a regulatory toolkit comprising co-regulation, sustainability-by-design principles, restrictions on training data, and consumption caps, including integration into the EU Emissions Trading Scheme. Finally, the paper argues that this regulatory toolkit could serve as a blueprint for regulating other high-emission technologies and infrastructures like blockchain, Metaverse applications, and data centers. The framework aims to cohesively address the crucial dual challenges of our era: digital transformation and climate change mitigation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Retail prices, environmental footprints, and nutritional profiles of commonly sold retail food items in 181 countries","Background: Transitions towards healthier, more environmentally sustainable diets would require large shifts in consumption patterns. Cost and affordability can be barriers to consuming healthy, sustainable diets.   Objective: This study provides the first worldwide test of how retail food prices relate to empirically estimated environmental footprints and nutritional profile scores between and within food groups.   Methods: We use 48,316 prices for 860 retail food items commonly sold in 181 countries during 2011 and 2017, matched to estimated carbon and water footprints and nutritional profiles, to test whether healthier and more sustainable foods are more expensive between and within food groups.   Results: Prices, environmental footprints, and nutritional profiles differ between food groups. Within almost all groups, more expensive items have significantly larger carbon and water footprints. Associations are strongest for animal source foods, where each 10% increment in price is associated with 21 grams higher carbon footprint and 5 liters higher water footprint per 100kcal of food. There is no such gradient for price and nutritional profile, as more expensive items are sometimes healthier and sometimes less healthy depending on the food group, price range, and nutritional attribute of interest.   Conclusions: Our finding that higher-priced items have larger environmental footprints is contrary to expectations that a more sustainable diet would be more expensive. Instead, we find that within each food group, meeting dietary needs with lower environmental footprints is possible by choosing items with a lower unit price. These findings are consistent with prior observations that higher-priced items typically use more resources, including energy and water, but may or may not be healthful as measured by nutrient profile scores.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CarbonClarity: Understanding and Addressing Uncertainty in Embodied Carbon for Sustainable Computing,"Embodied carbon footprint modeling has become an area of growing interest due to its significant contribution to carbon emissions in computing. However, the deterministic nature of the existing models fail to account for the spatial and temporal variability in the semiconductor supply chain. The absence of uncertainty modeling limits system designers' ability to make informed, carbon-aware decisions. We introduce CarbonClarity, a probabilistic framework designed to model embodied carbon footprints through distributions that reflect uncertainties in energy-per-area, gas-per-area, yield, and carbon intensity across different technology nodes. Our framework enables a deeper understanding of how design choices, such as chiplet architectures and new vs. old technology node selection, impact emissions and their associated uncertainties. For example, we show that the gap between the mean and 95th percentile of embodied carbon per cm$^2$ can reach up to 1.6X for the 7nm technology node. Additionally, we demonstrate through case studies that: (i) CarbonClarity is a valuable resource for device provisioning, help maintaining performance under a tight carbon budget; and (ii) chiplet technology and mature nodes not only reduce embodied carbon but also significantly lower its associated uncertainty, achieving an 18% reduction in the 95th percentile compared to monolithic designs for the mobile application.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Efficient Power-Splitting and Resource Allocation for Cellular V2X Communications,"The research efforts on cellular vehicle-to-everything (V2X) communications are gaining momentum with each passing year. It is considered as a paradigm-altering approach to connect a large number of vehicles with minimal cost of deployment and maintenance. This article aims to further push the state-of-the-art of cellular V2X communications by providing an optimization framework for wireless charging, power allocation, and resource block assignment. Specifically, we design a network model where roadside objects use wireless power from RF signals of electric vehicles for charging and information processing. Moreover, due to the resource-constraint nature of cellular V2X, the power allocation and resource block assignment are performed to efficiently use the resources. The proposed optimization framework shows an improvement in terms of the overall energy efficiency of the network when compared with the baseline technique. The performance gains of the proposed solution clearly demonstrate its feasibility and utility for cellular V2X communications.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability in HPC: Vision and Opportunities,"Tackling climate change by reducing and eventually eliminating carbon emissions is a significant milestone on the path toward establishing an environmentally sustainable society. As we transition into the exascale era, marked by an increasing demand and scale of HPC resources, the HPC community must embrace the challenge of reducing carbon emissions from designing and operating modern HPC systems. In this position paper, we describe challenges and highlight different opportunities that can aid HPC sites in reducing the carbon footprint of modern HPC systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Aware End-to-End Data Movement,"The latest trends in the adoption of cloud, edge, and distributed computing, as well as a rise in applying AI/ML workloads, have created a need to measure, monitor, and reduce the carbon emissions of these compute-intensive workloads and the associated communication costs. The data movement over networks has considerable carbon emission that has been neglected due to the difficulty in measuring the carbon footprint of a given end-to-end network path. We present a novel network carbon footprint measuring mechanism and propose three ways in which users can optimize scheduling network-intensive tasks to enable carbon savings through shifting tasks in time, space, and overlay networks based on the geographic carbon intensity.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions,"Cloud computing drives innovation but also poses significant environmental challenges due to its high-energy consumption and carbon emissions. Data centers account for 2-4% of global energy usage, and the ICT sector's share of electricity consumption is projected to reach 40% by 2040. As the goal of achieving net-zero emissions by 2050 becomes increasingly urgent, there is a growing need for more efficient and transparent solutions, particularly for private cloud infrastructures, which are utilized by 87% of organizations, despite the dominance of public-cloud systems.   This study evaluates the MAIZX framework, designed to optimize cloud operations and reduce carbon footprint by dynamically ranking resources, including data centers, edge computing nodes, and multi-cloud environments, based on real-time and forecasted carbon intensity, Power Usage Effectiveness (PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor operations. Tested across geographically distributed data centers, the framework demonstrates scalability and effectiveness, directly interfacing with hypervisors to optimize workloads in private, hybrid, and multi-cloud environments. MAIZX integrates real-time data on carbon intensity, power consumption, and carbon footprint, as well as forecasted values, into cloud management, providing a robust tool for enhancing climate performance potential while maintaining operational efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Untangling Carbon-free Energy Attribution and Carbon Intensity Estimation for Carbon-aware Computing,"Many organizations, including governments, utilities, and businesses, have set ambitious targets to reduce carbon emissions for their Environmental, Social, and Governance (ESG) goals. To achieve these targets, these organizations increasingly use power purchase agreements (PPAs) to obtain renewable energy credits, which they use to compensate for the ``brown'' energy consumed from the grid. However, the details of these PPAs are often private and not shared with important stakeholders, such as grid operators and carbon information services, who monitor and report the grid's carbon emissions. This often results in incorrect carbon accounting, where the same renewable energy production could be factored into grid carbon emission reports and separately claimed by organizations that own PPAs. Such ``double counting'' of renewable energy production could lead organizations with PPAs to understate their carbon emissions and overstate their progress toward sustainability goals, and also provide significant challenges to consumers using common carbon reduction measures to decrease their carbon footprint. Unfortunately, there is no consensus on accurately computing the grid's carbon intensity by properly accounting for PPAs. The goal of our work is to shed quantitative and qualitative light on the renewable energy attribution and the incorrect carbon intensity estimation problems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy,"The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Effect of the Network in Cutting Carbon for Geo-shifted Workloads,"Organizations are increasingly offloading their workloads to cloud platforms. For workloads with relaxed deadlines, this presents an opportunity to reduce the total carbon footprint of these computations by moving workloads to datacenters with access to low-carbon power. Recently published results have shown that the carbon footprint of the wide-area network (WAN) can be a significant share of the total carbon output of executing the workload itself, and so careful selection of the time and place where these computations are offloaded is critical. In this paper, we propose an approach to geographic workload migration that uses high-fidelity maps of physical Internet infrastructure to better estimate the carbon costs of WAN transfers. Our findings show that space-shifting workloads can achieve much higher carbon savings than time-shifting alone, if accurate estimates of WAN carbon costs are taken into account.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Footprinting in a course on energy,Footprints provide a way to estimate the relative impact of processes and products on the global climate. Including footprint analysis in a course on energy simultaneously provides students with an understanding of this tool and a quantitative guide to approaches that address climate change. College-level classroom activities for (primarily) process-based life cycle carbon footprinting are discussed.,"all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Taxonomy and Future Directions for Sustainable Cloud Computing: 360 Degree View,"The cloud computing paradigm offers on-demand services over the Internet and supports a wide variety of applications. With the recent growth of Internet of Things (IoT) based applications the usage of cloud services is increasing exponentially. The next generation of cloud computing must be energy-efficient and sustainable to fulfil the end-user requirements which are changing dynamically. Presently, cloud providers are facing challenges to ensure the energy efficiency and sustainability of their services. The usage of large number of cloud datacenters increases cost as well as carbon footprints, which further effects the sustainability of cloud services. In this paper, we propose a comprehensive taxonomy of sustainable cloud computing. The taxonomy is used to investigate the existing techniques for sustainability that need careful attention and investigation as proposed by several academic and industry groups. Further, the current research on sustainable cloud computing is organized into several categories: application design, sustainability metrics, capacity planning, energy management, virtualization, thermal-aware scheduling, cooling management, renewable energy and waste heat utilization. The existing techniques have been compared and categorized based on the common characteristics and properties. A conceptual model for sustainable cloud computing has been proposed along with discussion on future research directions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability assessment of 4G and 5G universal mobile broadband strategies,"With infrastructure systems lasting for decades, even centuries, there is growing need to assess sustainability impacts. However, compared to energy or transportation networks (which each contribute roughly one third of global emissions), broadband networks have arguably received less attention due to their much smaller footprint (~1.8-3.9% of global emissions). Nevertheless, many countries are looking to provide universal mobile broadband over the next decade to meet Goal 9 of the Sustainable Development Goals (SDGs). Therefore, this paper evaluates the future sustainability impacts of providing either 4G or 5G mobile broadband, quantifying the carbon and other environmental emissions associated with each universal broadband strategy. This paper contributes the first ex ante sustainability assessment of global universal mobile broadband strategies aimed at delivering SDG Goal 9.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Clover: Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service,"This paper presents a solution to the challenge of mitigating carbon emissions from hosting large-scale machine learning (ML) inference services. ML inference is critical to modern technology products, but it is also a significant contributor to carbon footprint. We introduce Clover, a carbon-friendly ML inference service runtime system that balances performance, accuracy, and carbon emissions through mixed-quality models and GPU resource partitioning. Our experimental results demonstrate that Clover is effective in substantially reducing carbon emissions while maintaining high accuracy and meeting service level agreement (SLA) targets.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Enabling Sustainable Clouds: The Case for Virtualizing the Energy System,"Cloud platforms' growing energy demand and carbon emissions are raising concern about their environmental sustainability. The current approach to enabling sustainable clouds focuses on improving energy-efficiency and purchasing carbon offsets. These approaches have limits: many cloud data centers already operate near peak efficiency, and carbon offsets cannot scale to near zero carbon where there is little carbon left to offset. Instead, enabling sustainable clouds will require applications to adapt to when and where unreliable low-carbon energy is available. Applications cannot do this today because their energy use and carbon emissions are not visible to them, as the energy system provides the rigid abstraction of a continuous, reliable energy supply. This vision paper instead advocates for a ``carbon first'' approach to cloud design that elevates carbon-efficiency to a first-class metric. To do so, we argue that cloud platforms should virtualize the energy system by exposing visibility into, and software-defined control of, it to applications, enabling them to define their own abstractions for managing energy and carbon emissions based on their own requirements.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Standby efficiency and thermocline degradation of a packed bed thermal energy storage: An experimental study,"The waste heat potential from industrial processes is huge and if it can be utilized it may contribute significantly to the mitigation of climate change. A packed bed thermal energy storage system is a low-cost storage technology that can be employed to enable the utilization of waste heat from industrial processes. This system can be used to store excess heat and release this energy when it is needed at a later time. To ensure the efficient operation of a packed bed thermal energy storage its characteristics in standby mode need to be studied in great detail. In the present study, the standby efficiency and thermocline degradation of a lab-scale packed bed thermal energy storage in standby mode is experimentally investigated for different flow directions of the heat transfer fluid during the preceding charging period. Results show that, for long standby periods, the standby efficiency is significantly affected by the flow direction. The maximum entropy generation rate for a 22-hour standby process with the flow direction of the heat transfer fluid from bottom to top in the preceding charging process is twice as high as for the same process with the reverse flow direction. Energy and exergy efficiencies are lower for the process with reverse flow direction by 5% and 18% respectively.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Sustainability in Dense Radio Access Networks via High Altitude Platform Stations,"The growing demand for radio access networks (RANs) driven by advanced wireless technology and the everincreasing mobile traffic, faces significant energy consumption challenges that threaten sustainability. To address this, an architecture referring to the vertical heterogeneous network (vHetNet) has recently been proposed. Our study seeks to enhance network operations in terms of energy efficiency and sustainability by examining a vHetNet configuration, comprising a high altitude platform station (HAPS) acting as a super macro base station (SMBS), along with a macro base station (MBS) and a set of small base stations (SBSs) in a densely populated area.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web Services,"There has been a significant societal push towards sustainable practices, including in computing. Modern interactive workloads such as geo-distributed web-services exhibit various spatiotemporal and performance flexibility, enabling the possibility to adapt the location, time, and intensity of processing to align with the availability of renewable and low-carbon energy. An example is a web application hosted across multiple cloud regions, each with varying carbon intensity based on their local electricity mix. Distributed load-balancing enables the exploitation of low-carbon energy through load migration across regions, reducing web applications carbon footprint. In this paper, we present CASPER, a carbon-aware scheduling and provisioning system that primarily minimizes the carbon footprint of distributed web services while also respecting their Service Level Objectives (SLO). We formulate CASPER as an multi-objective optimization problem that considers both the variable carbon intensity and latency constraints of the network. Our evaluation reveals the significant potential of CASPER in achieving substantial reductions in carbon emissions. Compared to baseline methods, CASPER demonstrates improvements of up to 70% with no latency performance degradation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon- and Precedence-Aware Scheduling for Data Processing Clusters,"As large-scale data processing workloads continue to grow, their carbon footprint raises concerns. Prior research on carbon-aware schedulers has focused on shifting computation to align with availability of low-carbon energy, but these approaches assume that each task can be executed independently. In contrast, data processing jobs have precedence constraints (i.e., outputs of one task are inputs for another) that complicate decisions, since delaying an upstream ``bottleneck'' task to a low-carbon period will also block downstream tasks, impacting the entire job's completion time. In this paper, we show that carbon-aware scheduling for data processing benefits from knowledge of both time-varying carbon and precedence constraints. Our main contribution is $\texttt{PCAPS}$, a carbon-aware scheduler that interfaces with modern ML scheduling policies to explicitly consider the precedence-driven importance of each task in addition to carbon. To illustrate the gains due to fine-grained task information, we also study $\texttt{CAP}$, a wrapper for any carbon-agnostic scheduler that adapts the key provisioning ideas of $\texttt{PCAPS}$. Our schedulers enable a configurable priority between carbon reduction and job completion time, and we give analytical results characterizing the trade-off between the two. Furthermore, our Spark prototype on a 100-node Kubernetes cluster shows that a moderate configuration of $\texttt{PCAPS}$ reduces carbon footprint by up to 32.9% without significantly impacting the cluster's total efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Improving the Representation of Energy Efficiency in an Energy System Optimization Model,"Energy system optimization models (ESOMs) are designed to examine the potential effects of a proposed policy, but often represent energy-efficient technologies and policies in an overly simplified way. Most ESOMs include different end-use technologies with varying efficiencies and select technologies for deployment based solely on least-cost optimization, which drastically oversimplifies consumer decision-making. In this paper, we change the structure of an existing ESOM to model energy efficiency in way that is consistent with microeconomic theory. The resulting model considers the effectiveness of energy-efficient technologies in meeting energy service demands, and their potential to substitute electricity usage by conventional technologies. To test the revised model, we develop a simple hypothetical case and use it to analyze the welfare gain from an energy efficiency subsidy versus a carbon tax policy. In the simple test case, the maximum recovered welfare from an efficiency subsidy is less than 50% of the first-best carbon tax policy.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Configurable Pythonic Data Center Model for Sustainable Cooling and ML Integration,"There have been growing discussions on estimating and subsequently reducing the operational carbon footprint of enterprise data centers. The design and intelligent control for data centers have an important impact on data center carbon footprint. In this paper, we showcase PyDCM, a Python library that enables extremely fast prototyping of data center design and applies reinforcement learning-enabled control with the purpose of evaluating key sustainability metrics including carbon footprint, energy consumption, and observing temperature hotspots. We demonstrate these capabilities of PyDCM and compare them to existing works in EnergyPlus for modeling data centers. PyDCM can also be used as a standalone Gymnasium environment for demonstrating sustainability-focused data center control.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Scoping Sustainable Collaborative Mixed Reality,"Mixed Reality (MR) is becoming ubiquitous as it finds its applications in education, healthcare, and other sectors beyond leisure. While MR end devices, such as headsets, have low energy intensity, the total number of devices and resource requirements of the entire MR ecosystem, which includes cloud and edge endpoints, can be significant. The resulting operational and embodied carbon footprint of MR has led to concerns about its environmental implications. Recent research has explored reducing the carbon footprint of MR devices by exploring hardware design space or network optimizations. However, many additional avenues for enhancing MR's sustainability remain open, including energy savings in non-processor components and carbon-aware optimizations in collaborative MR ecosystems. In this paper, we aim to identify key challenges, existing solutions, and promising research directions for improving MR sustainability. We explore adjacent fields of embedded and mobile computing systems for insights and outline MR-specific problems requiring new solutions. We identify the challenges that must be tackled to enable researchers, developers, and users to avail themselves of these opportunities in collaborative MR systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Emissions and Large Neural Network Training,"The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective,"Software sustainability is a key multifaceted non-functional requirement that encompasses environmental, social, and economic concerns, yet its integration into the development of Machine Learning (ML)-enabled systems remains an open challenge. While previous research has explored high-level sustainability principles and policy recommendations, limited empirical evidence exists on how sustainability is practically managed in ML workflows. Existing studies predominantly focus on environmental sustainability, e.g., carbon footprint reduction, while missing the broader spectrum of sustainability dimensions and the challenges practitioners face in real-world settings. To address this gap, we conduct an empirical study to characterize sustainability in ML-enabled systems from a practitioner's perspective. We investigate (1) how ML engineers perceive and describe sustainability, (2) the software engineering practices they adopt to support it, and (3) the key challenges hindering its adoption. We first perform a qualitative analysis based on interviews with eight experienced ML engineers, followed by a large-scale quantitative survey with 203 ML practitioners. Our key findings reveal a significant disconnection between sustainability awareness and its systematic implementation, highlighting the need for more structured guidelines, measurement frameworks, and regulatory support.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Studies on the Electron Reconstruction Efficiency for the Beam Calorimeter of an ILC Detector,In this talk recent simulation results on the single high energy electron reconstruction with the Beam Calorimeter for the ILD detector are presented. Guinea Pig is used to generate the e+e- pair background and GEANT4 for the simulation of electron showers in the calorimeter. An algorithm was developed for the sHEe reconstruction on top of the large e+e- background. The efficiency of the sHEe reconstruction is estimated for the nominal and SB-2009 ILC beam parameters.,"all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Achieving Dispatchability in Data Centers: Carbon and Cost-Aware Sizing of Energy Storage and Local Photovoltaic Generation,"Data centers are large electricity consumers due to the high consumption needs of servers and their cooling systems. Given the current crypto-currency and artificial intelligence trends, the data center electricity demand is bound to grow significantly. With the electricity sector being responsible for a large share of global greenhouse gas (GHG) emissions, it is important to lower the carbon footprint of data centers to meet GHG emissions targets set by international agreements. Moreover, uncontrolled integration of data centers in power distribution grids contributes to increasing the stochasticity of the power system demand, thus increasing the need for capacity reserves, which leads to economic and environmental inefficiencies in the power grid operation. This work provides a method to size a PhotoVoltaic (PV) system and an Energy Storage System (ESS) for an existing data center looking to reduce both its carbon footprint and demand stochasticity via dispatching. The proposed scenario-based optimization framework allows to size the ESS and the PV system to minimize the expected operational and capital costs, along with the carbon footprint of the data center complex. The life cycle assessment of the resources, as well as the dynamic carbon emissions of the upstream power distribution grid, are accounted for while computing the day-ahead planning of the data center aggregated demand and PV generation. Case studies in different Swiss cantons and regions of Germany emphasize the need for location-aware sizing processes since the obtained optimal solutions strongly depend on the local electricity carbon footprint, cost and on the local irradiance conditions. Some regions show potential in carbon footprint reduction, while other regions do not.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Clustering Time-Series Energy Data from Smart Meters,"Investigations have been performed into using clustering methods in data mining time-series data from smart meters. The problem is to identify patterns and trends in energy usage profiles of commercial and industrial customers over 24-hour periods, and group similar profiles. We tested our method on energy usage data provided by several U.S. power utilities. The results show accurate grouping of accounts similar in their energy usage patterns, and potential for the method to be utilized in energy efficiency programs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Enhancing Energy Efficiency for Reconfigurable Intelligent Surfaces with Practical Power Models,"Reconfigurable intelligent surfaces (RISs) are widely considered a promising technology for future wireless communication systems. As an important indicator of RIS-assisted communication systems in green wireless communications, energy efficiency (EE) has recently received intensive research interest as an optimization target. However, most previous works have ignored the different power consumption between ON and OFF states of the PIN diodes attached to each RIS element. This oversight results in extensive unnecessary power consumption and reduction of actual EE due to the inaccurate power model. To address this issue, in this paper, we first utilize a practical power model for a RIS-assisted multi-user multiple-input single-output (MU-MISO) communication system, which takes into account the difference in power dissipation caused by ON-OFF states of RIS's PIN diodes. Based on this model, we formulate a more accurate EE optimization problem. However, this problem is non-convex and has mixed-integer properties, which poses a challenge for optimization. To solve the problem, an effective alternating optimization (AO) algorithm framework is utilized to optimize the base station and RIS beamforming precoder separately. To obtain the essential RIS beamforming precoder, we develop two effective methods based on maximum gradient search and SDP relaxation respectively. Theoretical analysis shows the exponential complexity of the original problem has been reduced to polynomial complexity. Simulation results demonstrate that the proposed algorithm outperforms the existing ones, leading to a significant increase in EE across a diverse set of scenarios.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification,"Federated Learning (FL) methods adopt efficient communication technologies to distribute machine learning tasks across edge devices, reducing the overhead in terms of data storage and computational complexity compared to centralized solutions. Rather than moving large data volumes from producers (sensors, machines) to energy-hungry data centers, raising environmental concerns due to resource demands, FL provides an alternative solution to mitigate the energy demands of several learning tasks while enabling new Artificial Intelligence of Things (AIoT) applications. This paper proposes a framework for real-time monitoring of the energy and carbon footprint impacts of FL systems. The carbon tracking tool is evaluated for consensus (fully decentralized) and classical FL policies. For the first time, we present a quantitative evaluation of different computationally and communication efficient FL methods from the perspectives of energy consumption and carbon equivalent emissions, suggesting also general guidelines for energy-efficient design. Results indicate that consensus-driven FL implementations should be preferred for limiting carbon emissions when the energy efficiency of the communication is low (i.e., < 25 Kbit/Joule). Besides, quantization and sparsification operations are shown to strike a balance between learning performances and energy consumption, leading to sustainable FL designs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The role of broadband connectivity in achieving Sustainable Development Goals (SDGs),"Broadband connectivity is a tool for catalyzing socio-economic development and reducing the societal inequalities. Recent studies have investigated the supporting role of broadband in addressing Sustainable Development Goals (SDGs). Relationally, emerging ultra-dense broadband networks such as 5/6G have been linked to increased power consumption and more carbon footprint. With SDGs recognized as interdependent and addressing one should not jeopardize the achievement of the other, there is need for sustainability research. Despite the need to narrow the digital divide and address the SDGs by 2030, it is surprising that limited comprehensive studies exist on broadband sustainability. To this end, we review 113 peer reviewed journals focusing on six key areas (SDGs addressed, application areas, country income, technology, methodology and spatial focus). We further discuss our findings before making four key recommendations on broadband sustainability research to fast-track SDG achievement by 2030 especially for developing economies.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Late Breaking Results: Leveraging Approximate Computing for Carbon-Aware DNN Accelerators,"The rapid growth of Machine Learning (ML) has increased demand for DNN hardware accelerators, but their embodied carbon footprint poses significant environmental challenges. This paper leverages approximate computing to design sustainable accelerators by minimizing the Carbon Delay Product (CDP). Using gate-level pruning and precision scaling, we generate area-aware approximate multipliers and optimize the accelerator design with a genetic algorithm. Results demonstrate reduced embodied carbon while meeting performance and accuracy requirements.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon and Reliability-Aware Computing for Heterogeneous Data Centers,"The rapid expansion of data centers (DCs) has intensified energy and carbon footprint, incurring a massive environmental computing cost. While carbon-aware workload migration strategies have been examined, existing approaches often overlook reliability metrics such as server lifetime degradation, and quality-of-service (QoS) that substantially affects both carbon and operational efficiency of DCs. Hence, this paper proposes a comprehensive optimization framework for spatio-temporal workload migration across distributed DCs that jointly minimizes operational and embodied carbon emissions while complying with service-level agreements (SLA). A key contribution is the development of an embodied carbon emission model based on servers' expected lifetime analysis, which explicitly considers server heterogeneity resulting from aging and utilization conditions. These issues are accommodated using new server dispatch strategies, and backup resource allocation model, accounting hardware, software and workload-induced failure. The overall model is formulated as a mixed-integer optimization problem with multiple linearization techniques to ensure computational tractability. Numerical case studies demonstrate that the proposed method reduces total carbon emissions by up to 21%, offering a pragmatic approach to sustainable DC operations.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards a Systematic Survey for Carbon Neutral Data Centers,"Data centers are carbon-intensive enterprises due to their massive energy consumption, and it is estimated that data center industry will account for 8\% of global carbon emissions by 2030. However, both technological and policy instruments for reducing or even neutralizing data center carbon emissions have not been thoroughly investigated. To bridge this gap, this survey paper proposes a roadmap towards carbon-neutral data centers that takes into account both policy instruments and technological methodologies. We begin by presenting the carbon footprint of data centers, as well as some insights into the major sources of carbon emissions. Following that, carbon neutrality plans for major global cloud providers are discussed to summarize current industrial efforts in this direction. In what follows, we introduce the carbon market as a policy instrument to explain how to offset data center carbon emissions in a cost-efficient manner. On the technological front, we propose achieving carbon-neutral data centers by increasing renewable energy penetration, improving energy efficiency, and boosting energy circulation simultaneously. A comprehensive review of existing technologies on these three topics is elaborated subsequently. Based on this, a multi-pronged approach towards carbon neutrality is envisioned and a digital twin-powered industrial artificial intelligence (AI) framework is proposed to make this solution a reality. Furthermore, three key scientific challenges for putting such a framework in place are discussed. Finally, several applications for this framework are presented to demonstrate its enormous potential.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Management Based on Multi-Agent Deep Reinforcement Learning for A Multi-Energy Industrial Park,"Owing to large industrial energy consumption, industrial production has brought a huge burden to the grid in terms of renewable energy access and power supply. Due to the coupling of multiple energy sources and the uncertainty of renewable energy and demand, centralized methods require large calculation and coordination overhead. Thus, this paper proposes a multi-energy management framework achieved by decentralized execution and centralized training for an industrial park. The energy management problem is formulated as a partially-observable Markov decision process, which is intractable by dynamic programming due to the lack of the prior knowledge of the underlying stochastic process. The objective is to minimize long-term energy costs while ensuring the demand of users. To solve this issue and improve the calculation speed, a novel multi-agent deep reinforcement learning algorithm is proposed, which contains the following key points: counterfactual baseline for facilitating contributing agents to learn better policies, soft actor-critic for improving robustness and exploring optimal solutions. A novel reward is designed by Lagrange multiplier method to ensure the capacity constraints of energy storage. In addition, considering that the increase in the number of agents leads to performance degradation due to large observation spaces, an attention mechanism is introduced to enhance the stability of policy and enable agents to focus on important energy-related information, which improves the exploration efficiency of soft actor-critic. Numerical results based on actual data verify the performance of the proposed algorithm with high scalability, indicating that the industrial park can minimize energy costs under different demands.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Know your footprint -- Evaluation of the professional carbon footprint for individual researchers in high energy physics and related fields,"As the climate crisis intensifies, understanding the environmental impact of professional activities is paramount, especially in sectors with historically significant resource utilisation. This includes High Energy Physics (HEP) and related fields, which investigate the fundamental laws of our universe. As members of the young High Energy Physicists (yHEP) association, we investigate the CO$_2$-equivalent emissions generated by HEP-related research on a personalised per-researcher level, for four distinct categories: Experiments, tied to collaborations with substantial infrastructure; Institutional, representing the resource consumption of research institutes and universities; Computing, focussing on simulations and data analysis; and Travel, covering professional trips to conferences, etc. The findings are integrated into a tool for self-evaluation, the Know-your-footprint (Kyf) calculator, allowing the assessment of the personal and professional footprint and optionally sharing the data with the yHEP association. The study aims to heighten awareness, foster sustainability, and inspire the community to adopt more environmentally responsible research practices urgently.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Power-Capping Metric Evaluation for Improving Energy Efficiency in HPC Applications,"With high-performance computing systems now running at exascale, optimizing power-scaling management and resource utilization has become more critical than ever. This paper explores runtime power-capping optimizations that leverage integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip. We evaluate energy-performance metrics that account for simultaneous CPU and GPU power-capping effects by using two complementary approaches: speedup-energy-delay and a Euclidean distance-based multi-objective optimization method. By targeting a mostly compute-bound exascale science application, the Locally Self-Consistent Multiple Scattering (LSMS), we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications, and we recognize that even modest reductions in energy consumption can have significant overall impacts. Our results highlight how GPU task-specific dynamic power-cap adjustments combined with integrated CPU-GPU power steering can improve the energy utilization of certain GPU tasks, thereby laying the groundwork for future adaptive optimization strategies.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
MOSAIC: A Multi-Objective Optimization Framework for Sustainable Datacenter Management,"In recent years, cloud service providers have been building and hosting datacenters across multiple geographical locations to provide robust services. However, the geographical distribution of datacenters introduces growing pressure to both local and global environments, particularly when it comes to water usage and carbon emissions. Unfortunately, efforts to reduce the environmental impact of such datacenters often lead to an increase in the cost of datacenter operations. To co-optimize the energy cost, carbon emissions, and water footprint of datacenter operation from a global perspective, we propose a novel framework for multi-objective sustainable datacenter management (MOSAIC) that integrates adaptive local search with a collaborative decomposition-based evolutionary algorithm to intelligently manage geographical workload distribution and datacenter operations. Our framework sustainably allocates workloads to datacenters while taking into account multiple geography- and time-based factors including renewable energy sources, variable energy costs, power usage efficiency, carbon factors, and water intensity in energy. Our experimental results show that, compared to the best-known prior work frameworks, MOSAIC can achieve 27.45x speedup and 1.53x improvement in Pareto Hypervolume while reducing the carbon footprint by up to 1.33x, water footprint by up to 3.09x, and energy costs by up to 1.40x. In the simultaneous three-objective co-optimization scenario, MOSAIC achieves a cumulative improvement across all objectives (carbon, water, cost) of up to 4.61x compared to the state-of-the-arts.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy efficiency analysis as a function of the working voltages in supercapacitors,"Supercapacitors are increasingly used as energy storage elements. Unlike batteries, their state of charge has a considerable influence on their voltage in normal operation, allowing them to work from zero to their maximum voltage. In this work, a theoretical and practical analysis is proposed of the energy efficiency of these devices according to their working voltages. To this end, several supercapacitors were subjected to charge and discharge cycles until the measurements of current and voltage stabilized. At this point their energy efficiency was calculated. These charge-discharge cycles were carried out: i) without rest between charging and discharging; and ii) with a rest of several minutes between the two stages. Using the information obtained from the tests, the energy efficiency is shown plotted against the minimum and maximum working voltages. By consulting the data and the graphs, the ideal working voltages to optimize the energy efficiency of these devices can be obtained.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
DE-LIoT: The Data-Energy Networking Paradigm for Sustainable Light-Based Internet of Things,"The growing demand for Internet of Things (IoT) networks has sparked interest in sustainable, zero-energy designs through Energy Harvesting (EH) to extend the lifespans of IoT sensors. Visible Light Communication (VLC) is particularly promising, integrating signal transmission with optical power harvesting to enable both data exchange and energy transfer in indoor network nodes. VLC indoor channels, however, can be unstable due to their line-of-sight nature and indoor movements. In conventional EH-based IoT networks, maximum Energy Storage (ES) capacity might halt further harvesting or waste excess energy, leading to resource inefficiency. Addressing these issues, this paper proposes a novel VLC-based WPANs concept that enhances both data and energy harvesting efficiency. The architecture employs densely distributed nodes and a central controller for simultaneous data and energy network operation, ensuring efficient energy exchange and resource optimisation. This approach, with centralised control and energy-state-aware nodes, aims for long-term energy autonomy. The feasibility of the Data-Energy Networking-enabled Light-based Internet of Things (DE-LIoT) concept is validated through real hardware implementation, demonstrating its sustainability and practical applicability. Results show significant improvements in the lifetime of resource-limited nodes, confirming the effectiveness of this new data and energy networking model in enhancing sustainability and resource optimisation in VLC-based WPANs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence","The rapid advancement of Artificial Intelligence (AI) has led to unprecedented computational demands, raising significant environmental and ethical concerns. This paper critiques the prevailing reliance on large-scale, static datasets and monolithic training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging dynamic architectures, HAI seeks to balance performance with ecological responsibility. We detail the theoretical foundations, system design, and operational principles that enable AI to learn continuously and contextually while minimizing carbon footprints and human annotation costs. Our approach addresses pressing challenges in active learning, continual adaptation, and energy-efficient model deployment, offering a pathway toward responsible, human-centered artificial intelligence.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Treehouse: A Case For Carbon-Aware Datacenter Software,"The end of Dennard scaling and the slowing of Moore's Law has put the energy use of datacenters on an unsustainable path. Datacenters are already a significant fraction of worldwide electricity use, with application demand scaling at a rapid rate. We argue that substantial reductions in the carbon intensity of datacenter computing are possible with a software-centric approach: by making energy and carbon visible to application developers on a fine-grained basis, by modifying system APIs to make it possible to make informed trade offs between performance and carbon emissions, and by raising the level of application programming to allow for flexible use of more energy efficient means of compute and storage. We also lay out a research agenda for systems software to reduce the carbon footprint of datacenter computing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Power Challenges of Large Scale Research Infrastructures: the Square Kilometer Array and Solar Energy Integration; Towards a zero-carbon footprint next generation telescope,"The Square Kilometer Array (SKA) will be the largest Global science project of the next two decades. It will encompass a sensor network dedicated to radioastronomy, covering two continents. It will be constructed in remote areas of South Africa and Australia, spreading over 3000Km, in high solar irradiance latitudes. Solar Power supply is therefore an option to power supply the SKA and contribute to a zero carbon footprint next generation telescope. Here we outline the major characteristics of the SKA and some innovation approaches on thermal solar energy Integration with SKA prototypes.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Sustainable Future is now: a dynamic model to advance investments in PV and Energy Storage,"We examine the relationship among photovoltaic (PV) investments, energy production, and environmental impact using a dynamic optimization model. Our findings show that increasing investment in renewables supports both energy generation and ecological sustainability, with the optimal path depending on policy priorities. Our analysis demonstrates that the economic and technological conditions for a transition to PV energy are already in place, challenging the idea that renewables will only become competitive in the future. We also account for the fact that PV optimality conditions improve over time as storage technology efficiency increases and production costs decrease. In this perspective we find that energy storage may be a more effective policy tool than carbon taxation for cutting emissions, as it faces less political resistance and further strengthens the long-term viability of renewable energy. Policy insights of the paper capture the evolving competitiveness of PV and its role in accelerating the energy transition. They also provide policymakers with strategies to align economic growth with long-term sustainability through renewable energy investments.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices,"LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Efficiency Will Not Lead to Sustainable Reasoning AI,"AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference,"Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Efficiency of Downlink Transmission Strategies for Cloud Radio Access Networks,"This paper studies the energy efficiency of the cloud radio access network (C-RAN), specifically focusing on two fundamental and different downlink transmission strategies, namely the data-sharing strategy and the compression strategy. In the data-sharing strategy, the backhaul links connecting the central processor (CP) and the base-stations (BSs) are used to carry user messages -- each user's messages are sent to multiple BSs; the BSs locally form the beamforming vectors then cooperatively transmit the messages to the user. In the compression strategy, the user messages are precoded centrally at the CP, which forwards a compressed version of the analog beamformed signals to the BSs for cooperative transmission. This paper compares the energy efficiencies of the two strategies by formulating an optimization problem of minimizing the total network power consumption subject to user target rate constraints, where the total network power includes the BS transmission power, BS activation power, and load-dependent backhaul power. To tackle the discrete and nonconvex nature of the optimization problems, we utilize the techniques of reweighted $\ell_1$ minimization and successive convex approximation to devise provably convergent algorithms. Our main finding is that both the optimized data-sharing and compression strategies in C-RAN achieve much higher energy efficiency as compared to the non-optimized coordinated multi-point transmission, but their comparative effectiveness in energy saving depends on the user target rate. At low user target rate, data-sharing consumes less total power than compression, however, as the user target rate increases, the backhaul power consumption for data-sharing increases significantly leading to better energy efficiency of compression at the high user rate regime.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Boolean Computation Using Self-Sustaining Nonlinear Oscillators,"Self-sustaining nonlinear oscillators of practically any type can function as latches and registers if Boolean logic states are represented physically as the phase of oscillatory signals. Combinational operations on such phase-encoded logic signals can be implemented using arithmetic negation and addition followed by amplitude limiting. With these, general-purpose Boolean computation using a wide variety of natural and engineered oscillators becomes potentially possible. Such phase-encoded logic shows promise for energy efficient computing. It also has inherent noise immunity advantages over traditional level-based logic.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights","The rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45\% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability","The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities. In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs. More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations. After that, we quantitatively analyzed the requirement of xPUs in training and inference. Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience (QoE)'s perspective of end users. Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact. Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon Explorer: A Holistic Approach for Designing Carbon Aware Datacenters,"Technology companies have been leading the way to a renewable energy transformation, by investing in renewable energy sources to reduce the carbon footprint of their datacenters. In addition to helping build new solar and wind farms, companies make power purchase agreements or purchase carbon offsets, rather than relying on renewable energy every hour of the day, every day of the week (24/7). Relying on renewable energy 24/7 is challenging due to the intermittent nature of wind and solar energy. Inherent variations in solar and wind energy production causes excess or lack of supply at different times. To cope with the fluctuations of renewable energy generation, multiple solutions must be applied. These include: capacity sizing with a mix of solar and wind power, energy storage options, and carbon aware workload scheduling. However, depending on the region and datacenter workload characteristics, the carbon-optimal solution varies. Existing work in this space does not give a holistic view of the trade-offs of each solution and often ignore the embodied carbon cost of the solutions. In this work, we provide a framework, Carbon Explorer, to analyze the multi-dimensional solution space by taking into account operational and embodided footprint of the solutions to help make datacenters operate on renewable energy 24/7. The solutions we analyze include capacity sizing with a mix of solar and wind power, battery storage, and carbon aware workload scheduling, which entails shifting the workloads from times when there is lack of renewable supply to times with abundant supply.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Energy- and QoS-aware Load Balancing for 6G: Leveraging O-RAN to Achieve Sustainable and Energy-Efficient 6G,"This paper addresses the critical challenge posed by the increasing energy consumption in mobile networks, particularly with the advent of Sixth Generation (6G) technologies. We propose an adaptive network management framework that leverages the Open Radio Access Network (O-RAN) architecture to enhance network adaptability and energy efficiency. By utilizing O-RAN's open interfaces and intelligent controllers, our approach implements dynamic resource management strategies that respond to fluctuating user demands while maintaining the quality of service. We design and implement O-RAN-compliant applications to validate our framework, demonstrating significant improvements in energy efficiency without compromising network performance. Our study offers a comprehensive guide for utilizing O-RAN's open architecture to achieve sustainable and energy-efficient 6G networks, aligning with global efforts to reduce the environmental impact of mobile communication systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
LCA and energy efficiency in buildings: mapping more than twenty years of research,"Research on Life Cycle Assessment (LCA) is being conducted in various sectors, from analyzing building materials and components to comprehensive evaluations of entire structures. However, reviews of the existing literature have been unable to provide a comprehensive overview of research in this field, leaving scholars without a definitive guideline for future investigations. This paper aims to fill this gap, mapping more than twenty years of research. Using an innovative methodology that combines social network analysis and text mining, the paper examined 8024 scientific abstracts. The authors identified seven key thematic groups, building and sustainability clusters (BSCs). To assess their significance in the broader discourse on building and sustainability, the semantic brand score (SBS) indicator was applied. Additionally, building and sustainability trends were tracked, focusing on the LCA concept. The major research topics mainly relate to building materials and energy efficiency. In addition to presenting an innovative approach to reviewing extensive literature domains, the article also provides insights into emerging and underdeveloped themes, outlining crucial future research directions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Energy- and Cost-Efficient 6G Networks,"As the world enters the journey toward the 6th generation (6G) of wireless technology, the promises of ultra-high data rates, unprecedented low latency, and a massive surge in connected devices require crucial exploration of network energy saving (NES) solutions to minimize the carbon footprint and overall energy usage of future cellular networks. On the other hand, network-controlled repeaters (NCRs) have been introduced by 3rd generation partnership project (3GPP) as a cost-effective solution to improve network coverage. However, their impact on network power consumption and energy efficiency has not been thoroughly investigated. This paper studies NES schemes for next-generation 6G networks aided by NCRs and proposes optimal NES strategies aiming at maximizing the overall energy efficiency of the network. Repeaters are shown to allow for power savings at next-generation nodeB (gNB), and offer higher overall energy efficiency (EE) and spectral efficiency (SE), thus providing an energy-efficient and cost-efficient alternative to increase the performance of future 6G networks","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Quantifying Potential Energy Efficiency Gain in Green Cellular Wireless Networks,"Conventional cellular wireless networks were designed with the purpose of providing high throughput for the user and high capacity for the service provider, without any provisions of energy efficiency. As a result, these networks have an enormous Carbon footprint. In this paper, we describe the sources of the inefficiencies in such networks. First we present results of the studies on how much Carbon footprint such networks generate. We also discuss how much more mobile traffic is expected to increase so that this Carbon footprint will even increase tremendously more. We then discuss specific sources of inefficiency and potential sources of improvement at the physical layer as well as at higher layers of the communication protocol hierarchy. In particular, considering that most of the energy inefficiency in cellular wireless networks is at the base stations, we discuss multi-tier networks and point to the potential of exploiting mobility patterns in order to use base station energy judiciously. We then investigate potential methods to reduce this inefficiency and quantify their individual contributions. By a consideration of the combination of all potential gains, we conclude that an improvement in energy consumption in cellular wireless networks by two orders of magnitude, or even more, is possible.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning,"Hybrid intelligence aims to enhance decision-making, problem-solving, and overall system performance by combining the strengths of both, human cognitive abilities and artificial intelligence. With the rise of Large Language Models (LLM), progressively participating as smart agents to accelerate machine learning development, Hybrid Intelligence is becoming an increasingly important topic for effective interaction between humans and machines. This paper presents an approach to leverage Hybrid Intelligence towards sustainable and energy-aware machine learning. When developing machine learning models, final model performance commonly rules the optimization process while the efficiency of the process itself is often neglected. Moreover, in recent times, energy efficiency has become equally crucial due to the significant environmental impact of complex and large-scale computational processes. The contribution of this work covers the interactive inclusion of secondary knowledge sources through Human-in-the-loop (HITL) and LLM agents to stress out and further resolve inefficiencies in the machine learning development process.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Adjustable Nonlinear Springs to Improve Efficiency of Vibration Energy Harvesters,"Vibration Energy Harvesting is an emerging technology aimed at turning mechanical energy from vibrations into electricity to power microsystems of the future. Most of present vibration energy harvesters are based on a mass spring structure introducing a resonance phenomenon that allows to increase the output power compared to non-resonant systems, but limits the working frequency bandwidth. Therefore, they are not able to harvest energy when ambient vibrations' frequencies shift. To follow shifts of ambient vibration frequencies and to increase the frequency band where energy can be harvested, one solution consists in using nonlinear springs. We present in this paper a model of adjustable nonlinear springs (H-shaped springs) and their benefits to improve velocity-damped vibration energy harvesters' (VEH) output powers. A simulation on a real vibration source proves that the output power can be higher in nonlinear devices compared to linear systems (up to +48%).","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CATransformers: Carbon Aware Transformers Through Joint Model-Hardware Optimization,"Machine learning solutions are rapidly adopted to enable a variety of key use cases, from conversational AI assistants to scientific discovery. This growing adoption is expected to increase the associated lifecycle carbon footprint, including both \emph{operational carbon} from training and inference and \emph{embodied carbon} from AI hardware manufacturing. We introduce \ourframework -- the first carbon-aware co-optimization framework for Transformer-based models and hardware accelerators. By integrating both operational and embodied carbon into early-stage design space exploration, \ourframework enables sustainability-driven model architecture and hardware accelerator co-design that reveals fundamentally different trade-offs than latency- or energy-centric approaches. Evaluated across a range of Transformer models, \ourframework consistently demonstrates the potential to reduce total carbon emissions -- by up to 30\% -- while maintaining accuracy and latency. We further highlight its extensibility through a focused case study on multi-modal models. Our results emphasize the need for holistic optimization methods that prioritize carbon efficiency without compromising model capability and execution time performance. The source code of \ourframework is available at {\small{\href{https://github.com/facebookresearch/CATransformers}{\texttt{https://github.com/facebookresearch/CATransformers}}}}.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A User Interface Study on Sustainable City Trip Recommendations,"The importance of promoting sustainable and environmentally responsible practices is becoming increasingly recognized in all domains, including tourism. The impact of tourism extends beyond its immediate stakeholders and affects passive participants such as the environment, local businesses, and residents. City trips, in particular, offer significant opportunities to encourage sustainable tourism practices by directing travelers towards destinations that minimize environmental impact while providing enriching experiences. Tourism Recommender Systems (TRS) can play a critical role in this. By integrating sustainability features in TRS, travelers can be guided towards destinations that meet their preferences and align with sustainability objectives.   This paper investigates how different user interface design elements affect the promotion of sustainable city trip choices. We explore the impact of various features on user decisions, including sustainability labels for transportation modes and their emissions, popularity indicators for destinations, seasonality labels reflecting crowd levels for specific months, and an overall sustainability composite score. Through a user study involving mockups, participants evaluated the helpfulness of these features in guiding them toward more sustainable travel options.   Our findings indicate that sustainability labels significantly influence users towards lower-carbon footprint options, while popularity and seasonality indicators guide users to less crowded and more seasonally appropriate destinations. This study emphasizes the importance of providing users with clear and informative sustainability information, which can help them make more sustainable travel choices. It lays the groundwork for future applications that can recommend sustainable destinations in real-time.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Save A Tree or 6 kg of CO2? Understanding Effective Carbon Footprint Interventions for Eco-Friendly Vehicular Choices,"From ride-hailing to car rentals, consumers are often presented with eco-friendly options. Beyond highlighting a ""green"" vehicle and CO2 emissions, CO2 equivalencies have been designed to provide understandable amounts; we ask which equivalencies will lead to eco-friendly decisions. We conducted five ride-hailing scenario surveys where participants picked between regular and eco-friendly options, testing equivalencies, social features, and valence-based interventions. Further, we tested a car-rental embodiment to gauge how an individual (needing a car for several days) might behave versus the immediate ride-hailing context. We find that participants are more likely to choose green rides when presented with additional information about emissions; CO2 by weight was found to be the most effective. Further, we found that information framing - be it individual or collective footprint, positive or negative valence - had an impact on participants' choices. Finally, we discuss how our findings inform the design of effective interventions for reducing car-based carbon-emissions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy-Efficient Cyclical Trajectory Design for UAV-Aided Maritime Data Collection in Wind,"Unmanned aerial vehicles (UAVs), especially fixed-wing ones that withstand strong winds, have great potential for oceanic exploration and research. This paper studies a UAV-aided maritime data collection system with a fixed-wing UAV dispatched to collect data from marine buoys. We aim to minimize the UAV's energy consumption in completing the task by jointly optimizing the communication time scheduling among the buoys and the UAV's flight trajectory subject to wind effect, which is a non-convex problem and difficult to solve optimally. Existing techniques such as the successive convex approximation (SCA) method provide efficient sub-optimal solutions for collecting small/moderate data volume, whereas the solution heavily relies on the trajectory initialization and has not explicitly considered the wind effect, while the computational complexity and resulted trajectory complexity both become prohibitive for the task with large data volume. To this end, we propose a new cyclical trajectory design framework that can handle arbitrary data volume efficiently subject to wind effect. Specifically, the proposed UAV trajectory comprises multiple cyclical laps, each responsible for collecting only a subset of data and thereby significantly reducing the computational/trajectory complexity, which allows searching for better trajectory initialization that fits the buoys' topology and the wind. Numerical results show that the proposed cyclical scheme outperforms the benchmark one-flight-only scheme in general. Moreover, the optimized cyclical 8-shape trajectory can proactively exploit the wind and achieve lower energy consumption compared with the case without wind.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Embodied Carbon Accounting through Spatial-Temporal Embodied Carbon Models,"Embodied carbon is the total carbon released from the processes associated with a product from cradle to gate. In many industry sectors, embodied carbon dominates the overall carbon footprint. Embodied carbon accounting, i.e., to estimate the embodied carbon of a product, has become an important research topic.   Existing studies derive the embodied carbon through life cycle analysis (LCA) reports. Current LCA reports only provide the carbon emission of a product class, e.g., 28nm CPU, yet a product instance can be manufactured from diverse regions and in diverse time periods, e.g., a winter period of Ireland (Intel). It is known that the carbon emission depends on the electricity generation process which has spatial and temporal dynamics. Therefore, the embodied carbon of a specific product instance can largely differ from its product class. In this paper, we present new spatial-temporal embodied carbon models for embodied carbon accounting. We observe significant differences between current embodied carbon models and our spatial-temporal embodied carbon models, e.g., for 7nm CPU the difference can be 13.69%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards Sustainability in 6G and beyond: Challenges and Opportunities of Open RAN,"The transition to 6G is expected to bring significant advancements, including much higher data rates, enhanced reliability and ultra-low latency compared to previous generations. Although 6G is anticipated to be 100 times more energy efficient, this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability. Network sustainability encompasses a broader scope, integrating business viability, environmental sustainability, and social responsibility. This paper explores the sustainability requirements for 6G and proposes Open RAN as a key architectural solution. By enabling network diversification, fostering open and continuous innovation, and integrating AI/ML, Open RAN can promote sustainability in 6G. The paper identifies high energy consumption and e-waste generation as critical sustainability challenges and discusses how Open RAN can address these issues through softwarisation, edge computing, and AI integration.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Optimizing electric vehicles charging through smart energy allocation and cost-saving,"As the global focus on combating environmental pollution intensifies, the transition to sustainable energy sources, particularly in the form of electric vehicles (EVs), has become paramount. This paper addresses the pressing need for Smart Charging for EVs by developing a comprehensive mathematical model aimed at optimizing charging station management. The model aims to efficiently allocate the power from charging sockets to EVs, prioritizing cost minimization and avoiding energy waste. Computational simulations demonstrate the efficacy of the mathematical optimization model, which can unleash its full potential when the number of EVs at the charging station is high.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards net-zero manufacturing: carbon-aware scheduling for GHG emissions reduction,"Detailed scheduling has traditionally been optimized for the reduction of makespan and manufacturing costs. However, growing awareness of environmental concerns and increasingly stringent regulations are pushing manufacturing towards reducing the carbon footprint of its operations. Scope 2 emissions, which are the indirect emissions related to the production and consumption of grid electricity, are in fact estimated to be responsible for more than one-third of the global GHG emissions. In this context, carbon-aware scheduling can serve as a powerful way to reduce manufacturing's carbon footprint by considering the time-dependent carbon intensity of the grid and the availability of on-site renewable electricity.   This study introduces a carbon-aware permutation flow-shop scheduling model designed to reduce scope 2 emissions. The model is formulated as a mixed-integer linear problem, taking into account the forecasted grid generation mix and available on-site renewable electricity, along with the set of jobs to be scheduled and their corresponding power requirements. The objective is to find an optimal day-ahead schedule that minimizes scope 2 emissions. The problem is addressed using a dedicated memetic algorithm, combining evolutionary strategy and local search.   Results from computational experiments confirm that by considering the dynamic carbon intensity of the grid and on-site renewable electricity availability, substantial reductions in carbon emissions can be achieved.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Operator learning for energy-efficient building ventilation control with computational fluid dynamics simulation of a real-world classroom,"Energy-efficient ventilation control plays a vital role in reducing building energy consumption while ensuring occupant health and comfort. While Computational Fluid Dynamics (CFD) simulations provide detailed and physically accurate representation of indoor airflow, their high computational cost limits their use in real-time building control. In this work, we present a neural operator learning framework that combines the physical accuracy of CFD with the computational efficiency of machine learning to enable building ventilation control with the high-fidelity fluid dynamics models. Our method jointly optimizes the airflow supply rates and vent angles to reduce energy use and adhere to air quality constraints. We train an ensemble of neural operator transformer models to learn the mapping from building control actions to airflow fields using high-resolution CFD data. This learned neural operator is then embedded in an optimization-based control framework for building ventilation control. Experimental results show that our approach achieves significant energy savings compared to maximum airflow rate control, rule-based control, as well as data-driven control methods using spatially averaged CO2 prediction and deep learning based reduced order model, while consistently maintaining safe indoor air quality. These results highlight the practicality and scalability of our method in maintaining energy efficiency and indoor air quality in real-world buildings.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards a 6G embedding sustainability,"From its conception, 6G is being designed with a particular focus on sustainability. The general philosophy of the H2020 Hexa-X project work on sustainability in 6G is based on two principles: to reduce direct negative life cycle impacts of 6G systems as much as possible (Sustainable 6G) and to analyze use cases that maximize positive environmental, social, and economic effects in other sectors of society (6G for Sustainability or its enablement effect). To apply this philosophy, Hexa-X is designing 6G with three sustainability objectives in mind: to enable the reduction of emissions in 6G-powered sectors of society, to reduce the total cost of ownership and to improve energy efficiency. This paper describes these objectives, their associated KPIs and quantitative targets, and the levers to reach them. Furthermore, to maximize the positive effects of 6G through the enablement effect, a link between 6G and the United Nations' Sustainable Development Goals (UN SDGs) framework is proposed and illustrated by Hexa-X use case families.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"An Extensive and Methodical Review of Smart Grids for Sustainable Energy Management-Addressing Challenges with AI, Renewable Energy Integration and Leading-edge Technologies","Energy management decreases energy expenditures and consumption while simultaneously increasing energy efficiency, reducing carbon emissions, and enhancing operational performance. Smart grids are a type of sophisticated energy infrastructure that increase the generation and distribution of electricity's sustainability, dependability, and efficiency by utilizing digital communication technologies. They combine a number of cutting-edge techniques and technology to improve energy resource management. A large amount of research study on the topic of smart grids for energy management has been completed in the last several years. The authors of the present study want to cover a number of topics, including smart grid benefits and components, technical developments, integrating renewable energy sources, using artificial intelligence and data analytics, cybersecurity, and privacy. Smart Grids for Energy Management are an innovative field of study aiming at tackling various difficulties and magnifying the efficiency, dependability, and sustainability of energy systems, including: 1) Renewable sources of power like solar and wind are intermittent and unpredictable 2) Defending smart grid system from various cyber-attacks 3) Incorporating an increasing number of electric vehicles into the system of power grid without overwhelming it. Additionally, it is proposed to use AI and data analytics for better performance on the grid, reliability, and energy management. It also looks into how AI and data analytics can be used to optimize grid performance, enhance reliability, and improve energy management. The authors will explore these significant challenges and ongoing research. Lastly, significant issues in this field are noted, and recommendations for further work are provided.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
LACS: Learning-Augmented Algorithms for Carbon-Aware Resource Scaling with Uncertain Demand,"Motivated by an imperative to reduce the carbon emissions of cloud data centers, this paper studies the online carbon-aware resource scaling problem with unknown job lengths (OCSU) and applies it to carbon-aware resource scaling for executing computing workloads. The task is to dynamically scale resources (e.g., the number of servers) assigned to a job of unknown length such that it is completed before a deadline, with the objective of reducing the carbon emissions of executing the workload. The total carbon emissions of executing a job originate from the emissions of running the job and excess carbon emitted while switching between different scales (e.g., due to checkpoint and resume). Prior work on carbon-aware resource scaling has assumed accurate job length information, while other approaches have ignored switching losses and require carbon intensity forecasts. These assumptions prohibit the practical deployment of prior work for online carbon-aware execution of scalable computing workload. We propose LACS, a theoretically robust learning-augmented algorithm that solves OCSU. To achieve improved practical average-case performance, LACS integrates machine-learned predictions of job length. To achieve solid theoretical performance, LACS extends the recent theoretical advances on online conversion with switching costs to handle a scenario where the job length is unknown. Our experimental evaluations demonstrate that, on average, the carbon footprint of LACS lies within 1.2% of the online baseline that assumes perfect job length information and within 16% of the offline baseline that, in addition to the job length, also requires accurate carbon intensity forecasts. Furthermore, LACS achieves a 32% reduction in carbon footprint compared to the deadline-aware carbon-agnostic execution of the job.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
New Co-Simulation Variants for Emissions and Cost Reduction of Sustainable District Heating Planning,"Classical heating of residential areas is very energy-intensive, so alternatives are needed, including renewable energies and advanced heating technologies. Thus, the present paper introduces a new methodology for comprehensive variant analysis for future district heating planning, aiming at optimizing emissions and costs. For this, an extensive Modelica-based modeling study comprising models of heating center, heat grid pipelines and heating interface units to buildings are coupled in co-simulations. These enable a comparative analysis of the economic feasibility and sustainability for various technologies and energy carriers to be carried out. The new modular and highly parameterizable building model serves for validation of the introduced heat grid model. The results show that bio-methane as an energy source reduces carbon equivalent emissions by nearly 70% compared to conventional natural gas heating, and the use of hydrogen as an energy source reduces carbon equivalent emissions by 77% when equipped with a heat pump. In addition, the use of ground source heat pumps has a high economic viability when economic benefits are taken into account. The study findings highlight the importance of strategic planning and flexible design in the early stages of district development in order to achieve improved energy efficiency and a reduced carbon footprint.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Power Efficiency Metric for Comparing Energy Consumption in Future Wireless Networks in the Millimeter Wave and Terahertz bands,"Future wireless cellular networks will utilize millimeter-wave and sub-THz frequencies and deploy small-cell base stations to achieve data rates on the order of hundreds of Gigabits per second per user. The move to sub-THz frequencies will require attention to sustainability and reduction of power whenever possible to reduce the carbon footprint while maintaining adequate battery life for the massive number of resource-constrained devices to be deployed. This article analyzes power consumption of future wireless networks using a new metric, the power waste factor ($ W $), which shows promise for the study and development of ""green G"" - green technology for future wireless networks. Using $ W $, power efficiency can be considered by quantifying the power wasted by all devices on a signal path in a cascade. We then show that the consumption efficiency factor ($CEF$), defined as the ratio of the maximum data rate achieved to the total power consumed, is a novel and powerful measure of power efficiency that shows less energy per bit is expended as the cell size shrinks and carrier frequency and channel bandwidth increase. Our findings offer a standard approach to calculating and comparing power consumption and energy efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks,"The transportation sector accounts for about 25% of global greenhouse gas emissions. Therefore, an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint. Efficiency is typically measured in terms of energy use per traveled distance, e.g. liters of fuel per kilometer. Leading factors that impact the energy efficiency are the type of vehicle, environment, driver behavior, and weather conditions. These varying factors introduce uncertainty in estimating the vehicles' energy efficiency. We propose in this paper an ensemble learning approach based on deep neural networks (ENN) that is designed to reduce the predictive uncertainty and to output measures of such uncertainty. We evaluated it using the publicly available Vehicle Energy Dataset (VED) and compared it with several baselines per vehicle and energy type. The results showed a high predictive performance and they allowed to output a measure of predictive uncertainty.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms,"In recent years, sustainability in software systems has gained significant attention, especially with the rise of cloud computing and the shift towards cloud-based architectures. This shift has intensified the need to identify sustainability in architectural discussions to take informed architectural decisions. One source to see these decisions is in online Q&A forums among practitioners' discussions. However, recognizing sustainability concepts within software practitioners' discussions remains challenging due to the lack of clear and distinct guidelines for this task. To address this issue, we introduce the notion of sustainability flags as pointers in relevant discussions, developed through thematic analysis of multiple sustainability best practices from cloud providers. This study further evaluates the effectiveness of these flags in identifying sustainability within cloud architecture posts, using a controlled experiment. Preliminary results suggest that the use of flags results in classifying fewer posts as sustainability-related compared to a control group, with moderately higher certainty and significantly improved performance. Moreover, sustainability flags are perceived as more useful and understandable than relying solely on definitions for identifying sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainability in Telecommunication Networks and Key Value Indicators: a Survey,"Telecommunication technologies are important enablers for both digital and ecological transitions. By offering digital alternatives to traditional modes of transportation and communication, they help reduce carbon footprints while improving access to fundamental services. Particularly in rural and remote areas, telecommunications facilitate access to education, healthcare, and employment, helping to bridge the digital divide. Additionally, telecommunications can promote sustainability by supporting renewable energy usage, gender equality, and circular economies. However, defining the role of telecommunications in sustainability remains complex due to the historical focus on performance rather than long-term societal goals. Given the significance of this theme, this paper aims to provide the reader with a deeper look at the concept of sustainability within the telecommunications sector by examining relevant initiatives and projects. It reviews the major approaches for measuring sustainability and outlines practical approaches for implementing these assessments. Furthermore, the paper explores the proposed network architectures that incorporate Key Value Indicators and discusses major technologies in this area, such as Network Digital Twins and Intent-Based Networking. Through this analysis, the paper aims to contribute to creating sustainable telecommunication networks and broader industries.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Optimal sizing of solar photovoltaic and lithium battery storage to reduce grid electricity reliance in buildings,"In alignment with the Paris Agreement, the city of Oxford in the UK aims to become carbon neutral by 2040. Renewable energy help achieve this target by reducing the reliance on carbon-intensive grid electricity. This research seeks to optimally size solar photovoltaic and lithium battery storage systems, reducing Oxford's grid electricity reliance in buildings. The analysis starts with modeling the electricity demand. The model uses Elexon electricity settlement profiles, and assembles them into the demand profile according to the quantity and types of buildings in Oxford. Then, solar generation is modeled using Pfenninger and Staffell's method. Solar photovoltaic and lithium storage systems are sized using a hybridized analytical and iterative method. First, the method calculates the solar system size search range, then iterates through the range. At each solar size, the method calculates and iterates through the storage system size search range. Within each iteration, the renewable system is simulated using demand and generation data with a simplified system set-up and the conventional operation strategy. The method outputs combinations of solar system capacity, storage system capacity, and grid electricity import. Each combination's levelized cost of electricity is calculated, and the lowest cost combination is the optimal sizing. Solar and storage system costs are projected from 2019 to 2100, and the optimal sizing is calculated for each year. The result shows that solar photovoltaic is economically competitive, but lithium storage cost is still too high. As solar and storage prices continue to drop, they will take up greater portions of the energy system. However, there will always be a need for the grid, as it provides flexibility and can meet demands that are too costly for solar and storage","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study,"Environmental sustainability in Systems-of-Systems (SoS) is an emerging field that seeks to integrate technological solutions to promote the efficient management of natural resources. While systematic reviews address sustainability in the context of Smart Cities (a category of SoS), a systematic study synthesizing the existing knowledge on environmental sustainability applied to SoS in general does not exist. Although literature includes other types of sustainability, such as financial and social, this study focuses on environmental sustainability, analyzing how SoS contribute to sustainable practices such as carbon emission reduction, energy efficiency, and biodiversity conservation. We conducted a Systematic Mapping Study to identify the application domains of SoS in sustainability, the challenges faced, and research opportunities. We planned and executed a research protocol including an automated search over four scientific databases. Of 926 studies retrieved, we selected, analyzed, and reported the results of 39 relevant studies. Our findings reveal that most studies focus on Smart Cities and Smart Grids, while applications such as sustainable agriculture and wildfire prevention are less explored. We identified challenges such as system interoperability, scalability, and data governance. Finally, we propose future research directions for SoS and environmental sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Efficient Software Design and Development: A Systematic Literature Review,"The ICT sector, responsible for 2% of global carbon emissions, is under scrutiny calling for methodologies and tools to design and develop software in an environmentally sustainable-by-design manner. However, the software engineering solutions for designing and developing carbon-efficient software are currently scattered over multiple different pieces of literature, which makes it difficult to consult the body of knowledge on the topic. In this article, we precisely conduct a systematic literature review on state-of-the-art proposals for designing and developing carbon-efficient software. We identify and analyse 65 primary studies by classifying them through a taxonomy aimed at answering the 5W1H questions of carbon-efficient software design and development. We first provide a reasoned overview and discussion of the existing guidelines, reference models, measurement solutions and techniques for measuring, reducing, or minimising the carbon footprint of software. Ultimately, we identify open challenges and research gaps, offering insights for future work in this field.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Data-Driven Distributionally Robust Scheduling of Community Integrated Energy Systems with Uncertain Renewable Generations Considering Integrated Demand Response,"A community integrated energy system (CIES) is an important carrier of the energy internet and smart city in geographical and functional terms. Its emergence provides a new solution to the problems of energy utilization and environmental pollution. To coordinate the integrated demand response and uncertainty of renewable energy generation (RGs), a data-driven two-stage distributionally robust optimization (DRO) model is constructed. A comprehensive norm consisting of the 1-norm and infinity-norm is used as the uncertainty probability distribution information set, thereby avoiding complex probability density information. To address multiple uncertainties of RGs, a generative adversarial network based on the Wasserstein distance with gradient penalty is proposed to generate RG scenarios, which has wide applicability. To further tap the potential of the demand response, we take into account the ambiguity of human thermal comfort and the thermal inertia of buildings. Thus, an integrated demand response mechanism is developed that effectively promotes the consumption of renewable energy. The proposed method is simulated in an actual CIES in North China. In comparison with traditional stochastic programming and robust optimization, it is verified that the proposed DRO model properly balances the relationship between economical operation and robustness while exhibiting stronger adaptability. Furthermore, our approach outperforms other commonly used DRO methods with better operational economy, lower renewable power curtailment rate, and higher computational efficiency.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead","The proliferation of software and AI comes with a hidden risk: its growing energy and carbon footprint. As concerns regarding environmental sustainability come to the forefront, understanding and optimizing how software impacts the environment becomes paramount. In this paper, we present a state-of-the-art review of methods and tools that enable the measurement of software and AI-related energy and/or carbon emissions. We introduce a taxonomy to categorize the existing work as Monitoring, Estimation, or Black-Box approaches. We delve deeper into the tools and compare them across different dimensions and granularity - for example, whether their measurement encompasses energy and carbon emissions and the components considered (like CPU, GPU, RAM, etc.). We present our observations on the practical use (component wise consolidation of approaches) as well as the challenges that we have identified across the current state-of-the-art. As we start an initiative to address these challenges, we emphasize active collaboration across the community in this important field.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI,"Rapid adoption of machine learning (ML) technologies has led to a surge in power consumption across diverse systems, from tiny IoT devices to massive datacenter clusters. Benchmarking the energy efficiency of these systems is crucial for optimization, but presents novel challenges due to the variety of hardware platforms, workload characteristics, and system-level interactions. This paper introduces MLPerf Power, a comprehensive benchmarking methodology with capabilities to evaluate the energy efficiency of ML systems at power levels ranging from microwatts to megawatts. Developed by a consortium of industry professionals from more than 20 organizations, MLPerf Power establishes rules and best practices to ensure comparability across diverse architectures. We use representative workloads from the MLPerf benchmark suite to collect 1,841 reproducible measurements from 60 systems across the entire range of ML deployment scales. Our analysis reveals trade-offs between performance, complexity, and energy efficiency across this wide range of systems, providing actionable insights for designing optimized ML solutions from the smallest edge devices to the largest cloud infrastructures. This work emphasizes the importance of energy efficiency as a key metric in the evaluation and comparison of the ML system, laying the foundation for future research in this critical area. We discuss the implications for developing sustainable AI solutions and standardizing energy efficiency benchmarking for ML systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Efficient Energy Management Policies for Networks with Energy Harvesting Sensor Nodes,"We study sensor networks with energy harvesting nodes. The generated energy at a node can be stored in a buffer. A sensor node periodically senses a random field and generates a packet. These packets are stored in a queue and transmitted using the energy available at that time at the node. For such networks we develop efficient energy management policies. First, for a single node, we obtain policies that are throughput optimal, i.e., the data queue stays stable for the largest possible data rate. Next we obtain energy management policies which minimize the mean delay in the queue. We also compare performance of several easily implementable suboptimal policies. A greedy policy is identified which, in low SNR regime, is throughput optimal and also minimizes mean delay. Next using the results for a single node, we develop efficient MAC policies.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Digital Twins based Day-ahead Integrated Energy System Scheduling under Load and Renewable Energy Uncertainties,"By constructing digital twins (DT) of an integrated energy system (IES), one can benefit from DT's predictive capabilities to improve coordinations among various energy converters, hence enhancing energy efficiency, cost savings and carbon emission reduction. This paper is motivated by the fact that practical IESs suffer from multiple uncertainty sources, and complicated surrounding environment. To address this problem, a novel DT-based day-ahead scheduling method is proposed. The physical IES is modelled as a multi-vector energy system in its virtual space that interacts with the physical IES to manipulate its operations. A deep neural network is trained to make statistical cost-saving scheduling by learning from both historical forecasting errors and day-ahead forecasts. Case studies of IESs show that the proposed DT-based method is able to reduce the operating cost of IES by 63.5%, comparing to the existing forecast-based scheduling methods. It is also found that both electric vehicles and thermal energy storages play proactive roles in the proposed method, highlighting their importance in future energy system integration and decarbonisation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Simulation Models for Sustainable, Resilient, and Optimized Global Electric Vehicles Supply Chain","While the transition to electric vehicles (EVs) is essential for decarbonizing the transportation system, the production and distribution of EVs entail substantial carbon costs. To ensure these emissions are accurately accounted for and effectively mitigated, this research introduces a digital twin of the EV's supply chain, addressing a critical gap in current EV life cycle analyses and providing the first comprehensive quantification of its environmental sustainability and resilience. This simulation model replicates global market dynamics and captures the complexity and uncertainty of the EV supply chain, enabling a thorough evaluation of its carbon footprint, sustainability, resilience, and what-if counterfactual scenarios for alternative market structures. The results reveal that average supply chain emissions range from 6.42 to 6.94 Kg e-CO2/KWh across different battery technologies. Additionally, the mass flow analysis shows unbalanced dependencies at all supply phases, with one geographical region significantly dominating the supply chain structure, highlighting the current supply chain architecture's low resilience and high vulnerability. In light of these findings, the study introduces an optimization model for hub and resource allocation configuration, effectively reducing vulnerability levels and supply chain emissions by up to 80%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Optimal scheduling of island integrated energy systems considering multi-uncertainties and hydrothermal simultaneous transmission: A deep reinforcement learning approach,"Multi-uncertainties from power sources and loads have brought significant challenges to the stable demand supply of various resources at islands. To address these challenges, a comprehensive scheduling framework is proposed by introducing a model-free deep reinforcement learning (DRL) approach based on modeling an island integrated energy system (IES). In response to the shortage of freshwater on islands, in addition to the introduction of seawater desalination systems, a transmission structure of ""hydrothermal simultaneous transmission"" (HST) is proposed. The essence of the IES scheduling problem is the optimal combination of each unit's output, which is a typical timing control problem and conforms to the Markov decision-making solution framework of deep reinforcement learning. Deep reinforcement learning adapts to various changes and timely adjusts strategies through the interaction of agents and the environment, avoiding complicated modeling and prediction of multi-uncertainties. The simulation results show that the proposed scheduling framework properly handles multi-uncertainties from power sources and loads, achieves a stable demand supply for various resources, and has better performance than other real-time scheduling methods, especially in terms of computational efficiency. In addition, the HST model constitutes an active exploration to improve the utilization efficiency of island freshwater.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
GreenDFL: a Framework for Assessing the Sustainability of Decentralized Federated Learning Systems,"Decentralized Federated Learning (DFL) is an emerging paradigm that enables collaborative model training without centralized data and model aggregation, enhancing privacy and resilience. However, its sustainability remains underexplored, as energy consumption and carbon emissions vary across different system configurations. Understanding the environmental impact of DFL is crucial for optimizing its design and deployment. This work aims to develop a comprehensive and operational framework for assessing the sustainability of DFL systems. To address it, this work provides a systematic method for quantifying energy consumption and carbon emissions, offering insights into improving the sustainability of DFL. This work proposes GreenDFL, a fully implementable framework that has been integrated into a real-world DFL platform. GreenDFL systematically analyzes the impact of various factors, including hardware accelerators, model architecture, communication medium, data distribution, network topology, and federation size, on the sustainability of DFL systems. Besides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node selection algorithm (GreenDFL-SN) are developed to optimize energy efficiency and reduce carbon emissions in DFL training. Empirical experiments are conducted on multiple datasets, measuring energy consumption and carbon emissions at different phases of the DFL lifecycle. The proposed GreenDFL provides a comprehensive and practical approach for assessing the sustainability of DFL systems. Furthermore, it offers best practices for improving environmental efficiency in DFL, making sustainability considerations more actionable in real-world deployments.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Cost and efficiency requirements for a successful electricity storage in a highly renewable European energy system,"Future highly renewable energy systems might require substantial storage deployment. At the current stage, the technology portfolio of dominant storage options is limited to pumped-hydro storage and Li-Ion batteries. It is uncertain which storage design will be able to compete with these options. Considering Europe as a case study, we derive the cost and efficiency requirements of a generic storage technology, which we refer to as storage-X, to be deployed in the cost-optimal system. This is performed while including existing pumped-hydro facilities and accounting for the competition from stationary Li-ion batteries, flexible generation technology, and flexible demand in a highly renewable sector-coupled energy system. Based on a sample space of 724 storage configurations, we show that energy capacity cost and discharge efficiency largely determine the optimal storage deployment, in agreement with previous studies. Here, we show that charge capacity cost is also important due to its impact on renewable curtailment. A significant deployment of storage-X in a cost-optimal system requires (a) discharge efficiency of at least 95%, (b) discharge efficiency of at least 50% together with low energy capacity cost (10EUR/kWh), or (c) discharge efficiency of at least 25% with very low energy capacity cost (2EUR/kWh). Comparing our findings with seven emerging technologies reveals that none of them fulfill these requirements. Thermal Energy Storage (TES) is, however, on the verge of qualifying due to its low energy capacity cost and concurrent low charge capacity cost. Exploring the space of storage designs reveals that system cost reduction from storage-X deployment can reach 9% at its best, but this requires high round-trip efficiency (90%) and low charge capacity cost (35EUR/kW).","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-aware Software Services,"The significant carbon footprint of the ICT sector calls for methodologies to contain carbon emissions of running software. This article proposes a novel framework for implementing, configuring and assessing carbon-aware interactive software services. First, we propose a methodology to implement carbon-aware services leveraging the Strategy design pattern to feature alternative service versions with different energy consumption. Then, we devise a bilevel optimisation scheme to configure which version to use at different times of the day, based on forecasts of carbon intensity and service requests, pursuing the two-fold goal of minimising carbon emissions and maintaining average output quality above a desired set-point. Last, an open-source prototype of such optimisation scheme is used to configure a software service implemented as per our methodology and assessed against traditional non-adaptive implementations of the same service. Results show the capability of our framework to control the average quality of output results of carbon-aware services and to reduce carbon emissions from 8% to 50%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Performance is not All You Need: Sustainability Considerations for Algorithms,"This work focuses on the high carbon emissions generated by deep learning model training, specifically addressing the core challenge of balancing algorithm performance and energy consumption. It proposes an innovative two-dimensional sustainability evaluation system. Different from the traditional single performance-oriented evaluation paradigm, this study pioneered two quantitative indicators that integrate energy efficiency ratio and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy consumption and performance parameters through the harmonic mean to reveal the algorithm performance under unit energy consumption; the area under the sustainability curve (ASC) constructs a performance-power consumption curve to characterize the energy efficiency characteristics of the algorithm throughout the cycle. To verify the universality of the indicator system, the study constructed benchmarks in various multimodal tasks, including image classification, segmentation, pose estimation, and batch and online learning. Experiments demonstrate that the system can provide a quantitative basis for evaluating cross-task algorithms and promote the transition of green AI research from theory to practice. Our sustainability evaluation framework code can be found here, providing methodological support for the industry to establish algorithm energy efficiency standards.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Consumer-based Carbon Costs: Integrating Consumer Carbon Preferences in Electricity Markets,"An increasing share of consumers care about the carbon footprint of their electricity. This paper proposes to integrate consumer carbon preferences in the electricity market-clearing through consumer-based carbon costs. Specifically, consumers can submit not only bids for power but also assign a cost to the carbon emissions incurred by their electricity use. We start from a centralized market clearing that maximizes social welfare under consideration of generation costs, consumer utility and consumer carbon costs. We then derive an equivalent equilibrium formulation which incorporates a carbon allocation problem and gives rise to a set of carbon-adjusted electricity prices for both consumers and generators. We prove that the carbon-adjusted prices are higher for low-emitting generators and consumers with high carbon costs. Further, we prove that this new paradigm satisfies the same desirable market properties as standard electricity markets based on locational marginal prices, namely revenue adequacy and individual rationality, and demonstrate that a carbon tax on generators is equivalent to imposing a uniform carbon cost on consumers. Using a simplified three-bus system and the RTS-GMLC system, we illustrate that consumer-based carbon costs contribute to greener electricity market clearing both through generation redispatch and reductions in demand.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows,"Scientific workflows are widely used to automate scientific data analysis and often involve computationally intensive processing of large datasets on compute clusters. As such, their execution tends to be long-running and resource-intensive, resulting in substantial energy consumption and, depending on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet little work has focused specifically on scientific workflows, even though they present a substantial opportunity for carbon-aware computing because they are often significantly delay tolerant, efficiently interruptible, highly scalable and widely heterogeneous. In this study, we first exemplify the problem of carbon emissions associated with running scientific workflows, and then show the potential for carbon-aware workflow execution. For this, we estimate the carbon footprint of seven real-world Nextflow workflows executed on different cluster infrastructures using both average and marginal carbon intensity data. Furthermore, we systematically evaluate the impact of carbon-aware temporal shifting, and the pausing and resuming of the workflow. Moreover, we apply resource scaling to workflows and workflow tasks. Finally, we report the potential reduction in overall carbon emissions, with temporal shifting capable of decreasing emissions by over 80%, and resource scaling capable of decreasing emissions by 67%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Peak energy clustering and efficiency in compact objects,"We study the properties of plasmas containing a low energy thermal photon component at comoving temperature \equiv kT'/m_e c^2 \sim 10^{-5} - 10^{-2} interacting with an energetic electron component, characteristic of, e.g., the dissipation phase of relativistic outflows in gamma-ray bursts (GRB's), X-ray flashes, and blazars. We show that, for scattering optical depths larger than a few, balance between Compton and inverse-Compton scattering leads to the accumulation of electrons at values of $~ 0.15 - 0.3$. For optical depths larger than ~ 100, this leads to a peak in the comoving photon spectrum at 1-10 keV, very weakly dependent on the values of the free parameters. In particular, these results are applicable to the internal shock model of GRB, as well as to slow dissipation models, e.g. as might be expected from reconnection, if the dissipation occurs at a sub-photospheric radii. For GRB bulk Lorentz factors ~ 100, this results in observed spectral peaks clustering in the 0.1-1 MeV range, with conversion efficiencies of electron into photon energy in the BATSE range of ~ 30%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Exploring the Potential of Carbon-Aware Execution for Scientific Workflows,"Scientific workflows are widely used to automate scientific data analysis and often involve processing large quantities of data on compute clusters. As such, their execution tends to be long-running and resource intensive, leading to significant energy consumption and carbon emissions.   Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet little work has focused specifically on scientific workflows, even though they present a substantial opportunity for carbon-aware computing because they are inherently delay tolerant, efficiently interruptible, and highly scalable.   In this study, we demonstrate the potential for carbon-aware workflow execution. For this, we estimate the carbon footprint of two real-world Nextflow workflows executed on cluster infrastructure. We use a linear power model for energy consumption estimates and real-world average and marginal CI data for two regions. We evaluate the impact of carbon-aware temporal shifting, pausing and resuming, and resource scaling. Our findings highlight significant potential for reducing emissions of workflows and workflow tasks.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Theoretical thermodynamic analysis of a closed-cycle process for the conversion of heat into electrical energy by means of a distiller and an electrochemical cell,"We analyse a device aimed at the conversion of heat into electrical energy, based on a closed cycle in which a distiller generates two solutions at different concentrations, and an electrochemical cell consumes the concentration difference, converting it into electrical current. We first study an ideal model of such a process. We show that, if the device works at a single fixed pressure (i.e. with a ``single effect''), then the efficiency of the conversion of heat into electrical power can approach the efficiency of a reversible Carnot engine operating between the boiling temperature of the concentrated solution and that of the pure solvent. When two heat reservoirs with a higher temperature difference are available, the overall efficiency can be incremented by employing an arrangement of multiple cells working at different pressures (``multiple effects''). We find that a given efficiency can be achieved with a reduced number of effects by using solutions with a high boiling point elevation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Computing -- Without the Hot Air,"The demand for computing is continuing to grow exponentially. This growth will translate to exponential growth in computing's energy consumption unless improvements in its energy-efficiency can outpace increases in its demand. Yet, after decades of research, further improving energy-efficiency is becoming increasingly challenging, as it is already highly optimized. As a result, at some point, increases in computing demand are likely to outpace increases in its energy-efficiency, potentially by a wide margin. Such exponential growth, if left unchecked, will position computing as a substantial contributor to global carbon emissions. While prominent technology companies have recognized the problem and sought to reduce their carbon emissions, they understandably focus on their successes, which has the potential to inadvertently convey the false impression that this is now, or will soon be, a solved problem. Such false impressions can be counterproductive if they serve to discourage further research in this area, since, as we discuss, eliminating computing's, and more generally society's, carbon emissions is far from a solved problem. To better understand the problem's scope, this paper distills the fundamental trends that determine computing's carbon footprint and their implications for achieving sustainable computing.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The Market Measure of Carbon Risk and its Impact on the Minimum Variance Portfolio,"Like ESG investing, climate change is an important concern for asset managers and owners, and a new challenge for portfolio construction. Until now, investors have mainly measured carbon risk using fundamental approaches, such as with carbon intensity metrics. Nevertheless, it has not been proven that asset prices are directly impacted by these fundamental-based measures. In this paper, we focus on another approach, which consists in measuring the sensitivity of stock prices with respect to a carbon risk factor. In our opinion, carbon betas are market-based measures that are complementary to carbon intensities or fundamental-based measures when managing investment portfolios, because carbon betas may be viewed as an extension or forward-looking measure of the current carbon footprint. In particular, we show how this new metric can be used to build minimum variance strategies and how they impact their portfolio construction.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Architectural Tactics to Improve the Environmental Sustainability of Microservices: A Rapid Review,"Microservices are a popular architectural style adopted by the industry when it comes to deploying software that requires scalability, maintainability, and agile development. There is an increasing demand for improving the sustainability of microservice systems in the industry. This rapid review gathers 22 peer-reviewed studies and synthesizes architectural tactics that improve the environmental sustainability of microservices from them. We list 6 tactics that are presented in an actionable way and categorized according to their sustainability aspects and context. The sustainability aspects include energy efficiency, carbon efficiency, and resource efficiency, among which resource efficiency is the most researched one while energy efficiency and carbon efficiency are still in the early stage of study. The context categorization, including serverless platforms, decentralized networks, etc., helps to identify the tactics that we can use in a specific setting. Additionally, we present how the evidence of optimization after adopting these tactics is presented, like the measurement unit and statistical methods, and how experiments are generally set up so that this review is both instructive for our future study and our industrial practitioners' interest. We further study the insufficiencies of the current study and hope to provide insight for other researchers and the industry.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Explainable automatic industrial carbon footprint estimation from bank transaction classification using natural language processing,"Concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint (CF). These protocols are manual, work-intensive, and expensive. All of the above have led to a shift towards automatic data-driven approaches to estimate the CF, including Machine Learning (ML) solutions. Unfortunately, the decision-making processes involved in these solutions lack transparency from the end user's point of view, who must blindly trust their outcomes compared to intelligible traditional manual approaches. In this research, manual and automatic methodologies for CF estimation were reviewed, taking into account their transparency limitations. This analysis led to the proposal of a new explainable ML solution for automatic CF calculations through bank transaction classification. Consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose. For classification, different ML models have been employed based on their promising performance in the literature, such as Support Vector Machine, Random Forest, and Recursive Neural Networks. The results obtained were in the 90 % range for accuracy, precision, and recall evaluation metrics. From their decision paths, the proposed solution estimates the CO2 emissions associated with bank transactions. The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models. The explainability terms were automatically validated using a similarity metric over the descriptions of the target categories. Conclusively, the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Robots and Social Sustainability,"Sustainability is no longer a matter of choice but is invariably linked to the survival of the entire ecosystem of our planet Earth. As robotics technology is growing at an exponential rate, it is crucial to examine its implications for sustainability. Our focus is on social sustainability, specifically analyzing the role of robotics technology in this domain by identifying six distinct ways robots influence social sustainability.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Long-Term Energy Management for Microgrid with Hybrid Hydrogen-Battery Energy Storage: A Prediction-Free Coordinated Optimization Framework,"This paper studies the long-term energy management of a microgrid coordinating hybrid hydrogen-battery energy storage. We develop an approximate semi-empirical hydrogen storage model to accurately capture the power-dependent efficiency of hydrogen storage. We introduce a prediction-free two-stage coordinated optimization framework, which generates the annual state-of-charge (SoC) reference for hydrogen storage offline. During online operation, it updates the SoC reference online using kernel regression and makes operation decisions based on the proposed adaptive virtual-queue-based online convex optimization (OCO) algorithm. We innovatively incorporate penalty terms for long-term pattern tracking and expert-tracking for step size updates. We provide theoretical proof to show that the proposed OCO algorithm achieves a sublinear bound of dynamic regret without using prediction information. Numerical studies based on the Elia and North China datasets show that the proposed framework significantly outperforms the existing online optimization approaches by reducing the operational costs and loss of load by around 30% and 80%, respectively. These benefits can be further enhanced with optimized settings for the penalty coefficient and step size of OCO, as well as more historical references.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
CarbonKit: Designing A Personal Carbon Tracking Platform,"Ubiquitous technology platforms have been created to track and improve health and fitness; similar technologies can help individuals monitor and reduce their carbon footprints. This paper proposes CarbonKit, a platform combining technology, markets, and incentives to empower and reward people for reducing their carbon footprint. We argue that a goal-and-reward behavioral feedback loop can be combined with the Big Data available from tracked activities, apps, and social media to make CarbonKit an integral part of individuals daily lives. CarbonKit comprises five modules that link personal carbon tracking, health and fitness, social media, and economic incentives. Protocols for safeguarding security, privacy and individuals control over their own data are essential to the design of the CarbonKit. Initially CarbonKit would operate on a voluntary basis, but such a system can also serve as part of a mandatory region-wide initiative. We use the example of the British Columbia to illustrate the regulatory framework and participating stakeholders that would be required to support the CarbonKit in specific jurisdictions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency","Deploying Large Language Models (LLMs) on edge devices presents significant challenges due to computational constraints, memory limitations, inference speed, and energy consumption. Model quantization has emerged as a key technique to enable efficient LLM inference by reducing model size and computational overhead. In this study, we conduct a comprehensive analysis of 28 quantized LLMs from the Ollama library, which applies by default Post-Training Quantization (PTQ) and weight-only quantization techniques, deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy efficiency, inference performance, and output accuracy across multiple quantization levels and task types. Models are benchmarked on five standardized datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and we employ a high-resolution, hardware-based energy measurement tool to capture real-world power consumption. Our findings reveal the trade-offs between energy efficiency, inference speed, and accuracy in different quantization settings, highlighting configurations that optimize LLM deployment for resource-constrained environments. By integrating hardware-level energy profiling with LLM benchmarking, this study provides actionable insights for sustainable AI, bridging a critical gap in existing research on energy-aware LLM deployment.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget,"The carbon footprint of data centers has recently become a critical concern. So far, most carbon-aware strategies have focused on leveraging the flexibility of scheduling decisions for batch processing by shifting the time and location of workload executions. However, such approaches cannot be applied to service-oriented cloud applications, since they have to be reachable at every point in time and often at low latencies. We propose a carbon-aware approach for operating microservices under hourly carbon budgets. By choosing the most appropriate version and horizontal scaleout for each microservice, our strategy maximizes user experience and revenue while staying within budget constraints. Experiments across various application configurations and carbon budgets demonstrate that the approach adapts properly to changing workloads and carbon intensities.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Large Language Models for Energy-Efficient Code: Emerging Results and Future Directions,"Energy-efficient software helps improve mobile device experiences and reduce the carbon footprint of data centers. However, energy goals are often de-prioritized in order to meet other requirements. We take inspiration from recent work exploring the use of large language models (LLMs) for different software engineering activities. We propose a novel application of LLMs: as code optimizers for energy efficiency. We describe and evaluate a prototype, finding that over 6 small programs our system can improve energy efficiency in 3 of them, up to 2x better than compiler optimizations alone. From our experience, we identify some of the challenges of energy-efficient LLM code optimization and propose a research agenda.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Multi-Objective Power Allocation for Energy Efficient Wireless Information and Power Transfer Systems,"Simultaneous wireless information and power transfer (SWIPT) provides a promising solution for enabling perpetual wireless networks. As energy efficiency (EE) is an im- portant evaluation of system performance, this thesis studies energy-efficient resource allocation algorithm designs in SWIPT systems. We first investigate the trade-off between the EE for information transmission, the EE for power transfer, and the total transmit power in a basic SWIPT system with separated receivers. A multi-objective optimization problem is formulated under the constraint of maximum transmit power. We propose an algorithm which achieves flexible resource allocation for energy efficiencies maxi- mization and transmit power minimization. The trade-off region of the system design objectives is shown in simulation results. Further, we consider secure communication in a SWIPT system with power splitting receivers. Artificial noise is injected to the com- munication channel to combat the eavesdropping capability of potential eavesdroppers. A power-efficient resource allocation algorithm is developed when multiple legitimate information receivers and multi-antenna potential eavesdroppers co-exist in the system. Simulation results demonstrate a significant performance gain by the proposed optimal algorithm compared to suboptimal baseline schemes.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Quantifying Carbon Emissions due to Online Third-Party Tracking,"In the past decade, global warming made several headlines and turned the attention of the whole world to it. Carbon footprint is the main factor that drives greenhouse emissions up and results in the temperature increase of the planet with dire consequences. While the attention of the public is turned to reducing carbon emissions by transportation, food consumption and household activities, we ignore the contribution of CO2eq emissions produced by online activities. In the current information era, we spend a big amount of our days browsing online. This activity consumes electricity which in turn produces CO2eq. While website browsing contributes to the production of greenhouse gas emissions, the impact of the Internet on the environment is further exacerbated by the web-tracking practice. Indeed, most webpages are heavily loaded by tracking content used mostly for advertising, data analytics and usability improvements. This extra content implies big data transmissions which results in higher electricity consumption and thus higher greenhouse gas emissions. In this work, we focus on the overhead caused by web tracking and analyse both its network and carbon footprint. By leveraging the browsing telemetry of 100k users and the results of a crawling experiment of 2.7M websites, we find that web tracking increases data transmissions upwards of 21%, which in turn implies the additional emission of around 11 Mt of greenhouse gases in the atmosphere every year. We find such contribution to be far from negligible, and comparable to many activities of modern life, such as meat production, transportation, and even cryptocurrency mining. Our study also highlights that there exist significant inequalities when considering the footprint of different countries, website categories, and tracking organizations, with a few actors contributing to a much greater extent than the remaining ones.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Criteria for Credible AI-assisted Carbon Footprinting Systems: The Cases of Mapping and Lifecycle Modeling,"As organizations face increasing pressure to understand their corporate and products' carbon footprints, artificial intelligence (AI)-assisted calculation systems for footprinting are proliferating, but with widely varying levels of rigor and transparency. Standards and guidance have not kept pace with the technology; evaluation datasets are nascent; and statistical approaches to uncertainty analysis are not yet practical to apply to scaled systems. We present a set of criteria to validate AI-assisted systems that calculate greenhouse gas (GHG) emissions for products and materials. We implement a three-step approach: (1) Identification of needs and constraints, (2) Draft criteria development and (3) Refinements through pilots. The process identifies three use cases of AI applications: Case 1 focuses on AI-assisted mapping to existing datasets for corporate GHG accounting and product hotspotting, automating repetitive manual tasks while maintaining mapping quality. Case 2 addresses AI systems that generate complete product models for corporate decision-making, which require comprehensive validation of both component tasks and end-to-end performance. We discuss the outlook for Case 3 applications, systems that generate standards-compliant models. We find that credible AI systems can be built and that they should be validated using system-level evaluations rather than line-item review, with metrics such as benchmark performance, indications of data quality and uncertainty, and transparent documentation. This approach may be used as a foundation for practitioners, auditors, and standards bodies to evaluate AI-assisted environmental assessment tools. By establishing evaluation criteria that balance scalability with credibility requirements, our approach contributes to the field's efforts to develop appropriate standards for AI-assisted carbon footprinting systems.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning,"Solar sensor-based monitoring systems have become a crucial agricultural innovation, advancing farm management and animal welfare through integrating sensor technology, Internet-of-Things, and edge and cloud computing. However, the resilience of these systems to cyber-attacks and their adaptability to dynamic and constrained energy supplies remain largely unexplored. To address these challenges, we propose a sustainable smart farm network designed to maintain high-quality animal monitoring under various cyber and adversarial threats, as well as fluctuating energy conditions. Our approach utilizes deep reinforcement learning (DRL) to devise optimal policies that maximize both monitoring effectiveness and energy efficiency. To overcome DRL's inherent challenge of slow convergence, we integrate transfer learning (TL) and decision theory (DT) to accelerate the learning process. By incorporating DT-guided strategies, we optimize monitoring quality and energy sustainability, significantly reducing training time while achieving comparable performance rewards. Our experimental results prove that DT-guided DRL outperforms TL-enhanced DRL models, improving system performance and reducing training runtime by 47.5%.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
The X-ray footprint of the CircumNuclear Disk,"We studied the central regions of the Galactic Centre to determine if the CircumNuclear Disk (CND) acts as an absorber or a barrier for the central X-rays diffuse emission. After reprocessing 4.6Ms of Chandra observations, we were able to detect, for the first time, a depression in the X-ray luminosity of the diffuse emission whose size and location correspond to those of the CND. We extracted the X-ray spectra for various regions inside the CND footprint as well as for the region where the footprint is observed and for a region located outside the footprint. We simultaneously fitted these spectra as an optically thin plasma whose absorption by the interstellar medium and by the local plasma were fitted independently using the MCMC method. The hydrogen column density of the ISM is 7.5x10^22 cm^-2. The X-ray diffuse emission inside the CND footprint is formed by a 2T plasma of 1 and 4keV with slightly super-solar abundances except for the iron and carbon which are sub-solar. The plasma from the CND, in turn, is better described by a 1T model with abundances and local hydrogen column density which are very different to those of the innermost regions. The large iron abundance in this region confirms that the CND is dominated by the shock-heated ejecta of the Sgr A East supernova remnant. We deduced that the CND rather acts as a barrier for the Galactic Centre plasma and that the plasma located outside the CND may correspond to the collimated outflow possibly created by Sgr A* or the interaction between the wind of massive stars and the mini-spiral material.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads","The rapid advancement of AI, particularly large language models (LLMs), has raised significant concerns about the energy use and carbon emissions associated with model training and inference. However, existing tools for measuring and reporting such impacts are often fragmented, lacking systematic metric integration and offering limited support for correlation analysis among them. This paper presents AIMeter, a comprehensive software toolkit for the measurement, analysis, and visualization of energy use, power draw, hardware performance, and carbon emissions across AI workloads. By seamlessly integrating with existing AI frameworks, AIMeter offers standardized reports and exports fine-grained time-series data to support benchmarking and reproducibility in a lightweight manner. It further enables in-depth correlation analysis between hardware metrics and model performance and thus facilitates bottleneck identification and performance enhancement. By addressing critical limitations in existing tools, AIMeter encourages the research community to weigh environmental impact alongside raw performance of AI workloads and advances the shift toward more sustainable ""Green AI"" practices. The code is available at https://github.com/SusCom-Lab/AIMeter.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Improve Machine Learning carbon footprint using Nvidia GPU and Mixed Precision training for classification models -- Part I,"This is the 1st part of the dissertation for my master degree and compares the power consumption using the default floating point (32bit) and Nvidia mixed precision (16bit and 32bit) while training a classification ML model. A custom PC with specific hardware was built to perform the experiments, and different ML hyper-parameters, such as batch size, neurons, and epochs, were chosen to build Deep Neural Networks (DNN). Additionally, various software was used during the experiments to collect the power consumption data in Watts from the Graphics Processing Unit (GPU), Central Processing Unit (CPU), Random Access Memory (RAM) and manually from a wattmeter connected to the wall. A benchmarking test with default hyper parameter values for the DNN was used as a reference, while the experiments used a combination of different settings. The results were recorded in Excel, and descriptive statistics were chosen to calculate the mean between the groups and compare them using graphs and tables. The outcome was positive when using mixed precision combined with specific hyper-parameters. Compared to the benchmarking, the optimisation for the classification reduced the power consumption between 7 and 11 Watts. Similarly, the carbon footprint is reduced because the calculation uses the same power consumption data. Still, a consideration is required when configuring hyper-parameters because it can negatively affect hardware performance. However, this research required inferential statistics, specifically ANOVA and T-test, to compare the relationship between the means. Furthermore, tests indicated no statistical significance of the relationship between the benchmarking and experiments. However, a more extensive implementation with a cluster of GPUs can increase the sample size significantly, as it is an essential factor and can change the outcome of the statistical analysis.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Practical Issues of Energy Harvesting and Data Transmissions in Sustainable IoT,"The sustainable Internet of Things (IoT) is becoming a promising solution for the green living and smart industries. In this article, we investigate the practical issues in the radio energy harvesting and data communication systems through extensive field experiments. A number of important characteristics of energy harvesting circuits and communication modules have been studied, including the non-linear energy consumption of the communication system relative to the transmission power, the wake-up time associated with the payload, and the varying system power during consecutive packet transmissions. In order to improve the efficiency of energy harvest and energy utilization, we propose a new model to accurately describe the energy harvesting process and the power consumption for sustainable IoT devices. Experiments are performed using commercial IoT devices and RF energy harvesters to verify the accuracy of the proposed model. The experiment results show that the new model matches the performance of sustainable IoT devices very well in the real scenario.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Green Recommender Systems: Optimizing Dataset Size for Energy-Efficient Algorithm Performance,"As recommender systems become increasingly prevalent, the environmental impact and energy efficiency of training large-scale models have come under scrutiny. This paper investigates the potential for energy-efficient algorithm performance by optimizing dataset sizes through downsampling techniques in the context of Green Recommender Systems. We conducted experiments on the MovieLens 100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of various recommender algorithms under different portions of dataset size. Our results indicate that while more training data generally leads to higher algorithm performance, certain algorithms, such as FunkSVD and BiasedMF, particularly with unbalanced and sparse datasets like Amazon Toys and Games, maintain high-quality recommendations with up to a 50% reduction in training data, achieving nDCG@10 scores within approximately 13% of full dataset performance. These findings suggest that strategic dataset reduction can decrease computational and environmental costs without substantially compromising recommendation quality. This study advances sustainable and green recommender systems by providing insights for reducing energy consumption while maintaining effectiveness.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Beyond Efficiency: Scaling AI Sustainably,"Barroso's seminal contributions in energy-proportional warehouse-scale computing launched an era where modern datacenters have become more energy efficient and cost effective than ever before. At the same time, modern AI applications have driven ever-increasing demands in computing, highlighting the importance of optimizing efficiency across the entire deep learning model development cycle. This paper characterizes the carbon impact of AI, including both operational carbon emissions from training and inference as well as embodied carbon emissions from datacenter construction and hardware manufacturing. We highlight key efficiency optimization opportunities for cutting-edge AI technologies, from deep learning recommendation models to multi-modal generative AI tasks. To scale AI sustainably, we must also go beyond efficiency and optimize across the life cycle of computing infrastructures, from hardware manufacturing to datacenter operations and end-of-life processing for the hardware.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Optimal quantum metrology under energy constraints,"The traditional framework of quantum metrology commonly assumes unlimited access to resources, overlooking resource constraints in realistic scenarios. As such, the optimal strategies therein can be infeasible in practice. Here, we investigate quantum metrology where the total energy consumption of the probe state preparation, intermediate control operations, and the final measurement is subject to a constraint. We establish a comprehensive theoretical framework for characterizing energy-constrained multi-step quantum processes, based on which we develop a general optimization method for energy-constrained quantum metrology that determines both the optimal precision and the corresponding strategy. Using the method, we determine the ultimate precision limit of energy-constrained phase estimation and identify a novel advantage of quantum superpositions of causal orders in enhancing the energy efficiency of adaptive quantum estimation.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Self-Sustainability in Full-Spectrum 6G,"Full-spectrum ranging from sub 6 GHz to THz and visible light will be exploited in 6G in order to reach unprecedented key-performance-indicators (KPIs). However, extraordinary amount of energy will be consumed by network infrastructure, while functions of massively deployed Internet of Everything (IoE) devices are limited by embedded batteries. Therefore, energy self-sustainable 6G is proposed in this article. First of all, it may achieve network-wide energy efficiency by exploiting cell-free and airborne access networks as well as by implementing intelligent holographic environments. Secondly, by exploiting radio-frequency/visible light signals for providing on-demand wireless power transfer (WPT) and for enabling passive backscatter communication, ``zero-energy'' devices may become a reality. Furthermore, IoE devices actively adapt their transceivers for better performance to a dynamic environment. This article aims to provide a first glance at primary designing principles of energy self-sustainable 6G.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Oscillations of hypothetical strange stars as an efficient source ultra-high-energy particles,"We investigate the dynamical behavior of strange quark matter (SQM) objects, such as stars and planets, when subjected to radial oscillations induced by tidal interactions in stellar systems. Our study demonstrates that SQM objects can efficiently convert mechanical energy into hadronic energy due to the critical mass density at their surfaces of 4.7*10^{14} g/cm^3, below which SQM becomes unstable and decays into photons, hadrons, and leptons. We show that even small-amplitude radial oscillations, with a radius change of as little as 0.1%, can result in significant excitation energies near the surface of SQM stars. This excitation energy is rapidly converted into electromagnetic energy over short timescales approximately 1 ms, potentially leading to observable astrophysical phenomena. Higher amplitude oscillations may cause fragmentation or dissolution of SQM stars, which has important implications for the evolution of binary systems containing SQM objects and the emission of gravitational waves.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Towards carbon neutral scientific societies: A case study with the International Adsorption Society,"With increasing concerns over climate change, scientists must imperatively acknowledge their share in CO2 emissions. Considering the large emissions associated with scientific traveling - especially international conferences - initiatives to mitigate such impact are blooming. With the COVID-19 pandemic shattering our notion of private/professional interactions, the moment should be seized to reinvent science conferences and collaborations with a model respectful of the environment. Yet, despite efforts to reduce the footprint of conferences, there is a lack of a robust approach based on reliable numbers (emissions, carbon offsetting/removals, etc.) to accompany this shift of paradigm. Here, considering a representative scientific society, the International Adsorption Society, we report on a case study of the problem: making conferences carbon neutral while respecting the needs of scientists. We first provide a quantitative analysis of the CO2 emissions for the IAS conference in 2022 related to accommodation, catering, flights, etc. Second, we conduct two surveys probing our community view on the carbon footprint of our activities. These surveys mirror each other, and were distributed two years before and in the aftermath of our triennial conference (also corresponding to pre/post COVID times). By combining the different parts, we propose ambitious recommendations to shape the future of conferences.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Energy Efficiency in Rate-Splitting Multiple Access with Mixed Criticality,"Future sixth generation (6G) wireless communication networks face the need to similarly meet unprecedented quality of service (QoS) demands while also providing a larger energy efficiency (EE) to minimize their carbon footprint. Moreover, due to the diverseness of network participants, mixed criticality QoS levels are assigned to the users of such networks. In this work, with a focus on a cloud-radio access network (C-RAN), the fulfillment of desired QoS and minimized transmit power use is optimized jointly within a rate-splitting paradigm. Thereby, the optimization problem is non-convex. Hence, a low-complexity algorithm is proposed based on fractional programming. Numerical results validate that there is a trade-off between the QoS fulfillment and power minimization. Moreover, the energy efficiency of the proposed rate-splitting algorithm is larger than in comparative schemes, especially with mixed criticality.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Report on the NSF Workshop on Sustainable Computing for Sustainability (NSF WSCS 2024),"This report documents the process that led to the NSF Workshop on ""Sustainable Computing for Sustainability"" held in April 2024 at NSF in Alexandria, VA, and reports on its findings. The workshop's primary goals were to (i) advance the development of research initiatives along the themes of both sustainable computing and computing for sustainability, while also (ii) helping develop and sustain the interdisciplinary teams those initiatives would need. The workshop's findings are in the form of recommendations grouped in three categories: General recommendations that cut across both themes of sustainable computing and computing for sustainability, and recommendations that are specific to sustainable computing and computing for sustainability, respectively.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
"Engineering Carbon Credits Towards A Responsible FinTech Era: The Practices, Implications, and Future","Carbon emissions significantly contribute to climate change, and carbon credits have emerged as a key tool for mitigating environmental damage and helping organizations manage their carbon footprint. Despite their growing importance across sectors, fully leveraging carbon credits remains challenging. This study explores engineering practices and fintech solutions to enhance carbon emission management. We first review the negative impacts of carbon emission non-disclosure, revealing its adverse effects on financial stability and market value. Organizations are encouraged to actively manage emissions and disclose relevant data to mitigate risks. Next, we analyze factors influencing carbon prices and review advanced prediction algorithms that optimize carbon credit purchasing strategies, reducing costs and improving efficiency. Additionally, we examine corporate carbon emission prediction models, which offer accurate performance assessments and aid in planning future carbon credit needs. By integrating carbon price and emission predictions, we propose research directions, including corporate carbon management cost forecasting. This study provides a foundation for future quantitative research on the financial and market impacts of carbon management practices and is the first systematic review focusing on computing solutions and engineering practices for carbon credits.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Assessing the Sustainability and Trustworthiness of Federated Learning Models,"Artificial intelligence (AI) increasingly influences critical decision-making across sectors. Federated Learning (FL), as a privacy-preserving collaborative AI paradigm, not only enhances data protection but also holds significant promise for intelligent network management, including distributed monitoring, adaptive control, and edge intelligence. Although the trustworthiness of FL systems has received growing attention, the sustainability dimension remains insufficiently explored, despite its importance for scalable real-world deployment. To address this gap, this work introduces sustainability as a distinct pillar within a comprehensive trustworthy FL taxonomy, consistent with AI-HLEG guidelines. This pillar includes three key aspects: hardware efficiency, federation complexity, and the carbon intensity of energy sources. Experiments using the FederatedScope framework under diverse scenarios, including varying participants, system complexity, hardware, and energy configurations, validate the practicality of the approach. Results show that incorporating sustainability into FL evaluation supports environmentally responsible deployment, enabling more efficient, adaptive, and trustworthy network services and management AI models.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Sustainable Edge Computing: Challenges and Future Directions,"An increasing amount of data is being injected into the network from IoT (Internet of Things) applications. Many of these applications, developed to improve society's quality of life, are latency-critical and inject large amounts of data into the network. These requirements of IoT applications trigger the emergence of Edge computing paradigm. Currently, data centers are responsible for a global energy use between 2% and 3%. However, this trend is difficult to maintain, as bringing computing infrastructures closer to the edge of the network comes with its own set of challenges for energy efficiency. In this paper, we propose our approach for the sustainability of future computing infrastructures to provide (i) an energy-efficient and economically viable deployment, (ii) a fault-tolerant automated operation, and (iii) a collaborative resource management to improve resource efficiency. We identify the main limitations of applying Cloud-based approaches close to the data sources and present the research challenges to Edge sustainability arising from these constraints. We propose two-phase immersion cooling, formal modeling, machine learning, and energy-centric federated management as Edge-enabling technologies. We present our early results towards the sustainability of an Edge infrastructure to demonstrate the benefits of our approach for future computing environments and deployments.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms,"We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we demonstrate that our FL-based monitoring system significantly outperforms existing methods in prediction accuracy, operational efficiency, system reliability (i.e., mean time between failures or MTBF), and social welfare maximization by the mechanism designer. Our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms. The experimental results show that SusFL significantly improves system performance, including a $10\%$ reduction in energy consumption, a $15\%$ increase in social welfare, and a $34\%$ rise in Mean Time Between Failures (MTBF), alongside a marginal increase in the global model's prediction accuracy.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
IoT on the Road to Sustainability: Vehicle or Bandit?,"The Internet of Things (IoT) can support the evolution towards a digital and green future. However, the introduction of the technology clearly has in itself a direct adverse ecological impact. This paper assesses this impact at both the IoT-node and at the network side. For the nodes, we show that the electronics production of devices comes with a carbon footprint that can be much higher than during operation phase. We highlight that the inclusion of IoT support in existing cellular networks comes with a significant ecological penalty, raising overall energy consumption by more than 15%. These results call for novel design approaches for the nodes and for early consideration of the support for IoT in future networks. Raising the 'Vehicle or bandit?' question on the nature of IoT in the broader sense of sustainability, we illustrate the need for multidisciplinary cooperation to steer applications in desirable directions.","all:""carbon footprint"" OR all:""energy efficiency"" OR all:sustainability",0
Wolfram Model and the Technological Architecture of the Fourth Industrial Revolution,"In this essay, we will defend the thesis that the multi-computational paradigm is a natural way of thinking about the fourth industrial revolution. This will be done considering the geometry that emerges as the continuum limit of multiway systems.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
History of Prime Movers and Future Implications,"Motive and electrical energy has played a crucial role in human civilization. Since Ancient times, motive energy played a primary role in agricultural and industrial production as well as transportation. At that time, motive energy was provided by work of humans and draft animals. Later, work of water and wind power was harnessed. During the 19$^{\text{th}}$ century, steam power became the main source of motive energy in USA and Britain. Modern transportation and industry depend on the work of heat engines that use fossil fuel. A brief history of different sources of energy is presented in this work. The energy consumptions in pre-industrial and industrial societies are calculated. The lost opportunities for the Second Industrial Revolution (such as fast breeder reactors and thermonuclear power stations) are discussed. The case that the Solar Power will become the main source of energy by the second half of this century is presented. It is calculated that the Solar Power has the potential to bring about the new Industrial Revolution. Based on material and energy resources available in the Solar System, it is demonstrated that the Solar System Civilization supporting a population of 10 Quadrillion with a high standard of living is possible.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Possible Material Outcomes of Solar Power Revolution,"Energy production plays a primary role in industrial capacity and thus material standard of living of any civilization. The Industrial Revolution was engendered by a vast growth of motive energy production. Solar Power Revolution has the potential of increasing global energy production by a factor of 160 and engendering a new technological revolution in industry, food production and transportation. Electric energy can be used for vast expansion of industry. Electric energy can be used to extract vast amount of hydrocarbon motor fuel and chemicals from unconventional oil resources and coal. It can also be used to produce liquid hydrocarbon fuel, chemicals, and animal feed from water and carbon dioxide. Electric energy can be used to connect the World by a network of rapid transit, such as Magnetic Levitation Trains (MagLev). The Solar Power Revolution will enable Earth to sustain a population of 50 billion at material living standards much higher than corresponding standards in USA 2020. Global civilization which harvests the maximum possible fraction of solar energy falling on Earth is called Kardashev 1 Civilization. It is also the stepping stone toward the Final Frontier of Humankind -- Kardashev 2 Civilization, which has colonized the Solar System.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A New Lens on the Sustainability of the AI Revolution,"We introduce the Economic Productivity of Energy (EPE), GDP generated per unit of energy consumed, as a quantitative lens to assess the sustainability of the Artificial Intelligence (AI) revolution. Historical evidence shows that the first industrial revolution, pre-scientific in the sense that technological adoption preceded scientific understanding, initially disrupted this ratio: EPE collapsed as profits outpaced efficiency, with poorly integrated technologies, and recovered only with the rise of scientific knowledge and societal adaptation. Later industrial revolutions, such as electrification and microelectronics, grounded in established scientific theory, did not exhibit comparable declines. Today's AI revolution, highly profitable yet energy-intensive, remains pre-scientific and may follow a similar trajectory in EPE. We combine this conceptual discussion with cross-country EPE data spanning the last three decades. We find that the advanced economies exhibit a consistent linear growth in EPE: those countries are the ones that contribute most to global GDP production and energy consumption, and are expected to be the most affected by the AI transition. Therefore, we advocate for regular monitoring of EPE: transparent reporting of AI-related energy use and productivity-linked incentives can expose hidden energy costs and prevent efficiency-blind economic expansion. Embedding EPE within sustainability frameworks would help align technological innovation with energy productivity, a critical condition for sustainable growth.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Analytical history,"The purpose of this note is to explain what is ""analytical history"", a modular and testable analysis of historical events introduced in a book published in 2002 (Roehner and Syme 2002). Broadly speaking, it is a comparative methodology for the analysis of historical events. Comparison is the keystone and hallmark of science. For instance, the extrasolar planets are crucial for understanding our own solar system. Until their discovery, astronomers could observe only one instance. Single instances can be described but they cannot be understood in a testable way. In other words, if one accepts that, as many historians say, ""historical events are unique"", then no testable understanding can be developed.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A brief social history of astrobiology in Ibero-america,"The work is divided into three sections: the first one describes the historical evolution of the main arguments presented about the plurality of inhabited worlds, from the presocratics to the birth of modern science. The second section analyzes the race to define the search for life beyond Earth as a scientific activity under a specific name. Finally, the third part presents a brief description of the social history of science that allowed the early development of astrobiology in Iberoamerica.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Demographic Confounding Causes Extreme Instances of Lifestyle Politics on Facebook,"Lifestyle politics emerge when activities that have no substantive relevance to ideology become politically aligned and polarized. Homophily and social influence are able generate these fault lines on their own; however, social identities from demographics may serve as coordinating mechanisms through which lifestyle politics are mobilized are spread. Using a dataset of 137,661,886 observations from 299,327 Facebook interests aggregated across users of different racial/ethnic, education, age, gender, and income demographics, we find that the most extreme instances of lifestyle politics are those which are highly confounded by demographics such as race/ethnicity (e.g., Black artists and performers). After adjusting political alignment for demographic effects, lifestyle politics decreased by 27.36% toward the political ""center"" and demographically confounded interests were no longer among the most polarized interests. Instead, after demographic deconfounding, we found that the most liberal interests included electric cars, Planned Parenthood, and liberal satire while the most conservative interests included the Republican Party and conservative commentators. We validate our measures of political alignment and lifestyle politics using the General Social Survey and find similar demographic entanglements with lifestyle politics existed before social media such as Facebook were ubiquitous, giving us strong confidence that our results are not due to echo chambers or filter bubbles. Likewise, since demographic characteristics exist prior to ideological values, we argue that the demographic confounding we observe is causally responsible for the extreme instances of lifestyle politics that we find among the aggregated interests. We conclude our paper by relating our results to Simpson's paradox, cultural omnivorousness, and network autocorrelation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The Grass of the Universe: Rethinking Technosphere, Planetary History, and Sustainability with Fermi Paradox","SETI is not a usual point of departure for environmental humanities. However, this paper argues that theories originating in this field have direct implications for how we think about viable inhabitation of the Earth. To demonstrate SETI's impact on environmental humanities, this paper introduces Fermi paradox as a speculative tool to probe possible trajectories of planetary history, and especially the ""Sustainability Solution"" proposed by Jacob Haqq-Misra and Seth Baum. This solution suggests that sustainable coupling between extraterrestrial intelligences and their planetary environments is the major factor in the possibility of their successful detection by remote observation. By positing that exponential growth is not a sustainable development pattern, this solution rules out space-faring civilizations colonizing solar systems or galaxies. This paper elaborates on Haqq-Misra's and Baum's arguments, and discusses speculative implications of the Sustainability Solution, thus rethinking three concepts in environmental humanities: technosphere, planetary history, and sustainability. The paper advocates that (1) technosphere is a transitory layer that shall fold back into biosphere; (2) planetary history must be understood in a generic perspective that abstracts from terrestrial particularities; and (3) sustainability is not sufficient vector of viable human inhabitation of the Earth, suggesting instead habitability and genesity as better candidates.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Researcher Population Pyramids for Tracking Global Demographic and Gender Trajectories,"The sustainability of the global academic ecosystem relies on researcher demographics and gender balance, yet assessing these dynamics in a timely manner for policy is challenging. Here, we propose a researcher population pyramids framework for tracking global demographic and gender trajectories using publication data. This framework provides a timely snapshot of historical and present demographics and gender balance, revealing three contrasting research systems: Emerging systems (e.g., Arab countries) exhibit high researcher inflows with widening gender gaps in cumulative productivity; Mature systems (e.g., the United States) show modest inflows with narrowing gender gaps; and Rigid systems (e.g., Japan) lag in both. Furthermore, by simulating future scenarios, the framework makes potential trajectories visible. If 2023 demographic patterns persist, Arab countries' systems could resemble mature or even rigid ones by 2050. Our framework provides a robust diagnostic tool for policymakers worldwide to foster sustainable talent pipelines and gender equality in academia.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Who Makes Trends? Understanding Demographic Biases in Crowdsourced Recommendations,"Users of social media sites like Facebook and Twitter rely on crowdsourced content recommendation systems (e.g., Trending Topics) to retrieve important and useful information. Contents selected for recommendation indirectly give the initial users who promoted (by liking or posting) the content an opportunity to propagate their messages to a wider audience. Hence, it is important to understand the demographics of people who make a content worthy of recommendation, and explore whether they are representative of the media site's overall population. In this work, using extensive data collected from Twitter, we make the first attempt to quantify and explore the demographic biases in the crowdsourced recommendations. Our analysis, focusing on the selection of trending topics, finds that a large fraction of trends are promoted by crowds whose demographics are significantly different from the overall Twitter population. More worryingly, we find that certain demographic groups are systematically under-represented among the promoters of the trending topics. To make the demographic biases in Twitter trends more transparent, we developed and deployed a Web-based service 'Who-Makes-Trends' at twitter-app.mpi-sws.org/who-makes-trends.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Toward a Comparative Cognitive History: Archimedes and D. H. J. Polymath,"Is collective intelligence just individual intelligence writ large, or are there fundamental differences? This position paper argues that a cognitive history methodology can shed light into the nature of collective intelligence and its differences from individual intelligence. To advance this proposed area of research, a small case study on the structure of argument and proof is presented. Quantitative metrics from network science are used to compare the artifacts of deduction from two sources. The first is the work of Archimedes of Syracuse, putatively an individual, and of other ancient Greek mathematicians. The second is work of the Polymath Project, a massively collaborative mathematics project that used blog posts and comments to prove new results in combinatorics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Report on a Boston University Conference December 7-8, 2012 on 'How Can the History and Philosophy of Science Contribute to Contemporary U.S. Science Teaching?'","This is an editorial report on the outcomes of an international conference sponsored by a grant from the National Science Foundation (NSF) (REESE-1205273) to the School of Education at Boston University and the Center for Philosophy and History of Science at Boston University for a conference titled: How Can the History and Philosophy of Science Contribute to Contemporary U.S. Science Teaching? The presentations of the conference speakers and the reports of the working groups are reviewed. Multiple themes emerged for K-16 education from the perspective of the history and philosophy of science. Key ones were that: students need to understand that central to science is argumentation, criticism, and analysis; students should be educated to appreciate science as part of our culture; students should be educated to be science literate; what is meant by the nature of science as discussed in much of the science education literature must be broadened to accommodate a science literacy that includes preparation for socioscientific issues; teaching for science literacy requires the development of new assessment tools; and, it is difficult to change what science teachers do in their classrooms. The principal conclusions drawn by the editors are that: to prepare students to be citizens in a participatory democracy, science education must be embedded in a liberal arts education; science teachers alone cannot be expected to prepare students to be scientifically literate; and, to educate students for scientific literacy will require a new curriculum that is coordinated across the humanities, history/social studies, and science classrooms.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Are Supernovae Recorded in Indigenous Astronomical Traditions?,"Novae and supernovae are rare astronomical events that would have had an influence on the sky-watching peoples who witnessed them. Although several bright novae/supernovae have been visible during recorded human history, there are many proposed but no confirmed accounts of supernovae in oral traditions or material culture. Criteria are established for confirming novae/supernovae in oral and material culture, and claims from around the world are discussed to determine if they meet these criteria. Australian Aboriginal traditions are explored for possible descriptions of novae/supernovae. Although representations of supernovae may exist in Indigenous traditions, and an account of a nova in Aboriginal traditions has been confirmed, there are currently no confirmed accounts in Indigenous oral or material traditions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Voter demographics and socio-economic factors in kinetic models for opinion formation,"Voter demographics and socio-economic factors like age, sex, ethnicity, education level, income, and other measurable factors like behaviour in previous elections or referenda are of key importance in modelling opinion formation dynamics. Here, we revisit the kinetic opinion formation model from Dring and Wright (2022) and compare in more detail the influence of different choices of characteristic demographic factors and initial conditions. The model is based on the kinetic opinion formation model by Toscani (2006) and the leader-follower model of Dring et al. (2009) which leads to a system of Fokker-Planck-type partial differential equations. In the run-up to the 2024 general election in the United Kingdom, we consider in our numerical experiments the situation in the so-called `Red Wall' in North and Midlands of England and compare our simulation results to other election forecasts.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Dreamtime Astronomy: development of a new Indigenous program at Sydney Observatory,"The Australian National Curriculum promotes Indigenous culture in school education programs. To foster a broader appreciation of cultural astronomy, to utilise the unique astronomical heritage of the site, and to develop an educational program within the framework of the National Curriculum, Sydney Observatory launched Dreamtime Astronomy, a program incorporating Australian Indigenous culture, astronomy, and Sydney's astronomical history and heritage. This paper reviews the development and implementation of this program and discusses modifications following an evaluation by schools.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Two proto-science-fiction novels written in French by 18th century women,"With Cyrano, Voltaire, and Verne, France provided important milestones in the history of early science fiction. However, even if the genre was not very common a few centuries ago, there were numerous additional contributions by French-speaking writers. In this paper, we review two cases of interplanetary novels written in the second half of the eighteenth century and sharing a rare particularity: their authors were female. Voyages de Milord Ceton was imagined by Marie-Anne de Roumier-Robert whereas Cornelie Wouters de Wasse conceived Le Char Volant. While their personal lives were very different, and their writing style too, both authors share in these novels a common philosophy in which equality -- between ranks but also between genders -- takes an important place. Their works thus clearly fit into the context of the Enlightenment.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Beyond gender: The intersectional impact of community demographics on the continuation rates of male and female students into high school physics,"This study examines the complex interplay of gender and other demographics on continuation rates in high school physics. Using a diverse dataset that combines demographics from the Canadian Census and eleven years of gendered enrolment data from the Ontario Ministry of Education, we track student cohorts as they transition from mandatory science to elective physics courses. We then employ hierarchical linear modelling to quantify the interaction effects between gender and other demographics, providing a detailed perspective on the on continuation in physics. Our results indicate the racial demographics of a school's neighbourhood have a limited impact on continuation once controlling for other factors such as socioeconomic status, though neighbourhoods with a higher Black population were a notable exception, consistently exhibiting significantly lower continuation rates for both male and female students. A potential role model effect related to parental education was also found as the proportion of parents with STEM degrees correlates positively with increased continuation, whereas an increase in non-STEM degrees corresponds with a reduced SCR. The most pronounced effects are school-level factors. Continuation rates in physics are very strongly correlated with continuation in chemistry or calculus - effects which are much stronger for male than female students. Conversely, continuation in biology positively correlates with the continuation of female students in physics, with little to no effect found for male students. Nevertheless, the effect sizes observed for chemistry and calculus markedly outweigh that for biology. This is further evidence that considering STEM as a homogeneous subject when examining gender disparities is misguided. These insights can guide future education policies and initiatives to increase continuation rates and foster greater gender equity in physics education.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A new understanding on the history of developing MRI for cancer detection,"Science is about facts and truth. Yet sometimes the truth and facts are not obvious. For example, in the field of MRI (Magnetic Resonance Imaging), there has been a long-lasting debate about who were the major contributors in its development. Particularly, there was a strong dispute between the followers of two scientists, R. Damadian and P. Lauterbur. In this review, we carefully trace the major developments in applying NMR for cancer detection starting almost 50 years ago. The research records show that the truth was beyond the claims of either research camps. The development of NMR for cancer detection involved multiple research groups, who made critical contributions at different junctures.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Climate of the Khagan. Observations on palaeo-environmental Factors of the History of the Avars (6th-9th century AD),"Based on palaeoenvironmental, historical and archaeological data, the paper proposes possible climatic impacts on the history of the Avar Khaganate, which comprised the Carpathian Basin between the late 6th and the early 9th century AD. While the establishment of the Avars in East Central Europe took place within a period characterised by cold and dry climatic conditions (recently identified as Late Antique Little Ice Age), more stable climatic parameters may have favoured the stabilisation of Avar rule after a crisis in the aftermath of 626 AD. Data indicates growth of settlement and agricultural activity up to the mid-8th century. These developments did not necessarily strengthen central power, but may have contributed to a greater autonomy of various groups on the basis of increased resources. The Khaganate quickly disintegrated faced by the Carolingian advance of the 790s; the last decades of documented Avar presence were again accompanied by environmental vicissitudes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Sensing the Pulse of the Pandemic: Geovisualizing the Demographic Disparities of Public Sentiment toward COVID-19 through Social Media,"Social media offers a unique lens to observe large-scale, spatial-temporal patterns of users reactions toward critical events. However, social media use varies across demographics, with younger users being more prevalent compared to older populations. This difference introduces biases in data representativeness, and analysis based on social media without proper adjustment will lead to overlooking the voices of digitally marginalized communities and inaccurate estimations. This study explores solutions to pinpoint and alleviate the demographic biases in social media analysis through a case study estimating the public sentiment about COVID-19 using Twitter data. We analyzed the pandemic-related Twitter data in the U.S. during 2020-2021 to (1) elucidate the uneven social media usage among demographic groups and the disparities of their sentiments toward COVID-19, (2) construct an adjusted public sentiment measurement based on social media, the Sentiment Adjusted by Demographics (SAD) index, to evaluate the spatiotemporal varying public sentiment toward COVID-19. The results show higher proportions of female and adolescent Twitter users expressing negative emotions to COVID-19. The SAD index unveils that the public sentiment toward COVID-19 was most negative in January and February 2020 and most positive in April 2020. Vermont and Wyoming were the most positive and negative states toward COVID-19.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modeling a demographic problem using the Leslie matrix,"The application of Leslie matrices in demographic research is considered in this paper. The Leslie matrix is first proposed in the 1940s and gained popularity in the mid-1960s, becoming fundamental tool for predicting population dynamics. The Leslie matrix allows to categorize individuals based on various attributes and calculate the expected population sizes for various demographic categories in subsequent time intervals. The universality of the Leslie matrix extends to diverse life cycles in plants and animals, making it ubiquitous tool in non-human species. In the paper is presented detailed application of Leslie matrices to the problem of the two countries, demonstrating their practical value in solving real demographic problems. In conclusion, the Leslie matrix remains a cornerstone of demographic analysis, reflecting the complexity of population dynamics and providing a robust framework for understanding the intricate interplay of factors shaping human society. Its enduring relevance and adaptability make it an essential component in the toolkit of demographers and ecologists.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
History effects on network growth,"Growth dynamic of real networks because of emerging complexities is an open and interesting question. Indeed it is not realistic to ignore history impact on the current events. The mystery behind that complexity could be in the role of history in some how. To regard this point, the average effect of history has been included by a kernel function in differential equation of Barabasi Albert (BA) model . This approach leads to a fractional order BA differential equation as a generalization of BA model. As opposed to unlimited growth for degree of nodes, our results show that over time the memory impact will cause a decay for degrees. This gives a higher chance to younger members for turning to a hub. In fact in a real network, there are two competitive processes. On one hand, based on preferential attachment mechanism nodes with higher degree are more likely to absorb links. On the other hand, node history through aging process prevents new connections. Our findings from simulating a network grown by considering these effects also from studying a real network of collaboration between Hollywood movie actors conforms the results and significant effects of history and time on dynamic.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Choosing among alternative histories of a tree,"The structure of an evolving network contains information about its past. Extracting this information efficiently, however, is, in general, a difficult challenge. We formulate a fast and efficient method to estimate the most likely history of growing trees, based on exact results on root finding. We show that our linear-time algorithm produces the exact stepwise most probable history in a broad class of tree growth models. Our formulation is able to treat very large trees and therefore allows us to make reliable numerical observations regarding the possibility of root inference and history reconstruction in growing trees. We obtain the general formula $\langle \ln \mathcal{N} \rangle \cong N \ln N - cN$ for the size-dependence of the mean logarithmic number of possible histories of a given tree, a quantity that largely determines the reconstructability of tree histories. We also reveal an uncertainty principle: a relationship between the inferrability of the root and that of the complete history, indicating that there is a tradeoff between the two tasks; the root and the complete history cannot both be inferred with high accuracy at the same time.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A conjecture on demographic mortality at high ages,"The possibility of modeling and therefore predicting the trend of demographic mortality is of great scientific and social interest. The article presents and discusses the hypothesis that the demographic distribution of mortality in advanced ages converges asymptotically to an S-system distribution as lifespan increases. The statistical distribution of the S-system was introduced by the author in a 2022 paper and was derived by applying the methods of Fermi statistics to a cellular automaton acting as an ""arbitrary oscillator"". This distribution is here recalled and formalized analytically and its characteristics are described. The conjecture is based on two case studies: mortality in the United States from 1900 to 2017 and mortality in Italy from 1974 to 2019. The conjecture, applied to both case studies, appears reasonable. Tables and comparison figures are provided to support this. Finally, an attempt to predict demographic mortality behavior and limitations for the years to come is provided.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Once Upon a Crime: Towards Crime Prediction from Demographics and Mobile Data,"In this paper, we present a novel approach to predict crime in a geographic space from multiple data sources, in particular mobile phone and demographic data. The main contribution of the proposed approach lies in using aggregated and anonymized human behavioral data derived from mobile network activity to tackle the crime prediction problem. While previous research efforts have used either background historical knowledge or offenders' profiling, our findings support the hypothesis that aggregated human behavioral data captured from the mobile network infrastructure, in combination with basic demographic information, can be used to predict crime. In our experimental results with real crime data from London we obtain an accuracy of almost 70% when predicting whether a specific area in the city will be a crime hotspot or not. Moreover, we provide a discussion of the implications of our findings for data-driven crime analysis.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The verification of virtual community members socio-demographic profile,"This article considers the current problem of investigation and development of the method of web-members' socio-demographic characteristics' profile validation based on analysis of socio-demographic characteristics. The topicality of the paper is determined by the necessity to identify the web-community member by means of computer-linguistic analysis of their information track (all information about web-community members, which posted on the Internet). The formal model of basic socio-demographic characteristics of virtual communities' member is formed. The algorithm of these characteristics verification is developed.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Sicily and the development of Econophysics: the pioneering work of Ettore Majorana and the Econophysics Workshop in Palermo,"Sicily has played an important role in the development of the new research area named ""Econophysics"". In fact some key ideas supporting this new hybrid discipline were originally formulated in a pioneering work of the Sicilian born physicist Ettore Majorana. The article he wrote was entitled ""The value of statistical laws in physics and social sciences"". I will discuss its origin and history that has been recently discovered in the study of Stefano Roncoroni. This recent study documents the true reasons and motivations that triggered the pioneering work of Majorana. It also shows that the description of this work provided by Edoardo Amaldi was shallow and misleading. In the second part of the talk I will recollect the first years of development of econophysics and in particular the role of the ""International Workshop on Econophysics and Statistical Finance"" held in Palermo on 28-30 September 1998 and the setting in 1999 of the ""Observatory of Complex Systems"" the research group on Econophysics of Palermo University and Istituto Nazionale di Fisica della Materia.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Figuring Out Art History,"World population and the number of cultural artifacts are growing exponentially or faster, while cultural interaction approaches the fidelity of a global nervous system. Every day hundreds of millions of images are loaded into social networks by users all over the world. As this myriad of new artifacts veils the view into the past, like city lights covering the night sky, it is easy to forget that there is more than one Starry Night, the painting by Van Gogh. Like in ecology, where saving rare species may help us in treating disease, art and architectural history can reveal insights into the past, which may hold keys to our own future. With humanism under threat, facing the challenge of understanding the structure and dynamics of art and culture, both qualitatively and quantitatively, is more crucial now than it ever was. The purpose of this article is to provide perspective in the aim of figuring out the process of art history - not art history as a discipline, but the actual history of all made things, in the spirit of George Kubler and Marcel Duchamp. In other words, this article deals with the grand challenge of developing a systematic science of art and culture, no matter what, and no matter how.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The Geography of Happiness: Connecting Twitter sentiment and expression, demographics, and objective characteristics of place","We conduct a detailed investigation of correlations between real-time expressions of individuals made across the United States and a wide range of emotional, geographic, demographic, and health characteristics. We do so by combining (1) a massive, geo-tagged data set comprising over 80 million words generated over the course of several recent years on the social network service Twitter and (2) annually-surveyed characteristics of all 50 states and close to 400 urban populations. Among many results, we generate taxonomies of states and cities based on their similarities in word use; estimate the happiness levels of states and cities; correlate highly-resolved demographic characteristics with happiness levels; and connect word choice and message length with urban characteristics such as education levels and obesity rates. Our results show how social media may potentially be used to estimate real-time levels and changes in population-level measures such as obesity rates.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Emotions, Demographics and Sociability in Twitter Interactions","The social connections people form online affect the quality of information they receive and their online experience. Although a host of socioeconomic and cognitive factors were implicated in the formation of offline social ties, few of them have been empirically validated, particularly in an online setting. In this study, we analyze a large corpus of geo-referenced messages, or tweets, posted by social media users from a major US metropolitan area. We linked these tweets to US Census data through their locations. This allowed us to measure emotions expressed in the tweets posted from an area, the structure of social connections, and also use that area's socioeconomic characteristics in analysis. %We extracted the structure of online social interactions from the people mentioned in tweets from that area. We find that at an aggregate level, places where social media users engage more deeply with less diverse social contacts are those where they express more negative emotions, like sadness and anger. Demographics also has an impact: these places have residents with lower household income and education levels. Conversely, places where people engage less frequently but with diverse contacts have happier, more positive messages posted from them and also have better educated, younger, more affluent residents. Results suggest that cognitive factors and offline characteristics affect the quality of online interactions. Our work highlights the value of linking social media data to traditional data sources, such as US Census, to drive novel analysis of online behavior.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
How big does a population need to be before demographers can ignore individual-level randomness in demographic events?,"When studying a national-level population, demographers can safely ignore the effect of individual-level randomness on age-sex structure. When studying a single community, or group of communities, however, the potential importance of individual-level randomness is less clear. We seek to measure the effect of individual-level randomness in births and deaths on standard summary indicators of age-sex structure, for populations of different sizes, focusing on on demographic conditions typical of historical populations. We conduct a microsimulation experiment where we simulate events and age-sex structure under a range of settings for demographic rates and population size. The experiment results suggest that individual-level randomness strongly affects age-sex structure for populations of about 100, but has a much smaller effect on populations of 1,000, and a negligible effect on populations of 10,000. Our conclusion is that analyses of age-sex structure in historical populations with sizes on the order 100 must account for individual-level randomness in demographic events. Analyses of populations with sizes on the order of 1,000 may need to make some allowance for individual-level variation, but other issues, such as measurement error, probably deserve more attention. Analyses of populations of 10,000 can safely ignore individual-level variation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Memory endowed US cities and their demographic interactions,"A quantitative understanding of cities' demographic dynamics is becoming a potentially useful tool for planning sustainable growth. The concomitant theory should reveal details of the cities' past and also of its interaction with nearby urban conglomerates for providing a reasonably complete picture. Using the exhaustive database of the Census Bureau in a time window of 170 years, we exhibit here empirical evidence for time and space correlations in the demographic dynamics of US counties, with a characteristic memory-time of 25 years and typical distances of interaction of 200 km. These correlations are much larger than those observed in an European country (Spain), giving to the US a more coherent evolution. We also measure the resilience of US cities to historical events, finding a demographical post-traumatic amnesia after wars (as the Civil War) or economic crisis (as the 1929 Stock Market Crash).","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Edge interventions can mitigate demographic and prestige disparities in the Computer Science coauthorship network,"Social factors such as demographic traits and institutional prestige structure the creation and dissemination of ideas in academic publishing. One place these effects can be observed is in how central or peripheral a researcher is in the coauthorship network. Here we investigate inequities in network centrality in a hand-collected data set of 5,670 U.S.-based faculty employed in Ph.D.-granting Computer Science departments and their DBLP coauthorship connections. We introduce algorithms for combining name- and perception-based demographic labels by maximizing alignment with self-reported demographics from a survey of faculty from our census. We find that women and individuals with minoritized race identities are less central in the computer science coauthorship network, implying worse access to and ability to spread information. Centrality is also highly correlated with prestige, such that faculty in top-ranked departments are at the core and those in low-ranked departments are in the peripheries of the computer science coauthorship network. We show that these disparities can be mitigated using simulated edge interventions, interpreted as facilitated collaborations. Our intervention increases the centrality of target individuals, chosen independently of the network structure, by linking them with researchers from highly ranked institutions. When applied to scholars during their Ph.D., the intervention also improves the predicted rank of their placement institution in the academic job market. This work was guided by an ameliorative approach: uncovering social inequities in order to address them. By targeting scholars for intervention based on institutional prestige, we are able to improve their centrality in the coauthorship network that plays a key role in job placement and longer-term academic success.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Scaling of variability measures in hierarchical demographic data,"Demographic heterogeneity is often studied through the geographical lens. Therefore it is considered at a predetermined spatial resolution, which is a suitable choice to understand scalefull phenomena. Spatial autocorrelation indices are well established for this purpose. Yet complex systems are often scale-free, and thus studying the scaling behavior of demographic heterogeneity may provide valuable insights. Furthermore, migration processes are not necessarily influenced by the physical landscape, which is accounted for by the spatial autocorrelation indices. The migration process may be more influenced by the socio-economic landscape, which is better reflected by the hierarchical demographic data. Here we explore the scaling behavior of variability measures in the United Kingdom 2011 census data set. As expected, all of the considered variability measures decrease as the hierarchical scale becomes coarser. Though the non-monotonicity is observed, it can be explained by accounting for the imperfect hierarchical relationships. We show that the scaling behavior of variability measures can be qualitatively understood in terms of Schelling's segregation model and Kawasaki-Ising","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Generating online social networks based on socio-demographic attributes,"Recent years have seen tremendous growth of many online social networks such as Facebook, LinkedIn and MySpace. People connect to each other through these networks forming large social communities providing researchers rich datasets to understand, model and predict social interactions and behaviors. New contacts in these networks can be formed due to an individual's demographic attributes such as age group, gender, geographic location, or due to a network's structural dynamics such as triadic closure and preferential attachment, or a combination of both demographic and structural characteristics.   A number of network generation models have been proposed in the last decade to explain the structure, evolution and processes taking place in different types of networks, and notably social networks. Network generation models studied in the literature primarily consider structural properties, and in some cases an individual's demographic profile in the formation of new social contacts. These models do not present a mechanism to combine both structural and demographic characteristics for the formation of new links. In this paper, we propose a new network generation algorithm which incorporates both these characteristics to model network formation. We use different publicly available Facebook datasets as benchmarks to demonstrate the correctness of the proposed network generation model. The proposed model is flexible and thus can generate networks with varying demographic and structural properties.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Phase transition in the recoverability of network history,"Network growth processes can be understood as generative models of the structure and history of complex networks. This point of view naturally leads to the problem of network archaeology: reconstructing all the past states of a network from its structure---a difficult permutation inference problem. In this paper, we introduce a Bayesian formulation of network archaeology, with a generalization of preferential attachment as our generative mechanism. We develop a sequential Monte Carlo algorithm to evaluate the posterior averages of this model, as well as an efficient heuristic that uncovers a history well correlated with the true one, in polynomial time. We use these methods to identify and characterize a phase transition in the quality of the reconstructed history, when they are applied to artificial networks generated by the model itself. Despite the existence of a no-recovery phase, we find that nontrivial inference is possible in a large portion of the parameter space as well as on empirical data.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Two universal pathways in demographic transition,"Demographic transition, characterized by declines in fertility and mortality, is a global phenomenon associated with modernization. While typical patterns of fertility decline are observed in Western countries, their applicability to other regions and the underlying mechanisms remain unclear. By analyzing demographic data from 195 countries over 200 years, this study identifies two universal pathways in the changes in the crude birth rate (i.e., births per 1,000 individuals ) and life expectancy at birth ({e_0}), characterized by the conservation of either {e0} or {\exp(e_0/18)}. These pathways define two distinct phases governed by different mechanisms. Phase I, characterized by the conservation of {e_0}, dominated until the mid-20th century, with high child mortality and steady population growth. In contrast, Phase II, conserving {\exp(e_0/18)}, has prevailed since 1950, featuring low child mortality and steady GDP per capita growth. A theoretical model considering the trade-off between reproduction and education elucidates the transition between these phases, demonstrating that population size is prioritized in Phase I, while productivity is maximized in Phase II. Modernization processes, such as declining educational costs and increasing social mobility, are identified as key accelerators of the transition to Phase II. The findings suggest that reducing educational costs can foster fertility recovery without compromising educational standards, offering potential policy interventions. This study provides a novel theoretical framework for understanding demographic transition by applying principles from statistical physics to uncover universal macroscopic laws and their underlying mechanisms.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Evidence of Demographic rather than Ideological Segregation in News Discussion on Reddit,"We evaluate homophily and heterophily among ideological and demographic groups in a typical opinion formation context: online discussions of current news. We analyze user interactions across five years in the r/news community on Reddit, one of the most visited websites in the United States. Then, we estimate demographic and ideological attributes of these users. Thanks to a comparison with a carefully-crafted network null model, we establish which pairs of attributes foster interactions and which ones inhibit them.   Individuals prefer to engage with the opposite ideological side, which contradicts the echo chamber narrative. Instead, demographic groups are homophilic, as individuals tend to interact within their own group - even in an online setting where such attributes are not directly observable. In particular, we observe age and income segregation consistently across years: users tend to avoid interactions when belonging to different groups. These results persist after controlling for the degree of interest by each demographic group in different news topics. Our findings align with the theory that affective polarization - the difficulty in socializing across political boundaries-is more connected with an increasingly divided society, rather than ideological echo chambers on social media.   We publicly release our anonymized data set and all the code to reproduce our results: https://github.com/corradomonti/demographic-homophily","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Analyzing User Activities, Demographics, Social Network Structure and User-Generated Content on Instagram","Instagram is a relatively new form of communication where users can instantly share their current status by taking pictures and tweaking them using filters. It has seen a rapid growth in the number of users as well as uploads since it was launched in October 2010. Inspite of the fact that it is the most popular photo sharing application, it has attracted relatively less attention from the web and social media research community. In this paper, we present a large-scale quantitative analysis on millions of users and pictures we crawled over 1 month from Instagram. Our analysis reveals several insights on Instagram which were never studied before: 1) its social network properties are quite different from other popular social media like Twitter and Flickr, 2) people typically post once a week, and 3) people like to share their locations with friends. To the best of our knowledge, this is the first in-depth analysis of user activities, demographics, social network structure and user-generated content on Instagram.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A Brief History of Economics: An Outsider's Account,"A dangerously brief history of the developments of the main ideas in economics, as observed by a physicist, is given. This was published in 'Econophysics of Stock and Other Markets', Eds. A. Chatterjee, B. K. Chakrabarti, New Economic Windows Series, Springer, Milan, 2006, pp~219-224.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Explore Spatiotemporal and Demographic Characteristics of Human Mobility via Twitter: A Case Study of Chicago,"Characterizing human mobility patterns is essential for understanding human behaviors and the interactions with socioeconomic and natural environment. With the continuing advancement of location and Web 2.0 technologies, location-based social media (LBSM) have been gaining widespread popularity in the past few years. With an access to locations of users, profiles and the contents of the social media posts, the LBSM data provided a novel modality of data source for human mobility study. By exploiting the explicit location footprints and mining the latent demographic information implied in the LBSM data, the purpose of this paper is to investigate the spatiotemporal characteristics of human mobility with a particular focus on the impact of demography. We first collect geo-tagged Twitter feeds posted in the conterminous United States area, and organize the collection of feeds using the concept of space-time trajectory corresponding to each Twitter user. Commonly human mobility measures, including detected home and activity centers, are derived for each user trajectory. We then select a subset of Twitter users that have detected home locations in the city of Chicago as a case study, and apply name analysis to the names provided in user profiles to learn the implicit demographic information of Twitter users, including race/ethnicity, gender and age. Finally we explore the spatiotemporal distribution and mobility characteristics of Chicago Twitter users, and investigate the demographic impact by comparing the differences across three demographic dimensions (race/ethnicity, gender and age). We found that, although the human mobility measures of different demographic groups generally follow the generic laws (e.g., power law distribution), the demographic information, particular the race/ethnicity group, significantly affects the urban human mobility patterns.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Religion-based Urbanization Process in Italy: Statistical Evidence from Demographic and Economic Data,"This paper analyzes some economic and demographic features of Italians living in cities containing a Saint name in their appellation (hagiotoponyms). Demographic data come from the surveys done in the 15th (2011) Italian Census, while the economic wealth of such cities is explored through their recent [2007-2011] aggregated tax income (ATI). This cultural problem is treated from various points of view. First, the exact list of hagiotoponyms is obtained through linguistic and religiosity criteria. Next, it is examined how such cities are distributed in the Italian regions. Demographic and economic perspectives are also offered at the Saint level, i.e. calculating the cumulated values of the number of inhabitants and the ATI, ""per Saint"", as well as the corresponding relative values taking into account the Saint popularity. On one hand, frequency-size plots and cumulative distribution function plots, and on the other hand, scatter plots and rank-size plots between the various quantities are shown and discussed in order to find the importance of correlations between the variables. It is concluded that rank-rank correlations point to a strong Saint effect, which explains what actually Saint-based toponyms imply in terms of comparing economic and demographic data.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Network analysis of graduate program support structures through experiences of various demographic groups,"Physics graduate studies are substantial efforts on the part of individual students, departments, and institutions of higher education. Understanding the factors that lead to student success and attrition is crucial for improving these programs. One factor that has recently started to be investigated is the broadly defined students' experiences related to support structures. The Aspects of Student Experience Scale (ASES), a Likert-style survey, was developed by researchers to do just that. In this study, we leverage the network approach for Likert-style surveys (NALS) methodology to provide a unique interpretation of responses to the ASES instrument for well-defined demographic groups. We confirm the validity of our findings by studying the stability of the NALS themes and investigating how they are expressed within demographic-based networks. We find that for all four themes in the original ASES study, certain thematic trends capturing students' experiences vary across the demographic-based networks in meaningful ways. We also reveal that for some demographic groups, there is an interesting interplay between, and mixing of, the original themes. Finally, our study showcases how NALS can be applied to other Likert-style datasets.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Diverse local epidemics reveal the distinct effects of population density, demographics, climate, depletion of susceptibles, and intervention in the first wave of COVID-19 in the United States","The SARS-CoV-2 pandemic has caused significant mortality and morbidity worldwide, sparing almost no community. As the disease will likely remain a threat for years to come, an understanding of the precise influences of human demographics and settlement, as well as the dynamic factors of climate, susceptible depletion, and intervention, on the spread of localized epidemics will be vital for mounting an effective response. We consider the entire set of local epidemics in the United States; a broad selection of demographic, population density, and climate factors; and local mobility data, tracking social distancing interventions, to determine the key factors driving the spread and containment of the virus. Assuming first a linear model for the rate of exponential growth (or decay) in cases/mortality, we find that population-weighted density, humidity, and median age dominate the dynamics of growth and decline, once interventions are accounted for. A focus on distinct metropolitan areas suggests that some locales benefited from the timing of a nearly simultaneous nationwide shutdown, and/or the regional climate conditions in mid-March; while others suffered significant outbreaks prior to intervention. Using a first-principles model of the infection spread, we then develop predictions for the impact of the relaxation of social distancing and local climate conditions. A few regions, where a significant fraction of the population was infected, show evidence that the epidemic has partially resolved via depletion of the susceptible population (i.e., ""herd immunity""), while most regions in the United States remain overwhelmingly susceptible. These results will be important for optimal management of intervention strategies, which can be facilitated using our online dashboard.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Analysis of COVID-19 first wave in the US based on demographic, mobility, and environmental variables","COVID-19 had a strong and disruptive impact on our society, and yet further analyses on most relevant factors explaining the spread of the pandemic are needed. Interdisciplinary studies linking epidemiological, mobility, environmental, and socio-demographic data analysis can help understanding how historical conditions, concurrent social policies and environmental factors impacted on the evolution of the pandemic crisis. This work deals with a regression analysis linking COVID-19 mortality to socio-demographic, mobility, and environmental data in the US during the first half of 2020, i.e., during the COVID-19 pandemic first wave. This study can provide very useful insights about risk factors enhancing mortality rates before non-pharmaceutical interventions or vaccination campaigns took place. Our cross-sectional ecological regression analysis demonstrates that, when considering the entire US area, the socio-demographic variables globally play the most important role with respect to environmental and mobility variables in describing COVID-19 mortality. Compared to the complete generalized linear model considering all socio-demographic, mobility, and environmental data, the regression based only on socio-demographic data provides a better approximation and proves to be a better explanatory model when compared to the mobility-based and environmental-based models. However, when looking at single entries within each of the three groups, we see that the mobility data can become relevant descriptive predictors at local scale, as in New Jersey where the time spent at work is one of the most relevant explanatory variables, while environmental data play contradictory roles.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Demographic growth and the distribution of language sizes,"It is argued that the present log-normal distribution of language sizes is, to a large extent, a consequence of demographic dynamics within the population of speakers of each language. A two-parameter stochastic multiplicative process is proposed as a model for the population dynamics of individual languages, and applied over a period spanning the last ten centuries. The model disregards language birth and death. A straightforward fitting of the two parameters, which statistically characterize the population growth rate, predicts a distribution of language sizes in excellent agreement with empirical data. Numerical simulations, and the study of the size distribution within language families, validate the assumptions at the basis of the model.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Virtual water controlled demographic growth of nations,"Population growth is in general constrained by food production, which in turn depends on the access to water resources. At a country level, some populations use more water than they control because of their ability to import food and the virtual water required for its production. Here, we investigate the dependence of demographic growth on available water resources for exporting and importing nations. By quantifying the carrying capacity of nations based on calculations of the virtual water available through the food trade network, we point to the existence of a global water unbalance. We suggest that current export rates will not be maintained and consequently we question the long-run sustainability of the food trade system as a whole. Water rich regions are likely to soon reduce the amount of virtual water they export, thus leaving import-dependent regions without enough water to sustain their populations. We also investigate the potential impact of possible scenarios that might mitigate these effects through (1) cooperative interactions among nations whereby water rich countries maintain a tiny fraction of their food production available for export; (2) changes in consumption patterns; and (3) a positive feedback between demographic growth and technological innovations. We find that these strategies may indeed reduce the vulnerability of water-controlled societies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Why modeling? The visual as a reflection of intellectual perspectives in medieval history,"This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history, despite their shared commitment to describing dynamic processes and changes of state. Yet, while historians have already digitized a large part of the cultural heritage from Antiquity to the 10th-13th centuries, exploration of this corpus using visualizations remains limited. There is therefore untapped potential in this field.This article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history. It examines two distinct intellectual approaches: 1. the use of visuals to support a scientific discourse (majority) and 2. the construction of a historical discourse based on observations made from visual figures with the aim of modeling phenomena invisible to the naked eye. The author thus examines the use of ''images'' in medievalism, focusing on the annual volumes of the Soci{}t{} des historiens m{}di{}vistes de l'enseignement sup{}rieur (SHMESP), up to 2006. Two other parts of the text look at the still-rare forms of visual representation in medieval history, particularly those with a ''heuristic vocation'', using iconographic objects, parchments, buildings and digitized texts. The article suggests various visualization techniques, such as network analysis, the creation of ''stemmas 2.0'' and interactive chronologies, which could benefit the discipline. These methods could potentially profoundly change our understanding of ancient societies, by showing the dynamic relationships between different aspects of these societies. One of the most important advances expected from these visual methods is a better understanding of the patterns of development in medieval Europe, which varied from region to region. The hypothesis is that the scarcity of heuristic graphics in medieval history stems from the relationship with ancient documents and the historical method based on narration and exemplarity. The article thus questions the value of ''visual modelling'' in medieval history, and highlights the challenges associated with the widespread adoption of this approach in the humanities and social sciences. Finally, the text invites us to reflect on the nature and functioning of heuristic visual devices, by comparing medieval ''images'' and contemporary scientific visuals. In both cases, the point is to materialize the invisible in order to show something that exists beyond the visual. The author suggests that this way of approaching visuals could play a growing role in the decades to come, particularly in the field of data science.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Link-centric analysis of variation by demographics in mobile phone communication patterns,"We present a link-centric approach to study variation in the mobile phone communication patterns of individuals. Unlike most previous research on call detail records that focused on the variation of phone usage across individual users, we examine how the calling and texting patterns obtained from call detail records vary among pairs of users and how these patterns are affected by the nature of relationships between users. To demonstrate this link-centric perspective, we extract factors that contribute to the variation in the mobile phone communication patterns and predict demographics-related quantities for pairs of users. The time of day and the channel of communication (calls or texts) are found to explain most of the variance among pairs that frequently call each other. Furthermore, we find that this variation can be used to predict the relationship between the pairs of users, as inferred from their age and gender, as well as the age of the younger user in a pair. From the classifier performance across different age and gender groups as well as the inherent class overlap suggested by the estimate of the bounds of the Bayes error, we gain insights into the similarity and differences of communication patterns across different relationships.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Race, Religion and the City: Twitter Word Frequency Patterns Reveal Dominant Demographic Dimensions in the United States","Recently, numerous approaches have emerged in the social sciences to exploit the opportunities made possible by the vast amounts of data generated by online social networks (OSNs). Having access to information about users on such a scale opens up a range of possibilities, all without the limitations associated with often slow and expensive paper-based polls. A question that remains to be satisfactorily addressed, however, is how demography is represented in the OSN content? Here, we study language use in the US using a corpus of text compiled from over half a billion geo-tagged messages from the online microblogging platform Twitter. Our intention is to reveal the most important spatial patterns in language use in an unsupervised manner and relate them to demographics. Our approach is based on Latent Semantic Analysis (LSA) augmented with the Robust Principal Component Analysis (RPCA) methodology. We find spatially correlated patterns that can be interpreted based on the words associated with them. The main language features can be related to slang use, urbanization, travel, religion and ethnicity, the patterns of which are shown to correlate plausibly with traditional census data. Our findings thus validate the concept of demography being represented in OSN language use and show that the traits observed are inherently present in the word frequencies without any previous assumptions about the dataset. Thus, they could form the basis of further research focusing on the evaluation of demographic data estimation from other big data sources, or on the dynamical processes that result in the patterns found here.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Effects of the distant population density on spatial patterns of demographic dynamics,"Spatiotemporal patterns of population changes within and across countries have various implications. Different geographical, demographic and econo-societal factors seem to contribute to migratory decisions made by individual inhabitants. Focussing on internal (i.e., domestic) migration, we ask whether individuals may take into account the information on the population density in distant locations to make migratory decisions. We analyse population census data in Japan recorded with a high spatial resolution (i.e., cells of size 500 m $\times$ 500 m) for the entirety of the country and simulate demographic dynamics induced by the gravity model and its variants. We show that, in the census data, the population growth rate in a cell is positively correlated with the population density in nearby cells up to a radius of 20 km as well as that of the focal cell. The ordinary gravity model does not capture this empirical observation. We then show that the empirical observation is better accounted for by extensions of the gravity model such that individuals are assumed to perceive the attractiveness, approximated by the population density, of the source or destination cell of migration as the spatial average over a radius of $\approx 1$ km.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Biographical Social Networks on Wikipedia - A cross-cultural study of links that made history,"It is arguable whether history is made by great men and women or vice versa, but undoubtably social connections shape history. Analysing Wikipedia, a global collective memory place, we aim to understand how social links are recorded across cultures. Starting with the set of biographies in the English Wikipedia we focus on the networks of links between these biographical articles on the 15 largest language Wikipedias. We detect the most central characters in these networks and point out culture-related peculiarities. Furthermore, we reveal remarkable similarities between distinct groups of language Wikipedias and highlight the shared knowledge about connections between persons across cultures.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A demographic microsimulation model with an integrated household alignment method,"Many dynamic microsimulation models have shown their ability to reasonably project detailed population and households using non-data based household formation and dissolution rules. Although, those rules allow modellers to simplify changes in the household construction, they typically fall short in replicating household projections or if applied retrospectively the observed household numbers. Consequently, such models with biased estimation for household size and other household related attributes lose their usefulness in applications that are sensitive to household size, such as in travel demand and housing demand modelling. Nonetheless, these demographic microsimulation models with their associated shortcomings have been commonly used to assess various planning policies which can result in misleading judgements. In this paper, we contribute to the literature of population microsimulation by introducing a fully integrated system of models for different life event where a household alignment method adjusts household size distribution to closely align with any given target distribution. Furthermore, some demographic events that are generally difficult to model, such as incorporating immigrant families into a population, can be included. We illustrated an example of the household alignment method and put it to test in a dynamic microsimulation model that we developed using dymiumCore, a general-purpose microsimulation toolkit in R, to show potential improvements and weaknesses of the method. The implementation of this model has been made publicly available on GitHub.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Fermi statistics method applied to model macroscopic demographic data,"The paper presents a recursive function able to mimic demographic mortality curves. This function is not a fitting algorithm and depends only from one parameter that has a precise meaning in a cellular automaton model. This model is also presented. For the function definition, the Fermi statistics method of calculation has been used, resulting in similarities with known statistical distribution curves. A continuous representation of the recursive equations is also provided. Implications with life span and more general life cycle concepts are outlined. A correlation with a more recent study using a multi-omics approach is pointed out.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Interplay of Demographic Variables and Social Distancing Scores in Deep Prediction of U.S. COVID-19 Cases,"With the severity of the COVID-19 outbreak, we characterize the nature of the growth trajectories of counties in the United States using a novel combination of spectral clustering and the correlation matrix. As the U.S. and the rest of the world are experiencing a severe second wave of infections, the importance of assigning growth membership to counties and understanding the determinants of the growth are increasingly evident. Subsequently, we select the demographic features that are most statistically significant in distinguishing the communities. Lastly, we effectively predict the future growth of a given county with an LSTM using three social distancing scores. This comprehensive study captures the nature of counties' growth in cases at a very micro-level using growth communities, demographic factors, and social distancing performance to help government agencies utilize known information to make appropriate decisions regarding which potential counties to target resources and funding to.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Optimizing SMS Reminder Campaigns for Pre- and Post-Diagnosis Cancer Check-Ups using Socio-Demographics: An In-Silco Investigation Into Bladder Cancer,"Timely pre- and post-diagnosis check-ups are critical for cancer patients, across all cancer types, as these often lead to better outcomes. Several socio-demographic properties have been identified as strongly connected with both cancer's clinical dynamics and (indirectly) with different individual check-up behaviors. Unfortunately, existing check-up policies typically consider only the former association explicitly. In this work, we propose a novel framework, accompanied by a high-resolution computer simulation, to investigate and optimize socio-demographic-based SMS reminder campaigns for cancer check-ups. We instantiate our framework and simulation for the case of bladder cancer, the 10th most prevalent cancer today, using extensive real-world data. Our results indicate that optimizing an SMS reminder campaign based solely on simple socio-demographic features can bring about a statistically significant reduction in mortality rate compared to alternative campaigns by up to 5.8%.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Inference of Demographic Attributes based on Mobile Phone Usage Patterns and Social Network Topology,"Mobile phone usage provides a wealth of information, which can be used to better understand the demographic structure of a population. In this paper, we focus on the population of Mexican mobile phone users. We first present an observational study of mobile phone usage according to gender and age groups. We are able to detect significant differences in phone usage among different subgroups of the population. We then study the performance of different machine learning (ML) methods to predict demographic features (namely, age and gender) of unlabeled users by leveraging individual calling patterns, as well as the structure of the communication graph. We show how a specific implementation of a diffusion model, harnessing the graph structure, has significantly better performance over other node-based standard ML methods. We provide details of the methodology together with an analysis of the robustness of our results to changes in the model parameters. Furthermore, by carefully examining the topological relations of the training nodes (seed nodes) to the rest of the nodes in the network, we find topological metrics which have a direct influence on the performance of the algorithm.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Urban Analytics: History, Trajectory, and Critique","Urban analytics combines spatial analysis, statistics, computer science, and urban planning to understand and shape city futures. While it promises better policymaking insights, concerns exist around its epistemological scope and impacts on privacy, ethics, and social control. This chapter reflects on the history and trajectory of urban analytics as a scholarly and professional discipline. In particular, it considers the direction in which this field is going and whether it improves our collective and individual welfare. It first introduces early theories, models, and deductive methods from which the field originated before shifting toward induction. It then explores urban network analytics that enrich traditional representations of spatial interaction and structure. Next it discusses urban applications of spatiotemporal big data and machine learning. Finally, it argues that privacy and ethical concerns are too often ignored as ubiquitous monitoring and analytics can empower social repression. It concludes with a call for a more critical urban analytics that recognizes its epistemological limits, emphasizes human dignity, and learns from and supports marginalized communities.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Reconstructing North Korea's Plutonium Production History with Bayesian Inference-Based Reprocessing Waste Analysis,"Although North Korea's nuclear program has been the subject of extensive scrutiny, estimates of its fissile material stockpiles remain fraught with uncertainty. In potential future disarmament agreements, inspectors may need to use nuclear archaeology methods to verify or gain confidence in a North Korean fissile material declaration. This study explores the potential utility of a Bayesian inference-based analysis of the isotopic composition of reprocessing waste to reconstruct the operating history of the 5 MWe reactor and estimate its plutonium production history. We simulate several scenarios that reflect different assumptions and varying levels of prior knowledge about the reactor. The results show that correct prior assumptions can be confirmed and incorrect prior information (or a false declaration) can be detected. Model comparison techniques can distinguish between scenarios with different numbers of core discharges, a capability that could provide important insights into the early stages of operation of the 5 MWe reactor. Using these techniques, a weighted plutonium estimate can be calculated, even in cases where the number of core discharges is not known with certainty.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"SETI, evolution and human history merged into a mathematical model","In this paper we propose a new mathematical model capable of merging Darwinian Evolution, Human History and SETI into a single mathematical scheme: 1) Darwinian Evolution over the last 3.5 billion years is defined as one particular realization of a certain stochastic process called Geometric Brownian Motion (GBM). This GBM yields the fluctuations in time of the number of species living on Earth. Its mean value curve is an increasing exponential curve, i.e. the exponential growth of Evolution. 2) In 2008 this author provided the statistical generalization of the Drake equation yielding the number N of communicating ET civilizations in the Galaxy. N was shown to follow the lognormal probability distribution. 3) We call ""b-lognormals"" those lognormals starting at any positive time b (""birth"") larger than zero. Then the exponential growth curve becomes the geometric locus of the peaks of a one-parameter family of b-lognormals: this is our way to re-define Cladistics. 4) b-lognormals may be also be interpreted as the lifespan of any living being (a cell, or an animal, a plant, a human, or even the historic lifetime of any civilization). Applying this new mathematical apparatus to Human History, leads to the discovery of the exponential progress between Ancient Greece and the current USA as the envelope of all b-lognormals of Western Civilizations over a period of 2500 years. 5) We then invoke Shannon's Information Theory. The b-lognormals' entropy turns out to be the index of ""development level"" reached by each historic civilization. We thus get a numerical estimate of the entropy difference between any two civilizations, like the Aztec-Spaniard difference in 1519. 6) In conclusion, we have derived a mathematical scheme capable of estimating how much more advanced than Humans an Alien Civilization will be when the SETI scientists will detect the first hints about ETs.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Reconstructing the evolution history of networked complex systems,"The evolution processes of complex systems carry key information in the systems' functional properties. Applying machine learning algorithms, we demonstrate that the historical formation process of various networked complex systems can be extracted, including protein-protein interaction, ecology, and social network systems. The recovered evolution process has demonstrations of immense scientific values, such as interpreting the evolution of protein-protein interaction network, facilitating structure prediction, and particularly revealing the key co-evolution features of network structures such as preferential attachment, community structure, local clustering, degree-degree correlation that could not be explained collectively by previous theories. Intriguingly, we discover that for large networks, if the performance of the machine learning model is slightly better than a random guess on the pairwise order of links, reliable restoration of the overall network formation process can be achieved. This suggests that evolution history restoration is generally highly feasible on empirical networks.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Stochastic differential equations for evolutionary dynamics with demographic noise and mutations,"We present a general framework to describe the evolutionary dynamics of an arbitrary number of types in finite populations based on stochastic differential equations (SDE). For large, but finite populations this allows to include demographic noise without requiring explicit simulations. Instead, the population size only rescales the amplitude of the noise. Moreover, this framework admits the inclusion of mutations between different types, provided that mutation rates, $$, are not too small compared to the inverse population size 1/N. This ensures that all types are almost always represented in the population and that the occasional extinction of one type does not result in an extended absence of that type. For $N\ll1$ this limits the use of SDE's, but in this case there are well established alternative approximations based on time scale separation. We illustrate our approach by a Rock-Scissors-Paper game with mutations, where we demonstrate excellent agreement with simulation based results for sufficiently large populations. In the absence of mutations the excellent agreement extends to small population sizes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Testing a priority-based queue model with Linux command histories,"We study human dynamics by analyzing Linux history files. The goodness-of-fit test shows that most of the collected datasets belong to the universality class suggested in the literature by a variable-length queueing process based on priority. In order to check the validity of this model, we design two tests based on mutual information between time intervals and a mathematical relationship known as the arcsine law. Since the previously suggested queueing process fails to pass these tests, the result suggests that the modelling of human dynamics should properly consider the statistical dependency in the temporal dimension.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Charting multidimensional ideological polarization across demographic groups in the United States,"Has ideological polarization actually increased in the last decades, or have voters simply sorted themselves into parties matching their ideology more closely? We present a novel methodology to quantify multidimensional ideological polarization, by embedding the respondents to a wide variety of political, social, and economic topics from the American National Election Studies (ANES) into a two-dimensional ideological space. By identifying several demographic attributes of the ANES respondents, we chart how political and socio-economic groups move through the ideological space in time. We observe that income and especially racial groups align into parties, but their ideological distance has not increased over time. Instead, Democrats and Republicans have become ideologically more distant in the last 30 years: Both parties moved away from the center, at different rates. Furthermore, Democratic voters have become ideologically more heterogeneous after 2010, indicating that partisan sorting has declined in the last decade.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Spatiotemporal impacts of human activities and socio-demographics during the COVID-19 outbreak in the U.S,"Understanding influencing factors is essential for the surveillance and prevention of infectious diseases, and the factors are likely to vary spatially and temporally as the disease progresses. Taking daily cases and deaths data during the coronavirus disease 2019 (COVID-19) outbreak in the U.S. as a case study, we develop a mobility-augmented geographically and temporally weighted regression (M-GTWR) model to quantify the spatiotemporal impacts of social-demographic factors and human activities on the COVID-19 dynamics. Different from the base GTWR model, we incorporate a mobility-adjusted distance weight matrix where travel mobility is used in addition to the spatial adjacency to capture the correlations among local observations. The model residuals suggest that the proposed model achieves a substantial improvement over other benchmark methods in addressing the spatiotemporal nonstationarity. Our results reveal that the impacts of social-demographic and human activity variables present significant spatiotemporal heterogeneity. In particular, a 1% increase in population density may lead to 0.63% and 0.71% more daily cases and deaths, and a 1% increase in the mean commuting time may result in 0.22% and 0.95% increases in daily cases and deaths. Although increased human activities will, in general, intensify the disease outbreak, we report that the effects of grocery and pharmacy-related activities are insignificant in areas with high population density. And activities at the workplace and public transit are found to either increase or decrease the number of cases and deaths, depending on particular locations. The results of our study establish a quantitative framework for identifying influencing factors during a disease outbreak, and the obtained insights may have significant implications in guiding the policy-making against infectious diseases.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Story about One Island and Four Cities. The Socio-Economic Soft Matter Model - Based Report,"The report discusses the emergence of the Socio-Economic Soft Matter (SE-SM) as the result of interactions between physics and economy. First, demographic changes since the Industrial Revolution onset are tested using Soft Matter science tools. Notable in the support of innovative derivative-based and distortions-sensitive analytic tools. It revealed the Weibull type powered exponential increase, with a notably lesser rising rate since the crossover detected near the year 1970. Subsequently, demographic (SE-SM) patterns are tested for Rapa Nui (Easter) Island model case and for four large 'hallmark cities' where the rise and decay phases have occurred. They are Detroit and Cleveland in the USA and Lodz (former textile industry center) and Bytom (former coal mining center) in Poland. The analysis explicitly revealed scaling patterns for demographic changes, influenced by the historical and socio-economic backgrounds and the long-lasting determinism in population changes. Universalistic features of demographic changes are discussed within the Socio-Economic Soft Matter concept.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
History of art paintings through the lens of entropy and complexity,"Art is the ultimate expression of human creativity that is deeply influenced by the philosophy and culture of the corresponding historical epoch. The quantitative analysis of art is therefore essential for better understanding human cultural evolution. Here we present a large-scale quantitative analysis of almost 140 thousand paintings, spanning nearly a millennium of art history. Based on the local spatial patterns in the images of these paintings, we estimate the permutation entropy and the statistical complexity of each painting. These measures map the degree of visual order of artworks into a scale of order-disorder and simplicity-complexity that locally reflects qualitative categories proposed by art historians. The dynamical behavior of these measures reveals a clear temporal evolution of art, marked by transitions that agree with the main historical periods of art. Our research shows that different artistic styles have a distinct average degree of entropy and complexity, thus allowing a hierarchical organization and clustering of styles according to these metrics. We have further verified that the identified groups correspond well with the textual content used to qualitatively describe the styles, and that the employed complexity-entropy measures can be used for an effective classification of artworks.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying the `end of history' through a Bayesian Markov-chain approach,"Political regimes have been changing throughout human history. After the apparent triumph of liberal democracies at the end of the twentieth century, Francis Fukuyama and others have been arguing that humankind is approaching an `end of history' (EoH) in the form of a universality of liberal democracies. This view has been challenged by recent developments that seem to indicate the rise of defective democracies across the globe. There has been no attempt to quantify the expected EoH with a statistical approach. In this study, we model the transition between political regimes as a Markov process and -- using a Bayesian inference approach -- we estimate the transition probabilities between political regimes from time-series data describing the evolution of political regimes from 1800--2018. We then compute the steady state for this Markov process which represents a mathematical abstraction of the EoH and predicts that approximately 46 % of countries will be full democracies. Furthermore, we find that, under our model, the fraction of autocracies in the world is expected to increase for the next half-century before it declines. Using random-walk theory, we then estimate survival curves of different types of regimes and estimate characteristic lifetimes of democracies and autocracies of 244 years and 69 years, respectively. Quantifying the expected EoH allows us to challenge common beliefs about the nature of political equilibria. Specifically, we find no statistical evidence that the EoH constitutes a fixed, complete omnipresence of democratic regimes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Latent event history models for quasi-reaction systems,"Various processes can be modelled as quasi-reaction systems of stochastic differential equations, such as cell differentiation and disease spreading. Since the underlying data of particle interactions, such as reactions between proteins or contacts between people, are typically unobserved, statistical inference of the parameters driving these systems is developed from concentration data measuring each unit in the system over time. While observing the continuous time process at a time scale as fine as possible should in theory help with parameter estimation, the existing Local Linear Approximation (LLA) methods fail in this case, due to numerical instability caused by small changes of the system at successive time points. On the other hand, one may be able to reconstruct the underlying unobserved interactions from the observed count data. Motivated by this, we first formalise the latent event history model underlying the observed count process. We then propose a computationally efficient Expectation-Maximation algorithm for parameter estimation, with an extended Kalman filtering procedure for the prediction of the latent states. A simulation study shows the performance of the proposed method and highlights the settings where it is particularly advantageous compared to the existing LLA approaches. Finally, we present an illustration of the methodology on the spreading of the COVID-19 pandemic in Italy.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
SDSS-IV from 2014 to 2016: A Detailed Demographic Comparison over Three Years,"The Sloan Digital Sky Survey (SDSS) is one of the largest international astronomy organizations. We present demographic data based on surveys of its members from 2014, 2015 and 2016, during the fourth phase of SDSS (SDSS-IV). We find about half of SDSS-IV collaboration members were based in North America, a quarter in Europe, and the remainder in Asia and Central and South America. Overall, 26-36% are women (from 2014 to 2016), up to 2% report non-binary genders. 11-14% report that they are racial or ethnic minorities where they live. The fraction of women drops with seniority, and is also lower among collaboration leadership. Men in SDSS-IV were more likely to report being in a leadership role, and for the role to be funded and formally recognized. SDSS-IV collaboration members are twice as likely to have a parent with a college degree, than the general population, and are ten times more likely to have a parent with a PhD. This trend is slightly enhanced for female collaboration members. Despite this, the fraction of first generation college students (FGCS) is significant (31%). This fraction increased among collaboration members who are racial or ethnic minorities (40-50%), and decreased among women (15-25%). SDSS-IV implemented many inclusive policies and established a dedicated committee, the Committee on INclusiveness in SDSS (COINS). More than 60% of the collaboration agree that the collaboration is inclusive; however, collaboration leadership more strongly agree with this than the general membership. In this paper, we explain these results in full, including the history of inclusive efforts in SDSS-IV. We conclude with a list of suggested recommendations based on our findings, which can be used to improve equity and inclusion in large astronomical collaborations, which we argue is not only moral, but will also optimize their scientific output.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The Journal Impact Factor: A brief history, critique, and discussion of adverse effects","The Journal Impact Factor (JIF) is, by far, the most discussed bibliometric indicator. Since its introduction over 40 years ago, it has had enormous effects on the scientific ecosystem: transforming the publishing industry, shaping hiring practices and the allocation of resources, and, as a result, reorienting the research activities and dissemination practices of scholars. Given both the ubiquity and impact of the indicator, the JIF has been widely dissected and debated by scholars of every disciplinary orientation. Drawing on the existing literature as well as on original research, this chapter provides a brief history of the indicator and highlights well-known limitations-such as the asymmetry between the numerator and the denominator, differences across disciplines, the insufficient citation window, and the skewness of the underlying citation distributions. The inflation of the JIF and the weakening predictive power is discussed, as well as the adverse effects on the behaviors of individual actors and the research enterprise. Alternative journal-based indicators are described and the chapter concludes with a call for responsible application and a commentary on future developments in journal indicators.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Web Routineness and Limits of Predictability: Investigating Demographic and Behavioral Differences Using Web Tracking Data,"Understanding human activities and movements on the Web is not only important for computational social scientists but can also offer valuable guidance for the design of online systems for recommendations, caching, advertising, and personalization. In this work, we demonstrate that people tend to follow routines on the Web, and these repetitive patterns of web visits increase their browsing behavior's achievable predictability. We present an information-theoretic framework for measuring the uncertainty and theoretical limits of predictability of human mobility on the Web. We systematically assess the impact of different design decisions on the measurement. We apply the framework to a web tracking dataset of German internet users. Our empirical results highlight that individual's routines on the Web make their browsing behavior predictable to 85% on average, though the value varies across individuals. We observe that these differences in the users' predictabilities can be explained to some extent by their demographic and behavioral attributes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Who is the best player ever? A complex network analysis of the history of professional tennis,"We consider all matches played by professional tennis players between 1968 and 2010, and, on the basis of this data set, construct a directed and weighted network of contacts. The resulting graph shows complex features, typical of many real networked systems studied in literature. We develop a diffusion algorithm and apply it to the tennis contact network in order to rank professional players. Jimmy Connors is identified as the best player of the history of tennis according to our ranking procedure. We perform a complete analysis by determining the best players on specific playing surfaces as well as the best ones in each of the years covered by the data set. The results of our technique are compared to those of two other well established methods. In general, we observe that our ranking method performs better: it has a higher predictive power and does not require the arbitrary introduction of external criteria for the correct assessment of the quality of players. The present work provides a novel evidence of the utility of tools and methods of network theory in real applications.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Challenges in identifying simple pattern-forming mechanisms in the development of settlements using demographic data,"The rapid increase of population and settlement structures in the Global South during recent decades motivates the development of suitable models to describe their formation and evolution. Such settlement formation has been previously suggested to be dynamically driven by simple pattern-forming mechanisms. Here, we explore the use of a data-driven white-box approach, called SINDy, to discover differential equation models directly from available spatiotemporal demographic data for three representative regions of the Global South. We show that the current resolution and observation time of the available data is insufficient to uncover relevant pattern-forming mechanisms in settlement development. Using synthetic data generated with a generic pattern-forming model, the Allen-Cahn equation, we characterize what the requirements are on spatial and temporal resolution, as well as observation time, to successfully identify possible model system equations. Overall, the study provides a theoretical framework for the analysis of large-scale geographical/ecological systems, and it motivates further improvements in optimization approaches and data collection.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Harnessing Mobile Phone Social Network Topology to Infer Users Demographic Attributes,"We study the structure of the social graph of mobile phone users in the country of Mexico, with a focus on demographic attributes of the users (more specifically the users' age). We examine assortativity patterns in the graph, and observe a strong age homophily in the communications preferences. We propose a graph based algorithm for the prediction of the age of mobile phone users. The algorithm exploits the topology of the mobile phone network, together with a subset of known users ages (seeds), to infer the age of remaining users. We provide the details of the methodology, and show experimental results on a network GT with more than 70 million users. By carefully examining the topological relations of the seeds to the rest of the nodes in GT, we find topological metrics which have a direct influence on the performance of the algorithm. In particular we characterize subsets of users for which the accuracy of the algorithm is 62% when predicting between 4 age categories (whereas a pure random guess would yield an accuracy of 25%). We also show that we can use the probabilistic information computed by the algorithm to further increase its inference power to 72% on a significant subset of users.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Interplay between HIV/AIDS Epidemics and Demographic Structures Based on Sexual Contact Networks,"In this article, we propose a network spread model for HIV epidemics, wherein each individual is represented by a node of the transmission network and the edges are the connections between individuals along which the infection may spread. The sexual activity of each individual, measured by its degree, is not homogeneous but obeys a power-law distribution. Due to the heterogeneity of activity, the infection can persistently exist at a very low prevalence, which has been observed in real data but can not be illuminated by previous models with homogeneous mixing hypothesis. Furthermore, the model displays a clear picture of hierarchical spread: In the early stage the infection is adhered to these high-risk persons, and then, diffuses toward low-risk population. The prediction results show that the development of epidemics can be roughly categorized into three patterns for different countries, and the pattern of a given country is mainly determined by the average sex-activity and transmission probability per sexual partner. In most cases, the effect of HIV epidemics on demographic structure is very small. However, for some extremely countries, like Botswana, the number of sex-active people can be depressed to nearly a half by AIDS.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying Cultural Histories via Person Networks in Wikipedia,"At least since Priestley's 1765 Chart of Biography, large numbers of individual person records have been used to illustrate aggregate patterns of cultural history. Wikidata, the structured database sister of Wikipedia, currently contains about 2.7 million explicit person records, across all language versions of the encyclopedia. These individuals, notable according to Wikipedia editing criteria, are connected via millions of hyperlinks between their respective Wikipedia articles. This situation provides us with the chance to go beyond the illustration of an idiosyncratic subset of individuals, as in the case of Priestly. In this work we summarize the overlap of nationalities and occupations, based on their co-occurrence in Wikidata individuals. We construct networks of co-occurring nationalities and occupations, provide insights into their respective community structure, and apply the results to select and color chronologically structured subsets of a large network of individuals, connected by Wikipedia hyperlinks. While the imagined communities of nationality are much more discrete in terms of co-occurrence than occupations, our quantifications reveal the existing overlap of nationality as much less clear-cut than in case of occupational domains. Our work contributes to a growing body of research using biographies of notable persons to analyze cultural processes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Leveraging History for Faster Sampling of Online Social Networks,"How to enable efficient analytics over such data has been an increasingly important research problem. Given the sheer size of such social networks, many existing studies resort to sampling techniques that draw random nodes from an online social network through its restrictive web/API interface. Almost all of them use the exact same underlying technique of random walk - a Markov Chain Monte Carlo based method which iteratively transits from one node to its random neighbor.   Random walk fits naturally with this problem because, for most online social networks, the only query we can issue through the interface is to retrieve the neighbors of a given node (i.e., no access to the full graph topology). A problem with random walks, however, is the ""burn-in"" period which requires a large number of transitions/queries before the sampling distribution converges to a stationary value that enables the drawing of samples in a statistically valid manner.   In this paper, we consider a novel problem of speeding up the fundamental design of random walks (i.e., reducing the number of queries it requires) without changing the stationary distribution it achieves - thereby enabling a more efficient ""drop-in"" replacement for existing sampling-based analytics techniques over online social networks. Our main idea is to leverage the history of random walks to construct a higher-ordered Markov chain. We develop two algorithms, Circulated Neighbors and Groupby Neighbors Random Walk (CNRW and GNRW) and prove that, no matter what the social network topology is, CNRW and GNRW offer better efficiency than baseline random walks while achieving the same stationary distribution. We demonstrate through extensive experiments on real-world social networks and synthetic graphs the superiority of our techniques over the existing ones.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Opinion dynamics with emergent collective memory: the impact of a long and heterogeneous news history,"In modern society people are being exposed to numerous information, with some of them being frequently repeated or more disruptive than others. In this paper we use a model of opinion dynamics to study how this news impact the society. In particular, our study aims to explain how the exposure of the society to certain events deeply change people's perception of the present and future. The evolution of opinions which we consider is influenced both by external information and the pressure of the society. The latter includes imitation, differentiation, homophily and its opposite, xenophobia. The combination of these ingredients gives rise to a collective memory effect, which is triggered by external information. In this paper we focus our attention on how this memory arises when the order of appearance of external news is random. We will show which characteristics a piece of news needs to have in order to be embedded in the society's memory. We will also provide an analytical way to measure how many information a society can remember when an extensive number of news items is presented. Finally we will show that, when a certain piece of news is present in the society's history, even a distorted version of it is sufficient to trigger the memory of the originally stored information.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Facebook Ads as a Demographic Tool to Measure the Urban-Rural Divide,"In the global move toward urbanization, making sure the people remaining in rural areas are not left behind in terms of development and policy considerations is a priority for governments worldwide. However, it is increasingly challenging to track important statistics concerning this sparse, geographically dispersed population, resulting in a lack of reliable, up-to-date data. In this study, we examine the usefulness of the Facebook Advertising platform, which offers a digital ""census"" of over two billions of its users, in measuring potential rural-urban inequalities. We focus on Italy, a country where about 30% of the population lives in rural areas. First, we show that the population statistics that Facebook produces suffer from instability across time and incomplete coverage of sparsely populated municipalities. To overcome such limitation, we propose an alternative methodology for estimating Facebook Ads audiences that nearly triples the coverage of the rural municipalities from 19% to 55% and makes feasible fine-grained sub-population analysis. Using official national census data, we evaluate our approach and confirm known significant urban-rural divides in terms of educational attainment and income. Extending the analysis to Facebook-specific user ""interests"" and behaviors, we provide further insights on the divide, for instance, finding that rural areas show a higher interest in gambling. Notably, we find that the most predictive features of income in rural areas differ from those for urban centres, suggesting researchers need to consider a broader range of attributes when examining rural wellbeing. The findings of this study illustrate the necessity of improving existing tools and methodologies to include under-represented populations in digital demographic studies -- the failure to do so could result in misleading observations, conclusions, and most importantly, policies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Turing Instability in an Economic-Demographic Dynamical System Can Lead to Pattern Formation on Geographical Scale,"Spatial distribution of the human population is distinctly heterogeneous, e.g. showing significant difference in the population density between urban and rural areas. In the historical perspective, i.e. on the timescale of centuries, the emergence of the densely populated areas at their present locations is widely believed to be linked to more favourable environmental and climatic conditions. In this paper, we challenge this point of view. We first identify a few areas at different parts of the world where the environmental conditions (quantified by the temperature, precipitation and elevation) are approximately uniform over thousands of miles. We then examine the population distribution across those areas to show that, in spite of the homogeneity of the environment, it exhibits a clear nearly-periodic spatial pattern. Based on this apparent disagreement, we hypothesize that there exists an inherent mechanism that can lead to pattern formation even in a uniform environment. We consider a mathematical model of the coupled demographic-economic dynamics and show that its spatially uniform, locally stable steady state can give rise to a periodic spatial pattern due to the Turing instability. Using computer simulations, we show that, interestingly, the emergence of the Turing patterns eventually leads to the system collapse.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Intellectual interchanges in the history of the massive online open-editing encyclopedia, Wikipedia","Wikipedia is a free Internet encyclopedia with an enormous amount of content. This encyclopedia is written by volunteers with various backgrounds in a collective fashion; anyone can access and edit most of the articles. This open-editing nature may give us prejudice that Wikipedia is an unstable and unreliable source; yet many studies suggest that Wikipedia is even more accurate and self-consistent than traditional encyclopedias. Scholars have attempted to understand such extraordinary credibility, but usually used the number of edits as the unit of time, without consideration of real time. In this work, we probe the formation of such collective intelligence through a systematic analysis using the entire history of 34,534,110 English Wikipedia articles, between 2001 and 2014. From this massive data set, we observe the universality of both timewise and lengthwise editing scales, which suggests that it is essential to consider the real-time dynamics. By considering real time, we find the existence of distinct growth patterns that are unobserved by utilizing the number of edits as the unit of time. To account for these results, we present a mechanistic model that adopts the article editing dynamics based on both editor-editor and editor-article interactions. The model successfully generates the key properties of real Wikipedia articles such as distinct types of articles for the editing patterns characterized by the interrelationship between the numbers of edits and editors, and the article size. In addition, the model indicates that infrequently referred articles tend to grow faster than frequently referred ones, and articles attracting a high motivation to edit counterintuitively reduce the number of participants. We suggest that this decay of participants eventually brings inequality among the editors, which will become more severe with time.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
History-dependent percolation on multiplex networks,"The structure of interconnected systems and its impact on the system dynamics is a much-studied cross-disciplinary topic. Although various critical phenomena have been found in different models, the study on the connections between different percolation transitions is still lacking. Here we propose a unified framework to study the origins of the discontinuous transitions of the percolation process on interacting networks. The model evolves in generations with the result of the present percolation depending on the previous state and thus is history-dependent. Both theoretical analysis and Monte Carlo simulations reveal that the nature of the transition remains the same at finite generations but exhibits an abrupt change for the infinite generation. We use brain functional correlation and morphological similarity data to show that our model also provides a general method to explore the network structure and can contribute to many practical applications, such as detecting the abnormal structures of human brain networks.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Individual-driven versus interaction-driven burstiness in human dynamics: The case of Wikipedia edit history,"The origin of non-Poissonian or bursty temporal patterns observed in various datasets for human social dynamics has been extensively studied, yet its understanding still remains incomplete. Considering the fact that humans are social beings, a fundamental question arises: Is the bursty human dynamics dominated by individual characteristics or by interaction between individuals? In this paper we address this question by analyzing the Wikipedia edit history to see how spontaneous individual editors are in initiating bursty periods of editing, i.e., individual-driven burstiness, and to what extent such editors' behaviors are driven by interaction with other editors in those periods, i.e., interaction-driven burstiness. We quantify the degree of initiative (DoI) of an editor of interest in each Wikipedia article by using the statistics of bursty periods containing the editor's edits. The integrated value of the DoI over all relevant timescales reveals which is dominant between individual-driven and interaction-driven burstiness. We empirically find that this value tends to be larger for weaker temporal correlations in the editor's editing behavior and/or stronger editorial correlations. These empirical findings are successfully confirmed by deriving an analytic form of the DoI from a model capturing the essential features of the edit sequence. Thus our approach provides a deeper insight into the origin and underlying mechanisms of bursts in human social dynamics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Apparent structural changes in contact patterns during COVID-19 were driven by survey design and long-term demographic trends,"Social contact patterns are key drivers of infectious disease transmission. During the COVID-19 pandemic, differences between pre-COVID and COVID-era contact rates were widely attributed to non-pharmaceutical interventions such as lockdowns. However, the factors that drive changes in the distribution of contacts between different subpopulations remain poorly understood. Here, we present a clustering analysis of 33 contact matrices generated from surveys conducted before and during the COVID-19 pandemic, and analyse key features distinguishing their topological structures. While we expected to identify aspects of pandemic scenarios responsible for these features, our analysis demonstrates that they can be explained by differences in study design and long-term demographic trends. Our results caution against using survey data from different studies in counterfactual analysis of epidemic mitigation strategies. Doing so risks attributing differences stemming from methodological choices or long-term changes to the short-term effects of interventions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Amaldi Conferences. Their Past and Their Potential Future,"In this paper the history of the founding and of the development of the Amaldi Conferences is described with special reference to the following aspects and questions:   1. The Origin   2. The Vision of a European CISAC (Committee on International Security and Arms Control)   3. Changes in the Political Landscape and their Consequences   4. Discussions on Widening the Scope of the Amaldi Conferences   5. The ""Amaldi Guidelines""   6. Are the Amaldi Conferences still serving their initial purpose?   7. Are there new chances for a European CISAC after the progress in European Unification?","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Global Population and Carrying Capacity in the Anthropocene: the Relative Growth Rate Insight,"This report provides insights into global population dynamics since the beginning of the Anthropocene, focusing on empirical data and minimizing a priori the impact of model assumptions. It explores the Relative Growth Rate concept, introduced recently to global population studies by Lehman et al. [PNAS 118, e2024150118 (2021)] and subsequently extended to its analytical counterpart [PLoS ONE 20, eo323165 (2025)]. The analysis reveals a general non-monotonic growth pattern in the Anthropocene, emphasizing the uniqueness of the Industrial Revolution era. For the first 290 years, the Doomsday critical scaling provides a superior description for population changes, with a singularity at 2026. This is followed by the crossover to an exceptional 'reversed criticality' patterm, which has held over the last six decades, to the present day. The analysis suggests that the evolution of the human Real Intelligence (RI) population during the Innovations-driven Industrial Revolution times - a period of rapidly increasing connectivity and complexity - can serve as a model counterpart for the puzzling dynamics of Artificial Intelligence (AI) growth. The final conclusion is positive: the catastropic Doomsday singularity can be avoided due to the generic system constraints , both for RI and AI.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Nanotechnology: a slightly different history,"Many introductory articles and books about nanotechnology have been written to disseminate this apparently new technology, which investigate and manipulates matter at dimension of a billionth of a meter. However, these texts show in general a common feature: there is very little about the origins of this multidisciplinary field. If anything is mentioned at all, a few dates, facts and characters are reinforced, which under the scrutiny of a careful historical digging do not sustain as really founding landmarks of the field. Nevertheless, in spite of these flaws, such historical narratives bring up important elements to understand and contextualize this human endeavor, as well as the corresponding dissemination among the public: would nanotechnology be a cultural imperative?","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The determinants of COVID-19 case fatality rate (CFR) in the Italian regions and provinces: an analysis of environmental, demographic, and healthcare factors","The Italian government has been one of the most responsive to COVID-19 emergency, through the adoption of quick and increasingly stringent measures to contain the outbreak. Despite this, Italy has suffered a huge human and social cost, especially in Lombardy. The aim of this paper is dual: i) first, to investigate the reasons of the case fatality rate (CFR) differences across Italian 20 regions and 107 provinces, using a multivariate OLS regression approach; and ii) second, to build a taxonomy of provinces with similar mortality risk of COVID-19, by using the Ward hierarchical agglomerative clustering method. I considered health system metrics, environmental pollution, climatic conditions, demographic variables, and three ad hoc indexes that represent the health system saturation. The results showed that overall health care efficiency, physician density, and average temperature helped to reduce the CFR. By the contrary, population aged 70 and above, car and firm density, level of air pollutants (NO2, O3, PM10, and PM2.5), relative average humidity, COVID-19 prevalence, and all three indexes of health system saturation were positively associated with the CFR. Population density, social vertical integration, and altitude were not statistically significant. In particular, the risk of dying increases with age, as 90 years old and above had a three-fold greater risk than the 80 to 89 years old and four-fold greater risk than 70 to 79 years old. Moreover, the cluster analysis showed that the highest mortality risk was concentrated in the north of the country, while the lowest risk was associated with southern provinces. Finally, since prevalence and health system saturation indexes played the most important role in explaining the CFR variability, a significant part of the latter may have been caused by the massive stress of the Italian health system.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Understanding scaling through history-dependent processes with collapsing sample space,"History-dependent processes are ubiquitous in natural and social systems. Many such stochastic processes, especially those that are associated with complex systems, become more constrained as they unfold, meaning that their sample-space, or their set of possible outcomes, reduces as they age. We demonstrate that these sample-space reducing (SSR) processes necessarily lead to Zipf's law in the rank distributions of their outcomes. We show that by adding noise to SSR processes the corresponding rank distributions remain exact power-laws, $p(x)\sim x^{-}$, where the exponent directly corresponds to the mixing ratio of the SSR process and noise. This allows us to give a precise meaning to the scaling exponent in terms of the degree to how much a given process reduces its sample-space as it unfolds. Noisy SSR processes further allow us to explain a wide range of scaling exponents in frequency distributions ranging from $= 2$ to $\infty$. We discuss several applications showing how SSR processes can be used to understand Zipf's law in word frequencies, and how they are related to diffusion processes in directed networks, or ageing processes such as in fragmentation processes. SSR processes provide a new alternative to understand the origin of scaling in complex systems without the recourse to multiplicative, preferential, or self-organised critical processes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modeling Urban Population Dynamics and City-to-City Migration,"Migration plays a crucial role in urban growth. Over time, individuals opting to relocate led to vast metropolises like London and Paris during the Industrial Revolution, Shanghai and Karachi during the last decades and thousands of smaller settlements. Here, we analyze the impact that migration has on population redistribution. We use a model of city-to-city migration as a process that occurs within a network, where the nodes represent cities, and the edges correspond to the flux of individuals. We analyze metrics characterizing the urban distribution and show how a slight preference for some destinations might result in the observed distribution of the population.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Long-Term Trends in Given Name Frequencies in England and Wales,"The frequency distribution of personal given names offers important evidence about the information economy. This paper presents data on the popularity of the most frequent personal given names (first names) in England and Wales over the past millennium. The popularity of a name is its frequency relative to the total name instances sampled. The data show that the popularity distribution of names, like the popularity of other symbols and artifacts associated with the information economy, can be helpfully viewed as a power law. Moreover, the data on name popularity suggest that historically distinctive changes in the information economy occurred in conjunction with the Industrial Revolution.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Between News and History: Identifying Networked Topics of Collective Attention on Wikipedia,"The digital information landscape has introduced a new dimension to understanding how we collectively react to new information and preserve it at the societal level. This, together with the emergence of platforms such as Wikipedia, has challenged traditional views on the relationship between current events and historical accounts of events, with an ever-shrinking divide between ""news"" and ""history"". Wikipedia's place as the Internet's primary reference work thus poses the question of how it represents both traditional encyclopaedic knowledge and evolving important news stories. In other words, how is information on and attention towards current events integrated into the existing topical structures of Wikipedia? To address this we develop a temporal community detection approach towards topic detection that takes into account both short term dynamics of attention as well as long term article network structures. We apply this method to a dataset of one year of current events on Wikipedia to identify clusters distinct from those that would be found solely from page view time series correlations or static network structure. We are able to resolve the topics that more strongly reflect unfolding current events vs more established knowledge by the relative importance of collective attention dynamics vs link structures. We also offer important developments by identifying and describing the emergent topics on Wikipedia. This work provides a means of distinguishing how these information and attention clusters are related to Wikipedia's twin faces of encyclopaedic knowledge and current events -- crucial to understanding the production and consumption of knowledge in the digital age.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diffusion-driven demographics -- Turing model as a concept for the emergence of sedentism,"Sedentism was a decisive moment in the history of humankind. In a review article Kay and Kaplan quantified land use for early human settlements and found that sedentism and the emergence of farming go hand in hand. For these settlements two primary land use categories, farming and living, can be identified, whereas for hunter gatherer societies no distinct differences can be made. It is natural to search for this in the behavior of two different groups, settlers and farmers. The development of two distinct zones and the two groups lead us to the hypothesis that the emergence of settlements is the result of diffusion-driven Turing instability. In this short communication we further specify this and show that this results in a regular settlement arrangement as can still be seen today in agricultural regions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
On the origins of Lagrangian hydrodynamic methods,"The intent of this paper is to discuss the history and origins of Lagrangian hydrodynamic methods for simulating shock driven flows. The majority of the pioneering research occurred within the Manhattan Project. A range of Lagrangian hydrodynamic schemes were created between 1943 and 1948 by John von Neumann, Rudolf Peierls, Tony Skyrme, and Robert Richtmyer. These schemes varied significantly from each other; however, they all used a staggered-grid and finite difference approximations of the derivatives in the governing equations, where the first scheme was by von Neumann. These ground-breaking schemes were principally published in Los Alamos laboratory reports that were eventually declassified many decades after authorship, which motivates us to document the work and describe the accompanying history in a paper that is accessible to the broader scientific community. Furthermore, we seek to correct historical omissions on the pivotal contributions made by Peierls and Skyrme to creating robust Lagrangian hydrodynamic methods for simulating shock driven flows. Understanding the history of Lagrangian hydrodynamic methods can help explain the origins of many modern schemes and may inspire the pursuit of new schemes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Power Plexus: A network based analysis,"Power generation and distribution remains an important topic of discussion since the industrial revolution. As the system continues to grow, it needs to evolve both in infrastructure, robustness and its resilience to deal with failures. One such potential failure that we target in this work is the cascading failure. This avalanche effect propagates through the network and we study this propagation by Percolation Theory and implement some solutions for mitigation. We have extended the percolation theory as given in Mark Newman. Networks: an introduction,for random nodes to targeted nodes having high load bearing which is eliminated from the network to study the cascade effect. We also implement mitigation strategy to improve the network performance.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The Instituto Argentino de Radioastronoma (IAR): Past, present, and future","I present a brief review of the history of the Instituto Argentino de Radioastronoma, a description of its current facilities and projects, and a view of his prospects for the future.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
History of Lattice Field Theory from a Statistical Perspective,"Researchers working in lattice field theory constitute an established community since the early 1990s, and around the same time the online open-access e-print repository arXiv was created. The fact that this field has a specific arXiv section, hep-lat, which is comprehensively used, provides a unique opportunity for a statistical study of its evolution over the last three decades. We present data for the number of entries, $E$, published papers, $P$, and citations, $C$, in total and separated by nations. We compare them to six other arXiv sections (hep-ph, hep-th, gr-qc, nucl-th, quant-ph, cond-mat) and to two socio-economic indices of the nations involved: the Gross Domestic Product (GDP) and the Education Index (EI). We present rankings, which are based either on the Hirsch Index H, or on the linear combination $= E + P + 0.05 C$. We consider both extensive and intensive national statistics, i.e. absolute and relative to the population or to the GDP.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
From England to Italy: the intriguing story of Poli's engine for the King of Naples,"An interesting, yet unknown, episode concerning the effective permeation of the scientific revolution in the XVIII century Kingdom of Naples (and, more generally, Italy) is recounted. The quite intriguing story of Watt's steam engine prepared for serving a Royal Estate of the King of Naples in Carditello reveals a fascinating piece of the history of that Kingdom, as well as an unknown step in the history of Watt's steam engine, whose final entrepreneurial success for the celebrated Boulton & Watt company was a direct consequence. That story unveils that, contrary to what claimed in the literature, the first introduction in Italy of the most important technological innovation of the XVIII century did not take place with the construction of the first steamship of the Mediterranean Sea, but rather 30 years before that, thanks to the incomparable work of Giuseppe Saverio Poli, a leading scholar and a very influential figure in the Kingdom of Naples. The tragic epilogue of Poli's engine testifies for its vanishing in the historical memory.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Gender Gaps in the Mathematical Sciences: The Creativity Factor,"This article presents an overview, and recent history, of studies of gender gaps in the mathematically-intensive sciences. Included are several statistics about gender differences in science, and about public resources aimed at addressing them. We then examine the role that gender differences in creativity play in explaining the recent and current gender differences in the mathematical sciences, and identify several constructive suggestions aimed at improving analytical creativity output in research institutions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Neutronics Calculation Advances at Los Alamos: Manhattan Project to Monte Carlo,"The history and advances of neutronics calculations at Los Alamos during the Manhattan Project through the present is reviewed. We briefly summarize early simpler, and more approximate neutronics methods. We then motivate the need to better predict neutronics behavior through consideration of theoretical equations, models and algorithms, experimental measurements, and available computing capabilities and their limitations. These, coupled with increasing post-war defense needs, and the invention of electronic computing led to the creation of Monte Carlo neutronics transport. As a part of the history, we note the crucial role that the scientific comradery between the great Los Alamos scientists played in the process. We focus heavily on these early developments and the subsequent successes of Monte Carlo and its applications to problems of national defense at Los Alamos. We cover the early methods, algorithms, and computers, electronic and women pioneers, that enabled Monte Carlo to spread to all areas of science.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Aboriginal Oral Traditions of Australian Impact Craters,"We explore Aboriginal oral traditions that relate to Australian meteorite craters. Using the literature, first-hand ethnographic records, and fieldtrip data, we identify oral traditions and artworks associated with four impact sites: Gosses Bluff, Henbury, Liverpool, and Wolfe Creek. Oral traditions describe impact origins for Gosses Bluff, Henbury, and Wolfe Creek craters and non-impact origins of Liverpool crater, with Wolfe Creek and Henbury having both impact and non-impact origins in oral tradition. Three impact sites that are believed to have formed during human habitation of Australia - Dalgaranga, Veevers, and Boxhole - do not have associated oral traditions that are reported in the literature.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Los Alamos Computing Facility during the Manhattan Project,"This article describes the history of the computing facility at Los Alamos during the Manhattan Project, 1944 to 1946. The hand computations are briefly discussed, but the focus is on the IBM Punch Card Accounting Machines (PCAM). During WWII the Los Alamos facility was one of most advanced PCAM facilities both in the machines and in the problems being solved.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Aboriginal Astronomical Traditions from Ooldea, South Australia, Part 2: Animals in the Ooldean Sky","Australian Indigenous astronomical traditions hint at a relationship between animals in the skyworld and the behaviour patterns of their terrestrial counterparts. In our continued study of Aboriginal astronomical traditions from the Great Victoria Desert, South Australia, we investigate the relationship between animal behaviour and stellar positions. We develop a methodology to test the hypothesis that the behaviour of these animals is predicted by the positions of their celestial counterparts at particular times of the day. Of the twelve animals identified in the Ooldean sky, the nine stellar (i.e. non-planet or non-galactic) associations were analysed and each demonstrated a close connection between animal behaviour and stellar positions. We suggest that this may be a recurring theme in Aboriginal astronomical traditions, requiring further development of the methodology.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying the influence of Vocational Education and Training with text embedding and similarity-based networks,"Assessing the potential influence of Vocational Education and Training (VET) courses on creating job opportunities and nurturing work skills has been considered challenging due to the ambiguity in defining their complex relationships and connections with the local economy. Here, we quantify the potential influence of VET courses and explain it with future economy and specialization by constructing a network of more than 17,000 courses, jobs, and skills in Singapore's SkillsFuture data based on their text similarities captured by a text embedding technique, Sentence Transformer. We find that VET courses associated with Singapore's 4th Industrial Revolution economy demonstrate higher influence than those related to other future economies. The course influence varies greatly across different sectors, attributed to the level of specificity of the skills covered. Lastly, we show a notable concentration of VET supply in certain occupation sectors requiring general skills, underscoring a disproportionate distribution of education supply for the labor market.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Status of e-Print Servers,"We make a short study of the history and evolution of scientific publications, in order to explain the format for near-term e-Prints servers, proposing a new scientific publication scheme via digital network, and exploring the new dynamics of publication.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Eclipses in Australian Aboriginal Astronomy,"We explore 50 Australian Aboriginal accounts of lunar and solar eclipses to determine how Aboriginal groups understood this phenomenon. We summarise the literature on Aboriginal references to eclipses, showing that many Aboriginal groups viewed eclipses negatively, frequently associating them with bad omens, evil magic, disease, blood and death. In many communities, Elders or medicine men were believed to have the ability to control or avert eclipses by magical means, solidifying their role as provider and protector within the community. We also show that many Aboriginal groups understood the motions of the sun-earth-moon system, the connection between the lunar phases and tides, and acknowledged that solar eclipses were caused by the moon blocking the sun.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Arrow of Time is Alive and Well but Forbidden Under the Received View of Physics,"This essay offers a meta-level analysis in the sociology and history of physics in the context of the so-called ""Arrow of Time Problem"" or ""Two Times Problem,"" which asserts that the empirically observed directionality of time is in conflict with physical theory. I argue that there is actually no necessary conflict between physics and the arrow of time, and that the observed directionality of time is perfectly consistent with physics unconstrained by certain optional metaphysical, epistemological and methodological beliefs and practices characterizing the conventional or Received View.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A fuzzy process of individuation,"It is shown that an aspect of the process of individuation may be thought of as a fuzzy set. The process of individuation has been interpreted as a two-valued problem in the history of philosophy. In this work, I intend to show that such a process in its psychosocial aspect is better understood in terms of a fuzzy set, characterized by a continuum membership function. According to this perspective, species and their members present different degrees of individuation. Such degrees are measured from the membership function of the psychosocial process of individuation. Thus, a social analysis is suggested by using this approach in human societies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Comets in Australian Aboriginal Astronomy,"We present 25 accounts of comets from 40 Australian Aboriginal communities, citing both supernatural perceptions of comets and historical accounts of bright comets. Historical and ethnographic descriptions include the Great Comets of 1843, 1861, 1901, 1910, and 1927. We describe the perceptions of comets in Aboriginal societies and show that they are typically associated with fear, death, omens, malevolent spirits, and evil magic, consistent with many cultures around the world. We also provide a list of words for comets in 16 different Aboriginal languages.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Sociophysics: a personal testimony,The origins of Sociophysics are discussed from a personal testimony. I trace back its history to the late seventies. My twenty years of activities and research to establish and promote the field are reviewed. In particular the conflicting nature of Sociophysics with the physics community is revealed from my own experience. Recent presentations of a supposed natural growth from Social Sciences are criticized.,"cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modellierungskonzepte der Synergetik und der Theorie der Selbstorganisation,"Mnay models situated in the current research landscape of modelling and simulating social processes have roots in physics. This is visible in the name of specialties as Econophysics or Sociophysics. This chapter describes the history of knowledge transfer from physics, in particular physics of self-organization and evolution, to the social sciences. We discuss why physicists felt called to describe social processes. Across models and simulations the question how to explain the emergence of something new is the most intriguing one. We present one model approach to this problem and introduce a game -- Evolino -- inviting a larger audience to get acquainted with abstract evolution-theory approaches to describe the quest for new ideas.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Nanotechnology R(evolution),"Nanotechnology as a social concept and investment focal point has drawn much attention. Here we consider the place of nanotechnology in the second great technological revolution of mankind that began some 200 years ago. The so-called nanotechnology revolution represents both a continuation of prior science and technology trends and a re-awakening to the benefits of significant investment in fundamental research. We consider the role the military might play in the development of nanotechnology innovations, nanotechnology's context in the history of technology, and the global competition to lead the next technological revolution.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"A Review Essay: ""Lise Meitner and the Dawn of the Nuclear Age"", by Patricia Rife","The recently published book, ``Lise Meitner and the Dawn of the Nuclear Age'', by Patricia Rife (Boston: Birkhuser, 1999) is reviewed in an essay for the lay audience. Meitner was a leading nuclear physicist at the time that the nucleus was the most exciting frontier of science. To establish her career, she had to overcome daunting prejudices against women in science and academia. Being of Jewish origin in Germany in the 1930's, she narrowly escaped certain disaster. Meitner was a crucial participant in the discovery of nuclear fission, yet did not share in the Nobel Prize that her collaborator, Otto Hahn, received in 1945. How these events came about, how they were intertwined with contemporary history and how they fit into the evolution of Meitner's social conscience and her abhorrence of war are some of the fascinating subjects discussed in the book and reviewed in this essay.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Correlations and dynamics of consumption patterns in social-economic networks,"We analyse a coupled dataset collecting the mobile phone communications and bank transactions history of a large number of individuals living in a Latin American country. After mapping the social structure and introducing indicators of socioeconomic status, demographic features, and purchasing habits of individuals we show that typical consumption patterns are strongly correlated with identified socioeconomic classes leading to patterns of stratification in the social structure. In addition we measure correlations between merchant categories and introduce a correlation network, which emerges with a meaningful community structure. We detect multivariate relations between merchant categories and show correlations in purchasing habits of individuals. Finally, by analysing individual consumption histories, we detect dynamical patterns in purchase behaviour and their correlations with the socioeconomic status, demographic characters and the egocentric social network of individuals. Our work provides novel and detailed insight into the relations between social and consuming behaviour with potential applications in resource allocation, marketing, and recommendation system design.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Dynamics of Meaning,"A formal theory of meaning (the process of knowledge accumulation) as multiplicative chaos is proposed. The epistemological process is understood as the process of subjective extraction of some knowledge from the incoming information. The concepts of nonsense are introduced as a meaning that has a minimum value equal to one and the level of intelligence as a geometric mean of the cumulative meaning. The thesis of the multiplicativity of meaning, its polymorphism is substantiated, and numerous examples from world history are provided. By analogy with classical thermodynamics, three laws of thermodynamics of meaning are postulated. Estimates of the cumulative meaning when the comprehension of information (multiplicative cascade) is a random process with given statistical characteristics are carried out.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
An Aboriginal Australian Record of the Great Eruption of Eta Carinae,"We present evidence that the Boorong Aboriginal people of northwestern Victoria observed the Great Eruption of Eta () Carinae in the nineteenth century and incorporated the event into their oral traditions. We identify this star, as well as others not specifically identified by name, using descriptive material presented in the 1858 paper by William Edward Stanbridge in conjunction with early southern star catalogues. This identification of a transient astronomical event supports the assertion that Aboriginal oral traditions are dynamic and evolving, and not static. This is the only definitive indigenous record of  Carinae's outburst identified in the literature to date.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Building an Inclusive AAS - The Critical Role of Diversity and Inclusion Training for AAS Council and Astronomy Leadership,"Diversity, equity and inclusion are the science leadership issues of our time. As our nation and the field of astronomy grow more diverse, we find ourselves in a position of enormous potential and opportunity: a multitude of studies show how groups of diverse individuals with differing viewpoints outperform homogenous groups to find solutions that are more innovative, creative, and responsive to complex problems, and promote higher-order thinking amongst the group. Research specifically into publications also shows that diverse author groups publish in higher quality journals and receive higher citation rates. As we welcome more diverse individuals into astronomy, we therefore find ourselves in a position of potential never before seen in the history of science, with the best minds and most diverse perspectives our field has ever seen. Despite this enormous growing potential, and the proven power of diversity, the demographics of our field are not keeping pace with the changing demographics of the nation, and astronomers of colour, women, LGBT individuals, people with disabilities, and those with more than one of these identities still face ""chilly"" or ""hostile"" work environments in the sciences. If we are to fully support all astronomers and students in reaching their full scientific potential, we must recognize that most of us tend to overestimate our ability to support our minoritized students and colleagues, that our formal education system fails to prepare us for working in a multicultural environment, and that most of us need some kind of training to help us know what we don't know and fill those gaps in our education. To that end, diversity and inclusion training for AAS council and leadership, heads of astronomy departments, and faculty search committees should be a basic requirement throughout our field.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
How a fake Kepler portrait became iconic,"For several decades a portrait of Johannes Kepler has been widely circulating among professional astronomers, scientific and academic institutions, and the general public. Despite its provenance and identification having been questioned in the early part of the last century, this painting has reached iconic status. We review its history from its first mention in the literature in the 1870s to a published but virtually unknown judgment of competent art experts of the 1920s that the work is in fact an early nineteenth century forgery. We display the painting in context with other more secure portraits and suggest that if it is based on anything, the painting may derive from the well known portrait from life of Michael Mstlin. This correction takes on certain urgency since 2021 is the 450th anniversary of Kepler's birth.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Global warming in figures and the question of its treatment: some historical and epistemological views,"We first recall fundamentals of elementary climate physics: solar constant, radiative balance, greenhouse effect, astronomical parameters of the climate (theory of Milankovitch). Without disputing the analyzes of climatologists and the famous Keeling curve revealing in an indisputable way the increase in CO$_{2}$ in the atmosphere since the industrial revolution, we nevertheless insist on the main contributor to the greenhouse effect which is, as we know, water vapor. Faced with the difficulties that there will be in imposing zero-carbon policies everywhere in the world (and especially in developing countries), we show that it would perhaps be in our interest to act on soil drought, which amounts, in fact, to being interested in the clouds. The decrease in cloud cover, due to a lack of water fixation in the soil, in fact increases the general temperature and therefore the greenhouse effect. Acting on CO$_{2}$ will always have, in this context, much less effect than acting on water vapor, even indirectly. Despite the difficulty of making this action sustainable, due to the balance of atmospheric water vapor and the oceans, it would be in our interest not to neglect this path and also possibly increase forest cover for this purpose, given the problems of setting up zero-carbon policy on a global scale. In desperation, one can also consider protecting the Earth with an artificial dust cloud.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Philosophical Implications of Inflationary Cosmology,"Recent developments in cosmology indicate that every history having a nonzero probability is realized in infinitely many distinct regions of spacetime. Thus, it appears that the universe contains infinitely many civilizations exactly like our own, as well as infinitely many civilizations that differ from our own in any way permitted by physical laws. We explore the implications of this conclusion for ethical theory and for the doomsday argument. In the infinite universe, we find that the doomsday argument applies only to effects which change the average lifetime of all civilizations, and not those which affect our civilization alone.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Barcelona, The Innovation District: between vulnerability and social equity","The municipal actors of the Catalan capital wish to obtain a reversal of the negative representations concerning the lack of attractiveness of the district of Poblenou considered as one of the poorest and most marginalized districts of the city while having as identity the expression of ""Manchester southern Europe"", recalling its role as a factory for Spain and an emblem of the Industrial Revolution for the Iberian Peninsula. Conceptualization, programming and construction of a new image of the city through this large urban wasteland are made concrete thanks to an urban marketing operation. The specifications allow the construction of innovative buildings, with resolutely contemporary architecture, breaking with the regulations in force on the limitation of the height of skyscrapers in order to make this pericentral territory visible and attractive. Catalan society in the 21st century has produced a fragmented and dominating neo-capitalist space through a thoughtless real estate frenzy and complex land games. Walking through the streets of Poblenou, one is forced to think of the work of Henri Lefebvre on the production of space and the three orders of understanding the design of an urban composition. Here, the city has become a stake and the place of economic growth based on innovation. Finally, neighborhood life is in complete mutation which seems for this territory to leave a chance for a long time on the sidelines. We present an economic analysis grid of a cluster with graphs and statistics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Accelerator Disaster Scenarios, the Unabomber, and Scientific Risks","The possibility that experiments at high-energy accelerators could create new forms of matter that would ultimately destroy the Earth has been considered several times in the past quarter century. One consequence of the earliest of these disaster scenarios was that the authors of a 1993 article in ""Physics Today"" who reviewed the experiments that had been carried out at the Bevalac at Lawrence Berkeley Laboratory were placed on the FBI's Unabomber watch list. Later, concerns that experiments at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory might create mini black holes or nuggets of stable strange quark matter resulted in a flurry of articles in the popular press. I discuss this history, as well as Richard A. Posner's provocative analysis and recommendations on how to deal with such scientific risks. I conclude that better communication between scientists and nonscientists would serve to assuage unreasonable fears and focus attention on truly serious potential threats to humankind.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Unveiling philosophy and social aspects of nanotechnology: A short review,"Philosophy has nurtured fundamental science by asking the right questions. This scientific growth has fuelled research in various domains and introduced diverse disciplines. Nanotechnology is an interdisciplinary domain with numerous applications ranging from medical diagnostics and food technology to electronics and psychology. Exploring nanotechnology's philosophical and social perspective can better understand these domains and may open new doors for research. This review addresses philosophical and other aspects of nanotechnology, such as history, definitions, vision, language, laws, politics, and ethics. This is an attempt to equip anyone in the field of nanotechnology with philosophical and social insights. We expect this review to provide an introductory understanding of philosophy and other aspects to the nanotechnologists, which are usually excluded from their degree curriculum.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Human wealth evolution is an accelerating expansion underpinned by a decelerating optimization process,"Optimization and expansion are two modes of staged evolution of complex systems where macroscopic observables change at a decreasing, respectively increasing, rate.   A prime example of evolutionary expansion, Gross Domestic Product (GDP) time series gauge economic activities in changing societal structures,} and the accelerating trend of their growth probably reflects a manyfold increase of the human interactions that drive change. We show how optimization and expansion can coexist by replacing `wall clock time' $t$ as independent variable with a measure of human interactions intensity $$.   Our analysis of eight centuries of yearly GDP data from three regions of Western Europe is carried out in two steps. First, a Monte Carlo algorithm is used to fit the GDP data to a piecewise continuous function comprising a sequence of exponentials with different exponents.   In a second step, GDP data are plotted vs. $$ and shown to display two logarithmic regimes, both decelerating, that are joined by a power-law cross-over period.   We connect the end of the first regime and the beginning of the second with the dawn of the Industrial Revolution and the societal impact of new transport, communication and production technologies that became widely available after World War I.   We conclude that wealth evolution in terms of $$ is a decelerating process with the hallmarks of record dynamics optimization.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Citation Statistics From More Than a Century of Physical Review,"We study the statistics of citations from all Physical Review journals for the 110-year period 1893 until 2003. In addition to characterizing the citation distribution and identifying publications with the highest citation impact, we investigate how citations evolve with time. There is a positive correlation between the number of citations to a paper and the average age of citations. Citations from a publication have an exponentially decaying age distribution; that is, old papers tend to not get cited. In contrast, the citations to a publication are consistent with a power-law age distribution, with an exponent close to -1 over a time range of 2 -- 20 years. We also identify a number of strongly-correlated citation bursts and other dramatic features in the time history of citations to individual publications.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Fusion divided: what prevented European collaboration on controlled thermonuclear fusion in 1958,"The European Organization for Nuclear Research (CERN) in Geneva is renowned for operating the world`s largest particle accelerator and is often regarded as a model of high-profile international collaboration. Less well known, however, is a key episode from the late 1950s, when CERN was confronted with the research priorities of similar organisations. The issue centred on a CERN-sponsored study group on controlled thermonuclear fusion, which brought together scientists from CERN member states, as well as representatives from the European Atomic Energy Community (EURATOM), the European Nuclear Energy Agency (ENEA), and the US Atomic Energy Commission (AEC). While the CERN Study Group on Fusion Problems succeeded in creating an international network for exchanging reports and coordinating projects to avoid duplication, it ultimately failed to establish joint fusion research programmes. This article explores the reasons behind this outcome to provide insights into intergovernmental power dynamics, underlying competition, and how these factors favoured the creation of a new fusion research institution in the UK, the Culham Laboratory. In doing so, the article contributes to a deeper understanding of the role of science in European integration, while also highlighting that CERN`s involvement in application-oriented research remains an underexplored aspect of its history.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Avoiding Intellectual Stagnation: The Starship as an Expander of Minds,"Interstellar exploration will advance human knowledge and culture in multiple ways. Scientifically, it will advance our understanding of the interstellar medium, stellar astrophysics, planetary science and astrobiology. In addition, significant societal and cultural benefits will result from a programme of interstellar exploration and colonisation. Most important will be the cultural stimuli resulting from expanding the horizons of human experience, and increased opportunities for the spread and diversification of life and culture through the Galaxy. Ultimately, a programme of interstellar exploration may be the only way for human (and post-human) societies to avoid the intellectual stagnation predicted for the ""end of history"".","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Science for Peace in the Benefit of Humankind. The Hippocratic Oath for Scientists concept,"This article shows the importance that has had the scientific research, the technological development and the innovation processes in increasing the lethality of the available weapons during the last century. A set of initiatives promoted by the scientific community to stop the nuclear arms race that threatened the continuation of life on the planet is described. At this point, a thorough survey of the texts and proposals of Hippocratic Oaths for Scientists presented at different epochs is made. It is observed that the interest in linking ethical aspects with science and technology issues shows an exponential growth behavior since the Second World War. It is shown how the several proposals of oaths and ethical commitments for scientists, engineers and technologists are disseminated following a logistic growth behavior, in the same manner as a disembodied technology in a particular niche. The data analysis shows that there is a coincidence between the maximum rate of proposals and the historical moment at which the world had deployed the largest number of nuclear warheads (70,586) as well as the largest world military expenditures in history (USD 1,485,000,000,000). Subsequently, the origin of the Hippocratic Oath for Scientists used for more than two decades in graduation ceremonies at the Faculty of Exact and Natural Sciences of the University of Buenos Aires is analyzed and linked with the historical circumstances of its birth.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Innovators and Interpreters: The Historic Role of Women in Science,"Until this century, the number of working female scientists has been indeterminate. Prevailing wisdom indicates that women, historically, have not excelled in the mathematics and sciences, for various reasons. These range from societal pressures to marry and bear children, to a lack of systematic scientific education for females, to the lack of opportunity in institutions and industry. Women, until comparatively recently, were not in control of their own financial lives, and were therefore dependent upon the goodwill of a husband, father or brother, should they attempt to enter academic life. Many times, their accomplishments, and the accomplishments of these collaborating relatives, become so merged as to become indistinguishable. Nevertheless, with all these factors working against them, history records an occasional bright star. Their diaries, memoirs and correspondence detail both the similarities and the differences between their roles and the roles of modern female scientists, in their own minds, and in the estimation of those closest to them. There exists, in science, a dichotomy between the desire to understand one's universe, and the desire to change the understanding of others. These two fundamental desires lead future scientists to the discipline. Historically, the role of women has largely been interpretive, rather than innovative, by their own estimation. But is interpretation the ""natural"" role of women scientists?","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diversity legitimizes science: Holding basic research in the physical sciences accountable to the public,"The American scientific community is reeling from funding cuts and policy directives that will debilitate scientific research and education. The underlying hostilities fueling these attacks have intensified in recent years as the COVID-19 pandemic increased suspicion of scientific experts and the institutional embrace of diversity, equity, and inclusion (DEI) policies in 2020 prompted a backlash along longstanding political fault lines. Under the banner of anti-elitism, opponents of science and DEI have formed a coalition that sees attacks on higher education as a strategic means to achieve their political ends. While some of their arguments contain legitimate criticisms, academics must resist these attacks that seek to dismantle higher education altogether. Instead, we should engage the public in our research process, build a scientific practice representative of and accountable to the communities we serve, and interrogate the aims of our work by critically studying the history of science.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Epistemic Phase Transitions in Mathematical Proofs,"Mathematical proofs are both paradigms of certainty and some of the most explicitly-justified arguments that we have in the cultural record. Their very explicitness, however, leads to a paradox, because the probability of error grows exponentially as the argument expands. When a mathematician encounters a proof, how does she come to believe it? Here we show that, under a cognitively-plausible belief formation mechanism combining deductive and abductive reasoning, belief in mathematical arguments can undergo what we call an epistemic phase transition: a dramatic and rapidly-propagating jump from uncertainty to near-complete confidence at reasonable levels of claim-to-claim error rates. To show this, we analyze an unusual dataset of forty-eight machine-aided proofs from the formalized reasoning system Coq, including major theorems ranging from ancient to 21st Century mathematics, along with five hand-constructed cases including Euclid, Apollonius, Hernstein's Topics in Algebra, and Andrew Wiles's proof of Fermat's Last Theorem. Our results bear both on recent work in the history and philosophy of mathematics on how we understand proofs, and on a question, basic to cognitive science, of how we justify complex beliefs.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Evolution of timekeeping from water clock to quartz clock -- the curious case of the Bulova ACCUTRON 214 the first transistorized wristwatch,"The technological discoveries and developments since dawn of civilization that resulted in the modern wristwatch are linked to the evolution of Science itself. A history of over 6000 years filled with amazing technical prowess since the emergence of the first cities in Mesopotamia established by the umer civilization. Usage of gears for timekeeping has its origin in the Islamic Golden Age about 1000 years ago. Although gears have been known for over 2000 years such as found in the Antikythera Mechanism. Only in the seventeenth century springs started to be used in clock making. In the eighteenth century the amazing \textit{Tourbillon} was designed and built to increase clock accuracy. In the nineteenth century the tuning fork was used for the first time as timebase. Wristwatches started to become popular in the beginning of the twentieth century. Later in the second half of the twentieth century the first electronic wristwatch was designed and produced, which brings us to the curious case of the Bulova \textit{ACCUTRON} caliber 214 the first transistorized wristwatch, another marvel of technological innovation and craftsmanship whose operation is frequently misunderstood. In this paper the historical evolution of timekeeping is presented. The goal is to show the early connection between Science and Engineering in the development of timekeeping devices. This linked development only became common along the twentieth century and beyond.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Social Network Analysis: Bibliographic Network Analysis of the Field and its Evolution / Part 1. Basic Statistics and Citation Network Analysis,"In this paper, we present the results of the study on the development of social network analysis (SNA) discipline and its evolution over time, using the analysis of bibliographic networks. The dataset consists of articles from the Web of Science Clarivate Analytics database and those published in the main journals in the field (70,000+ publications), created by searching for the key word ""social network*."" From the collected data, we constructed several networks (citation and two-mode, linking publications with authors, keywords and journals). Analyzing the obtained networks, we evaluated the trends in the field`s growth, noted the most cited works, created a list of authors and journals with the largest amount of works, and extracted the most often used keywords in the SNA field. Next, using the Search path count approach, we extracted the main path, key-route paths and link islands in the citation network. Based on the probabilistic flow node values, we identified the most important articles. Our results show that authors from the social sciences, who were most active through the whole history of the field development, experienced the ""invasion"" of physicists from 2000's. However, starting from the 2010's, a new very active group of animal social network analysis has emerged.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
(Non)-neutrality of science and algorithms: Machine Learning between fundamental physics and society,"The impact of Machine Learning (ML) algorithms in the age of big data and platform capitalism has not spared scientific research in academia. In this work, we will analyse the use of ML in fundamental physics and its relationship to other cases that directly affect society. We will deal with different aspects of the issue, from a bibliometric analysis of the publications, to a detailed discussion of the literature, to an overview on the productive and working context inside and outside academia. The analysis will be conducted on the basis of three key elements: the non-neutrality of science, understood as its intrinsic relationship with history and society; the non-neutrality of the algorithms, in the sense of the presence of elements that depend on the choices of the programmer, which cannot be eliminated whatever the technological progress is; the problematic nature of a paradigm shift in favour of a data-driven science (and society). The deconstruction of the presumed universality of scientific thought from the inside becomes in this perspective a necessary first step also for any social and political discussion. This is the subject of this work in the case study of ML.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Speculative Physics: the Ontology of Theory and Experiment in High Energy Particle Physics and Science Fiction,"The dissertation brings together approaches across the fields of physics, critical theory, literary studies, philosophy of physics, sociology of science, and history of science to synthesize a hybrid approach for instigating more rigorous and intense cross-disciplinary interrogations between the sciences and the humanities. There are two levels of conversations going on in the dissertation; at the first level, the discussion is centered on a critical historiography and philosophical implications of the discovery Higgs boson in relation to its position at the intersection of old (current) and the potential for new possibilities in quantum physics; I then position my findings on the Higgs boson in connection to the double-slit experiment that represents foundational inquiries into quantum physics, to demonstrate the bridge between fundamental physics and high energy particle physics. The conceptualization of the variants of the double-slit experiment informs the aforementioned critical comparisons. At the second level of the conversation, theories are produced from a close study of the physics objects as speculative engine for new knowledge generation that are then reconceptualized and re-articulated for extrapolation into the speculative ontology of hard science fiction, particularly the hard science fiction written with the double intent of speaking to the science while producing imaginative and socially conscious science through the literary affordances of science fiction. The works of science fiction examined here demonstrate the tension between the internal values of physics in the practice of theory and experiment and questions on ethics, culture, and morality.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Feyerabend and physics,"Feyerabend frequently discussed physics. He also referred to the history of the subject when motivating his philosophy of science. Alas, as some examples show, his understanding of physics remained superficial. In this respect, Feyerabend is like Popper; the difference being his self-criticism later on, and the much more tolerant attitude toward the allowance of methods. Quite generally, partly due to the complexity of the formalism and the new challenges of their findings, which left philosophy proper at a loss, physicists have attempted to developed their own meaning of their subject. For instance, in recent years, the interpretation of quantum mechanics has stimulated a new type of experimental philosophy, which seeks to operationalize emerging philosophical issues; issues which are incomprehensible for most philosophers. In this respect, physics often appears to be a continuation of philosophy by other means. Yet, Feyerabend has also expressed profound insights into the possibilities for the progress of physics, a legacy which remains to be implemented in the times to come: the conquest of abundance, the richness of reality, the many worlds which still await discovery, and the vast openness of the physical universe.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Taming of Plutonium: Pu Metallurgy and the Manhattan Project,"We describe the wartime challenges associated with the rapid developments in plutonium chemistry and metallurgy that were necessary to produce the core of the Trinity Device. Beginning with microgram quantities of plutonium metal late in 1943, initial measurements showed a wide and confusing variance in density and other properties. These confusing results were the first clues to the astounding complexity of plutonium. As this complexity was revealed, it introduced new challenges for the fabrication of kilogram-scale parts. In a remarkable period from January 1944 to June 1945, Manhattan Project scientists made rapid progress in understanding plutonium chemistry and metallurgy. By early 1945, they had discovered five of the six ambient-pressure phases of unalloyed plutonium and reported the density of these phases to within a value of 0.1 g/cm$^3$ of those accepted today. They solved the stability problem introduced by these phases with a rapid alloy development program that ultimately identified gallium as the preferred element to stabilize the delta-phase, producing a plutonium alloy still of scientific and technical interest today. We conclude with a description of post-war developments in these areas, including applications of wartime plutonium metallurgy to civilian applications in nuclear reactors. We dedicate this paper to the memory of Ed Hammel, the Manhattan Project plutonium metallurgist whose previous description and documentation of plutonium history during the war has been essential in our research.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diverse Misinformation: Impacts of Human Biases on Detection of Deepfakes on Networks,"Social media platforms often assume that users can self-correct against misinformation. However, social media users are not equally susceptible to all misinformation as their biases influence what types of misinformation might thrive and who might be at risk. We call ""diverse misinformation"" the complex relationships between human biases and demographics represented in misinformation. To investigate how users' biases impact their susceptibility and their ability to correct each other, we analyze classification of deepfakes as a type of diverse misinformation. We chose deepfakes as a case study for three reasons: 1) their classification as misinformation is more objective; 2) we can control the demographics of the personas presented; 3) deepfakes are a real-world concern with associated harms that must be better understood. Our paper presents an observational survey (N=2,016) where participants are exposed to videos and asked questions about their attributes, not knowing some might be deepfakes. Our analysis investigates the extent to which different users are duped and which perceived demographics of deepfake personas tend to mislead. We find that accuracy varies by demographics, and participants are generally better at classifying videos that match them. We extrapolate from these results to understand the potential population-level impacts of these biases using a mathematical model of the interplay between diverse misinformation and crowd correction. Our model suggests that diverse contacts might provide ""herd correction"" where friends can protect each other. Altogether, human biases and the attributes of misinformation matter greatly, but having a diverse social group may help reduce susceptibility to misinformation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The End of Hyperbolic Growth in Human Population and CO$_2$ Emissions,"Using current empirical data from 10,000 BCE to 2023 CE, we re-examine a hyperbolic pattern of human population growth, which was identified by von Foerster et al. in 1960 with a predicted singularity in 2026. We find that human population initially grew exponentially in time as $N(t)\propto e^{t/T}$ with $T$=2080 years. This growth then gradually evolved to be super-exponential with a form similar to the Bose function in statistical physics. Around 1700, population growth further accelerated, entering the hyperbolic regime as $N(t)\propto(t_s-t)^{-1}$ with the extrapolated singularity year $t_s$=2030, which is close to the prediction by von Foerster et al. We attribute the switch from the super-exponential to the hyperbolic regime to the onset of the Industrial Revolution and the transition to massive use of fossil fuels. This claim is supported by a linear relation that we find between the increase in the atmospheric CO$_2$ level and population from 1700 to 2000. In the 21st century, we observe that the inverse population curve $1/N(t)$ deviates from a straight line and follows a pattern of ""avoided crossing"" described by the square root of the Lorentzian function. Thus, instead of a singularity, we predict a peak in human population at $t_s$=2030 of the time width $$=32 years. We also find that the increase in CO$_2$ level since 1700 is well fitted by ${\rm arccot}[(t_s-t)/_F]$ with $_F$=40 years, which implies a peak in the annual CO$_2$ emissions at the same year $t_s$=2030.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
First echoes of relativity in Argentine astronomy,"We consider the attitude of astronomers in Argentina in connection with the new problems posed by relativity theory, before and after GR was presented. We begin considering the sequence of ""technical"" publications that appeared and use it to attempt to identify who were the relativity leaders and authors in the Argentina scientific community of the 1910-1920s. Among them there are natives of Argentina, permanent resident scientists, and occasional foreign visitors. They are either academic scientists, or high school teachers; we leave aside the {\it philosophers} and the {\it aficionados}. We discuss the scientific facts and publications they handled, the modernity of their information and the ""language"" they use to transmit their ideas. Finally, we consider astronomers proper; first Charles Perrine, an astronomer interested in astrophysics, contracted by the government of Argentina in the USA as director of its main observatory. He became interested in testing the possible deflection of light rays by the Sun towards 1912; his Argentine expedition was the first to attempt that test. Perhaps Perrine was not so much interested in relativity as in testing the particular astronomical effects it predicted. In any case, he attempted the test with the acquiescence and financial support of the Argentine state, and as a leading member of its official scientific elite. We contrast his very specific and strictly scientific efforts with those of our second astronomer, Jos Ubach, SJ, a secondary school teacher of science at a leading Buenos Aires Catholic school who reported in response to Eddington's expedition. Finally, our third astronomer is Flix Aguilar, who made an effort to contribute to the public understanding of Einstein's theories in 1924, when Einstein's visit to Argentina had become a certainty. [abridged]","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Small-area Population Forecast in a Segregated City using Density-Functional Fluctuation Theory,"Decisions regarding housing, transportation, and resource allocation would all benefit from accurate small-area population forecasts. While various tried-and-tested forecast methods exist at regional scales, developing an accurate neighborhood-scale forecast remains a challenge partly due to complex drivers of residential choice ranging from housing policies to social preferences and economic status that cumulatively cause drastic neighborhood-scale segregation. Here, we show how to forecast the dynamics of neighborhood-scale demographics by extending a novel statistical physics approach called Density-Functional Fluctuation Theory (DFFT) to multi-component time-dependent systems. In particular, this technique observes the fluctuations in neighborhood-scale demographics to extract effective drivers of segregation. As a demonstration, we simulate a segregated city using a Schelling-type segregation model, and found that DFFT accurately predicts how a city-scale demographic change trickles down to block scales. Should these results extend to actual human populations, DFFT could capitalize on the recent advances in demographic data collection and regional-scale forecasts to improve upon current small-area population forecasts.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
On a kinetic opinion formation model for pre-election polling,"Motivated by recent successes in model-based pre-election polling, we propose a kinetic model for opinion formation which includes voter demographics and socio-economic factors like age, sex, ethnicity, education level, income and other measurable factors like behaviour in previous elections or referenda as a key driver in the opinion formation dynamics. The model is based on Toscani's kinetic opinion formation model and the leader-follower model of Dring et al., and leads to a system of coupled Boltzmann-type equations and associated, approximate Fokker-Planck-type systems. Numerical examples using data from general elections in the United Kingdom show the effect different demographics have on the opinion formation process and the outcome of elections.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Moons Are Planets: Scientific Usefulness Versus Cultural Teleology in the Taxonomy of Planetary Science,"We argue that taxonomical concept development is vital for planetary science as in all branches of science, but its importance has been obscured by unique historical developments. The literature shows that the concept of planet developed by scientists during the Copernican Revolution was theory-laden and pragmatic for science. It included both primaries and satellites as planets due to their common intrinsic, geological characteristics. About two centuries later the non-scientific public had just adopted heliocentrism and was motivated to preserve elements of geocentrism including teleology and the assumptions of astrology. This motivated development of a folk concept of planet that contradicted the scientific view. The folk taxonomy was based on what an object orbits, making satellites out to be non-planets and ignoring most asteroids. Astronomers continued to keep primaries and moons classed together as planets and continued teaching that taxonomy until the 1920s. The astronomical community lost interest in planets ca. 1910 to 1955 and during that period complacently accepted the folk concept. Enough time has now elapsed so that modern astronomers forgot this history and rewrote it to claim that the folk taxonomy is the one that was created by the Copernican scientists. Starting ca. 1960 when spacecraft missions were developed to send back detailed new data, there was an explosion of publishing about planets including the satellites, leading to revival of the Copernican planet concept. We present evidence that taxonomical alignment with geological complexity is the most useful scientific taxonomy for planets. It is this complexity of both primary and secondary planets that is a key part of the chain of origins for life in the cosmos.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Who Invented the Trinity Nuclear Test's Christy Gadget? Patents and Evidence from the Archives,"The Christy Gadget is the informal name for the plutonium device detonated in the Trinity test on July 16, 1945. In September 1944, Robert Christy, working in the theoretical implosion group, proposed a novel concept that altered the design of the nuclear core in Fat Man. While scientists originally intended to use a hollow sphere of plutonium, this design entailed substantial risk, due to the likelihood of asymmetries resulting from implosion. Christy proposed changing the design to a solid sphere of plutonium with a modulated neutron source, and the design was eventually adopted, tested at Trinity, and used in the attack on Nagasaki. While there is no question regarding the important role that Christy played in demonstrating its feasibility as a reliable design, there is a debate as to who initially proposed the idea; though most sources have attributed this invention to Christy, some historical sources have attributed credit to Christy's group leader, Rudolf Peierls, or indeed other scientists. This paper seeks to outline and resolve this dispute. We present new unclassified evidence extracted from previously unavailable sources (to unclassified audiences) from the National Security Research Center archives at Los Alamos National Laboratory. This evidence consists of 1945-1946 patent documentation, oral history interview tapes of Christy and Peierls, and monthly 1944 progress reports from the Theoretical Division. Though Christy and Peierls share joint credit on the patent, both Christy's and Peierls' words and writings, together with sources from Hans Bethe and Edward Teller, support the traditional view that Christy was indeed the originator of the idea. While Christy does deserve the majority of the credit for the invention and design, we acknowledge the important role Peierls and von Neumann played in its development.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"""But since the affairs of men rest still uncertain, let's reason with the worst that may befall"": Probability, risk, and the 2009 L'Aquila Earthquake","This article is a commentary on the verdict of the ""L'Aquila Six"", the group of bureaucrats and scientists tried by an Italian court as a result of their public statements in advance of the quake of 2009 Apr. 6 that left the city in ruins and cause more than 300 deaths. It was not the worst such catastrophic event in recent Italian history, but it was one of -- if not the -- worst failures of risk assessment and preventive action. The six were found guilty and condemned by a first level of the justice system to substantial prison terms. The outcry provoked by the verdict in the world press and the international scientific community has fueled the already fiery debate over whether the six should have been tried at all. They have been presented as martyrs to science being treated as scapegoats by a scientifically illiterate justice system and inflamed local population for not being able to perform the impossible (predict the event). Petitions of support have been drafted and signed by thousands of working scientists and technical experts in many fields excoriating the court and the country for such an outrage against the scientific community, often accompanied by ominous warnings about the chilling effect this will have on the availability of expert advice in times of need. My purpose in this essay is to explain why this view of the events of the trial is misguided, however well intentioned, and misinformed.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The unsustainable legacy of the Nuclear Age,"It is seldom acknowledged the tremendous burden that the Nuclear Age leaves on future generations, and the environment, for an extremely long time. Nuclear processes, and products, are activated at energies millions of times higher than the energies of chemical processes, and consequently they cannot be eliminated by the natural environment on Earth. So it turns out that hundreds of nuclear tests performed in the atmosphere left a huge radioactive contamination; Rosalie Bertell estimated 1,300 millions victims of the Nuclear Age; civil nuclear programs have produced enormous quantities of radioactive waste, whose final disposal has not been solved by any country; decommissioning of tens of shut down nuclear plants shall involve costs which were underestimated in the past; spent nuclear fuel accumulates in decontamination pools, or in dry cask storage, but no final storage has been carried out yet; radioactivity of spent fuel will last for tens of thousand years; military nuclear programs leave, besides almost 15,000 nuclear warheads, approximately 1,300 metric tons of plutonium; even mining of natural uranium was, and is, carried out mainly by poor and exploited populations, which suffer serious health consequences; paradoxically enough (or maybe not), French territory itself is widely contaminated. All these facts have been downplayed during the whole history of the Nuclear Age. Future generations shall not be grateful.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Correlations of consumption patterns in social-economic networks,"We analyze a coupled anonymized dataset collecting the mobile phone communication and bank transactions history of a large number of individuals. After mapping the social structure and introducing indicators of socioeconomic status, demographic features, and purchasing habits of individuals we show that typical consumption patterns are strongly correlated with identified socioeconomic classes leading to patterns of stratification in the social structure. In addition we measure correlations between merchant categories and introduce a correlation network, which emerges with a meaningful community structure. We detect multivariate relations between merchant categories and show correlations in purchasing habits of individuals. Our work provides novel and detailed insight into the relations between social and consuming behaviour with potential applications in recommendation system design.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The geographic spread of COVID-19 correlates with the structure of social networks as measured by Facebook,"We use aggregated data from Facebook to show that COVID-19 is more likely to spread between regions with stronger social network connections. Areas with more social ties to two early COVID-19 ""hotspots"" (Westchester County, NY, in the U.S. and Lodi province in Italy) generally had more confirmed COVID-19 cases by the end of March. These relationships hold after controlling for geographic distance to the hotspots as well as the population density and demographics of the regions. As the pandemic progressed in the U.S., a county's social proximity to recent COVID-19 cases and deaths predicts future outbreaks over and above physical proximity and demographics. In part due to its broad coverage, social connectedness data provides additional predictive power to measures based on smartphone location or online search data. These results suggest that data from online social networks can be useful to epidemiologists and others hoping to forecast the spread of communicable diseases such as COVID-19.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Critical network effect induces business oscillations in multi-level marketing systems,"The ""social-networking revolution"" of late (e.g., with the advent of social media, Facebook, and the like) has been propelling the crusade to elucidate the embedded networks that underlie economic activity. An unexampled synthesis of network science and economics uncovers how the web of human interactions spurred by familiarity and similarity could potentially induce the ups and downs ever so common to our economy. Zeroing in on the million-strong global industry known as multi-level marketing, this study finds that such a socially-powered enterprise can only work stably through discrimination about who to make entrepreneurial connections with.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Dissecting the Spatial Structure of Cities from Human Mobility Patterns to Define Functional Urban Boundaries,"Since the industrial revolution, accelerated urban growth has overflown administrative divisions, merged cities into large built extensions, and blurred the boundaries between urban and rural land-uses. These traits, present in most of contemporary metropolis, complicate the definition of cities, a crucial issue considering that objective and comparable metrics are the basic inputs needed for the planning and design of sustainable urban environments. In this context, city definitions that respond to administrative or political criteria usually overlook human dynamics, a key factor that could help to make cities comparable across the urban fabric of diverse social, cultural and economic realities. Using a technique based on the spectral analysis of complex networks, we rank places in 11 of the major Chilean urban regions from a high-resolution human mobility dataset: Official origin-destination (OD) surveys. We propose a method for further distinguishing urban and rural land-uses within these regions, by means of a network centrality measure from which we construct a spectre of geographic places. This spectre, constructed from the ranking of locations as measured by their approximate number of embedded human flows, allows us to probe several urban boundaries. From the analysis of the urban scaling exponent of trips in relation to the population across these city delineations, we identify two clearly distinct scaling regimes occurring in urban and rural areas. The comparison of our results with land cover derived from remote sensing suggests that, for the case of trips, the scaling exponent in urban areas is close to linear. We conclude with estimations for well-formed cities in the Chilean urban system, which according to our analysis could emerge from clusters composed by places that capture at least ~138 trips (over the expectation) of the underlying mobility network.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diffusion of Lexical Change in Social Media,"Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity -- especially with regard to race -- plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified ""netspeak"" dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
It's good to be popular in high school: A look at disparities in STEM AP offerings in Northern California public high schools,"In 2018, in response to the proposed elimination of physics at a predominately Hispanic and socioeconomically disadvantaged (SED) high school, the Northern California/Nevada chapter of the AAPT investigated school demographics and their effect on physics offerings in public high schools in our region. As access was a key issue, the focus was on public, non-charter high schools, which are free to students and do not require winning a lottery for attendance. As reported previously, the data revealed that the percentage of Hispanic students and the percentage of SED students at a high school are highly correlated (r^2=0.60). Additionally, these factors could be used as predictors of a school's physics offerings. To determine if the disparities in course offerings extended through other Advanced Placement (AP) STEM classes the data was further analyzed, revealing that as the popularity of an AP exam drops, so do the relative odds of it being offered, when comparing schools with different demographics. A Northern California public high school student is much more likely to get a strong selection of AP STEM classes if their school serves an affluent, non-Hispanic student majority rather than mostly poor, Hispanic students.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Epicast 2.0: A large-scale, demographically detailed, agent-based model for simulating respiratory pathogen spread in the United States","The recent history of respiratory pathogen epidemics, including those caused by influenza and SARS-CoV-2, has highlighted the urgent need for advanced modeling approaches that can accurately capture heterogeneous disease dynamics and outcomes at the national scale, thereby enhancing the effectiveness of resource allocation and decision-making. In this paper, we describe Epicast 2.0, an agent-based model that utilizes a highly detailed, synthetic population and high-performance computing techniques to simulate respiratory pathogen transmission across the entire United States. This model replicates the contact patterns of over 320 million agents as they engage in daily activities at school, work, and within their communities. Epicast 2.0 supports vaccination and an array of non-pharmaceutical interventions that can be promoted or relaxed via highly granular, user specified policies. We illustrate the model's capabilities using a wide range of outbreak scenarios, highlighting the model's varied dynamics as well as its extensive support for policy exploration. This model provides a robust platform for conducting what if scenario analysis and providing insights into potential strategies for mitigating the impacts of infectious diseases.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
On the emergence of scale-free production networks,We propose a simple dynamical model of the formation of production networks among monopolistically competitive firms. The model subsumes the standard general equilibrium approach  la Arrow-Debreu but displays a wide set of potential dynamic behaviors. It robustly reproduces key stylized facts of firms' demographics. Our main result is that competition between intermediate good producers generically leads to the emergence of scale-free production networks.,"cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Introducing AstroGen: The Astronomy Genealogy Project,"The Astronomy Genealogy Project (""AstroGen""), a project of the Historical Astronomy Division of the American Astronomical Society (AAS), will soon appear on the AAS website. Ultimately, it will list the world's astronomers with their highest degrees, theses for those who wrote them, academic advisors (supervisors), universities, and links to the astronomers or their obituaries, their theses when on-line, and more. At present the AstroGen team is working on those who earned doctorates with astronomy-related theses. We show what can be earned already, with just ten countries essentially completed.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Case for the double-blind peer review,"Peer review is a process designed to produce a fair assessment of research quality before the publication of scholarly work in a journal. Demographics, nepotism, and seniority have been all shown to affect reviewer behavior suggesting the most common, single-blind review method (or the less common open review method) might be biased. A survey of current research indicates that double-blind review offers a solution to many biases stemming from author's gender, seniority, or location without imposing any significant downsides.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modeling peer and external influence in online social networks,"Opinion polls mediated through a social network can give us, in addition to usual demographics data like age, gender and geographic location, a friendship structure between voters and the temporal dynamics of their activity during the voting process. Using a Facebook application we collected friendship relationships, demographics and votes of over ten thousand users on the referendum on the definition of marriage in Croatia held on 1st of December 2013. We also collected data on online news articles mentioning our application. Publication of these articles align closely with large peaks of voting activity, indicating that these external events have a crucial influence in engaging the voters. Also, existence of strongly connected friendship communities where majority of users vote during short time period, and the fact that majority of users in general tend to friend users that voted the same suggest that peer influence also has its role in engaging the voters. As we are not able to track activity of our users at all times, and we do not know their motivations for expressing their votes through our application, the question is whether we can infer peer and external influence using friendship network of users and the times of their voting. We propose a new method for estimation of magnitude of peer and external influence in friendship network and demonstrate its validity on both simulated and actual data.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Landscape of the Quantum Start-up Ecosystem,"The second quantum revolution has been producing groundbreaking scientific and technological outputs since the early 2000s; however, the scientific literature on the impact of this revolution on the industry, specifically on start-ups, is limited. In this paper, we present a landscaping study with a gathered dataset of 441 companies from 42 countries that we identify as quantum start-ups, meaning that they mainly focus on quantum technologies (QT) as their primary priority business. We answer the following questions: (1) What are the temporal and geographical distributions of the quantum start-ups? (2) How can we categorize them, and how are these categories populated? (3) Are there any patterns that we can derive from empirical data on trends? We found that more than 92% of these companies have been founded within the last 10 years, and more than 50% of them are located in the US, the UK, and Canada. We categorized the QT start-ups into six fields: (i) complementary technologies, (ii) quantum computing (hardware), (iii) quantum computing (software/application/simulation), (iv) quantum cryptography/communication, (v) quantum sensing and metrology, and (vi) supporting companies, and analyzed the population of each field both for countries, and temporally. Finally, we argue that low levels of quantum start-up activity in a country might be an indicator of a national initiative to be adopted afterwards, which later sees both an increase in the number of start-ups, and a diversification of activity in different QT fields.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Combining surveys and sensors to explore student behaviour,"Student belongingness is important for successful study paths, and group work forms an important part of modern university physics education. To study the group dynamics of introductory physics students at the University of Helsinki, we collected network data from seven laboratory course sections of approximately 20 students each for seven consecutive weeks. The data was collected via the SocioPatterns platform, and supplemented with students' major subject, year of study and gender. We also collected the Mechanics Baseline Test to measure physics knowledge and the Colorado Learning Attitudes about Science Survey to measure attitudes. We developed metrics for studying the small networks of the laboratory sessions by using connections of the teaching assistant as a constant. In the network, we found both demographically homogeneous and heterogeneous groups that are stable. While some students are consistently loosely connected to their networks, we were not able to identify risk factors. Based on our results, the physics laboratory course is equally successful in building strongly connected groups regardless of student demographics in the sections or the formed small groups. SocioPatterns supplemented with surveys thus provides an opportunity to look into the dynamics of students' social networks.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
SeCoNet: A Heterosexual Contact Network Growth Model for Human Papillomavirus Disease Simulation,"Human Papillomavirus infection is the most common sexually transmitted infection, and causes serious complications such as cervical cancer in vulnerable female populations in regions such as East Africa. Due to the scarcity of empirical data about sexual relationships in varying demographics, computationally modelling the underlying sexual contact networks is important to understand Human Papillomavirus infection dynamics and prevention strategies. In this work we present SeCoNet, a heterosexual contact network growth model for Human Papillomavirus disease simulation. The growth model consists of three mechanisms that closely imitate real-world relationship forming and discontinuation processes in sexual contact networks. We demonstrate that the networks grown from this model are scale-free, as are the real world sexual contact networks, and we demonstrate that the model can be calibrated to fit different demographic contexts by using a range of parameters. We also undertake disease dynamics analysis of Human Papillomavirus infection using a compartmental epidemic model on the grown networks. The presented SeCoNet growth model is useful to computational epidemiologists who study sexually transmitted infections in general and Human Papillomavirus infection in particular.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Strong gender differences in reproductive success variance, and the times to the most recent common ancestors","The Time To the Most Recent Common Ancestor (TMRCA) based on human mitochondrial DNA (mtDNA) is estimated to be twice that based on the non-recombining part of the Y chromosome (NRY). These TMRCAs have special demographic implications because mtDNA is transmitted only from mother to child, and NRY from father to son. Therefore, mtDNA reflects female history, and NRY, male history. To investigate what caused the two-to-one female-male TMRCA ratio in humans, we develop a forward-looking agent-based model (ABM) with overlapping generations and individual life cycles. We implement two main mating systems: polygynandry and polygyny with different degrees in between. In each mating system, the male population can be either homogeneous or heterogeneous. In the latter case, some males are `alphas' and others are `betas', which reflects the extent to which they are favored by female mates. A heterogeneous male population implies a competition among males with the purpose of signaling as alphas. The introduction of a heterogeneous male population is found to reduce by a factor 2 the probability of finding equal female and male TMRCAs and shifts the distribution of the TMRCA ratio to higher values. We find that high male-male competition is necessary to reproduce a TMRCA ratio of 2: less than half the males can be alphas and betas can have at most half the fitness of alphas. In addition, in the modes that maximize the probability of having a TMRCA ratio between 1.5 and 2.5, the present generation has 1.4 times as many female as male ancestors. We also tested the effect of sex-biased migration and sex-specific death rates and found that these are unlikely to explain alone the sex-biased TMRCA ratio observed in humans. Our results support the view that we are descended from males who were successful in a highly competitive context, while females were facing a much smaller female-female competition.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Statistical Machine Learning Meets High-Dimensional Spatiotemporal Challenges -- A Case Study of COVID-19 Modeling,"Diverse non-pharmacological interventions (NPIs), serving as the primary approach for COVID-19 control prior to pharmaceutical interventions, showed heterogeneous spatiotemporal effects on pandemic management. Investigating the dynamic compounding impacts of NPIs on pandemic spread is imperative. However, the challenges posed by data availability of high-dimensional human behaviors and the complexity of modeling changing and interrelated factors are substantial. To address these challenges, this study analyzed social media data, COVID-19 case rates, Apple mobility data, and the stringency of stay-at-home policies in the United States throughout the year 2020, aiming to (1) uncover the spatiotemporal variations in NPIs during the COVID-19 pandemic utilizing geospatial big data; (2) develop a statistical machine learning model that incorporates spatiotemporal dependencies and temporal lag effects for the detection of relationships; (3) dissect the impacts of NPIs on the pandemic across space and time. Three indices were computed based on Twitter (currently known as X) data: the Negative and Positive Sentiments Adjusted by Demographics (N-SAD and P-SAD) and the Ratio Adjusted by Demographics (RAD), representing negative sentiment, positive sentiment, and public awareness of COVID-19, respectively. The Multivariate Bayesian Structural Time Series Time Lagged model (MBSTS-TL) was proposed to investigate the effects of NPIs, accounting for spatial dependencies and temporal lag effects. The developed MBSTS-TL model exhibited a high degree of accuracy. Determinants of COVID-19 health impacts transitioned from an emphasis on human mobility during the initial outbreak period to a combination of human mobility and stay-at-home policies during the rapid spread phase, and ultimately to the compound of human mobility, stay-at-home policies, and public awareness of COVID-19.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Generating functional analysis of minority games with inner product strategy definitions,"We use generating functional methods to solve the so-called inner product versions of the minority game (MG), with fake and/or real market histories, by generalizing the theory developed recently for look-up table MGs with real histories. The phase diagrams of the lookup table and inner product MG versions are generally found to be identical, with the exception of inner product MGs where histories are sampled linearly, which are found to be structurally critical. However, we encounter interesting differences both in the theory (where the role of the history frequency distribution in lookup table MGs is taken over by the eigenvalue spectrum of a history covariance matrix in inner product MGs) and in the static and dynamic phenomenology of the models. Our theoretical predictions are supported by numerical simulations.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Towards European Standards for Quantum Technologies,"The Second Quantum Revolution facilitates the engineering of new classes of sensors, communication technologies, and computers with unprecedented capabilities. Supply chains for quantum technologies are emerging, some focussed on commercially available components for enabling technologies and/or quantum-technologies research infrastructures, others with already higher technology-readiness levels, near to the market.   In 2018, the European Commission has launched its large-scale and long-term Quantum Flagship research initiative to support and foster the creation and development of a competitive European quantum technologies industry, as well as the consolidation and expansion of leadership and excellence in European quantum technology research. One of the measures to achieve an accelerated development and uptake has been identified by the Quantum Flagship in its Strategic Research Agenda: the promotion of coordinated, dedicated standardisation and certification efforts. Standardisation is indeed of paramount importance to facilitate the growth of new technologies, and the development of efficient and effective supply chains. The harmonisation of technologies, methodologies, and interfaces enables interoperable products, innovation, and competition, all leading to structuring and hence growth of markets. As quantum technologies are maturing, time has come to start thinking about further standardisation needs.   This article presents insights on standardisation for quantum technologies from the perspective of the CEN-CENELEC Focus Group on Quantum Technologies (FGQT), which was established in June 2020 to coordinate and support the development of standards relevant for European industry and research.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
To Apply or Not to Apply: A Survey Analysis of Grant Writing Costs and Benefits,"We surveyed 113 astronomers and 82 psychologists active in applying for federally funded research on their grant-writing history between January, 2009 and November, 2012. We collected demographic data, effort levels, success rates, and perceived non-financial benefits from writing grant proposals. We find that the average proposal takes 116 PI hours and 55 CI hours to write; although time spent writing was not related to whether the grant was funded. Effort did translate into success, however, as academics who wrote more grants received more funding. Participants indicated modest non-monetary benefits from grant writing, with psychologists reporting a somewhat greater benefit overall than astronomers. These perceptions of non-financial benefits were unrelated to how many grants investigators applied for, the number of grants they received, or the amount of time they devoted to writing their proposals. We also explored the number of years an investigator can afford to apply unsuccessfully for research grants and our analyses suggest that funding rates below approximately 20%, commensurate with current NIH and NSF funding, are likely to drive at least half of the active researchers away from federally funded research. We conclude with recommendations and suggestions for individual investigators and for department heads.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantum Index Report 2025,"The inaugural edition of the MIT Quantum Index Report (QIR). Quantum technologies are evolving from theoretical concepts into tangible technologies with commercial promise. Their rapid progress is capturing global attention and suggests we stand on the cusp of a second quantum revolution. Unlocking the quantum opportunity is not simple. One challenge is that quantum technologies can present a high barrier to understanding for nonexperts because they often rely on complex principles and concepts from a variety of specialist fields. This can lead to confusion and intimidation for business leaders, educators, policymakers and others. The Quantum Index Report aims to reduce the complexity and make it possible for a wider audience to have a deeper understanding of the quantum landscape. The Quantum Index Report provides a comprehensive, data-driven assessment of the state of quantum technologies. For this inaugural edition we have focused on quantum computing and networking. The report tracks, measures, and visualizes trends across research, development, education and public acceptance. It aggregates data from academia, industry and policy sources and aims to provide nonpartisan insights.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Whos Ditching the Bus?,"This paper uses stop-level passenger count data in four cities to understand the nation-wide bus ridership decline between 2012 and 2018. The local characteristics associated with ridership change are evaluated in Portland, Miami, Minneapolis/St-Paul, and Atlanta. Poisson models explain ridership as a cross-section and the change thereof as a panel. While controlling for the change in frequency, jobs, and population, the correlation with local socio-demographic characteristics are investigated using data from the American Community Survey. The effect of changing neighborhood demographics on bus ridership are modeled using Longitudinal Employer-Household Dynamics data. At a point in time, neighborhoods with high proportions of non-white, carless, and most significantly, high-school-educated residents are the most likely to have high ridership. Over time, white neighborhoods are losing the most ridership across all four cities. In Miami and Atlanta, places with high concentrations of residents with college education and without access to a car also lose ridership at a faster rate. In Minneapolis/St-Paul, the proportion of college-educated residents is linked to ridership gain. The sign and significance of these results remain consistent even when controlling for intra-urban migration. Although bus ridership is declining across neighborhood characteristics, these results suggest that the underlying cause of bus ridership decline must be primarily affecting the travel behavior of white bus riders.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
An inevitably aging world -- Analysis on the evolutionary pattern of age structure in 200 countries,"Ignoring the differences between countries, human reproductive and dispersal behaviors can be described by some standardized models, so whether there is a universal law of population growth hidden in the abundant and unstructured data from various countries remains unclear. The age-specific population data constitute a three-dimensional tensor containing more comprehensive information. The existing literature often describes the characteristics of global or regional population evolution by subregion aggregation and statistical analysis, which makes it challenging to identify the underlying rules by ignoring national or structural details. Statistical physics can be used to summarize the macro characteristics and evolution laws of complex systems based on the attributes and motions of masses of individuals by decomposing high-dimensional tensors. Specifically, it can be used to assess the evolution of age structure in various countries over the past approximately 70 years, rather than simply focusing on the regions where aging has become apparent. It provides a universal scheme for the growing elderly and working age populations, indicating that the demographics on all continents are inevitably moving towards an aging population, including the current ""young"" continents of Africa, and Asia, South America with a recent ""demographic dividend"". It is a force derived from the ""life cycle"", and most countries have been unable to avoid this universal evolutionary path in the foreseeable future.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Racial Impact on Infections and Deaths due to COVID-19 in New York City,"Redlining is the discriminatory practice whereby institutions avoided investment in certain neighborhoods due to their demographics. Here we explore the lasting impacts of redlining on the spread of COVID-19 in New York City (NYC). Using data available through the Home Mortgage Disclosure Act, we construct a redlining index for each NYC census tract via a multi-level logistical model. We compare this redlining index with the COVID-19 statistics for each NYC Zip Code Tabulation Area. Accurate mappings of the pandemic would aid the identification of the most vulnerable areas and permit the most effective allocation of medical resources, while reducing ethnic health disparities.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Common indicators hurt armed conflict prediction,"Are big conflicts different from small or medium size conflicts? To answer this question, we leverage fine-grained conflict data, which we map to climate, geography, infrastructure, economics, raw demographics, and demographic composition in Africa. With an unsupervised learning model, we find three overarching conflict types representing ``major unrest,'' ``local conflict,'' and ``sporadic and spillover events.'' Major unrest predominantly propagates around densely populated areas with well-developed infrastructure and flat, riparian geography. Local conflicts are in regions of median population density, are diverse socio-economically and geographically, and are often confined within country borders. Finally, sporadic and spillover conflicts remain small, often in low population density areas, with little infrastructure and poor economic conditions. The three types stratify into a hierarchy of factors that highlights population, infrastructure, economics, and geography, respectively, as the most discriminative indicators. Specifying conflict type negatively impacts the predictability of conflict intensity such as fatalities, conflict duration, and other measures of conflict size. The competitive effect is a general consequence of weak statistical dependence. Hence, we develop an empirical and bottom-up methodology to identify conflict types, knowledge of which can hurt predictability and cautions us about the limited utility of commonly available indicators.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The chanciness of time,"Digital network failures stemming from instabilities in measurements of temporal order motivate attention to concurrent events. A century of attempts to resolve the instabilities have never eliminated them. Do concurrent events occur at indeterminate times, or are they better seen as events to which the very concept of temporal order cannot apply? Logical dependencies of messages propagating through digital networks can be represented by marked graphs on which tokens are moved in formal token games. However, available mathematical formulations of these token games invoke ""markings"" -- global snapshots of the locations of tokens on the graph. The formulation in terms of global snapshots is misleading, because distributed networks are never still: they exhibit concurrent events inexpressible by global snapshots. We reformulate token games used to represent digital networks so as to express concurrency. The trick is to replace global snapshots with ""local snapshots."" Detached from any central clock, a local snapshot records an action at a node during a play of a token game. Assemblages of local records define acyclic directed graphs that we call history graphs. We show how history graphs represent plays of token games with concurrent motions, and, importantly, how history graphs can represent the history of a network operating while undergoing unpredictable changes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Can an interdisciplinary field contribute to one of the parent disciplines from which it emerged?,"In the light of contemporary discussions of inter and transdisciplinarity, this paper approaches econophysics and sociophysics to seek a response to the question -- whether these interdisciplinary fields could contribute to physics and economics. Drawing upon the literature on history and philosophy of science, the paper argues that the two way traffic between physics and economics has a long history and this is likely to continue in the future.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Sustained Online Amplification of COVID-19 Elites in the United States,"The ongoing, fluid nature of the COVID-19 pandemic requires individuals to regularly seek information about best health practices, local community spreading, and public health guidelines. In the absence of a unified response to the pandemic in the United States and clear, consistent directives from federal and local officials, people have used social media to collectively crowdsource COVID-19 elites, a small set of trusted COVID-19 information sources. We take a census of COVID-19 crowdsourced elites in the United States who have received sustained attention on Twitter during the pandemic. Using a mixed methods approach with a panel of Twitter users linked to public U.S. voter registration records, we find that journalists, media outlets, and political accounts have been consistently amplified around COVID-19, while epidemiologists, public health officials, and medical professionals make up only a small portion of all COVID-19 elites on Twitter. We show that COVID-19 elites vary considerably across demographic groups, and that there are notable racial, geographic, and political similarities and disparities between various groups and the demographics of their elites. With this variation in mind, we discuss the potential for using the disproportionate online voice of crowdsourced COVID-19 elites to equitably promote timely public health information and mitigate rampant misinformation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
You Tweet What You Eat: Studying Food Consumption Through Twitter,"Food is an integral part of our lives, cultures, and well-being, and is of major interest to public health. The collection of daily nutritional data involves keeping detailed diaries or periodic surveys and is limited in scope and reach. Alternatively, social media is infamous for allowing its users to update the world on the minutiae of their daily lives, including their eating habits. In this work we examine the potential of Twitter to provide insight into US-wide dietary choices by linking the tweeted dining experiences of 210K users to their interests, demographics, and social networks. We validate our approach by relating the caloric values of the foods mentioned in the tweets to the state-wide obesity rates, achieving a Pearson correlation of 0.77 across the 50 US states and the District of Columbia. We then build a model to predict county-wide obesity and diabetes statistics based on a combination of demographic variables and food names mentioned on Twitter. Our results show significant improvement over previous CHI research (Culotta'14). We further link this data to societal and economic factors, such as education and income, illustrating that, for example, areas with higher education levels tweet about food that is significantly less caloric. Finally, we address the somewhat controversial issue of the social nature of obesity (first raised by Christakis & Fowler in 2007) by inducing two social networks using mentions and reciprocal following relationships.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modeling PKT at a global level: A machine learning approach,"It is well-accepted that the ability to go from one place to another, or mobility, contributes significantly to one's wellbeing. The need for mobility is universal, but the demand for mobility shows a great variation on a country basis. This particular study looks at what are some of the most important factors on a global level that can help in predicting the passengerkilometers-travelled or passenger-miles-travelled (PKT/PMT) on a country by country basis. This particular work tries to quantify the impact of some of the key variables like Gross Domestic Product (GDP), population growth, employment rate, number of households, age demographics within the population and macroeconomic variables on the total vehicle-based travel within each country. A panel-based regression model is developed to identify the effect of some of the key macroeconomic variables on the countries' PKT growth.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Zipf's law, power laws, and maximum entropy","Zipf's law, and power laws in general, have attracted and continue to attract considerable attention in a wide variety of disciplines - from astronomy to demographics to software structure to economics to linguistics to zoology, and even warfare. A recent model of random group formation [RGF] attempts a general explanation of such phenomena based on Jaynes' notion of maximum entropy applied to a particular choice of cost function. In the present article I argue that the cost function used in the RGF model is in fact unnecessarily complicated, and that power laws can be obtained in a much simpler way by applying maximum entropy ideas directly to the Shannon entropy subject only to a single constraint: that the average of the logarithm of the observable quantity is specified.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Understanding infection risks of COVID-19 in the city: an investigation of infected neighborhoods in Wuhan,"During the COVID-19 pandemic, built environments in dense urban settings become major sources of infection. This study tests the difference of demographics and surrounding built environments across high-, medium- and low-infection neighborhoods, to inform the high-risk areas in the city. We found that high-infection neighborhoods own a higher ratio of aged population than other neighborhoods on average. However, it shows no statistical difference in terms of population density. Additionally, high-infection neighborhoods are closer to high-risk built environments than the others. In a walking distance, they also can access more of the high-risk built environments except for the wholesale markets and shopping malls. These findings advise policy-makers to deploy social distancing measures in precision, regulating the access of high-risk facilities to mitigate the impacts of COVID-19.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Tracing scientific influence,"Scientometrics is the field of quantitative studies of scholarly activity. It has been used for systematic studies of the fundamentals of scholarly practice as well as for evaluation purposes. Although advocated from the very beginning the use of scientometrics as an additional method for science history is still under explored. In this paper we show how a scientometric analysis can be used to shed light on the reception history of certain outstanding scholars. As a case, we look into citation patterns of a specific paper by the American sociologist Robert K. Merton.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Swayed by Friends or by the Crowd?,"We have conducted three empirical studies of the effects of friend recommendations and general ratings on how online users make choices. These two components of social influence were investigated through user studies on Mechanical Turk. We find that for a user deciding between two choices an additional rating star has a much larger effect than an additional friend's recommendation on the probability of selecting an item. Equally important, negative opinions from friends are more influential than positive opinions, and people exhibit more random behavior in their choices when the decision involves less cost and risk. Our results can be generalized across different demographics, implying that individuals trade off recommendations from friends and ratings in a similar fashion.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Exploring the Bitcoin Mesoscale,"The open availability of the entire history of the Bitcoin transactions opens up the possibility to study this system at an unprecedented level of detail. This contribution is devoted to the analysis of the mesoscale structural properties of the Bitcoin User Network (BUN), across its entire history (i.e. from 2009 to 2017). What emerges from our analysis is that the BUN is characterized by a core-periphery structure a deeper analysis of which reveals a certain degree of bow-tieness (i.e. the presence of a Strongly-Connected Component, an IN- and an OUT-component together with some tendrils attached to the IN-component). Interestingly, the evolution of the BUN structural organization experiences fluctuations that seem to be correlated with the presence of bubbles, i.e. periods of price surge and decline observed throughout the entire Bitcoin history: our results, thus, further confirm the interplay between structural quantities and price movements observed in previous analyses.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying Information Flow During Emergencies,"Recent advances on human dynamics have focused on the normal patterns of human activities, with the quantitative understanding of human behavior under extreme events remaining a crucial missing chapter. This has a wide array of potential applications, ranging from emergency response and detection to traffic control and management. Previous studies have shown that human communications are both temporally and spatially localized following the onset of emergencies, indicating that social propagation is a primary means to propagate situational awareness. We study real anomalous events using country-wide mobile phone data, finding that information flow during emergencies is dominated by repeated communications. We further demonstrate that the observed communication patterns cannot be explained by inherent reciprocity in social networks, and are universal across different demographics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Dynamic Behaviors of Mix-game Model and Its Applications,"This paper proposes a modification to Minority Game (MG) by adding some agents who play majority game into MG. So it is referred to as mix-game. The highlight of this model is that the two groups of agents in mix-game have different bounded abilities to deal with history information and to count their own performance. Through simulations, this paper finds out that the local volatilities change a lot by adding some agents who play majority game into MG, and the change of local volatilities largely depends on different combinations of history memories of the two groups. Furthermore this paper analyses the underlying mechanisms for this finding. It also gives an example of applications of mix-game.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Experimental study of the impact of historical information in human coordination,"We perform laboratory experiments to elucidate the role of historical information in games involving human coordination. Our approach follows prior work studying human network coordination using the task of graph coloring. We first motivate this research by showing empirical evidence that the resolution of coloring conflicts is dependent upon the recent local history of that conflict. We also conduct two tailored experiments to manipulate the game history that can be used by humans in order to determine (i) whether humans use historical information, and (ii) whether they use it effectively. In the first variant, during the course of each coloring task, the network positions of the subjects were periodically swapped while maintaining the global coloring state of the network. In the second variant, participants completed a series of 2-coloring tasks, some of which were restarts from checkpoints of previous tasks. Thus, the participants restarted the coloring task from a point in the middle of a previous task without knowledge of the history that led to that point. We report on the game dynamics and average completion times for the diverse graph topologies used in the swap and restart experiments.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Indigenization of Urban Mobility,"The identification of urban mobility patterns is very important for predicting and controlling spatial events. In this study, we analyzed millions of geographical check-ins crawled from a leading Chinese location-based social networking service (Jiepang.com), which contains demographic information that facilitates group-specific studies. We determined the distinct mobility patterns of natives and non-natives in all five large cities that we considered. We used a mixed method to assign different algorithms to natives and non-natives, which greatly improved the accuracy of location prediction compared with the basic algorithms. We also propose so-called indigenization coefficients to quantify the extent to which an individual behaves like a native, which depends only on their check-in behavior, rather than requiring demographic information. Surprisingly, the hybrid algorithm weighted using the indigenization coefficients outperformed a mixed algorithm that used additional demographic information, suggesting the advantage of behavioral data in characterizing individual mobility compared with the demographic information. The present location prediction algorithms can find applications in urban planning, traffic forecasting, mobile recommendation, and so on.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
An age structured demographic theory of technological change,"At the heart of technology transitions lie complex processes of social and industrial dynamics. The quantitative study of sustainability transitions requires modelling work, which necessitates a theory of technology substitution. Many, if not most, contemporary modelling approaches for future technology pathways overlook most aspects of transitions theory, for instance dimensions of heterogenous investor choices, dynamic rates of diffusion and the profile of transitions. A significant body of literature however exists that demonstrates how transitions follow S-shaped diffusion curves or Lotka-Volterra systems of equations. This framework is used ex-post since timescales can only be reliably obtained in cases where the transitions have already occurred, precluding its use for studying cases of interest where nascent innovations in protective niches await favourable conditions for their diffusion. In principle, scaling parameters of transitions can, however, be derived from knowledge of industrial dynamics, technology turnover rates and technology characteristics. In this context, this paper presents a theory framework for evaluating the parameterisation of S-shaped diffusion curves for use in simulation models of technology transitions without the involvement of historical data fitting, making use of standard demography theory applied to technology at the unit level. The classic Lotka-Volterra competition system emerges from first principles from demography theory, its timescales explained in terms of technology lifetimes and industrial dynamics. The theory is placed in the context of the multi-level perspective on technology transitions, where innovation and the diffusion of new socio-technical regimes take a prominent place, as well as discrete choice theory, the primary theoretical framework for introducing agent diversity.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Peer Ratings in Massive Online Social Networks,"Instant quality feedback in the form of online peer ratings is a prominent feature of modern massive online social networks (MOSNs). It allows network members to indicate their appreciation of a post, comment, photograph, etc. Some MOSNs support both positive and negative (signed) ratings. In this study, we rated 11 thousand MOSN member profiles and collected user responses to the ratings. MOSN users are very sensitive to peer ratings: 33% of the subjects visited the researcher's profile in response to rating, 21% also rated the researcher's profile picture, and 5% left a text comment. The grades left by the subjects are highly polarized: out of the six available grades, the most negative and the most positive are also the most popular. The grades fall into three almost equally sized categories: reciprocal, generous, and stingy. We proposed quantitative measures for generosity, reciprocity, and benevolence, and analyzed them with respect to the subjects' demographics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Moderate immigration may promote a peak of cooperation among natives,"In a world of hardening borders, nations may deprive themselves of enjoying the benefits of cooperative immigrants. Here, we analyze the effect of efficient cooperative immigrants on a population playing public goods games. We considered a population structured on a square lattice with individuals playing public goods games with their neighbors. The demographics are determined by stochastic birth, death, and migration. The strategies spread through imitation dynamics. Our model shows that cooperation among natives can emerge due to social contagion of good role-model agents that can provide better quality public goods. Only a small fraction of efficient cooperators, among immigrants, is enough to trigger cooperation across the native population. We see that native cooperation achieves its peak at moderate values of immigration rate. Such efficient immigrant cooperators act as nucleation centers for the growth of cooperative clusters, that eventually dominate defection.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Canadian Physics Counts: An exploration of the diverse identities of physics students and professionals in Canada,"The lack of diversity in physics remains a persistent worldwide problem. Despite being a quantitative discipline which relies on measurements to construct and validate hypotheses, there remains a paucity of data on both demographics and experiences of marginalized groups. In Canada, there has never been a nationwide assessment of those studying or working in physics. Here, we present findings from Canadian Physics Counts: the first national survey of equity, diversity, and inclusion (EDI) in the Canadian physics community. Our intersectional approach allowed us to gather a wealth of information on gender identity, sexual orientation, race, disability, and more. Analyses revealed key findings, including the first data on physicists who identify as non-binary or gender diverse, as well as the first data on Black and Indigenous scholars. Black physicists (1.2%) and Indigenous physicists (.3%) were found to be the most underrepresented, while White men were overrepresented across all sectors. Among respondents with a disability, 5% reported receiving full accommodations for their required needs at their place of work or study. One in four respondents from BIPOC gender diverse backgrounds identified as being disabled, and the proportion of sexually diverse students who reported having a disability was more than three times higher than the proportion of heterosexual students with a disability. The data also revealed that students represented more demographic diversity than working professionals, highlighting the importance of acting today in order to retain the diverse physicists of tomorrow. Our analysis identifies areas for intervention and offers recommendations for building a diverse and inclusive physics community in Canada that can be a global exemplar.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
To Switch or Not To Switch: Understanding Social Influence in Recommender Systems,"We designed and ran an experiment to test how often people's choices are reversed by others' recommendations when facing different levels of confirmation and conformity pressures. In our experiment participants were first asked to provide their preferences between pairs of items. They were then asked to make second choices about the same pairs with knowledge of others' preferences. Our results show that others people's opinions significantly sway people's own choices. The influence is stronger when people are required to make their second decision sometime later (22.4%) than immediately (14.1%). Moreover, people are most likely to reverse their choices when facing a moderate number of opposing opinions. Finally, the time people spend making the first decision significantly predicts whether they will reverse their decisions later on, while demographics such as age and gender do not. These results have implications for consumer behavior research as well as online marketing strategies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Measuring unequal distribution of pandemic severity across census years, variants of concern and interventions","Diverse and complex intervention policies deployed over the last years have shown varied effectiveness in controlling the COVID-19 pandemic. However, a systematic analysis and modelling of the combined effects of different viral lineages and complex intervention policies remains a challenge. Using large-scale agent-based modelling and a high-resolution computational simulation matching census-based demographics of Australia, we carried out a systematic comparative analysis of several COVID-19 pandemic scenarios. The scenarios covered two most recent Australian census years (2016 and 2021), three variants of concern (ancestral, Delta and Omicron), and five representative intervention policies. In addition, we introduced pandemic Lorenz curves measuring an unequal distribution of the pandemic severity across local areas. We quantified nonlinear effects of population heterogeneity on the pandemic severity, highlighting that (i) the population growth amplifies pandemic peaks, (ii) the changes in population size amplify the peak incidence more than the changes in density, and (iii) the pandemic severity is distributed unequally across local areas. We also examined and delineated the effects of urbanisation on the incidence bimodality, distinguishing between urban and regional pandemic waves. Finally, we quantified and examined the impact of school closures, complemented by partial interventions, and identified the conditions when inclusion of school closures may decisively control the transmission. Our results suggest that (a) public health response to long-lasting pandemics must be frequently reviewed and adapted to demographic changes, (b) in order to control recurrent waves, mass-vaccination rollouts need to be complemented by partial NPIs, and (c) healthcare and vaccination resources need to be prioritised towards the localities and regions with high population growth and/or high density.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Unraveling the Dynamic Importance of County-level Features in Trajectory of COVID-19,"The objective of this study was to investigate the importance of multiple county-level features in the trajectory of COVID-19. We examined feature importance across 2,787 counties in the United States using a data-driven machine learning model. We trained random forest models using 23 features representing six key influencing factors affecting pandemic spread: social demographics of counties, population activities, mobility within the counties, movement across counties, disease attributes, and social network structure. Also, we categorized counties into multiple groups according to their population densities, and we divided the trajectory of COVID-19 into three stages: the outbreak stage, the social distancing stage, and the reopening stage. The study aims to answer two research questions: (1) The extent to which the importance of heterogeneous features evolves in different stages; (2) The extent to which the importance of heterogeneous features varies across counties with different characteristics. We fitted a set of random forest models to determine weekly feature importance. The results showed that: (1) Social demographic features, such as gross domestic product, population density, and minority status maintained high-importance features throughout stages of COVID-19 across the 2787 studied counties; (2) Within-county mobility features had the highest importance in county clusters with higher population densities; (3) The feature reflecting the social network structure (Facebook, social connectedness index), had higher importance in the models for counties with higher population densities. The results show that the data-driven machine learning models could provide important insights to inform policymakers regarding feature importance for counties with various population densities and in different stages of a pandemic life cycle.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Individual differences in knowledge network navigation,"With the rapid accumulation of online information, efficient web navigation has grown vital yet challenging. To create an easily navigable cyberspace catering to diverse demographics, understanding how people navigate differently is paramount. While previous research has unveiled individual differences in spatial navigation, such differences in knowledge space navigation remain sparse. To bridge this gap, we conducted an online experiment where participants played a navigation game on Wikipedia and completed personal information questionnaires. Our analysis shows that age negatively affects knowledge space navigation performance, while multilingualism enhances it. Under time pressure, participants' performance improves across trials and males outperform females, an effect not observed in games without time pressure. In our experiment, successful route-finding is usually not related to abilities of innovative exploration of routes. Our results underline the importance of age, multilingualism and time constraint in the knowledge space navigation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"The gradual transformation of inland areas -- human plowing, horse plowing and equity incentives","Many modern areas have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Friends FTW! Friendship, Collaboration and Competition in Halo: Reach","How important are friendships in determining success by individuals and teams in complex collaborative environments? By combining a novel data set containing the dynamics of millions of ad hoc teams from the popular multiplayer online first person shooter Halo: Reach with survey data on player demographics, play style, psychometrics and friendships derived from an anonymous online survey, we investigate the impact of friendship on collaborative and competitive performance. In addition to finding significant differences in player behavior across these variables, we find that friendships exert a strong influence, leading to both improved individual and team performance--even after controlling for the overall expertise of the team--and increased pro-social behaviors. Players also structure their in-game activities around social opportunities, and as a result hidden friendship ties can be accurately inferred directly from behavioral time series. Virtual environments that enable such friendship effects will thus likely see improved collaboration and competition.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Homophily of Music Listening in Online Social Networks,"Homophily, ranging from demographics to sentiments, breeds connections in social networks, either offline or online. However, with the prosperous growth of music streaming service, whether homophily exists in online music listening remains unclear. In this study, two online social networks of a same group of active users are established respectively in Netease Music and Weibo. Through presented multiple similarity measures, it is evidently demonstrated that homophily does exist in music listening of both online social networks. The unexpected music similarity in Weibo also implies that knowledge from generic social networks can be confidently transfered to domain-oriented networks for context enrichment and algorithm enhancement. Comprehensive factors that might function in formation of homophily are further probed and many interesting patterns are profoundly revealed. It is found that female friends are more homogeneous in music listening and positive and energetic songs significantly pull users close. Our methodology and findings would shed lights on realistic applications in online music services.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Eugene Garfield and Algorithmic Historiography: Co-Words, Co-Authors, and Journal Names","Algorithmic historiography was proposed by Eugene Garfield in collaboration with Irving Sher in the 1960s, but further developed only recently into HistCite^{TM} with Alexander Pudovkin. As in history writing, HistCite^{TM} reconstructs by drawing intellectual lineages. In addition to cited references, however, documents can be attributed a multitude of other variables such as title words, keywords, journal names, author names, and even full texts. New developments in multidimensional scaling (MDS) enable us not only to visualize these patterns at each moment of time, but also to animate them over time. Using title words, co-authors, and journal names in Garfield's oeuvre, the method is demonstrated and further developed in this paper (and in the animation at http://www.leydesdorff.net/garfield/animation). The variety and substantive content of the animation enables us to write, visualize, and animate the author's intellectual history.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Temporal Analysis of Worldwide War,"Analysis of wars and conflicts between regions has been an important topic of interest throughout the history of humankind. In the latter part of the 20th century, in the aftermath of two World Wars and the shadow of nuclear, biological, and chemical holocaust, more was written on the subject than ever before. Wars have a negative impact on a country's economy, social order, infrastructure, and public health. In this paper, we study the wars fought in history and draw conclusions from that. We explore the participation of countries in wars and the nature of relationships between various countries during different timelines. A big part of today's wars is fought against terrorism. Therefore, this study also attempts to shed light on different countries' exposure to terrorist encounters and analyses the impact of wars on a country's economy in terms of change in GDP.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Effects of infection fatality ratio and social contact matrices on vaccine prioritization strategies,"Effective strategies of vaccine prioritization are essential to mitigate the impacts of severe infectious diseases. We investigate the role of infection fatality ratio (IFR) and social contact matrices on vaccination prioritization using a compartmental epidemic model fueled by real-world data of different diseases and countries. Our study confirms that massive and early vaccination is extremely effective to reduce the disease fatality if the contagion is mitigated, but the effectiveness is increasingly reduced as vaccination beginning delays in an uncontrolled epidemiological scenario. The optimal and least effective prioritization strategies depend non-linearly on epidemiological variables. Regions of the epidemiological parameter space, in which prioritizing the most vulnerable population is more effective than the most contagious individuals, depend strongly on the IFR age profile being, for example, substantially broader for COVID-19 in comparison with seasonal influenza. Demographics and social contact matrices deform the phase diagrams but do not alter their qualitative shapes.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Socio-economic models of divorces in different societies,"Population dynamic of getting divorced depends on many global factors, including social norms, economy, law or demographics as well as individual factors like the level of interpersonal or problem-solving skills of the spouses. We sought to find such a relationship incorporating only quantitative variables and test theoretical model considering phase transition between coupling (pairs) and free (single) preferential states as a function of social and economic. The analyzed data has been collected by UN across almost all the countries since 1948. Our first approach is followed by Bouchaud's model of social network of opinions, which works well with dynamics of fertility rates in postwar Europe. Unfortunately, we postulate that this pure sociological and pure economic approach fail in general. Thus, we did some observation about why it went wrong and where economy (e. g. Poland) or law (e. g. Portugal) has bigger impact on getting divorce than social pressure.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Exploring the conditions for sustainability with open-ended innovation,"Can sustained open-ended technological progress preserve natural resources in a finite planet? We address this question on the basis of a stylized model with genuine open-ended technological innovation, where an innovation event corresponds to a random draw of a technology in the space of the parameters that define how it impacts the environment and how it interacts with the population. Technological innovation is endogenous because an innovation may invade if it satisfies constraints which depend on the state of the environment and of the population. We find that open-ended innovation leads either to a sustainable future where global population saturates and the environment is preserved, or to exploding population and a vanishing environment. What drives the transition between these two phases is not the level of environmental impact of technologies, but rather the demographic effects of technologies and labor productivity. Low demographic impact and high labor productivity (as in several western countries today) result in a Schumpeterian dynamics where new ""greener"" technologies displace older ones, thereby reducing the overall environmental impact. In this scenario, global population saturates to a finite value, imposing strong selective pressure on technological innovation. When technologies contribute significantly to demographic growth and/or labor productivity is low, technological innovation runs unrestrained, population grows unbounded, while the environment collapses. As such, our model captures subtle feedback effects between technological progress, demography and sustainability that rationalize and align with empirical observations of a demographic transition and the environmental Kuznets curve, without deriving it from profit maximization based on individual incentives.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Does Transport Inequality Perpetuate Housing Insecurity?,"With trends of urbanisation on the rise, providing adequate housing to individuals remains a complex issue to be addressed. Often, the slow output of relevant housing policies, coupled with quickly increasing housing costs, leaves individuals with the burden of finding housing that is affordable and safe. In this paper, we unveil how urban planning, not just housing policies, can prevent individuals from accessing better housing conditions. We begin by proposing a clustering approach to characterising levels of housing insecurity in a city, by considering multiple dimensions of housing. Then we define levels of transit efficiency in 20 US cities by comparing public transit journeys to car-based journeys. Finally, we use geospatial autocorrelation to highlight how commuting to areas associated with better housing conditions results in transit commute times of over 30 minutes in most cities, and commute times of over an hour in some cases. Ultimately, we show the role that public transportation plays in locking vulnerable demographics into a cycle of poverty, thus motivating a more holistic approach to addressing housing insecurity that extends beyond changing housing policies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Gendered Networks and Communicability in Medieval Historical Narratives,"One of the defining representations of women from medieval times is in the role of peace-weaver, that is, a woman was expected to 'weave' peace between warring men. The underlying assumption in scholarship on this topic is that female mediation lessens male violence. This stance can however be questioned since it may be the result of gender-based peace and diplomacy models that relegate women's roles to that of conduits between men. By analyzing the concept of communicability and relevance of certain nodes in complex networks we show how our sources afford women more complex and nuanced social roles. As a case study we consider a historical narrative, namely Bede's ""Ecclesiastical History of the English People"", which is a history of Britain from the first to eighth centuries AD and was immensely popular all over Europe in the Middle Ages.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Thermo-kinetic explosions: safety first or safety last?,"Gas and vapour explosions have been involved in industrial accidents since the beginnings of industry. A century ago, at 11:55 am on Friday 24th September 1920, the petroleum barge Warwick exploded in London's docklands and seven men were killed. Understanding what happened when it blew up as it was being refurbished, and how to prevent similar explosions, involves fluid mechanics and thermodynamics plus chemistry. I recount the 1920 accident as an example, together with the history of thermo-kinetic explosions prior to 1920 and up to the present day, and I review the history and the actual state of the science of explosion and the roles of fluid mechanics, thermodynamics, and chemistry in that science. The science of explosions has been aware of its societal implications from the beginning, but, despite advances in health and safety over the past century, is there still work to do?","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Heaps' Law and Vocabulary Richness in the History of Classical Music Harmony,"Music is a fundamental human construct, and harmony provides the building blocks of musical language. Using the Kunstderfuge corpus of classical music, we analyze the historical evolution of the richness of harmonic vocabulary of 76 classical composers, covering almost 6 centuries. Such corpus comprises about 9500 pieces, resulting in more than 5 million tokens of music codewords. The fulfilment of Heaps' law for the relation between the size of the harmonic vocabulary of a composer (in codeword types) and the total length of his works (in codeword tokens), with an exponent around 0.35, allows us to define a relative measure of vocabulary richness that has a transparent interpretation. When coupled with the considered corpus, this measure allows us to quantify harmony richness across centuries, unveiling a clear increasing linear trend. In this way, we are able to rank the composers in terms of richness of vocabulary, in the same way as for other related metrics, such as entropy. We find that the latter is particularly highly correlated with our measure of richness. Our approach is not specific for music and can be applied to other systems built by tokens of different types, as for instance natural language.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Space-time correlations in urban sprawl,"Understanding demographic and migrational patterns constitutes a great challenge. Millions of individual decisions, motivated by economic, political, demographic, rational, and/or emotional reasons underlie the high complexity of demographic dynamics. Significant advances in quantitatively understanding such complexity have been registered in recent years, as those involving the growth of cities [Bettencourt LMA, Lobo J, Helbing D, Kuehnert C, West GB (2007) Growth,. Innovation, Scaling, and the Pace of Life in Cities, Proc Natl Acad Sci USA 104 (17) 7301-7306] but many fundamental issues still defy comprehension. We present here compelling empirical evidence of a high level of regularity regarding time and spatial correlations in urban sprawl, unraveling patterns about the inertia in the growth of cities and their interaction with each other. By using one of the world's most exhaustive extant demographic data basis ---that of the Spanish Government's Institute INE, with records covering 111 years and (in 2011) 45 million people, distributed amongst more than 8000 population nuclei--- we show that the inertia of city growth has a characteristic time of 15 years, and its interaction with the growth of other cities has a characteristic distance of 70 km. Distance is shown to be the main factor that entangles two cities (a 60% of total correlations). We present a mathematical model for population flows that i) reproduces all these regularities and ii) can be used to predict the population-evolution of cities. The power of our current social theories is thereby enhanced.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Diversity + Inclusion at Belle II: Where We Are, What We've Done and Where We Want To Be","The Belle II Collaboration comprises over 1000 international high energy physicists, who investigate the properties of $b$-quarks and other particles at the luminosity frontier. In order to achieve our aim of a successful physics program, it is essential that we emphasise contributions from a diverse community. Belle II has thus far focused on diversity in gender and sexuality, among other efforts within our collaboration. These efforts are led by our two Diversity Officers, elected to the newly created positions in 2018. Their role has been to promote an inclusive atmosphere, raising awareness of diversity and being a safe first point of call for issues of discrimination and harassment. These proceedings accompany the short talk delivered during ICHEP 2020, marking the first conference the Belle II Collaboration has presented in the diversity and inclusion stream. It details the efforts described above, as well as examining the evolving gender demographics of our community, since membership began in 2011.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Which early works are cited most frequently in climate change research literature? A bibliometric approach based on Reference Publication Year Spectroscopy,"This bibliometric analysis focuses on the general history of climate change research and, more specifically, on the discovery of the greenhouse effect. First, the Reference Publication Year Spectroscopy (RPYS) is applied to a large publication set on climate change of 222,060 papers published between 1980 and 2014. The references cited therein were extracted and analyzed with regard to publications, which are cited most frequently. Second, a new method for establishing a more subject-specific publication set for applying RPYS (based on the co-citations of a marker reference) is proposed (RPYS-CO). The RPYS of the climate change literature focuses on the history of climate change research in total. We identified 35 highly-cited publications across all disciplines, which include fundamental early scientific works of the 19th century (with a weak connection to climate change) and some cornerstones of science with a stronger connection to climate change. By using the Arrhenius (1896) paper as a RPYS-CO marker paper, we selected only publications specifically discussing the discovery of the greenhouse effect and the role of carbon dioxide. Also, we focused on the time period 1800-1850 to reveal the contributions of J.B.J Fourier in terms of cited references. Using different RPYS approaches in this study, we were able to identify the complete range of works of the celebrated icons as well as many less known works relevant for the history of climate change research. The analyses confirmed the potential of the RPYS method for historical studies: Seminal papers are detected on the basis of the references cited by the overall community without any further assumptions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Circadian patterns of Wikipedia editorial activity: A demographic analysis,"Wikipedia (WP) as a collaborative, dynamical system of humans is an appropriate subject of social studies. Each single action of the members of this society, i.e. editors, is well recorded and accessible. Using the cumulative data of 34 Wikipedias in different languages, we try to characterize and find the universalities and differences in temporal activity patterns of editors. Based on this data, we estimate the geographical distribution of editors for each WP in the globe. Furthermore we also clarify the differences among different groups of WPs, which originate in the variance of cultural and social features of the communities of editors.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Optimizing Urban Mobility Restrictions: a Multi-Agent System (MAS) for SARS-CoV-2,"Infectious epidemics can be simulated by employing dynamical processes as interactions on network structures. Here, we introduce techniques from the Multi-Agent System (MAS) domain in order to account for individual level characterization of societal dynamics for the SARS-CoV-2 pandemic. We hypothesize that a MAS model which considers rich spatial demographics, hourly mobility data and daily contagion information from the metropolitan area of Toronto can explain significant emerging behavior. To investigate this hypothesis we designed, with our modeling framework of choice, GAMA, an accurate environment which can be tuned to reproduce mobility and healthcare data, in our case coming from TomTom's API and Toronto's Open Data. We observed that some interesting contagion phenomena are directly influenced by mobility restrictions and curfew policies. We conclude that while our model is able to reproduce non-trivial emerging properties, large-scale simulation are needed to further investigate the role of different parameters. Finally, providing such an end-to-end model can be critical for policy-makers to compare their outcomes with past strategies in order to devise better plans for future measures.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Unravelling the Spatial Properties of Individual Mobility Patterns using Longitudinal Travel Data,"The analysis of longitudinal travel data enables investigating how mobility patterns vary across the population and identify the spatial properties thereof. The objective of this study is to identify the extent to which users explore different parts of the network as well as identify distinctive user groups in terms of the spatial extent of their mobility patterns. To this end, we propose two means for representing spatial mobility profiles and clustering travellers accordingly. We represent users patterns in terms of zonal visiting frequency profiles and grid-cells spatial extent heatmaps. We apply the proposed analysis to a large-scale multi-modal mobility data set from the public transport system in Stockholm, Sweden. We unravel three clusters - locals, commuters and explorers - that best describe the zonal visiting frequency and show that their composition varies considerably across users' place of residence and related demographics. We also identify 18 clusters of visiting spatial extent which form four groups that follow similar shapes of travel extent yet oriented in different directions. The approach proposed and applied in this study could be applied for any longitudinal individual travel demand data.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Who Votes for Library Bonds? A Principal Component Exploration,"Previous research has shown a relationship between voter characteristics and voter support for tax bonds. These findings, however, are difficult to interpret because of the high degree of collinearity across the measures. From 13 demographic measures of voters in a library bond election, seven independent principal components were extracted which accounted for 95 percent of the variance. Whereas the direct demographic measures showed inconsistent relationships with voting, the principal components of low SES, college experience, female and service job were related to affirmative voting, while high home value was related to negative voting.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Foursquare to The Rescue: Predicting Ambulance Calls Across Geographies,"Understanding how ambulance incidents are spatially distributed can shed light to the epidemiological dynamics of geographic areas and inform healthcare policy design. Here we analyze a longitudinal dataset of more than four million ambulance calls across a region of twelve million residents in the North West of England. With the aim to explain geographic variations in ambulance call frequencies, we employ a wide range of data layers including open government datasets describing population demographics and socio-economic characteristics, as well as geographic activity in online services such as Foursquare. Working at a fine level of spatial granularity we demonstrate that daytime population levels and the deprivation status of an area are the most important variables when it comes to predicting the volume of ambulance calls at an area. Foursquare check-ins on the other hand complement these government sourced indicators, offering a novel view to population nightlife and commercial activity locally. We demonstrate how check-in activity can provide an edge when predicting certain types of emergency incidents in a multi-variate regression model.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Extracting Spatiotemporal Demand for Public Transit from Mobility Data,"With people constantly migrating to different urban areas, our mobility needs for work, services and leisure are transforming rapidly. The changing urban demographics pose several challenges for the efficient management of transit services. To forecast transit demand, planners often resort to sociological investigations or modelling that are either difficult to obtain, inaccurate or outdated. How can we then estimate the variegated demand for mobility? We propose a simple method to identify the spatiotemporal demand for public transit in a city. Using a Gaussian mixture model, we decompose empirical ridership data into a set of temporal demand profiles representative of ridership over any given day. A case of approximately 4.6 million daily transit traces from the Greater London region reveals distinct demand profiles. We find that a weighted mixture of these profiles can generate any station traffic remarkably well, uncovering spatially concentric clusters of mobility needs. Our method of analysing the spatiotemporal geography of a city can be extended to other urban regions with different modes of public transit.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Network Dimensions in the Getty Provenance Index,"In this article we make a case for a systematic application of complex network science to study art market history and more general collection dynamics. We reveal social, temporal, spatial, and conceptual network dimensions, i.e. network node and link types, previously implicit in the Getty Provenance Index (GPI). As a pioneering art history database active since the 1980s, the GPI provides online access to source material relevant for research in the history of collecting and art markets. Based on a subset of the GPI, we characterize an aggregate of more than 267,000 sales transactions connected to roughly 22,000 actors in four countries over 20 years at daily resolution from 1801 to 1820. Striving towards a deeper understanding on multiple levels we disambiguate social dynamics of buying, brokering, and selling, while observing a general broadening of the market, where large collections are split into smaller lots. Temporally, we find annual market cycles that are shifted by country and obviously favor international exchange. Spatially, we differentiate near-monopolies from regions driven by competing sub-centers, while uncovering asymmetries of international market flux. Conceptually, we track dynamics of artist attribution that clearly behave like product categories in a very slow supermarket. Taken together, we introduce a number of meaningful network perspectives dealing with historical art auction data, beyond the analysis of social networks within a single market region. The results presented here have inspired a Linked Open Data conversion of the GPI, which is currently in process and will allow further analysis by a broad set of researchers.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A Dynamic Niche Model for the Emergence and Evolution of Mutualistic Network Structures,"Mutualistic interactions are vital constituents of ecological and socio-economic systems. Empirical studies have found that the patterns of reciprocal relations among the participants often shows the salient features of being simultaneously nested and modular. Whether and how these two structural properties of mutualistic networks can emerge out of a common mechanism however remains unclear. We propose a unified dynamic model based on the adaptation of niche relations that gives rise to both structural features. We apply Hutchinson's concept of niche interaction to networked cooperative species. Their niche relation evolves under the assumption of fitness maximization. Modularity and nestedness emerge concurrently through the accumulated local advantages in the structural and demographic distribution. A rich ensemble of key dynamical behaviors are unveiled in the dynamical framework. We demonstrate that mutualism can exhibit either a stabilizing or destabilizing effect on the evolved network, which undergoes a drastic transition with the overall competition level. Most strikingly, the adaptive network may exhibit a profound nature of history-dependency in response to environmental changes, allowing it to be found in alternative stable structures. The adaptive nature of niche interactions, as captured in our framework, can underlie a broad class of ecological relations and also socio-economic networks that engage in bipartite cooperation.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
On some pioneering Usenet newsgroups in astrophysics,"The foundation of two very early Usenet newsgroups in astrophysics, still existent today, and some milestones in their history have been tracked from the origins at Princeton University in 1983 to 1994. They result to be pioneering experiences in this discipline, and among the earliest ones of this kind in academic disciplines at large. To the best of our knowledge, an account of their birth and evolution is given here for the first time. Following key recommendations from the recent discipline of web history, this research has combined multiple and different-type sources, building mainly on online archives of Usenet newsgroups and on human contributions from the concerned scholarly community. A final overview is proposed on how these early online communication tools have been perceived and used by the scholarly community involved. This research reconstructs computer mediated communication experiences which were at risk of being forgotten; provides a view of this environment's uptake of new communication technology; and contributes knowledge of some social dynamics of the astrophysics community in the last twenty years of the twentieth century.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Complex-network approach for visualizing and quantifying the evolution of a scientific topic,"Tracing the evolution of specific topics is a subject area which belongs to the general problem of mapping the structure of scientific knowledge. Often bibliometric data bases are used to study the history of scientific topic evolution from its appearance to its extinction or merger with other topics. In this chapter the authors present an analysis of the academic response to the disaster that occurred in 1986 in Chornobyl (Chernobyl), Ukraine, considered as one of the most devastating nuclear power plant accidents in history. Using a bibliographic database the distributions of Chornobyl-related papers in different scientific fields are analysed, as are their growth rates and properties of co-authorship networks. Elements of descriptive statistics and tools of complex-network theory are used to highlight interdisciplinary as well as international effects. In particular, tools of complex-network science enable information visualization complemented by further quantitative analysis. A further goal of the chapter is to provide a simple pedagogical introduction to the application of complex-network analysis for visual data representation and interdisciplinary communication.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Wikipedia and Digital Currencies: Interplay Between Collective Attention and Market Performance,"The production and consumption of information about Bitcoin and other digital-, or 'crypto'-, currencies have grown together with their market capitalisation. However, a systematic investigation of the relationship between online attention and market dynamics, across multiple digital currencies, is still lacking. Here, we quantify the interplay between the attention towards digital currencies in Wikipedia and their market performance. We consider the entire edit history of currency-related pages, and their view history from July 2015. First, we quantify the evolution of the cryptocurrency presence in Wikipedia by analysing the editorial activity and the network of co-edited pages. We find that a small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in Wikipedia. Then, we show that a simple trading strategy informed by Wikipedia views performs better, in terms of returns on investment, than classic baseline strategies for most of the covered period. Our results contribute to the recent literature on the interplay between online information and investment markets, and we anticipate it will be of interest for researchers as well as investors.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A Study of Age and Gender seen through Mobile Phone Usage Patterns in Mexico,"Mobile phone usage provides a wealth of information, which can be used to better understand the demographic structure of a population. In this paper we focus on the population of Mexican mobile phone users. Our first contribution is an observational study of mobile phone usage according to gender and age groups. We were able to detect significant differences in phone usage among different subgroups of the population. Our second contribution is to provide a novel methodology to predict demographic features (namely age and gender) of unlabeled users by leveraging individual calling patterns, as well as the structure of the communication graph. We provide details of the methodology and show experimental results on a real world dataset that involves millions of users.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modularity maximization and tree clustering: Novel ways to determine effective geographic borders,"Territorial subdivisions and geographic borders are essential for understanding phenomena in sociology, political science, history, and economics. They influence the interregional flow of information and cross-border trade and affect the diffusion of innovation and technology. However, most existing administrative borders were determined by a variety of historic and political circumstances along with some degree of arbitrariness. Societies have changed drastically, and it is doubtful that currently existing borders reflect the most logical divisions. Fortunately, at this point in history we are in a position to actually measure some aspects of the geographic structure of society through human mobility. Large-scale transportation systems such as trains and airlines provide data about the number of people traveling between geographic locations, and many promising human mobility proxies are being discovered, such as cell phones, bank notes, and various online social networks. In this chapter we apply two optimization techniques to a human mobility proxy (bank note circulation) to investigate the effective geographic borders that emerge from a direct analysis of human mobility.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Modeling Bike Share Station Activity: Effects of Nearby Businesses and Jobs on Trips to and from Stations,"The purpose of this research is to identify correlates of bike station activity for Nice Ride Minnesota, a bike share system in Minneapolis - St. Paul Metropolitan Area in Minnesota. We obtained the number of trips to and from each of the 116 bike share stations operating in 2011 from Nice Ride Minnesota. Data for independent variables included in models come from a variety of sources; including the 2010 US Census, the Metropolitan Council, a regional planning agency, and the cities of Minneapolis and St. Paul. We use log-linear and negative binomial regression models to evaluate the marginal effects of these factors on average daily station trips. Our models have high goodness of fit, and each of 13 independent variables is significant at the 10% level or higher. The number of trips at Nice Ride stations is associated with neighborhood socio demographics (i.e., age and race), proximity to the central business district, proximity to water, accessibility to trails, distance to other bike share stations, and measures of economic activity. Analysts can use these results to optimize bike share operations, locate new stations, and evaluate the potential of new bike share programs.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Measuring the Change in European and US COVID-19 Death Rates,"By fitting a compartment ODE model for Covid-19 propagation to cumulative case and death data for US states and European countries, we find that the case mortality rate seems to have decreased by at least 80% in most of the US and at least 90% in most of Europe. These are much larger and faster changes than reported in empirical studies, such as the 18% decrease in mortality found for the New York City hospital system from March to August 2020 (Horwitz et al, Trends in Covid-19 risk-adjusted mortality rates, J. Hosp. Med. 2020). Our reported decreases surprisingly do not have strong correlations to other model parameters (such as contact rate) or other standard state/national metrics such as population density, GDP, and median age. Almost all the decreases occurred between mid-April and mid-June, which unexpectedly corresponds to the time when many state and national lockdowns were released resulting in surges of new cases. Several plausible causes for this drop are examined, such as improvements in treatment, face mask wearing, a new virus strain, and potentially changing demographics of infected patients, but none are overwhelmingly convincing given the currently available evidence.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Do Cascades Recur?,"Cascades of information-sharing are a primary mechanism by which content reaches its audience on social media, and an active line of research has studied how such cascades, which form as content is reshared from person to person, develop and subside. In this paper, we perform a large-scale analysis of cascades on Facebook over significantly longer time scales, and find that a more complex picture emerges, in which many large cascades recur, exhibiting multiple bursts of popularity with periods of quiescence in between. We characterize recurrence by measuring the time elapsed between bursts, their overlap and proximity in the social network, and the diversity in the demographics of individuals participating in each peak. We discover that content virality, as revealed by its initial popularity, is a main driver of recurrence, with the availability of multiple copies of that content helping to spark new bursts. Still, beyond a certain popularity of content, the rate of recurrence drops as cascades start exhausting the population of interested individuals. We reproduce these observed patterns in a simple model of content recurrence simulated on a real social network. Using only characteristics of a cascade's initial burst, we demonstrate strong performance in predicting whether it will recur in the future.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Projecting Multimorbidity and Mortality under Demographic Change and Preventive Interventions,"As populations age, the rise of multimorbidity poses a significant healthcare challenge. However, our ability to quantitatively forecast the progression of multimorbidity remains limited. Leveraging a nationwide dataset comprising approximately 45 million hospital stays spanning 17 years in Austria, we develop a new compartmental model for chronic disease trajectories across 132 distinct multimorbidity patterns (compartments). Each compartment represents a distinct constellation of co-occurring chronic conditions, with transitions modeled as age- and sex-dependent probabilities. We use the compartmental disease trajectory model (CDTM) to simulate disease trajectories to 2030, estimating the frequency of all empirically observed co-occurrence patterns among more than 100 diagnosis groups. We demonstrate the model's utility in identifying high-impact prevention targets. A 5% reduction in new cases of hypertensive disease (I10--I15) leads to a 0.57 (SD 0.06)% reduction in all-cause mortality over a 15-year period, and a 0.57 (SD 0.07)% reduction in mortality for malignant neoplasms (C00--C97). We also evaluate long-term impacts of SARS-CoV-2 sequelae, projecting earlier and more frequent hospitalizations across a range of diagnoses. Our fully data-driven modelling approach identifies leverage points for proactive preparation by physicians and policymakers to reduce the overall disease burden in the population, emphasizing patient-centered healthcare planning in aging societies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Identifying latent activity behaviors and lifestyles using mobility data to describe urban dynamics,"Urbanization and its problems require an in-depth and comprehensive understanding of urban dynamics, especially the complex and diversified lifestyles in modern cities. Digitally acquired data can accurately capture complex human activity, but it lacks the interpretability of demographic data. In this paper, we study a privacy-enhanced dataset of the mobility visitation patterns of 1.2 million people to 1.1 million places in 11 metro areas in the U.S. to detect the latent mobility behaviors and lifestyles in the largest American cities. Despite the considerable complexity of mobility visitations, we found that lifestyles can be automatically decomposed into only 12 latent interpretable activity behaviors on how people combine shopping, eating, working, or using their free time. Rather than describing individuals with a single lifestyle, we find that city dwellers' behavior is a mixture of those behaviors. Those detected latent activity behaviors are equally present across cities and cannot be fully explained by main demographic features. Finally, we find those latent behaviors are associated with dynamics like experienced income segregation, transportation, or healthy behaviors in cities, even after controlling for demographic features. Our results signal the importance of complementing traditional census data with activity behaviors to understand urban dynamics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
What is research?,"Doing research is fighting, what any other thing the human being could do? Fight against powers or to get powers, that depends on us. Science can be a revolution or deadlocked idleness. Still waters, without hitting the stones along their history, trend to form bogs.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Scientific evaluation of Charles Dickens,"I report the results of the test, where the takers had to tell the prose of Charles Dickens from that of Edward Bulwer-Lytton, who is considered by many to be the worst writer in history of letters. The average score is about 50%, which is on the level of random guessing. This suggests that the quality of Dickens' prose is the same as of that of Bulwer-Lytton.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Emergence of universality in the transmission dynamics of COVID-19,"The complexities involved in modelling the transmission dynamics of COVID-19 has been a roadblock in achieving predictability in the spread and containment of the disease. In addition to understanding the modes of transmission, the effectiveness of the mitigation methods also needs to be built into any effective model for making such predictions. We show that such complexities can be circumvented by appealing to scaling principles which lead to the emergence of universality in the transmission dynamics of the disease. The ensuing data collapse renders the transmission dynamics largely independent of geopolitical variations, the effectiveness of various mitigation strategies, population demographics, etc. We propose a simple two-parameter model -- the Blue Sky model -- and show that one class of transmission dynamics can be explained by a solution that lives at the edge of a blue sky bifurcation. In addition, the data collapse leads to an enhanced degree of predictability in the disease spread for several geographical scales which can also be realized in a model-independent manner as we show using a deep neural network. The methodology adopted in this work can potentially be applied to the transmission of other infectious diseases and new universality classes may be found. The predictability in transmission dynamics and the simplicity of our methodology can help in building policies for exit strategies and mitigation methods during a pandemic.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Deeply nested structure of mythological traditions worldwide,"All human societies present unique narratives that shape their customs and beliefs. Despite cultural differences, some symbolic elements (e.g., heroes and tricksters) are common across many cultures. Here, we reconcile these seemingly contradictory aspects by analyzing mythological themes and traditions at various scales. Our analysis revealed that global mythologies exhibit both geographic and thematic nesting across different scales, manifesting in a layered structure. The largest geographic clusters correspond to the New and Old Worlds, which further divide into smaller bioregions. This hierarchical manifestation closely aligns with historical human migration patterns at a large scale, suggesting that narrative themes were carried through deep history. At smaller scales, the correspondence with bioregions indicates that these themes are locally adapted and diffused into variations across cultures over time. Our approach, which treats myths and traditions as random variables without considering factors like geography, history, or story lineage, suggests that the manifestation of mythology has been well-preserved over time and thus opens exciting research avenues to reconstruct historical patterns and provide insight into human cultural narratives.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Attributing equity gaps to course structure in introductory physics,"We add to a growing literature suggesting that demographic grade gaps should be attributed to biases embedded in the courses themselves. Changes in the structure of two different introductory physics classes were made while leaving the topics covered and the level of coverage unchanged. First, a class where conceptual issues were studied before doing any complicated calculations had zero final exam grade gap between students from underrepresented racial/ethnic groups and their peers. Next, four classes that offered students a retake exam each week between the regular bi-weekly exams during the term had zero gender gap in course grades. Our analysis indicates that demographic grade gaps can be attributed to the course structure (a Course Deficit Model) rather than to student preparation (a Student Deficit Model).","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diversity and inclusion activities in Belle II,"These proceedings accompany the Belle II talk in the Science in Society parallel session delivered during Lepton Photon 2021. In this talk we present updated membership statistics using 10 years of data with a diversity and inclusion lens, and we present Belle II's most recent activities to aid and improve diversity and inclusion. This report has the intention to bring light to the social working environment and population representation within our collaboration and, by extension, within high energy physics.   Belle II is a particle physics collaboration that has over 1000 people from institutions in 26 countries who work together to achieve its physics goals. Belle II is committed to fostering an open, diverse, and inclusive environment; as part of this commitment it created a diversity office to raise awareness of diversity and inclusion issues, promote an inclusive atmosphere within the collaboration, provide a safe and confidential point to contact for collaborators to report any issues, particularly those related to discrimination and harassment, and ensure that persons from underrepresented groups are considered for positions of responsibility within the collaboration. Diversity and inclusion activities and initiatives at Belle II and analysis of the demographics of the collaboration will be presented.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Competitive market for multiple firms and economic crisis,"The origin of economic crises is a key problem for economics. We present a model of long-run competitive markets to show that the multiplicity of behaviors in an economic system, over a long time scale, emerge as statistical regularities (perfectly competitive markets obey Bose-Einstein statistics and purely monopolistic-competitive markets obey Boltzmann statistics) and that how interaction among firms influences the evolutionary of competitive markets. It has been widely accepted that perfect competition is most efficient. Our study shows that the perfectly competitive system, as an extreme case of competitive markets, is most efficient but not stable, and gives rise to economic crises as society reaches full employment. In the economic crisis revealed by our model, many firms condense (collapse) into the lowest supply level (zero supply, namely bankruptcy status), in analogy to Bose-Einstein condensation. This curious phenomenon arises because perfect competition (homogeneous competitions) equals symmetric (indistinguishable) investment direction, a fact abhorred by nature. Therefore, we urge the promotion of monopolistic competition (heterogeneous competitions) rather than perfect competition. To provide early warning of economic crises, we introduce a resolving index of investment, which approaches zero in the run-up to an economic crisis. On the other hand, our model discloses, as a profound conclusion, that the technological level for a long-run social or economic system is proportional to the freedom (disorder) of this system; in other words, technology equals the entropy of system. As an application of this new concept, we give a possible answer to the Needham question: ""Why was it that despite the immense achievements of traditional China it had been in Europe and not in China that the scientific and industrial revolutions occurred?""","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Disadvantaged Communities Have Lower Access to Urban Infrastructure,"Disparity in spatial accessibility is strongly associated with growing inequalities among urban communities. Since improving levels of accessibility for certain communities can provide them with upward social mobility and address social exclusion and inequalities in cities, it is important to understand the nature and distribution of spatial accessibility among urban communities. To support decision-makers in achieving inclusion and fairness in policy interventions in cities, we present an open-source and data-driven framework to understand the spatial nature of accessibility to infrastructure among the different demographics. We find that accessibility to a wide range of infrastructure in any city (54 cities) converges to a Zipf's law, suggesting that inequalities also appear proportional to growth processes in these cities. Then, assessing spatial inequalities among the socioeconomically clustered urban profiles for 10 of those cities, we find urban communities are distinctly segregated along social and spatial lines. We find low accessibility scores for populations who have a larger share of minorities, earn less, and have a relatively lower number of individuals with a university degree. These findings suggest that the reproducible framework we propose may be instrumental in understanding processes leading to spatial inequalities and in supporting cities to devise targeted measures for addressing inequalities for certain underprivileged communities.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Emergence of collective memories,"We understand the dynamics of the world around us as by associating pairs of events, where one event has some influence on the other. These pairs of events can be aggregated into a web of memories representing our understanding of an episode of history. The events and the associations between them need not be directly experienced-they can also be acquired by communication. In this paper we take a network approach to study the dynamics of memories of history. First we investigate the network structure of a data set consisting of reported events by several individuals and how associations connect them. We focus our measurement on degree distributions, degree correlations, cycles (which represent inconsistencies as they would break the time ordering) and community structure. We proceed to model effects of communication using an agent-based model. We investigate the conditions for the memory webs of different individuals to converge to collective memories, how groups where the individuals have similar memories (but different from other groups) can form. Our work outlines how the cognitive representation of memories and social structure can co-evolve as a contagious process. We generate some testable hypotheses including that the number of groups is limited as a function of the total population size.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Prevention of infectious diseases by public vaccination and individual protection,"In the face of serious infectious diseases, governments endeavour to implement containment measures such as public vaccination at a macroscopic level. Meanwhile, individuals tend to protect themselves by avoiding contacts with infections at a microscopic level. However, a comprehensive understanding of how such combined strategy influences epidemic dynamics is still lacking. We study a susceptible-infected-susceptible epidemic model with imperfect vaccination on dynamic contact networks, where the macroscopic intervention is represented by random vaccination of the population and the microscopic protection is characterised by susceptible individuals rewiring contacts from infective neighbours. In particular, the model is formulated both in populations without and then with demographic effects. Using the pairwise approximation and the probability generating function approach, we investigate both dynamics of the epidemic and the underlying network. For populations without demography, the emerging degree correlations, bistable states, and oscillations demonstrate the combined effects of the public vaccination program and individual protective behavior. Compared to either strategy in isolation, the combination of public vaccination and individual protection is more effective in preventing and controlling the spread of infectious diseases by increasing both the invasion threshold and the persistence threshold. For populations with additional demographic factors, the integration between vaccination intervention and individual rewiring may promote epidemic spreading due to the birth effect. Moreover, the degree distributions of both networks in the steady state is closely related to the degree distribution of newborns, which leads to uncorrelated connectivity. All the results demonstrate the importance of both local protection and global intervention, as well as the demographic effects.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Analyzing recreational fishing effort -- Gender differences and the impact of Covid-19,"Recreational fishing is an important economic driver and provides multiple social benefits. To predict fishing activity, identifying variables related to variation, such as gender or Covid-19, is helpful. We conducted a Canada-wide email survey of users of an online fishing platform and analyzed responses focusing on gender, the impact of Covid-19, and variables directly related to fishing effort. Genders (90% men and 10% women) significantly differed in demographics, socioeconomic status, and fishing skills but showed similar fishing preferences, fishing effort in terms of trip frequency, and travel distance. Covid-19 altered trip frequency for almost half of fishers, with changes varying by gender and activity level. A Bayesian network revealed travel distance as the main determinant of trip frequency, negatively impacting fishing activity for 61% of fishers, with fishing expertise also playing a role. The results suggest that among active fishers, socio-economic differences between genders do not drive fishing effort, but responses to Covid-19 were gender-specific. Recognizing these patterns is critical for equitable policy-making and accurate socio-ecological models, thereby improving resource management and sustainability.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Models of the World human population growth- critical analysis,We analyze mathematical models of the global human population growth and compare them to actual dynamics of the world population and of the world surplus product. We consider a possibility that the so-called world's demographic transition is not a dynamic crossover but a phase transition that affects all aspects of our life.,"cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Unprecedented reach and rich online journeys drive hate and extremism globally,"Hate and extremism cannot be controlled globally without understanding how they operate at scale. Both have escalated dramatically during the Israel-Hamas and Ukraine-Russia wars. Here we show how the online hate-extremism system is now operating at unprecedented scale across 26 social media platforms of all sizes, audience demographics, and geographic locations; and we analyze individuals' journeys through it. This new picture contradicts notions of rabbit-hole activity at the fringe of the Internet. Instead, it shows that hate-extremism support now enjoys a direct link to more than a billion of the general global population, and that newcomers now enjoy a rich variety of online journey experiences during which they get to mingle with experienced violent actors, discuss topics from diverse news sources, and learn to collectively adapt in order to bypass platform shutdowns. Our results mean that law enforcement must expect future mass shooters to have increasingly hard-to-understand online journeys; that new E.U. laws will fall short because the combined impact of many smaller, lesser-known platforms outstrips larger ones like Twitter; and that the current global hate-extremism infrastructure will become increasingly robust in 2024 and beyond. Fortunately, it also reveals a new opportunity for system-wide control akin to adaptive vs. extinction treatments for cancer.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Assessing the Impact of Case Correction Methods on the Fairness of COVID-19 Predictive Models,"One of the central difficulties of addressing the COVID-19 pandemic has been accurately measuring and predicting the spread of infections. In particular, official COVID-19 case counts in the United States are under counts of actual caseloads due to the absence of universal testing policies. Researchers have proposed a variety of methods for recovering true caseloads, often through the estimation of statistical models on more reliable measures, such as death and hospitalization counts, positivity rates, and demographics. However, given the disproportionate impact of COVID-19 on marginalized racial, ethnic, and socioeconomic groups, it is important to consider potential unintended effects of case correction methods on these groups. Thus, we investigate two of these correction methods for their impact on a downstream COVID-19 case prediction task. For that purpose, we tailor an auditing approach and evaluation protocol to analyze the fairness of the COVID-19 prediction task by measuring the difference in model performance between majority-White counties and majority-minority counties. We find that one of the correction methods improves fairness, decreasing differences in performance between majority-White and majority-minority counties, while the other method increases differences, introducing bias. While these results are mixed, it is evident that correction methods have the potential to exacerbate existing biases in COVID-19 case data and in downstream prediction tasks. Researchers planning to develop or use case correction methods must be careful to consider negative effects on marginalized groups.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The wisdom of the few: Predicting collective success from individual behavior,"Can we predict top-performing products, services, or businesses by only monitoring the behavior of a small set of individuals? Although most previous studies focused on the predictive power of ""hub"" individuals with many social contacts, which sources of customer behavioral data are needed to address this question remains unclear, mostly due to the scarcity of available datasets that simultaneously capture individuals' purchasing patterns and social interactions. Here, we address this question in a unique, large-scale dataset that combines individuals' credit-card purchasing history with their social and mobility traits across an entire nation. Surprisingly, we find that the purchasing history alone enables the detection of small sets of ``discoverers"" whose early purchases offer reliable success predictions for the brick-and-mortar stores they visit. In contrast with the assumptions by most existing studies on word-of-mouth processes, the hubs selected by social network centrality are not consistently predictive of success. Our findings show that companies and organizations with access to large-scale purchasing data can detect the discoverers and leverage their behavior to anticipate market trends, without the need for social network data.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Structure of Phonological Networks Across Multiple Languages,"The network characteristics based on the phonological similarities in the lexicons of several languages were examined. These languages differed widely in their history and linguistic structure, but commonalities in the network characteristics were observed. These networks were also found to be different from other networks studied in the literature. The properties of these networks suggest explanations for various aspects of linguistic processing and hint at deeper organization within human language.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Causal Impacts of Protected Bike Lanes on Cycling Behavior with Demographic Disparities,"Cities around the world face significant barriers to grow urban cycling, including competing budgetary priorities and car-centric streets. Thus, when making decisions regarding the installation of bicycle infrastructure, it is crucial to understand if and to what extent different bicycle-lane types increase bicycle ridership. However, associations between bicycle infrastructure and bicycle ridership have primarily been studied in the context of individual lanes and corridors, or when analyzed at the scale of entire cities, generalized across different bike-lane types. Drawing upon 72 million bikeshare trips from Citi Bike in New York, we demonstrate that there is an approximately 18% increase in bikeshare trips at adjacent stations in the 12 months following the installation of protected bike lanes (those with a physical barrier between cyclists and automobile traffic) and a 14% increase associated with painted bike lanes (where a line of pavement marking is present) and `sharrows' (where a normal traffic lane is marked with a bike stencil). However, using a difference-in-differences analysis, we detect a causal effect on bikeshare ridership only following the installation of protected bike lanes, with an average monthly increase of 379 rides per station (p<0.001). Despite this causal effect being pronounced among census block groups with higher percentages of older adults (688 rides per month per station, p<0.001), the causal effect of protected bike lanes on bikeshare ridership is absent in census block groups where the percentage of Black residents is medium to high. Taken together, these findings indicate that planners must emphasize protected bike lanes to spur ridership, and simultaneously target policies and programming to communities of color, to ensure that such infrastructure makes urban cycling a viable option for all residents.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Re-inventing Willis,"Scientists often re-invent things that were long known. Here we review these activities as related to the mechanism of producing power law distributions, originally proposed in 1922 by Yule to explain experimental data on the sizes of biological genera, collected by Willis. We also review the history of re-invention of closely related branching processes, random graphs and coagulation models.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Spices form the basis of food pairing in Indian cuisine,"Culinary practices are influenced by climate, culture, history and geography. Molecular composition of recipes in a cuisine reveals patterns in food preferences. Indian cuisine encompasses a number of diverse sub-cuisines separated by geographies, climates and cultures. Its culinary system has a long history of health-centric dietary practices focused on disease prevention and promotion of health. We study food pairing in recipes of Indian cuisine to show that, in contrast to positive food pairing reported in some Western cuisines, Indian cuisine has a strong signature of negative food pairing; more the extent of flavor sharing between any two ingredients, lesser their co-occurrence. This feature is independent of recipe size and is not explained by ingredient category-based recipe constitution alone. Ingredient frequency emerged as the dominant factor specifying the characteristic flavor sharing pattern of the cuisine. Spices, individually and as a category, form the basis of ingredient composition in Indian cuisine. We also present a culinary evolution model which reproduces ingredient use distribution as well as negative food pairing of the cuisine. Our study provides a basis for designing novel signature recipes, healthy recipe alterations and recipe recommender systems.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying the Social Costs of Power Outages and Restoration Disparities Across Four U.S. Hurricanes,"The multifaceted nature of disaster impact shows that densely populated areas contribute more to aggregate burden, while sparsely populated but heavily affected regions suffer disproportionately at the individual level. This study introduces a framework for quantifying the societal impacts of power outages by translating customer weighted outage exposure into deprivation measures, integrating welfare metrics with three recovery indicators, average outage days per customer, restoration duration, and relative restoration rate, computed from sequential EAGLE I observations and linked to Zip Code Tabulation Area demographics. Applied to four United States hurricanes, Beryl 2024 Texas, Helene 2024 Florida, Milton 2024 Florida, and Ida 2021 Louisiana, this standardized pipeline provides the first cross event, fine scale evaluation of outage impacts and their drivers. Results demonstrate regressive patterns with greater burdens in lower income areas, mechanistic analysis shows deprivation increases with longer restoration durations and decreases with faster restoration rates, explainable modeling identifies restoration duration as the dominant driver, and clustering reveals distinct recovery typologies not captured by conventional reliability metrics. This framework delivers a transferable method for assessing outage impacts and equity, comparative cross event evidence linking restoration dynamics to social outcomes, and actionable spatial analyses that support equity informed restoration planning and resilience investment.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Complex systems science and urban science: towards applications to sustainability trade-offs in territorial systems,"Urban systems are at the core of current sustainability concerns, and their study from a complexity perspective has a long history in several disciplines. We survey this literature and discuss future research directions relevant to sustainable planning, in particular the construction of integrative approaches. We finally illustrate this research program with the coupling of urban simulation models to explore trade-offs between sustainable development goals in systems of cities.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Contraction of online response to major events,"Quantifying regularities in behavioral dynamics is of crucial interest for understanding collective social events such as panics or political revolutions. With the widespread use of digital communication media it has become possible to study massive data streams of user-created content in which individuals express their sentiments, often towards a specific topic. Here we investigate messages from various online media created in response to major, collectively followed events such as sport tournaments, presidential elections or a large snow storm. We relate content length and message rate, and find a systematic correlation during events which can be described by a power law relation - the higher the excitation the shorter the messages. We show that on the one hand this effect can be observed in the behavior of most regular users, and on the other hand is accentuated by the engagement of additional user demographics who only post during phases of high collective activity. Further, we identify the distributions of content lengths as lognormals in line with statistical linguistics, and suggest a phenomenological law for the systematic dependence of the message rate to the lognormal mean parameter. Our measurements have practical implications for the design of micro-blogging and messaging services. In the case of the existing service Twitter, we show that the imposed limit of 140 characters per message currently leads to a substantial fraction of possibly dissatisfying to compose tweets that need to be truncated by their users.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Analyzing travel time reliability of a bus route in a limited data set scenario: A case study,"In this information era commuters prefer to know a reliable travel time to plan ahead of their journey using both public and private modes. In this direction reliability analysis using the location data of the buses is conducted in two folds in the current work; (i) Reliability analysis of a public transit service at route level, and (ii) Travel time reliability analysis of a route utilizing the location data of the buses. The reliability parameters assessed for public transit service are headway, passenger waiting time, travel speed, and travel time as per the Service Level Benchmarks for Urban Transport by the National Urban Transport Policy, Government of India. And travel time reliability parameters such as Buffer Time Index, Travel Time Index, and Planning Time Index are assessed as per Federal Highway Administration, Department of Transportation, U S. The study is conducted in Tumakuru city, India for a significant bus route in a limited data sources scenario. The results suggest that (i) the Level of Service of the public transit service needs improvement. (ii)around 30% excess of average travel time is needed as buffer time. (iii) more than double the amount of free flow travel time must be planned during peak hours and in the worst case. In the future, the analysis conducted for the route can be extended for citywide performance analysis in both folds. Also, the same method can be applied to cities with similar demographics and traffic-related infrastructure.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Honolulu Rail Transit: International Lessons from Barcelona in Linking Urban Form, Design, and Transportation","The city of Honolulu, Hawaii is currently planning and developing a new rail transit system. While Honolulu has supportive density and topography for rail transit, questions remain about its ability to effectively integrate urban design and accessibility across the system. Every transit trip begins and ends with a walking trip from origins and to destinations: transportation planning must account for pedestrian safety, comfort, and access. Ildefons Cerda's 19th century utopian plan for Barcelona's Eixample district produced a renowned, livable urban form. The Eixample, with its well-integrated rail transit, serves as a model of urban design, land use, transportation planning, and pedestrian-scaled streets working in synergy to produce accessibility. This study discusses the urban form of Honolulu and the history and planning of its new rail transit system. Then it reviews the history of Cerda's plan for the Eixample and discusses its urban form and performance today. Finally it draws several lessons from Barcelona's urban design, accessibility, and rail transit planning and critically discusses their applicability to policy and design in Honolulu. This discussion is situated within wider debates around livable cities and social justice as it contributes several form and design lessons to the livability and accessibility literature while identifying potential concerns with privatization and displacement.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Expectation-driven interaction: a model based on Luhmann's contingency approach,"We introduce an agent-based model of interaction, drawing on the contingency approach from Luhmann's theory of social systems. The agent interactions are defined by the exchange of distinct messages. Message selection is based on the history of the interaction and developed within the confines of the problem of double contingency. We examine interaction strategies in the light of the message-exchange description using analytical and computational methods.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Quantifying individual performance in Cricket $-$ A network analysis of Batsmen and Bowlers,Quantifying individual performance in the game of Cricket is critical for team selection in International matches. The number runs scored by batsmen and wickets taken by bowlers serves as a natural way of quantifying the performance of a cricketer. Traditionally the batsmen and bowlers are rated on their batting or bowling average respectively. However in a game like Cricket it is always important the manner in which one scores the runs or claims a wicket. Scoring runs against a strong bowling line-up or delivering a brilliant performance against a team with strong batting line-up deserves more credit. A player's average is not able to capture this aspect of the game. In this paper we present a refined method to quantify the `quality' of runs scored by a batsman or wickets taken by a bowler. We explore the application of Social Network Analysis (SNA) to rate the players in a team performance. We generate directed and weighted network of batsmen-bowlers using the player-vs-player information available for Test cricket and ODI cricket. Additionally we generate network of batsmen and bowlers based on the dismissal record of batsmen in the history of cricket - Test (1877-2011) and ODI (1971-2011). Our results show that {\it M Muralitharan} is the most successful bowler in history of Cricket. Our approach could potentially be applied in domestic matches to judge a player's performance which in turn pave the way for a balanced team selection for International matches.,"cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Analysing Time-Stamped Co-Editing Networks in Software Development Teams using git2net,"Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts. Because this neglects detailed information on code changes and code ownership we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. We apply our tool in two case studies using GitHub repositories of multiple Open Source as well as a commercial software project. Specifically, we use data on more than 1.2 million commits and more than 25'000 developers to test a hypothesis on the relation between developer productivity and co-editing patterns in software teams. We argue that git2net opens up a massive new source of high-resolution data on human collaboration patterns that can be used to advance theory in empirical software engineering, computational social science, and organisational studies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Effects of the globalization in the Korean financial markets,"We study the effect of globalization on the Korean market, one of the emerging markets. Some characteristics of the Korean market are different from those of the mature market according to the latest market data, and this is due to the influence of foreign markets or investors. We concentrate on the market network structures over the past two decades with knowledge of the history of the market, and determine the globalization effect and market integration as a function of time.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Finite Population Dynamics Resolve the Central Paradox of the Inspection Game,"The Inspection Game is the canonical model for the strategic conflict between law enforcement (inspectors) and citizens (potential criminals). Its classical Mixed-Strategy Nash Equilibrium (MSNE) is afflicted by a paradox: the equilibrium crime rate is independent of both the penalty size ($p$) and the crime gain ($g$), undermining the efficacy of deterrence policy. We re-examine this challenge using evolutionary game theory, focusing on the long-term fixation probabilities of strategies in finite, asymmetric population sizes subject to demographic noise. The deterministic limit of our model exhibits stable limit cycles around the MSNE, which coincides with the neutral fixed point of the equilibrium analysis. Crucially, in finite populations, demographic noise drives the system away from this cycle and toward absorbing states. Our results demonstrate that high absolute penalties $p$ are highly effective at suppressing crime by influencing the geometry of the deterministic dynamics, which in turn biases the fixation probability toward the criminal extinction absorbing state, thereby restoring the intuitive role of $p$. Furthermore, we reveal a U-shaped policy landscape where both high penalties and light penalties (where $p \approx g$) are successful suppressors, maximizing criminal risk at intermediate penalty levels. Most critically, we analyze the realistic asymptotic limit of extreme population sizes asymmetry, where inspectors are exceedingly rare. In this limit, the system's dynamic outcome is entirely decoupled from the citizen payoff parameters $p$ and $g$, and is instead determined by the initial frequency of crime relative to the deterrence threshold (the ratio of inspection cost to reward for catching a criminal). This highlights that effective crime suppression requires managing the interaction between deterministic dynamics, demographic noise, and initial conditions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Emergent Democracy,"This essay argues that a new form of democracy - an ""Emergent Democracy"" - will develop as a result of the use of Internet communication tools and platforms such as blogs. The essay explores a variety of tools available and explores the history of democracy, modern experiments with democracy and how these tools might support democracy. The essay also explores concerns as these new tools emerge. These issues include concerns such as privacy and the societally negative use of these tools by corporations, totalitarian regimes and terrorists.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
On the knowledge production function,"Knowledge amount is an integral indicator of the development of society. Humanity produces knowledge in response to challenges from nature and society. Knowledge production depends on population size and human productivity. Productivity is a function of knowledge amount. The purpose of this study is to find this function and verify it on empirical material, including global demographic and information data. The productivity function is a basic element of the theory that results in the dynamic equations of knowledge production and population growth. A separate problem is the quantitative assessment of knowledge. To solve it, we consider knowledge representations in the form of patents, articles and books. Knowledge is stored in various types of devices, which together form a global informational storage. Storage capacity is increasing rapidly as digital technology advances. We compare storage capacity with the memory occupied by the forms of knowledge representation. The results obtained in this study contribute to the theory of knowledge production and related demographic dynamics and allow us to deepen our understanding of civilization development.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Outbreak-size distributions under fluctuating rates,"We study the effect of noisy infection (contact) and recovery rates on the distribution of outbreak sizes in the stochastic SIR model. The rates are modeled as Ornstein-Uhlenbeck processes with finite correlation time and variance, which we illustrate using outbreak data from the RSV 2019-2020 season in the US. In the limit of large populations, we find analytical solutions for the outbreak-size distribution in the long-correlated (adiabatic) and short-correlated (white) noise regimes, and demonstrate that the distribution can be highly skewed with significant probabilities for large fluctuations away from mean-field theory. Furthermore, we assess the relative contribution of demographic and reaction-rate noise on the outbreak-size variance, and show that demographic noise becomes irrelevant in the presence of slowly varying reaction-rate noise but persists for large system sizes if the noise is fast. Finally, we show that the crossover to the white-noise regime typically occurs for correlation times that are on the same order as the characteristic recovery time in the model.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Epidemic Forecast Follies,"We introduce a simple multiplicative model to describe the temporal behavior and the ultimate outcome of an epidemic. Our model accounts, in a minimalist way, for the competing influences of imposing public-health restrictions when the epidemic is severe, and relaxing restrictions when the epidemic is waning. Our primary results are that different instances of an epidemic with identical starting points have disparate outcomes and each epidemic temporal history is strongly fluctuating.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Non-negative Tensor Factorization for Human Behavioral Pattern Mining in Online Games,"Multiplayer online battle arena has become a popular game genre. It also received increasing attention from our research community because they provide a wealth of information about human interactions and behaviors. A major problem is extracting meaningful patterns of activity from this type of data, in a way that is also easy to interpret. Here, we propose to exploit tensor decomposition techniques, and in particular Non-negative Tensor Factorization, to discover hidden correlated behavioral patterns of play in a popular game: League of Legends. We first collect the entire gaming history of a group of about one thousand players, totaling roughly $100K$ matches. By applying our methodological framework, we then separate players into groups that exhibit similar features and playing strategies, as well as similar temporal trajectories, i.e., behavioral progressions over the course of their gaming history: this will allow us to investigate how players learn and improve their skills.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Geo-Spotting: Mining Online Location-based Services for Optimal Retail Store Placement,"The problem of identifying the optimal location for a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable.   In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility, which includes transitions between venues or the incoming flow of mobile users from distant areas. Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. We also show that performance improves significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple factors.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Impact of Declining Proposal Success Rates on Scientific Productivity,"Over the last decade proposal success rates in the fundamental sciences have dropped significantly. Astronomy and related fields funded by NASA and NSF are no exception. Data across agencies show that this is not principally the result of a decline in proposal merit (the proportion of proposals receiving high rankings is largely unchanged), nor of a shift in proposer demographics (seniority, gender, and institutional affiliation have all remained unchanged), nor of an increase (beyond inflation) in the average requested funding per proposal, nor of an increase in the number of proposals per investigator in any one year. Rather, the statistics are consistent with a scenario in which agency budgets for competed research are flat or decreasing in inflation-adjusted dollars, the overall population of investigators has grown, and a larger proportion of these investigators are resubmitting meritorious but unfunded proposals. This White Paper presents statistics which support this conclusion, as well as recent research on the time cost of proposal writing versus that of producing publishable results. We conclude that an aspirational proposal success rate of 30-35% would still provide a healthily competitive environment for researchers, would more fully utilize the scientific capacity of the community's facilities and missions, and provide relief to the funding agencies who face the logistics of ever-increasing volumes of proposals.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Responding to complexity in socio-economic systems: How to build a smart and resilient society?,"The world is changing at an ever-increasing pace. And it has changed in a much more fundamental way than one would think, primarily because it has become more connected and interdependent than in our entire history. Every new product, every new invention can be combined with those that existed before, thereby creating an explosion of complexity: structural complexity, dynamic complexity, functional complexity, and algorithmic complexity. How to respond to this challenge? And what are the costs?","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
git2net - Mining Time-Stamped Co-Editing Networks from Large git Repositories,"Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
How the competitive altruism leads to bistable homogeneous states of cooperation or defection,"Our recent minimal model of cooperation (P. Gawronski et al, Physica A 388 (2009) 3581) is modified as to allow for time-dependent altruism. This evolution is based on reputation of other agents, which in turn depends on history. We show that this modification leads to two absorbing states of the whole system, where the cooperation flourishes in one state and is absent in another one. The effect is compared with the results obtained with the model of indirect reciprocity, where the altruism of agents is constant.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Economic Hysteresis and Its Mathematical Modeling,"Hysteresis is treated as a history dependent branching, and the use of the classical Preisach model for the analysis of macroeconomic hysteresis is first discussed. Then, a new Preisach-type model is introduced as a macroeconomic aggregation of more realistic microeconomic hysteresis than in the case of the classical Preisach model. It is demonstrated that this model is endowed with a more general mechanism of branching and may account for the continuous evolution of the economy and its effect on hysteresis. Furthermore, it is shown that the sluggishness of economic recovery is an intrinsic manifestation of hysteresis branching.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Atlas for the Aspiring Network Scientist,"Network science is the field dedicated to the investigation and analysis of complex systems via their representations as networks. We normally model such networks as graphs: sets of nodes connected by sets of edges and a number of node and edge attributes. This deceptively simple object is the starting point of never-ending complexity, due to its ability to represent almost every facet of reality: chemical interactions, protein pathways inside cells, neural connections inside the brain, scientific collaborations, financial relations, citations in art history, just to name a few examples. If we hope to make sense of complex networks, we need to master a large analytic toolbox: graph and probability theory, linear algebra, statistical physics, machine learning, combinatorics, and more.   This book aims at providing the first access to all these tools. It is intended as an ""Atlas"", because its interest is not in making you a specialist in using any of these techniques. Rather, after reading this book, you will have a general understanding about the existence and the mechanics of all these approaches. You can use such an understanding as the starting point of your own career in the field of network science. This has been, so far, an interdisciplinary endeavor. The founding fathers of this field come from many different backgrounds: mathematics, sociology, computer science, physics, history, digital humanities, and more. This Atlas is charting your path to be something different from all of that: a pure network scientist.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Higher-order Network Analysis Takes Off, Fueled by Classical Ideas and New Data","Higher-order network analysis uses the ideas of hypergraphs, simplicial complexes, multilinear and tensor algebra, and more, to study complex systems. These are by now well established mathematical abstractions. What's new is that the ideas can be tested and refined on the type of large-scale data arising in today's digital world. This research area therefore is making an impact across many applications. Here, we provide a brief history, guide, and survey.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
High Socioeconomic Status is Associated with Diverse Consumption across Brands and Price Levels,"Consumption practices are determined by a combination of economic, social, and cultural forces. We posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestyle-based niche consumption. We provide empirical evidence for this diversity hypothesis by analysing millions of mobile-tracked visits from thousands of Census Block Groups to thousands of stores in New York State. The results show that high income is significantly associated with diverse consumption across brands and price levels. The associations between diversity and income persist but are less prominent for necessity-based consumption and for the densely populated and demographically diverse New York City. The associations replicate for education as an alternative measure of socioeconomic status and for the state of Texas. We further illustrate that the associations cannot be explained by simple geographic constraints, including the neighbourhoods' demographic diversity, the residents' geographic mobility and the stores' local availability, so deeper social and cultural factors must be at play.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The computational power of a human society: a new model of social evolution,"Social evolutionary theory seeks to explain increases in the scale and complexity of human societies, from origins to present. Over the course of the twentieth century, social evolutionary theory largely fell out of favor as a way of investigating human history, just when advances in complex systems science and computer science saw the emergence of powerful new conceptions of complex systems, and in particular new methods of measuring complexity. We propose that these advances in our understanding of complex systems and computer science should be brought to bear on our investigations into human history. To that end, we present a new framework for modeling how human societies co-evolve with their biotic environments, recognizing that both a society and its environment are computers. This leads us to model the dynamics of each of those two systems using the same, new kind of computational machine, which we define here. For simplicity, we construe a society as a set of interacting occupations and technologies. Similarly, under such a model, a biotic environment is a set of interacting distinct ecological and environmental processes. This provides novel ways to characterize social complexity, which we hope will cast new light on the archaeological and historical records. Our framework also provides a natural way to formalize both the energetic (thermodynamic) costs required by a society as it runs, and the ways it can extract thermodynamic resources from the environment in order to pay for those costs -- and perhaps to grow with any left-over resources.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Analyzing Common Social and Physical Features of Flash-Flood Vulnerability Hotspots in Urban Areas,"Flash flooding events, with their intense and sudden nature, present unique challenges for disaster researchers and emergency planners. To quantify the extent to which hotspots of flash flooding share similar social and physical features, the research uses community scale crowdsourced data and k means clustering. Crowdsourced data offers the potential to allocate limited resources, to improve spatial understanding, and to minimize the future effects of natural hazards. The research evaluates the impacts of Tropical Storm Imelda on Houston Metropolitan and Hurricane Ida on New York City. It develops a combined flash flood impact index based on FEMA claims, 311 calls, and Waze traffic reports which is able to capture a combination of crowdsourced data for the societal impact of flash flooding. In addition, k means clustering offers an essential tool for evaluating attributes associated with flash flooding events by grouping data into k number clusters of features. Thus, k means clustering evaluates the significance of a community's socio demographic, social capital, and physical features to the combined flood impact index. To ensure accessibility and replicability to different types of communities, our research uses publicly available datasets to understand how socio demographic data, social capital, and physical connectivity and development affect flash flood resilience. The findings reveal the intricate relationships associated with flash flooding impact as a combination of socio demographic and physical features. For instance, the cluster with the highest scores of the flash flood impact index from Tropical Storm Imelda had the highest percentage of minority population and lower income while the cluster with the second highest score flash flood impact index had the highest percentage of impervious surface and number of POIs.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Decoding the Manhattan Project's Network: Unveiling Science, Collaboration, and Human Legacy","The Manhattan Project was one of the largest scientific collaborations ever undertaken. It operated thanks to a complex social network of extraordinary minds and it became undoubtedly one of the most remarkable intellectual efforts of human history. It also had devastating consequences during and after the atomic bombings of Hiroshima and Nagasaki. Despite the loss of hundreds of thousands of human lives during the bombing and the subsequent events, the scientific journey itself stands as a testament to human achievement, as highlighted in Christopher Nolan's film portrayal of Oppenheimer.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Preprint Dj Vu: an FAQ,"I give a brief overview of arXiv history, and describe the current state of arXiv practice, both technical and sociological. This commentary originally appeared in the EMBO Journal, 19 Oct 2016. It was intended as an update on comments from the late 1990s regarding use of preprints by biologists (or lack thereof), but may be of interest to practitioners of other disciplines. It is based largely on a keynote presentation I gave to the ASAPbio inaugural meeting in Feb 2016, and responds as well to some follow-up questions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Cues to gender and racial identity reduce creativity in diverse social networks,"The characteristics of social partners have long been hypothesized as influential in guiding group interactions. Understanding how demographic cues impact networks of creative collaborators is critical for elevating creative performances therein. We conducted a randomized experiment to investigate how the knowledge of peers' gender and racial identities distorts people's connection patterns and the resulting creative outcomes in a dynamic social network. Consistent with prior work, we found that creative inspiration links are primarily formed with top idea-generators. However, when gender and racial identities are known, not only is there (1) an increase of 82.03% in the odds of same-gender connections (but not for same-race connections), but (2) the semantic similarity of idea-sets stimulated by these connections also increase significantly compared to demography-agnostic networks, negatively impacting the outcomes of divergent creativity. We found that ideas tend to be more homogeneous within demographic groups than between, taking away diversity-bonuses from similarity-based links and partly explaining the results. These insights can inform intelligent interventions to enhance network-wide creative performances.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Internal migration and mobile communication patterns among pairs with strong ties,"Using large-scale call detail records of anonymised mobile phone service subscribers with demographic and location information, we investigate how a long-distance residential move within the country affects the mobile communication patterns between an ego who moved and a frequently called alter who did not move. By using clustering methods in analysing the call frequency time series, we find that such ego-alter pairs are grouped into two clusters, those with the call frequency increasing and those with the call frequency decreasing after the move of the ego. This indicates that such residential moves are correlated with a change in the communication pattern soon after moving. We find that the pre-move calling behaviour is a relevant predictor for the post-move calling behaviour. While demographic and location information can help in predicting whether the call frequency will rise or decay, they are not relevant in predicting the actual call frequency volume. We also note that at four months after the move, most of these close pairs maintain contact, even if the call frequency is decreased.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Optimizing COVID-19 vaccine distribution adding spatio-temporal criteria,"Massive vaccination against pandemics such as Coronavirus SARS-CoV-2 presents several complexities. The criteria to assess public health policies are fundamental to distribute vaccines in an effective way in order to avoid as many infections and deaths as possible. Usually these policies are focused on determining socio-demographic groups of people and establishing a vaccination order among these groups.   This work focuses on optimizing the way of distributing vaccines among the different populations of a region for a period of time once established the priority socio-demographic groups. For this aim we use a SEIR model which takes into account vaccination. Also, for this model we prove theoretical results concerning the convergence of solutions on the long-term and the stability of fixed points and analyze the impact of an hypothetical vaccination during the COVID-19 pandemics in Spain.   After that, we introduce a heuristic approach in order to minimize the COVID-19 spreading by planning effective vaccine distributions among the populations of a region over a period of time. As an application, the impact of distributing vaccines in the Valencian Community (Spain) according to this method is computed in terms of the number of saved infected individuals.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Interdependent Diffusion: The social contagion of interacting beliefs,"Social contagion is the process in which people adopt a belief, idea, or practice from a neighbor and pass it along to someone else. For over 100 years, scholars of social contagion have almost exclusively made the same implicit assumption: that only one belief, idea, or practice spreads through the population at a time. It is a default assumption that we don't bother to state, let alone justify. The assumption is so ingrained that our literature doesn't even have a word for ""whatever is to be diffused"", because we have never needed to discuss more than one of them. But this assumption is obviously false. Millions of beliefs, ideas, and practices (let's call them ""diffusants"") spread through social contagion every day. To assume that diffusants spread one at a time - or more generously, that they spread independently of one another - is to assume that interactions between diffusants have no influence on adoption patterns. This could be true, or it could be wildly off the mark. We've never stopped to find out. This paper makes a direct comparison between the spread of independent and interdependent beliefs using simulations, observational data, and a 2400-subject laboratory experiment. I find that in assuming independence between diffusants, scholars have overlooked social processes that fundamentally change the outcomes of social contagion. Interdependence between beliefs generates polarization, irrespective of social network structure, homophily, demographics, politics, or any other commonly cited cause. It also coordinates structures of beliefs that can have both internal justification and social support without any grounding in external truth.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Network-based indicators of Bitcoin bubbles,"The functioning of the cryptocurrency Bitcoin relies on the open availability of the entire history of its transactions. This makes it a particularly interesting socio-economic system to analyse from the point of view of network science. Here we analyse the evolution of the network of Bitcoin transactions between users. We achieve this by using the complete transaction history from December 5th 2011 to December 23rd 2013. This period includes three bubbles experienced by the Bitcoin price. In particular, we focus on the global and local structural properties of the user network and their variation in relation to the different period of price surge and decline. By analysing the temporal variation of the heterogeneity of the connectivity patterns we gain insights on the different mechanisms that take place during bubbles, and find that hubs (i.e., the most connected nodes) had a fundamental role in triggering the burst of the second bubble. Finally, we examine the local topological structures of interactions between users, we discover that the relative frequency of triadic interactions experiences a strong change before, during and after a bubble, and suggest that the importance of the hubs grows during the bubble. These results provide further evidence that the behaviour of the hubs during bubbles significantly increases the systemic risk of the Bitcoin network, and discuss the implications on public policy interventions.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Qbits, la funcion de onda del universo, diagramas de Penrose: Una nueva perspectiva para una fisica simbolica (I)","The present research is about the analysis of the wave function of the universe applied to cosmological and quantum states. Also the wave function includes the history of the universe itself in several states, starting from the big bang and going to the actual time. Regarding the entropic consideration of Hawking for a black hole, the wave function contains all the information of the matter that can collapse into a black hole, and the information is transmitted as radiation. As a consequence, the information is analyzed as a quantum information from qubits idea.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Say Their Names: Resurgence in the collective attention toward Black victims of fatal police violence following the death of George Floyd,"The murder of George Floyd by police in May 2020 sparked international protests and renewed attention in the Black Lives Matter movement. Here, we characterize ways in which the online activity following George Floyd's death was unparalleled in its volume and intensity, including setting records for activity on Twitter, prompting the saddest day in the platform's history, and causing George Floyd's name to appear among the ten most frequently used phrases in a day, where he is the only individual to have ever received that level of attention who was not known to the public earlier that same week. Further, we find this attention extended beyond George Floyd and that more Black victims of fatal police violence received attention following his death than during other past moments in Black Lives Matter's history. We place that attention within the context of prior online racial justice activism by showing how the names of Black victims of police violence have been lifted and memorialized over the last 12 years on Twitter. Our results suggest that the 2020 wave of attention to the Black Lives Matter movement centered past instances of police violence in an unprecedented way, demonstrating the impact of the movement's rhetorical strategy to ""say their names.""","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Rethinking Resource Allocation in Science,"US funding agencies alone distribute a yearly total of roughly $65B dollars largely through the process of proposal peer review: scientists compete for project funding by submitting grant proposals which are evaluated by selected panels of peer reviewers. Similar funding systems are in place in most advanced democracies. However, in spite of its venerable history, proposal peer review is increasingly struggling to deal with the increasing mismatch between demand and supply of research funding.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Justice in the Shadow of Self-Interest. An Experiment on Redistributive Behavior,"By means of laboratory experiment I examine the relation between fairness judgments made `behind the veil of ignorance' and actual behavior in a model situation of income inequality. As the evidence shows, when material self-interest is at stake vast majority of subjects tends to abandon the fairness norm. Rather small regard for efficiency is present in the data. Furthermore, as low income players go through a sequence of games against high earners and experience changes in income disparity, the history effect proves to override structural characteristics of the redistribution game.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A Systematic Identification and Analysis of Scientists on Twitter,"Metrics derived from Twitter and other social media---often referred to as altmetrics---are increasingly used to estimate the broader social impacts of scholarship. Such efforts, however, may produce highly misleading results, as the entities that participate in conversations about science on these platforms are largely unknown. For instance, if altmetric activities are generated mainly by scientists, does it really capture broader social impacts of science? Here we present a systematic approach to identifying and analyzing scientists on Twitter. Our method can identify scientists across many disciplines, without relying on external bibliographic data, and be easily adapted to identify other stakeholder groups in science. We investigate the demographics, sharing behaviors, and interconnectivity of the identified scientists. We find that Twitter has been employed by scholars across the disciplinary spectrum, with an over-representation of social and computer and information scientists; under-representation of mathematical, physical, and life scientists; and a better representation of women compared to scholarly publishing. Analysis of the sharing of URLs reveals a distinct imprint of scholarly sites, yet only a small fraction of shared URLs are science-related. We find an assortative mixing with respect to disciplines in the networks between scientists, suggesting the maintenance of disciplinary walls in social media. Our work contributes to the literature both methodologically and conceptually---we provide new methods for disambiguating and identifying particular actors on social media and describing the behaviors of scientists, thus providing foundational information for the construction and use of indicators on the basis of social media metrics.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
What does making money have to do with crime?: A dive into the National Crime Victimization survey,"In this short article, I leverage the National Crime Victimization Survey from 1992 to 2022 to examine how income, education, employment, and key demographic factors shape the type of crime victims experience (violent vs property). Using balanced classification splits and logistic regression models evaluated by F1-score, there is an isolation of the socioeconomic drivers of victimization ""Group A"" models and then an introduction of demographic factors such as age, gender, race, and marital status controls called ""Group B"" models. The results consistently proves that higher income and education lower the odds of violent relative to property crime, while men younger individuals and racial minorities face disproportionately higher violentcrime risks. On the geographic spectrum, the suburban models achieve the strongest predictive performance with an accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education and employment predictors and crime in rural areas are still unpredictable using these current factors. The patterns found in this study shows the need for specific interventions like educational investments in metropolitan settings economic support in rural communities and demographicaware prevention strategies.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Migration and Trade: A Complex-Network Approach,This paper explores the relationships between migration and trade using a complex-network approach. We show that: (i) both weighted and binary versions of the networks of international migration and trade are strongly correlated; (ii) such correlations can be mostly explained by country economic/demographic size and geographical distance; (iii) pairs of countries that are more central in the international-migration network trade more.,"cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Investigating population dynamics of the Kumbh Mela through the lens of cell phone data,"The Kumbh is a religious Hindu festival that has been celebrated for centuries. The 2013 Kumbh Mela, a grander form of the annual Kumbh, was purportedly the largest gathering of people in human history. Many of the participants carried cell phones, making it possible for us to use a data-driven approach to document this magnificent festival. We used Call Detail Records (CDRs) from participants attending the event, a total of 390 million records, to investigate its population dynamics. We report here on some of our preliminary findings.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
The Role of Social Interactions in Mitigating Psychological Distress During the COVID-19 Pandemic: A Study in Sri Lanka,"Massive changes in many aspects related to social groups of different socioeconomic backgrounds were caused by the COVID-19 pandemic and as a result, the overall state of mental health was severely affected globally. This study examined how the pandemic affected Sri Lankan citizens representing a range of socioeconomic backgrounds in terms of their mental health. The data used in this research was gathered from 3,020 households using a nationwide face-to-face survey, from which a processed dataset of 921 responses was considered for the final analysis. Four distinct factors were identified by factor analysis (FA) that was conducted and subsequently, the population was clustered using unsupervised clustering to determine which population subgroups were affected similarly. Two such subgroups were identified where the respective relationships to the retrieved principal factors and their demographics were thoroughly examined and interpreted. This resulted in the identification of contrasting perspectives between the two groups toward the maintenance and the state of social relationships during the pandemic, which revealed that one group was more 'socially connected' in nature resulting in their mental state being comparatively better in coping with the pandemic. The other group was seen to be more 'socially reserved' showing an opposite reaction toward social connections while their mental well-being declined showing symptoms such as loneliness, and emptiness in response to the pandemic. The study examined the role of social media, and it was observed that social media was perceived as a substitute for the lack of social connections or primarily used as a coping mechanism in response to the challenges of the pandemic","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Cooperation in Public Goods Games: Leveraging Other-Regarding Reinforcement Learning on Hypergraphs,"Cooperation as a self-organized collective behavior plays a significant role in the evolution of ecosystems and human society. Reinforcement learning (RL) offers a new perspective, distinct from imitation learning in evolutionary games, for exploring the mechanisms underlying its emergence. However, most existing studies with the public good game (PGG) employ a self-regarding setup or are on pairwise interaction networks. Players in the real world, however, optimize their policies based not only on their histories but also on the histories of their co-players, and the game is played in a group manner. In the work, we investigate the evolution of cooperation in the PGG under the other-regarding reinforcement learning evolutionary game (OR-RLEG) on hypergraph by combining the Q-learning algorithm and evolutionary game framework, where other players' action history is incorporated and the game is played on hypergraphs. Our results show that as the synergy factor increases, the parameter interval is divided into three distinct regions, the absence of cooperation (AC), medium cooperation (MC), and high cooperation (HC), accompanied by two abrupt transitions in the cooperation level near two transition points, respectively. Interestingly, we identify regular and anti-coordinated chessboard structures in the spatial pattern that positively contribute to the first cooperation transition but adversely affect the second. Furthermore, we provide a theoretical treatment for the first transition with an approximated first transition point and reveal that players with a long-sighted perspective and low exploration rate are more likely to reciprocate kindness with each other, thus facilitating the emergence of cooperation. Our findings contribute to understanding the evolution of human cooperation, where other-regarding information and group interactions are commonplace.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Encouraging moderation: Clues from a simple model of ideological conflict,"Some of the most pivotal moments in intellectual history occur when a new ideology sweeps through a society, supplanting an established system of beliefs in a rapid revolution of thought. Yet in many cases the new ideology is as extreme as the old. Why is it then that moderate positions so rarely prevail? Here, in the context of a simple model of opinion spreading, we test seven plausible strategies for deradicalizing a society and find that only one of them significantly expands the moderate subpopulation without risking its extinction in the process.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Cities as they could be: Artificial Life and Urban Systems,"The metaphor of cities as organisms has a long history in urban planning, and a few urban modeling approaches have explicitly been linked to Artificial Life. We propose in that paper to explore the extent of Artificial Life and Artificial Intelligence application to urban issues, by constructing and exploring a citation network of around 225,000 papers. It shows that most of the literature is indeed application of methodologies and a rather strong modularity of approaches. We finally develop ALife concepts which have a strong potential for the development of new urban theories.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Emergence and resilience of social networks: a general theoretical framework,"We introduce and study a general model of social network formation and evolution based on the concept of preferential link formation between similar nodes and increased similarity between connected nodes. The model is studied numerically and analytically for three definitions of similarity. In common with real-world social networks, we find coexistence of high and low connectivity phases and history dependence. We suggest that the positive feedback between linking and similarity which is responsible for the model's behaviour is also an important mechanism in real social networks.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
One size does not fit all: the complex relationship between biodiversity and psychological well-being,"Enhancing urban biodiversity is increasingly advanced as a nature-based solution that can help align public health and biodiversity conservation agendas. Yet, research on the relationship between biodiversity and psychological well-being provides inconsistent results. The goal of this interdisciplinary research was to understand how components of psychological well-being of green space users relate to species richness and abundance. Additionally, we investigated how key characteristics that shape the way people interact with nature, affinity towards nature and ecological knowledge, moderate the well-being biodiversity relationship. We sampled bird, butterfly and plant in 24 urban gardens in Israel and distributed 600 close-ended questionnaires in-situ to measure psychological well-being, nature-relatedness, ecological knowledge, perceived species richness and demographics. Components of psychological well-being were mostly associated with perceived species richness and to lesser extent with actual species richness and abundance for all taxa. Nature-relatedness moderated these relationships. Respondents with high nature-relatedness demonstrated positive well-being-richness relationships, while those with intermediate, or low, nature-relatedness showed no, or even negative relationships, respectively. Opposite relationships were recorded for bird abundance, i.e., negative versus positive well-being-abundance relationship for individuals with high or low nature-relatedness, respectively. Overall, individuals demonstrated poor ecological knowledge of species and this variable moderated the relationships between well-being components and perceived butterfly richness and bird abundance. Our results demonstrate that one-size-does-not-fit-all when considering the relationship between psychological well-being and biodiversity and that affinity to nature is a key moderator for this relationship.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
How time and pollster history affect U.S. election forecasts under a compartmental modeling approach,"In the months leading up to political elections in the United States, forecasts are widespread and take on multiple forms, including projections of what party will win the popular vote, state ratings, and predictions of vote margins at the state level. It can be challenging to evaluate how accuracy changes in the lead up to Election Day or to put probabilistic forecasts into historical context. Moreover, forecasts differ between analysts, highlighting the many choices in the forecasting process. With this as motivation, here we take a more comprehensive view and begin to unpack some of the choices involved in election forecasting. Building on a prior compartmental model of election dynamics, we present the forecasts of this model across months, years, and types of race. By gathering together monthly forecasts of presidential, senatorial, and gubernatorial races from 2004--2022, we provide a larger-scale perspective and discuss how treating polling data in different ways affects forecast accuracy. We conclude with our 2024 election forecasts (upcoming at the time of writing).","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Regional Development in the Knowledge-Based Economy: The Construction of Advantage,"In this introduction the editors showcase the papers by way of a structured project and seek to clarify the two key concepts cited in the title. We consider the history of the idea that knowledge is an economic factor, and discuss the question of whether regions provide the relevant system of reference for knowledge-based economic development. Current transformations in university-industry-government relations at various levels can be considered as a metamorphosis in industry organization. The concept of constructed advantage will be elaborated. The various papers arising from a conference on this subject hosted by Memorial University, Newfoundland, Canada are approached from this perspective.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Patterns of Linguistic Diffusion in Space and Time: The Case of Mazatec,"In the framework of complexity theory, which provides a unified framework for natural and social sciences, we study the complex and interesting problem of the internal structure, similarities, and differences between the Mazatec dialects, an endangered Otomanguean language spoken in south-east Mexico. The analysis is based on some databases, which are used to compute linguistic distances between the dialects. The results are interpreted in the light of linguistics as well as statistical considerations and used to infer the history of the development of the observed pattern of diversity.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
"Scientific elite revisited: Patterns of productivity, collaboration, authorship and impact","Throughout history, a relatively small number of individuals have made a profound and lasting impact on science and society. Despite long-standing, multi-disciplinary interests in understanding careers of elite scientists, there have been limited attempts for a quantitative, career-level analysis. Here, we leverage a comprehensive dataset we assembled, allowing us to trace the entire career histories of nearly all Nobel laureates in physics, chemistry, and physiology or medicine over the past century. We find that, although Nobel laureates were energetic producers from the outset, producing works that garner unusually high impact, their careers before winning the prize follow relatively similar patterns as ordinary scientists, being characterized by hot streaks and increasing reliance on collaborations. We also uncovered notable variations along their careers, often associated with the Nobel prize, including shifting coauthorship structure in the prize-winning work, and a significant but temporary dip in the impact of work they produce after winning the Nobel. Together, these results document quantitative patterns governing the careers of scientific elites, offering an empirical basis for a deeper understanding of the hallmarks of exceptional careers in science.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Diversity of reproduction time scale promotes cooperation in spatial prisoner's dilemma games,"We study an evolutionary spatial prisoner's dilemma game where the fitness of the players is determined by both the payoffs from the current interaction and their history. We consider the situation where the selection timescale is slower than the interaction timescale. This is done by implementing probabilistic reproduction on an individual level. We observe that both too fast and too slow reproduction rates hamper the emergence of cooperation. In other words, there exists an intermediate selection timescale that maximizes cooperation. Another factor we find to promote cooperation is a diversity of reproduction timescales.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Towards inferring reactor operations from high-level waste,"Nuclear archaeology research provides scientific methods to reconstruct the operating histories of fissile material production facilities to account for past fissile material production. While it has typically focused on analyzing material in permanent reactor structures, spent fuel or high-level waste also hold information about the reactor operation. In this computational study, we explore a Bayesian inference framework for reconstructing the operational history from measurements of isotope ratios from a sample of nuclear waste . We investigate two different inference models. The first model discriminates between three potential reactors of origin (Magnox, PWR, and PHWR) while simultaneously reconstructing the fuel burnup, time since irradiation, initial enrichment, and average power density. The second model reconstructs the fuel burnup and time since irradiation of two batches of waste in a mixed sample. Each of the models is applied to a set of simulated test data, and the performance is evaluated by comparing the highest posterior density regions to the corresponding parameter values of the test dataset. Both models perform well on the simulated test cases, which highlights the potential of the Bayesian inference framework and opens up avenues for further investigation","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
A County-level Dataset for Informing the United States' Response to COVID-19,"As the coronavirus disease 2019 (COVID-19) continues to be a global pandemic, policy makers have enacted and reversed non-pharmaceutical interventions with various levels of restrictions to limit its spread. Data driven approaches that analyze temporal characteristics of the pandemic and its dependence on regional conditions might supply information to support the implementation of mitigation and suppression strategies. To facilitate research in this direction on the example of the United States, we present a machine-readable dataset that aggregates relevant data from governmental, journalistic, and academic sources on the U.S. county level. In addition to county-level time-series data from the JHU CSSE COVID-19 Dashboard, our dataset contains more than 300 variables that summarize population estimates, demographics, ethnicity, housing, education, employment and income, climate, transit scores, and healthcare system-related metrics. Furthermore, we present aggregated out-of-home activity information for various points of interest for each county, including grocery stores and hospitals, summarizing data from SafeGraph and Google mobility reports. We compile information from IHME, state and county-level government, and newspapers for dates of the enactment and reversal of non-pharmaceutical interventions. By collecting these data, as well as providing tools to read them, we hope to accelerate research that investigates how the disease spreads and why spread may be different across regions. Our dataset and associated code are available at github.com/JieYingWu/COVID-19_US_County-level_Summaries.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
How to be an extremist,"We present a toy model of opinion spreading in a society which combines a self-reinforcing mechanism with diffusion. The relative strength of these two mechanisms - called the affectability of the system - is a free parameter of the model. The model is run on a scale-free network and its asymptotic behaviour is investigated. A surprising emergent effect is observed: If every individual becomes more attentive to the opinions of others, the society as a whole switches from a plurality of nuanced opinions into the state of an absolute consensus on an extreme opinion. This counter-intuitive emergent behaviour may help to explain certain paradoxes in human history.","cat:physics.soc-ph AND (history OR ""industrial revolution"" OR demographics)",0
Propositional computability logic I,"In the same sense as classical logic is a formal theory of truth, the recently initiated approach called computability logic is a formal theory of computability. It understands (interactive) computational problems as games played by a machine against the environment, their computability as existence of a machine that always wins the game, logical operators as operations on computational problems, and validity of a logical formula as being a scheme of ""always computable"" problems. The present contribution gives a detailed exposition of a soundness and completeness proof for an axiomatization of one of the most basic fragments of computability logic. The logical vocabulary of this fragment contains operators for the so called parallel and choice operations, and its atoms represent elementary problems, i.e. predicates in the standard sense. This article is self-contained as it explains all relevant concepts. While not technically necessary, however, familiarity with the foundational paper ""Introduction to computability logic"" [Annals of Pure and Applied Logic 123 (2003), pp.1-99] would greatly help the reader in understanding the philosophy, underlying motivations, potential and utility of computability logic, -- the context that determines the value of the present results. Online introduction to the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html and http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Quantitative Logic Reasoning,"In this paper we show several similarities among logic systems that deal simultaneously with deductive and quantitative inference. We claim it is appropriate to call the tasks those systems perform as Quantitative Logic Reasoning. Analogous properties hold throughout that class, for whose members there exists a set of linear algebraic techniques applicable in the study of satisfiability decision problems. In this presentation, we consider as Quantitative Logic Reasoning the tasks performed by propositional Probabilistic Logic; first-order logic with counting quantifiers over a fragment containing unary and limited binary predicates; and propositional Lukasiewicz Infinitely-valued Probabilistic Logic","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Beyond Logic Programming for Legal Reasoning,"Logic programming has long being advocated for legal reasoning, and several approaches have been put forward relying upon explicit representation of the law in logic programming terms. In this position paper we focus on the PROLEG logic-programming-based framework for formalizing and reasoning with Japanese presupposed ultimate fact theory. Specifically, we examine challenges and opportunities in leveraging deep learning techniques for improving legal reasoning using PROLEG identifying four distinct options ranging from enhancing fact extraction using deep learning to end-to-end solutions for reasoning with textual legal descriptions. We assess advantages and limitations of each option, considering their technical feasibility, interpretability, and alignment with the needs of legal practitioners and decision-makers. We believe that our analysis can serve as a guideline for developers aiming to build effective decision-support systems for the legal domain, while fostering a deeper understanding of challenges and potential advancements by neuro-symbolic approaches in legal applications.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Modal Extensions of ukasiewicz Logic for Modeling Coalitional Power,"Modal logics for reasoning about the power of coalitions capture the notion of effectivity functions associated with game forms. The main goal of coalition logics is to provide formal tools for modeling the dynamics of a game frame whose states may correspond to different game forms. The two classes of effectivity functions studied are the families of playable and truly playable effectivity functions, respectively. In this paper we generalize the concept of effectivity function beyond the yes/no truth scale. This enables us to describe the situations in which the coalitions assess their effectivity in degrees, based on functions over the outcomes taking values in a finite ukasiewicz chain. Then we introduce two modal extensions of ukasiewicz finite-valued logic together with many-valued neighborhood semantics in order to encode the properties of many-valued effectivity functions associated with game forms. As our main results we prove completeness theorems for the two newly introduced modal logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Towards applied theories based on computability logic,"Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a recently launched program for redeveloping logic as a formal theory of computability, as opposed to the formal theory of truth that logic has more traditionally been. Formulas in it represent computational problems, ""truth"" means existence of an algorithmic solution, and proofs encode such solutions. Within the line of research devoted to finding axiomatizations for ever more expressive fragments of CL, the present paper introduces a new deductive system CL12 and proves its soundness and completeness with respect to the semantics of CL. Conservatively extending classical predicate calculus and offering considerable additional expressive and deductive power, CL12 presents a reasonable, computationally meaningful, constructive alternative to classical logic as a basis for applied theories. To obtain a model example of such theories, this paper rebuilds the traditional, classical-logic-based Peano arithmetic into a computability-logic-based counterpart. Among the purposes of the present contribution is to provide a starting point for what, as the author wishes to hope, might become a new line of research with a potential of interesting findings -- an exploration of the presumably quite unusual metatheory of CL-based arithmetic and other CL-based applied systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Neutral Temporal Deontic STIT Logic,"In this work we answer a long standing request for temporal embeddings of deontic STIT logics by introducing the multi-agent STIT logic TDS. The logic is based upon atemporal utilitarian STIT logic. Yet, the logic presented here will be neutral: instead of committing ourselves to utilitarian theories, we prove the logic TDS sound and complete with respect to relational frames not employing any utilitarian function. We demonstrate how these neutral frames can be transformed into utilitarian temporal frames, while preserving validity. Last, we discuss problems that arise from employing binary utility functions in a temporal setting.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
From Propositional Logic to Plausible Reasoning: A Uniqueness Theorem,"We consider the question of extending propositional logic to a logic of plausible reasoning, and posit four requirements that any such extension should satisfy. Each is a requirement that some property of classical propositional logic be preserved in the extended logic; as such, the requirements are simpler and less problematic than those used in Cox's Theorem and its variants. As with Cox's Theorem, our requirements imply that the extended logic must be isomorphic to (finite-set) probability theory. We also obtain specific numerical values for the probabilities, recovering the classical definition of probability as a theorem, with truth assignments that satisfy the premise playing the role of the ""possible cases.""","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Trichotomy and Dichotomy Results on the Complexity of Reasoning with Disjunctive Logic Programs,"We present trichotomy results characterizing the complexity of reasoning with disjunctive logic programs. To this end, we introduce a certain definition schema for classes of programs based on a set of allowed arities of rules. We show that each such class of programs has a finite representation, and for each of the classes definable in the schema we characterize the complexity of the existence of an answer set problem. Next, we derive similar characterizations of the complexity of skeptical and credulous reasoning with disjunctive logic programs. Such results are of potential interest. On the one hand, they reveal some reasons responsible for the hardness of computing answer sets. On the other hand, they identify classes of problem instances, for which the problem is ""easy"" (in P) or ""easier than in general"" (in NP). We obtain similar results for the complexity of reasoning with disjunctive programs under the supported-model semantics. To appear in Theory and Practice of Logic Programming (TPLP)","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Topological Modal Logics with Difference Modality,"We consider propositional modal logic with two modal operators $\Box$ and $\D$. In topological semantics $\Box$ is interpreted as an interior operator and $\D$ as difference. We show that some important topological properties are expressible in this language. In addition, we present a few logics and proofs of f.m.p. and of completeness theorems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The complexity of admissible rules of ukasiewicz logic,"We investigate the computational complexity of admissibility of inference rules in infinite-valued ukasiewicz propositional logic (). It was shown in [13] that admissibility in  is checkable in PSPACE. We establish that this result is optimal, i.e., admissible rules of  are PSPACE-complete. In contrast, derivable rules of  are known to be coNP-complete.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Embedding Defeasible Logic into Logic Programming,"Defeasible reasoning is a simple but efficient approach to nonmonotonic reasoning that has recently attracted considerable interest and that has found various applications. Defeasible logic and its variants are an important family of defeasible reasoning methods. So far no relationship has been established between defeasible logic and mainstream nonmonotonic reasoning approaches.   In this paper we establish close links to known semantics of logic programs. In particular, we give a translation of a defeasible theory D into a meta-program P(D). We show that under a condition of decisiveness, the defeasible consequences of D correspond exactly to the sceptical conclusions of P(D) under the stable model semantics. Without decisiveness, the result holds only in one direction (all defeasible consequences of D are included in all stable models of P(D)). If we wish a complete embedding for the general case, we need to use the Kunen semantics of P(D), instead.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Game-Theoretic Semantics for Alternating-Time Temporal Logic,"We introduce versions of game-theoretic semantics (GTS) for Alternating-Time Temporal Logic (ATL). In GTS, truth is defined in terms of existence of a winning strategy in a semantic evaluation game, and thus the game-theoretic perspective appears in the framework of ATL on two semantic levels: on the object level in the standard semantics of the strategic operators, and on the meta-level where game-theoretic logical semantics is applied to ATL. We unify these two perspectives into semantic evaluation games specially designed for ATL. The game-theoretic perspective enables us to identify new variants of the semantics of ATL based on limiting the time resources available to the verifier and falsifier in the semantic evaluation game. We introduce and analyse an unbounded and (ordinal) bounded GTS and prove these to be equivalent to the standard (Tarski-style) compositional semantics. We show that in these both versions of GTS, truth of ATL formulae can always be determined in finite time, i.e., without constructing infinite paths. We also introduce a non-equivalent finitely bounded semantics and argue that it is natural from both logical and game-theoretic perspectives.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
From Intuitionism to Many-Valued Logics through Kripke Models,"Intuitionistic Propositional Logic is proved to be an infinitely many valued logic by Kurt Gdel (1932), and it is proved by Stanisaw Jakowski (1936) to be a countably many valued logic. In this paper, we provide alternative proofs for these theorems by using models of Saul Kripke (1959). Gdel's proof gave rise to an intermediate propositional logic (between intuitionistic and classical), that is known nowadays as Gdel or the Gdel-Dummet Logic, and is studied by fuzzy logicians as well. We also provide some results on the inter-definablility of propositional connectives in this logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Disjunctive Logic Programs with Inheritance,"The paper proposes a new knowledge representation language, called DLP<, which extends disjunctive logic programming (with strong negation) by inheritance. The addition of inheritance enhances the knowledge modeling features of the language providing a natural representation of default reasoning with exceptions.   A declarative model-theoretic semantics of DLP< is provided, which is shown to generalize the Answer Set Semantics of disjunctive logic programs.   The knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in DLP<.   The complexity of DLP< is analyzed, proving that inheritance does not cause any computational overhead, as reasoning in DLP< has exactly the same complexity as reasoning in disjunctive logic programming. This is confirmed by the existence of an efficient translation from DLP< to plain disjunctive logic programming. Using this translation, an advanced KR system supporting the DLP< language has been implemented on top of the DLV system and has subsequently been integrated into DLV.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Logic for Conditional Local Strategic Reasoning,"We consider systems of rational agents who act and interact in pursuit of their individual and collective objectives. We study and formalise the reasoning of an agent, or of an external observer, about the expected choices of action of the other agents based on their objectives, in order to assess the reasoner's ability, or expectation, to achieve their own objective.   To formalize such reasoning we extend Pauly's Coalition Logic with three new modal operators of conditional strategic reasoning, thus introducing the Logic for Local Conditional Strategic Reasoning ConStR. We provide formal semantics for the new conditional strategic operators in concurrent game models, introduce the matching notion of bisimulation for each of them, prove bisimulation invariance and Hennessy-Milner property for each of them, and discuss and compare briefly their expressiveness. Finally, we also propose systems of axioms for each of the basic operators of ConStR and for the full logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The prospects for mathematical logic in the twenty-first century,"The four authors present their speculations about the future developments of mathematical logic in the twenty-first century. The areas of recursion theory, proof theory and logic for computer science, model theory, and set theory are discussed independently.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Exploiting Game Theory for Analysing Justifications,"Justification theory is a unifying semantic framework. While it has its roots in non-monotonic logics, it can be applied to various areas in computer science, especially in explainable reasoning; its most central concept is a justification: an explanation why a property holds (or does not hold) in a model. In this paper, we continue the study of justification theory by means of three major contributions. The first is studying the relation between justification theory and game theory. We show that justification frameworks can be seen as a special type of games. The established connection provides the theoretical foundations for our next two contributions. The second contribution is studying under which condition two different dialects of justification theory (graphs as explanations vs trees as explanations) coincide. The third contribution is establishing a precise criterion of when a semantics induced by justification theory yields consistent results. In the past proving that such semantics were consistent took cumbersome and elaborate proofs. We show that these criteria are indeed satisfied for all common semantics of logic programming. This paper is under consideration for acceptance in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Similarity-based Reasoning in Qualified Logic Programming,"Similarity-based Logic Programming (briefly, SLP ) has been proposed to enhance the LP paradigm with a kind of approximate reasoning which supports flexible information retrieval applications. This approach uses a fuzzy similarity relation R between symbols in the program's signature, while keeping the syntax for program clauses as in classical LP. Another recent proposal is the QLP(D) scheme for Qualified Logic Programming, an extension of the LP paradigm which supports approximate reasoning and more. This approach uses annotated program clauses and a parametrically given domain D whose elements qualify logical assertions by measuring their closeness to various users' expectations. In this paper we propose a more expressive scheme SQLP(R,D) which subsumes both SLP and QLP(D) as particular cases. We also show that SQLP(R,D) programs can be transformed into semantically equivalent QLP(D) programs. As a consequence, existing QLP(D) implementations can be used to give efficient support for similarity-based reasoning.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Annotated Defeasible Logic,"Defeasible logics provide several linguistic features to support the expression of defeasible knowledge. There is also a wide variety of such logics, expressing different intuitions about defeasible reasoning. However, the logics can only combine in trivial ways. This limits their usefulness in contexts where different intuitions are at play in different aspects of a problem. In particular, in some legal settings, different actors have different burdens of proof, which might be expressed as reasoning in different defeasible logics.   In this paper, we introduce annotated defeasible logic as a flexible formalism permitting multiple forms of defeasibility, and establish some properties of the formalism.   This paper is under consideration for acceptance in Theory and Practice of Logic Programming.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logic Programming for Finding Models in the Logics of Knowledge and its Applications: A Case Study,"The logics of knowledge are modal logics that have been shown to be effective in representing and reasoning about knowledge in multi-agent domains. Relatively few computational frameworks for dealing with computation of models and useful transformations in logics of knowledge (e.g., to support multi-agent planning with knowledge actions and degrees of visibility) have been proposed. This paper explores the use of logic programming (LP) to encode interesting forms of logics of knowledge and compute Kripke models. The LP modeling is expanded with useful operators on Kripke structures, to support multi-agent planning in the presence of both world-altering and knowledge actions. This results in the first ever implementation of a planner for this type of complex multi-agent domains.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Exploring the Jungle of Intuitionistic Temporal Logics,"The importance of intuitionistic temporal logics in Computer Science and Artificial Intelligence has become increasingly clear in the last few years. From the proof-theory point of view, intuitionistic temporal logics have made it possible to extend functional languages with new features via type theory, while from its semantical perspective several logics for reasoning about dynamical systems and several semantics for logic programming have their roots in this framework. In this paper we consider several axiomatic systems for intuitionistic linear temporal logic and show that each of these systems is sound for a class of structures based either on Kripke frames or on dynamic topological systems. Our topological semantics features a new interpretation for the `henceforth' modality that is a natural intuitionistic variant of the classical one. Using the soundness results, we show that the seven logics obtained from the axiomatic systems are distinct.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Sequent-Type Proof Systems for Three-Valued Default Logic,"Sequent-type proof systems constitute an important and widely-used class of calculi well-suited for analysing proof search. In my master's thesis, I introduce sequent-type calculi for a variant of default logic employing \Lukasiewicz's three-valued logic as the underlying base logic. This version of default logic has been introduced by Radzikowska addressing some representational shortcomings of standard default logic. More specifically, the calculi discussed in my thesis axiomatise brave and skeptical reasoning for this version of default logic, respectively following the sequent method first introduced in the context of nonmonotonic reasoning by Bonatti and Olivetti, which employ a complementary calculus for axiomatising invalid formulas, taking care of expressing the consistency condition of defaults.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Inquisitive Logic as an Epistemic Logic of Knowing How,"In this paper, we present an alternative interpretation of propositional inquisitive logic as an epistemic logic of knowing how. In our setting, an inquisitive logic formula $$ being supported by a state is formalized as ""knowing how to resolve $$"" (more colloquially, ""knowing how $$ is true"") holds on the S5 epistemic model corresponding to the state. Based on this epistemic interpretation, we use a dynamic epistemic logic with both know-how and know-that operators to capture the epistemic information behind the innocent-looking connectives in inquisitive logic. We show that the set of valid know-how formulas corresponds precisely to the inquisitive logic. The main result is a complete axiomatization with intuitive axioms using the full dynamic epistemic language. Moreover, we show that the know-how operator and the dynamic operator can both be eliminated without changing the expressivity over models, which is consistent with the modal translation of inquisitive logic existing in the literature. We hope our framework can give an intuitive alternative interpretation of various concepts and technical results in inquisitive logic, and also provide a powerful and flexible tool to do inquisitive reasoning in an epistemic context.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Qualitative Theory of Cognitive Attitudes and their Change,"We present a general logical framework for reasoning about agents' cognitive attitudes of both epistemic type and motivational type. We show that it allows us to express a variety of relevant concepts for qualitative decision theory including the concepts of knowledge, belief, strong belief, conditional belief, desire, conditional desire, strong desire and preference. We also present two extensions of the logic, one by the notion of choice and the other by dynamic operators for belief change and desire change, and we apply the former to the analysis of single-stage games under incomplete information. We provide sound and complete axiomatizations for the basic logic and for its two extensions. The paper is under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Category theory, logic and formal linguistics: some connections, old and new","We seize the opportunity of the publication of selected papers from the \emph{Logic, categories, semantics} workshop in the \emph{Journal of Applied Logic} to survey some current trends in logic, namely intuitionistic and linear type theories, that interweave categorical, geometrical and computational considerations. We thereafter present how these rich logical frameworks can model the way language conveys meaning.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Papers presented at the 32nd International Conference on Logic Programming (ICLP 2016),"This is the list of the full papers accepted for presentation at the 32nd International Conference on Logic Programming, New York City, USA, October 18-21, 2016.   In addition to the main conference itself, ICLP hosted four pre-conference workshops, the Autumn School on Logic Programing, and a Doctoral Consortium.   The final versions of the full papers will be published in a special issue of the journal Theory and Practice of Logic Programming (TPLP). We received eighty eight abstract submissions, of which twenty seven papers were accepted for publication as TPLP rapid communications.   Papers deemed of sufficiently high quality to be presented as the conference, but not enough to be appear in TPLP, will be published as Technical Communications in the OASIcs series. Fifteen papers fell into this category.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Optimizing Probabilities in Probabilistic Logic Programs,"Probabilistic Logic Programming is an effective formalism for encoding problems characterized by uncertainty. Some of these problems may require the optimization of probability values subject to constraints among probability distributions of random variables. Here, we introduce a new class of probabilistic logic programs, namely Probabilistic Optimizable Logic Programs, and we provide an effective algorithm to find the best assignment to probabilities of random variables, such that a set of constraints is satisfied and an objective function is optimized. This paper is under consideration for acceptance in Theory and Practice of Logic Programming.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Splitting Epistemic Logic Programs,"Epistemic logic programs constitute an extension of the stable models semantics to deal with new constructs called subjective literals. Informally speaking, a subjective literal allows checking whether some regular literal is true in all stable models or in some stable model. As it can be imagined, the associated semantics has proved to be non-trivial, as the truth of the subjective literal may interfere with the set of stable models it is supposed to query. As a consequence, no clear agreement has been reached and different semantic proposals have been made in the literature. Unfortunately, comparison among these proposals has been limited to a study of their effect on individual examples, rather than identifying general properties to be checked. In this paper, we propose an extension of the well-known splitting property for logic programs to the epistemic case. To this aim, we formally define when an arbitrary semantics satisfies the epistemic splitting property and examine some of the consequences that can be derived from that, including its relation to conformant planning and to epistemic constraints. Interestingly, we prove (through counterexamples) that most of the existing proposals fail to fulfill the epistemic splitting property, except the original semantics proposed by Gelfond in 1991.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Inference and learning in probabilistic logic programs using weighted Boolean formulas,"Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. This paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs. Several such tasks such as computing the marginals given evidence and learning from (partial) interpretations have not really been addressed for probabilistic logic programs before.   The first contribution of this paper is a suite of efficient algorithms for various inference tasks. It is based on a conversion of the program and the queries and evidence to a weighted Boolean formula. This allows us to reduce the inference tasks to well-studied tasks such as weighted model counting, which can be solved using state-of-the-art methods known from the graphical model and knowledge compilation literature. The second contribution is an algorithm for parameter estimation in the learning from interpretations setting. The algorithm employs Expectation Maximization, and is built on top of the developed inference algorithms.   The proposed approach is experimentally evaluated. The results show that the inference algorithms improve upon the state-of-the-art in probabilistic logic programming and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Why teach an introductory course in Mathematical Logic in the Philosophy curriculum?,This paper tries to justify the relevance of an introductory course in Mathematical Logic in the Philosophy curriculum for analyzing philosophical arguments in natural language. It is argued that the representation of the structure of natural language arguments in Freeman's diagramming system can provide an intuitive foundation for the inferential processes involved in the use of First Order Logic natural deduction rules.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A two-level logic approach to reasoning about computations,"Relational descriptions have been used in formalizing diverse computational notions, including, for example, operational semantics, typing, and acceptance by non-deterministic machines. We therefore propose a (restricted) logical theory over relations as a language for specifying such notions. Our specification logic is further characterized by an ability to explicitly treat binding in object languages. Once such a logic is fixed, a natural next question is how we might prove theorems about specifications written in it. We propose to use a second logic, called a reasoning logic, for this purpose. A satisfactory reasoning logic should be able to completely encode the specification logic. Associated with the specification logic are various notions of binding: for quantifiers within formulas, for eigenvariables within sequents, and for abstractions within terms. To provide a natural treatment of these aspects, the reasoning logic must encode binding structures as well as their associated notions of scope, free and bound variables, and capture-avoiding substitution. Further, to support arguments about provability, the reasoning logic should possess strong mechanisms for constructing proofs by induction and co-induction. We provide these capabilities here by using a logic called G which represents relations over lambda-terms via definitions of atomic judgments, contains inference rules for induction and co-induction, and includes a special generic quantifier. We show how provability in the specification logic can be transparently encoded in G. We also describe an interactive theorem prover called Abella that implements G and this two-level logic approach and we present several examples that demonstrate the efficacy of Abella in reasoning about computations.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A conventional expansion of first-order Belnap-Dunn logic,"This paper concerns an expansion of first-order Belnap-Dunn logic whose connectives and quantifiers all have a counterpart in classical logic. The language and logical consequence relation of this logic are defined, a proof system for this logic is presented, and the soundness and completeness of this proof system is established. The minor differences between the presented proof system for the defined logic and a sound and complete proof system for the version of classical logic with the same language illustrates the close relationship between the logical consequence relations of these logics. A clear characterization of the classical nature of the connectives and quantifiers of the defined logic is given by means of classical laws of logical equivalence. Moreover, a simple embedding of this logic in classical logic is presented and the potential of the logic for dealing with inconsistencies and incompletenesses in inductive machine learning is discussed.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Third Life of Quantum Logic: Quantum Logic Inspired by Quantum Computing,"We begin by discussing the history of quantum logic, dividing it into three eras or lives. The first life has to do with Birkhoff and von Neumann's algebraic approach in the 1930's. The second life has to do with attempt to understand quantum logic as logic that began in the late 1950's and blossomed in the 1970's. And the third life has to do with recent developments in quantum logic coming from its connections to quantum computation. We discuss our own work connecting quantum logic to quantum computation (viewing quantum logic as the logic of quantum registers storing qubits), and make some speculations about mathematics based on quantum principles.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Deterministic modal Bayesian Logic: derive the Bayesian inference within the modal logic T,"In this paper a conditional logic is defined and studied. This conditional logic, DmBL, is constructed as a deterministic counterpart to the Bayesian conditional. The logic is unrestricted, so that any logical operations are allowed. A notion of logical independence is also defined within the logic itself. This logic is shown to be non-trivial and is not reduced to classical propositions. A model is constructed for the logic. Completeness results are proved. It is shown that any unconditioned probability can be extended to the whole logic DmBL. The Bayesian conditional is then recovered from the probabilistic DmBL. At last, it is shown why DmBL is compliant with Lewis' triviality.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Magic of Logical Inference in Probabilistic Programming,"Today, many different probabilistic programming languages exist and even more inference mechanisms for these languages. Still, most logic programming based languages use backward reasoning based on SLD resolution for inference. While these methods are typically computationally efficient, they often can neither handle infinite and/or continuous distributions, nor evidence. To overcome these limitations, we introduce distributional clauses, a variation and extension of Sato's distribution semantics. We also contribute a novel approximate inference method that integrates forward reasoning with importance sampling, a well-known technique for probabilistic inference. To achieve efficiency, we integrate two logic programming techniques to direct forward sampling. Magic sets are used to focus on relevant parts of the program, while the integration of backward reasoning allows one to identify and avoid regions of the sample space that are inconsistent with the evidence.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Tight Logic Programs,"This note is about the relationship between two theories of negation as failure -- one based on program completion, the other based on stable models, or answer sets. Francois Fages showed that if a logic program satisfies a certain syntactic condition, which is now called ``tightness,'' then its stable models can be characterized as the models of its completion. We extend the definition of tightness and Fages' theorem to programs with nested expressions in the bodies of rules, and study tight logic programs containing the definition of the transitive closure of a predicate.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Simple Logic of Functional Dependence,"This paper presents a simple decidable logic of functional dependence LFD, based on an extension of classical propositional logic with dependence atoms plus dependence quantifiers treated as modalities, within the setting of generalized assignment semantics for first order logic. The expressive strength, complete proof calculus and meta-properties of LFD are explored. Various language extensions are presented as well, up to undecidable modal-style logics for independence and dynamic logics of changing dependence models. Finally, more concrete settings for dependence are discussed: continuous dependence in topological models, linear dependence in vector spaces, and temporal dependence in dynamical systems and games.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On the Implementation of the Probabilistic Logic Programming Language ProbLog,"The past few years have seen a surge of interest in the field of probabilistic logic learning and statistical relational learning. In this endeavor, many probabilistic logics have been developed. ProbLog is a recent probabilistic extension of Prolog motivated by the mining of large biological networks. In ProbLog, facts can be labeled with probabilities. These facts are treated as mutually independent random variables that indicate whether these facts belong to a randomly sampled program. Different kinds of queries can be posed to ProbLog programs. We introduce algorithms that allow the efficient execution of these queries, discuss their implementation on top of the YAP-Prolog system, and evaluate their performance in the context of large networks of biological entities.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Specifying a Game-Theoretic Extensive Form as an Abstract 5-ary Relation,"This paper specifies an extensive form as a 5-ary relation (that is, as a set of quintuples) which satisfies eight abstract axioms. Each quintuple is understood to list a player, a situation (that is, a name for an information set), a decision node, an action, and a successor node. Accordingly, the axioms are understood to specify abstract relationships between players, situations, nodes, and actions. Such an extensive form is called a ""pentaform"". Finally, a ""pentaform game"" is defined to be a pentaform together with utility functions.   To ground this new specification in the literature, the paper defines the concept of a ""traditional game"" to represent the literature's many specifications of finite-horizon and infinite-horizon games. The paper's main result is to construct an intuitive bijection between pentaform games and traditional games. Secondary results concern disaggregating pentaforms by subsets, constructing pentaforms by unions, and initial pentaform applications to Selten subgames and perfect-recall (an extensive application to dynamic programming is in Streufert 2023, arXiv:2302.03855).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Logic Framework for P2P Deductive Databases,"This paper presents a logic framework for modeling the interaction among deductive databases in a P2P (Peer to Peer) environment. Each peer joining a P2P system provides or imports data from its neighbors by using a set of mapping rules, i.e. a set of semantic correspondences to a set of peers belonging to the same environment. Two different types of mapping rules are defined: mapping rules allowing to import a maximal set of atoms not leading to inconsistency (called maximal mapping rules) and mapping rules allowing to import a minimal set of atoms needed to restore consistency (called minimal mapping rules). Implicitly, the use of maximal mapping rules states it is preferable to import as long as no inconsistencies arise; whereas the use of minimal mapping rules states that it is preferable not to import unless a inconsistency exists. The paper presents three different declarative semantics of a P2P system: (i) the Max Weak Model Semantics, in which mapping rules are used to import as much knowledge as possible} from a peer's neighborhood without violating local integrity constraints; (ii) the Min Weak Model Semantics, in which the P2P system can be locally inconsistent and the information provided by the neighbors is used to restore consistency, that is to only integrate the missing portion of a correct, but incomplete database; (iii) the Max-Min Weak Model Semantics that unifies the previous two different perspectives captured by the Max Weak Model Semantics and Min Weak Model Semantics. This last semantics allows to characterize each peer in the neighborhood as a resource used either to enrich (integrate) or to fix (repair) the knowledge, so as to define a kind of integrate-repair strategy for each peer.   Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Geometry of Reasoning: Flowing Logics in Representation Space,"We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structure from semantics by employing the same natural deduction propositions with varied semantic carriers, allowing us to test whether LLMs internalize logic beyond surface form. This perspective connects reasoning with geometric quantities such as position, velocity, and curvature, enabling formal analysis in representation and concept spaces. Our theory establishes: (1) LLM reasoning corresponds to smooth flows in representation space, and (2) logical statements act as local controllers of these flows' velocities. Using learned representation proxies, we design controlled experiments to visualize and quantify reasoning flows, providing empirical validation of our theoretical framework. Our work serves as both a conceptual foundation and practical tools for studying reasoning phenomenon, offering a new lens for interpretability and formal analysis of LLMs' behavior.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The logic of contextuality,"Contextuality is a key signature of quantum non-classicality, which has been shown to play a central role in enabling quantum advantage for a wide range of information-processing and computational tasks. We study the logic of contextuality from a structural point of view, in the setting of partial Boolean algebras introduced by Kochen and Specker in their seminal work. These contrast with traditional quantum logic  la Birkhoff and von Neumann in that operations such as conjunction and disjunction are partial, only being defined in the domain where they are physically meaningful.   We study how this setting relates to current work on contextuality such as the sheaf-theoretic and graph-theoretic approaches. We introduce a general free construction extending the commeasurability relation on a partial Boolean algebra, i.e. the domain of definition of the binary logical operations. This construction has a surprisingly broad range of uses. We apply it in the study of a number of issues, including:   - establishing the connection between the abstract measurement scenarios studied in the contextuality literature and the setting of partial Boolean algebras;   - formulating various contextuality properties in this setting, including probabilistic contextuality as well as the strong, state-independent notion of contextuality given by Kochen-Specker paradoxes, which are logically contradictory statements validated by partial Boolean algebras, specifically those arising from quantum mechanics;   - investigating a Logical Exclusivity Principle, and its relation to the Probabilistic Exclusivity Principle widely studied in recent work on contextuality as a step towards closing in on the set of quantum-realisable correlations;   - developing some work towards a logical presentation of the Hilbert space tensor product, using logical exclusivity to capture some of its salient quantum features.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Note on the Complexity of the Satisfiability Problem for Graded Modal Logics,"Graded modal logic is the formal language obtained from ordinary (propositional) modal logic by endowing its modal operators with cardinality constraints. Under the familiar possible-worlds semantics, these augmented modal operators receive interpretations such as ""It is true at no fewer than 15 accessible worlds that..."", or ""It is true at no more than 2 accessible worlds that..."". We investigate the complexity of satisfiability for this language over some familiar classes of frames. This problem is more challenging than its ordinary modal logic counterpart--especially in the case of transitive frames, where graded modal logic lacks the tree-model property. We obtain tight complexity bounds for the problem of determining the satisfiability of a given graded modal logic formula over the classes of frames characterized by any combination of reflexivity, seriality, symmetry, transitivity and the Euclidean property.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Predicate Logic as a Modeling Language: Modeling and Solving some Machine Learning and Data Mining Problems with IDP3,"This paper provides a gentle introduction to problem solving with the IDP3 system. The core of IDP3 is a finite model generator that supports first order logic enriched with types, inductive definitions, aggregates and partial functions. It offers its users a modeling language that is a slight extension of predicate logic and allows them to solve a wide range of search problems. Apart from a small introductory example, applications are selected from problems that arose within machine learning and data mining research. These research areas have recently shown a strong interest in declarative modeling and constraint solving as opposed to algorithmic approaches. The paper illustrates that the IDP3 system can be a valuable tool for researchers with such an interest.   The first problem is in the domain of stemmatology, a domain of philology concerned with the relationship between surviving variant versions of text. The second problem is about a somewhat related problem within biology where phylogenetic trees are used to represent the evolution of species. The third and final problem concerns the classical problem of learning a minimal automaton consistent with a given set of strings. For this last problem, we show that the performance of our solution comes very close to that of a state-of-the art solution. For each of these applications, we analyze the problem, illustrate the development of a logic-based model and explore how alternatives can affect the performance.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logics for complexity classes,"A new syntactic characterization of problems complete via Turing reductions is presented. General canonical forms are developed in order to define such problems. One of these forms allows us to define complete problems on ordered structures, and another form to define them on unordered non-Aristotelian structures. Using the canonical forms, logics are developed for complete problems in various complexity classes. Evidence is shown that there cannot be any complete problem on Aristotelian structures for several complexity classes. Our approach is extended beyond complete problems. Using a similar form, a logic is developed to capture the complexity class $NP\cap coNP$ which very likely contains no complete problem.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reasoning about Medical Triage Optimization with Logic Programming,"We present a logic programming framework that orchestrates multiple variants of an optimization problem and reasons about their results to support high-stakes medical decision-making. The logic programming layer coordinates the construction and evaluation of multiple optimization formulations, translating solutions into logical facts that support further symbolic reasoning and ensure efficient resource allocation-specifically targeting the ""right patient, right platform, right escort, right time, right destination"" principle. This capability is integrated into GuardianTwin, a decision support system for Forward Medical Evacuation (MEDEVAC), where rapid and explainable resource allocation is critical. Through a series of experiments, our framework demonstrates an average reduction in casualties by 35.75 % compared to standard baselines. Additionally, we explore how users engage with the system via an intuitive interface that delivers explainable insights, ultimately enhancing decision-making in critical situations. This work demonstrates how logic programming can serve as a foundation for modular, interpretable, and operationally effective optimization in mission-critical domains.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Modal logic, fundamentally","Non-classical generalizations of classical modal logic have been developed in the contexts of constructive mathematics and natural language semantics. In this paper, we discuss a general approach to the semantics of non-classical modal logics via algebraic representation theorems. We begin with complete lattices $L$ equipped with an antitone operation $\neg$ sending $1$ to $0$, a completely multiplicative operation $\Box$, and a completely additive operation $\Diamond$. Such lattice expansions can be represented by means of a set $X$ together with binary relations $\vartriangleleft$, $R$, and $Q$, satisfying some first-order conditions, used to represent $(L,\neg)$, $\Box$, and $\Diamond$, respectively. Indeed, any lattice $L$ equipped with such a $\neg$, a multiplicative $\Box$, and an additive $\Diamond$ embeds into the lattice of propositions of a frame $(X,\vartriangleleft,R,Q)$. Building on our recent study of ""fundamental logic"", we focus on the case where $\neg$ is dually self-adjoint ($a\leq \neg b$ implies $b\leq\neg a$) and $\Diamond \neg a\leq\neg\Box a$. In this case, the representations can be constrained so that $R=Q$, i.e., we need only add a single relation to $(X,\vartriangleleft)$ to represent both $\Box$ and $\Diamond$. Using these results, we prove that a system of fundamental modal logic is sound and complete with respect to an elementary class of bi-relational structures $(X,\vartriangleleft, R)$.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Founded (Auto)Epistemic Equilibrium Logic Satisfies Epistemic Splitting,"In a recent line of research, two familiar concepts from logic programming semantics (unfounded sets and splitting) were extrapolated to the case of epistemic logic programs. The property of epistemic splitting provides a natural and modular way to understand programs without epistemic cycles but, surprisingly, was only fulfilled by Gelfond's original semantics (G91), among the many proposals in the literature. On the other hand, G91 may suffer from a kind of self-supported, unfounded derivations when epistemic cycles come into play. Recently, the absence of these derivations was also formalised as a property of epistemic semantics called foundedness. Moreover, a first semantics proved to satisfy foundedness was also proposed, the so-called Founded Autoepistemic Equilibrium Logic (FAEEL). In this paper, we prove that FAEEL also satisfies the epistemic splitting property something that, together with foundedness, was not fulfilled by any other approach up to date. To prove this result, we provide an alternative characterisation of FAEEL as a combination of G91 with a simpler logic we called Founded Epistemic Equilibrium Logic (FEEL), which is somehow an extrapolation of the stable model semantics to the modal logic S5. Under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Counterfactuals Modulo Temporal Logics,"Lewis' theory of counterfactuals is the foundation of many contemporary notions of causality. In this paper, we extend this theory in the temporal direction to enable symbolic counterfactual reasoning on infinite sequences, such as counterexamples found by a model checker and trajectories produced by a reinforcement learning agent. In particular, our extension considers a more relaxed notion of similarity between worlds and proposes two additional counterfactual operators that close a semantic gap between the previous two in this more general setting. Further, we consider versions of counterfactuals that minimize the distance to the witnessing counterfactual worlds, a common requirement in causal analysis. To automate counterfactual reasoning in the temporal domain, we introduce a logic that combines temporal and counterfactual operators, and outline decision procedures for the satisfiability and trace-checking problems of this logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
How well do SOTA legal reasoning models support abductive reasoning?,"We examine how well the state-of-the-art (SOTA) models used in legal reasoning support abductive reasoning tasks. Abductive reasoning is a form of logical inference in which a hypothesis is formulated from a set of observations, and that hypothesis is used to explain the observations. The ability to formulate such hypotheses is important for lawyers and legal scholars as it helps them articulate logical arguments, interpret laws, and develop legal theories. Our motivation is to consider the belief that deep learning models, especially large language models (LLMs), will soon replace lawyers because they perform well on tasks related to legal text processing. But to do so, we believe, requires some form of abductive hypothesis formation. In other words, while LLMs become more popular and powerful, we want to investigate their capacity for abductive reasoning. To pursue this goal, we start by building a logic-augmented dataset for abductive reasoning with 498,697 samples and then use it to evaluate the performance of a SOTA model in the legal field. Our experimental results show that although these models can perform well on tasks related to some aspects of legal text processing, they still fall short in supporting abductive reasoning tasks.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Dynamic Cantor Derivative Logic,"Topological semantics for modal logic based on the Cantor derivative operator gives rise to derivative logics, also referred to as $d$-logics. Unlike logics based on the topological closure operator, $d$-logics have not previously been studied in the framework of dynamical systems, which are pairs $(X,f)$ consisting of a topological space $X$ equipped with a continuous function $f\colon X\to X$. We introduce the logics $\bf{wK4C}$, $\bf{K4C}$ and $\bf{GLC}$ and show that they all have the finite Kripke model property and are sound and complete with respect to the $d$-semantics in this dynamical setting. In particular, we prove that $\bf{wK4C}$ is the $d$-logic of all dynamic topological systems, $\bf{K4C}$ is the $d$-logic of all $T_D$ dynamic topological systems, and $\bf{GLC}$ is the $d$-logic of all dynamic topological systems based on a scattered space. We also prove a general result for the case where $f$ is a homeomorphism, which in particular yields soundness and completeness for the corresponding systems $\bf{wK4H}$, $\bf{K4H}$ and $\bf{GLH}$. The main contribution of this work is the foundation of a general proof method for finite model property and completeness of dynamic topological $d$-logics. Furthermore, our result for $\bf{GLC}$ constitutes the first step towards a proof of completeness for the trimodal topo-temporal language with respect to a finite axiomatisation -- something known to be impossible over the class of all spaces.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Deterministic modal Bayesian Logic: derive the Bayesian within the modal logic T,"In this paper a conditional logic is defined and studied. This conditional logic, DmBL, is constructed as close as possible to the Bayesian and is unrestricted, that is one is able to use any operator without restriction. A notion of logical independence is also defined within the logic itself. This logic is shown to be non trivial and is not reduced to classical propositions. A model is constructed for the logic. Completeness results are proved. It is shown that any unconditioned probability can be extended to the whole logic DmBL. The Bayesian is then recovered from the probabilistic DmBL. At last, it is shown why DmBL is compliant with Lewis triviality.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Syntactic Interpolation for Tense Logics and Bi-Intuitionistic Logic via Nested Sequents,"We provide a direct method for proving Craig interpolation for a range of modal and intuitionistic logics, including those containing a ""converse"" modality. We demonstrate this method for classical tense logic, its extensions with path axioms, and for bi-intuitionistic logic. These logics do not have straightforward formalisations in the traditional Gentzen-style sequent calculus, but have all been shown to have cut-free nested sequent calculi. The proof of the interpolation theorem uses these calculi and is purely syntactic, without resorting to embeddings, semantic arguments, or interpreted connectives external to the underlying logical language. A novel feature of our proof includes an orthogonality condition for defining duality between interpolants.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Base-extension Semantics for Modal Logic,"In proof-theoretic semantics, meaning is based on inference. It may seen as the mathematical expression of the inferentialist interpretation of logic. Much recent work has focused on base-extension semantics, in which the validity of formulas is given by an inductive definition generated by provability in a `base' of atomic rules. Base-extension semantics for classical and intuitionistic propositional logic have been explored by several authors. In this paper, we develop base-extension semantics for the classical propositional modal systems K, KT , K4, and S4, with $\square$ as the primary modal operator. We establish appropriate soundness and completeness theorems and establish the duality between $\square$ and a natural presentation of $\lozenge$. We also show that our semantics is in its current form not complete with respect to euclidean modal logics. Our formulation makes essential use of relational structures on bases.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Efficient Description Logic Reasoning in Prolog: The DLog system,"This paper describes a resolution based Description Logic reasoning system called DLog. DLog transforms Description Logic axioms into a Prolog program and uses the standard Prolog execution for efficiently answering instance retrieval queries. From the Description Logic point of view, DLog is an ABox reasoning engine for the full SHIQ language. The DLog approach makes it possible to store the individuals in a database instead of memory, which results in better scalability and helps using description logic ontologies directly on top of existing information sources.   To appear in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Coalescing: Syntactic Abstraction for Reasoning in First-Order Modal Logics,We present a syntactic abstraction method to reason about first-order modal logics by using theorem provers for standard first-order logic and for propositional modal logic.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Modular many-valued semantics for combined logics,"We obtain, for the first time, a modular many-valued semantics for combined logics, which is built directly from many-valued semantics for the logics being combined, by means of suitable universal operations over partial non-deterministic logical matrices. Our constructions preserve finite-valuedness in the context of multiple-conclusion logics whereas, unsurprisingly, it may be lost in the context of single-conclusion logics. Besides illustrating our constructions over a wide range of examples, we also develop concrete applications of our semantic characterizations, namely regarding the semantics of strengthening a given many-valued logic with additional axioms, the study of conditions under which a given logic may be seen as a combination of simpler syntactically defined fragments whose calculi can be obtained independently and put together to form a calculus for the whole logic, and also general conditions for decidability to be preserved by the combination mechanism.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Formula size games for modal logic and $$-calculus,"We propose a new version of formula size game for modal logic. The game characterizes the equivalence of pointed Kripke-models up to formulas of given numbers of modal operators and binary connectives. Our game is similar to the well-known Adler-Immerman game. However, due to a crucial difference in the definition of positions of the game, its winning condition is simpler, and the second player does not have a trivial optimal strategy. Thus, unlike the Adler-Immerman game, our game is a genuine two-person game. We illustrate the use of the game by proving a non-elementary succinctness gap between bisimulation invariant first-order logic $\mathrm{FO}$ and (basic) modal logic $\mathrm{ML}$. We also present a version of the game for the modal $$-calculus $\mathrm{L}_$ and show that $\mathrm{FO}$ is also non-elementarily more succinct than $\mathrm{L}_$.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Under Lock and Key: A Proof System for a Multimodal Logic,"We present a proof system for a multimodal logic, based on our previous work on a multimodal Martin-Loef type theory. The specification of modes, modalities, and implications between them is given as a mode theory, i.e. a small 2-category. The logic is extended to a lambda calculus, establishing a Curry-Howard correspondence.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Abductive Reasoning in Intuitionistic Propositional Logic via Theorem Synthesis,"With help of a compact Prolog-based theorem prover for Intuitionistic Propositional Logic, we synthesize minimal assumptions under which a given formula formula becomes a theorem.   After applying our synthesis algorithm to cover basic abductive reasoning mechanisms, we synthesize conjunctions of literals that mimic rows of truth tables in classical or intermediate logics and we abduce conditional hypotheses that turn the theorems of classical or intermediate logics into theorems in intuitionistic logic. One step further, we generalize our abductive reasoning mechanism to synthesize more expressive sequent premises using a minimal set of canonical formulas, to which arbitrary formulas in the calculus can be reduced while preserving their provability.   Organized as a self-contained literate Prolog program, the paper supports interactive exploration of its content and ensures full replicability of our results.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Parameterized Complexity View on Description Logic Reasoning,"Description logics are knowledge representation languages that have been designed to strike a balance between expressivity and computational tractability. Many different description logics have been developed, and numerous computational problems for these logics have been studied for their computational complexity. However, essentially all complexity analyses of reasoning problems for description logics use the one-dimensional framework of classical complexity theory. The multi-dimensional framework of parameterized complexity theory is able to provide a much more detailed image of the complexity of reasoning problems.   In this paper we argue that the framework of parameterized complexity has a lot to offer for the complexity analysis of description logic reasoning problems---when one takes a progressive and forward-looking view on parameterized complexity tools. We substantiate our argument by means of three case studies. The first case study is about the problem of concept satisfiability for the logic ALC with respect to nearly acyclic TBoxes. The second case study concerns concept satisfiability for ALC concepts parameterized by the number of occurrences of union operators and the number of occurrences of full existential quantification. The third case study offers a critical look at data complexity results from a parameterized complexity point of view. These three case studies are representative for the wide range of uses for parameterized complexity methods for description logic problems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Complex Logical Reasoning over Knowledge Graphs using Large Language Models,"Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Framework for Intuitionistic Grammar Logics,"We generalize intuitionistic tense logics to the multi-modal case by placing grammar logics on an intuitionistic footing. We provide axiomatizations for a class of base intuitionistic grammar logics as well as provide axiomatizations for extensions with combinations of seriality axioms and what we call ""intuitionistic path axioms"". We show that each axiomatization is sound and complete with completeness being shown via a typical canonical model construction.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Principles and Examples of Plausible Reasoning and Propositional Plausible Logic,"Plausible reasoning concerns situations whose inherent lack of precision is not quantified; that is, there are no degrees or levels of precision, and hence no use of numbers like probabilities. A hopefully comprehensive set of principles that clarifies what it means for a formal logic to do plausible reasoning is presented. A new propositional logic, called Propositional Plausible Logic (PPL), is defined and applied to some important examples. PPL is the only non-numeric non-monotonic logic we know of that satisfies all the principles and correctly reasons with all the examples. Some important results about PPL are proved.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Uniform Substitution for Differential Game Logic,"This paper presents a uniform substitution calculus for differential game logic (dGL). Church's uniform substitutions substitute a term or formula for a function or predicate symbol everywhere. After generalizing them to differential game logic and allowing for the substitution of hybrid games for game symbols, uniform substitutions make it possible to only use axioms instead of axiom schemata, thereby substantially simplifying implementations. Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones, the resulting axiomatization adopts only a finite number of ordinary dGL formulas as axioms, which uniform substitutions instantiate soundly. This paper proves soundness and completeness of uniform substitutions for the monotone modal logic dGL. The resulting axiomatization admits a straightforward modular implementation of dGL in theorem provers.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"An asymptotic analysis of probabilistic logic programming, with implications for expressing projective families of distributions","Probabilistic logic programming is a major part of statistical relational artificial intelligence, where approaches from logic and probability are brought together to reason about and learn from relational domains in a setting of uncertainty. However, the behaviour of statistical relational representations across variable domain sizes is complex, and scaling inference and learning to large domains remains a significant challenge. In recent years, connections have emerged between domain size dependence, lifted inference and learning from sampled subpopulations. The asymptotic behaviour of statistical relational representations has come under scrutiny, and projectivity was investigated as the strongest form of domain-size dependence, in which query marginals are completely independent of the domain size.   In this contribution we show that every probabilistic logic program under the distribution semantics is asymptotically equivalent to an acyclic probabilistic logic program consisting only of determinate clauses over probabilistic facts. We conclude that every probabilistic logic program inducing a projective family of distributions is in fact everywhere equivalent to a program from this fragment, and we investigate the consequences for the projective families of distributions expressible by probabilistic logic programs.   To facilitate the application of classical results from finite model theory, we introduce the abstract distribution semantics, defined as an arbitrary logical theory over probabilistic facts. This bridges the gap to the distribution semantics underlying probabilistic logic programming. In this representation, determinate logic programs correspond to quantifier-free theories, making asymptotic quantifier elimination results available for the setting of probabilistic logic programming.   This paper is under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Complete Game Logic with Sabotage,"Game Logic with sabotage ($\mathsf{GL_s}$) is introduced as a simple and natural extension of Parikh's game logic with a single additional primitive, which allows players to lay traps for the opponent. $\mathsf{GL_s}$ can be used to model infinite sabotage games, in which players can change the rules during game play. In contrast to game logic, which is strictly less expressive, $\mathsf{GL_s}$ is exactly as expressive as the modal $$-calculus. This reveals a close connection between the entangled nested recursion inherent in modal fixpoint logics and adversarial dynamic rule changes characteristic for sabotage games. A natural Hilbert-style proof calculus for $\mathsf{GL_s}$ is presented and proved complete using syntactic equiexpressiveness reductions. The completeness of a simple extension of Parikh's calculus for game logic follows.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Abstract Reasoning via Logic-guided Generation,"Abstract reasoning, i.e., inferring complicated patterns from given observations, is a central building block of artificial general intelligence. While humans find the answer by either eliminating wrong candidates or first constructing the answer, prior deep neural network (DNN)-based methods focus on the former discriminative approach. This paper aims to design a framework for the latter approach and bridge the gap between artificial and human intelligence. To this end, we propose logic-guided generation (LoGe), a novel generative DNN framework that reduces abstract reasoning as an optimization problem in propositional logic. LoGe is composed of three steps: extract propositional variables from images, reason the answer variables with a logic layer, and reconstruct the answer image from the variables. We demonstrate that LoGe outperforms the black box DNN frameworks for generative abstract reasoning under the RAVEN benchmark, i.e., reconstructing answers based on capturing correct rules of various attributes from observations.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Strong Equivalence and Program Structure in Arguing Essential Equivalence between Logic Programs,"Answer set programming is a prominent declarative programming paradigm used in formulating combinatorial search problems and implementing different knowledge representation formalisms. Frequently, several related and yet substantially different answer set programs exist for a given problem. Sometimes these encodings may display significantly different performance. Uncovering precise formal links between these programs is often important and yet far from trivial. This paper presents formal results carefully relating a number of interesting program rewritings. It also provides the proof of correctness of system Projector concerned with automatic program rewritings for the sake of efficiency. Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Induction and Recursion Principles in a Higher-Order Quantitative Logic for Probability,"Quantitative logic reasons about the degree to which formulas are satisfied. This paper studies the fundamental reasoning principles of higher-order quantitative logic and their application to reasoning about probabilistic programs and processes.   We construct an affine calculus for $1$-bounded complete metric spaces and the monad for probability measures equipped with the Kantorovic distance. The calculus includes a form of guarded recursion interpreted via Banach's fixed point theorem, useful, e.g., for recursive programming with processes. We then define an affine higher-order quantitative logic for reasoning about terms of our calculus. The logic includes novel principles for guarded recursion, and induction over probability measures and natural numbers.   We illustrate the expressivity of the logic by a sequence of case studies: Proving upper limits on bisimilarity distances of Markov processes, showing convergence of a temporal learning algorithm and of a random walk using a coupling argument. Finally we show how to encode a probabilistic Hoare logic in our logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Propositional Computability Logic II,"Computability logic is a formal theory of computational tasks and resources. Its formulas represent interactive computational problems, logical operators stand for operations on computational problems, and validity of a formula is understood as being a scheme of problems that always have algorithmic solutions. A comprehensive online source on the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html . The earlier article ""Propositional computability logic I"" proved soundness and completeness for the (in a sense) minimal nontrivial fragment CL1 of computability logic. The present paper extends that result to the significantly more expressive propositional system CL2. What makes CL2 more expressive than CL1 is the presence of two sorts of atoms in its language: elementary atoms, representing elementary computational problems (i.e. predicates), and general atoms, representing arbitrary computational problems. CL2 conservatively extends CL1, with the latter being nothing but the general-atom-free fragment of the former.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Expressive Power of k-ary Exclusion Logic,"In this paper we study the expressive power of k-ary exclusion logic, EXC[k], that is obtained by extending first order logic with k-ary exclusion atoms. It is known that without arity bounds exclusion logic is equivalent with dependence logic. By observing the translations, we see that the expressive power of EXC[k] lies in between k-ary and (k+1)-ary dependence logics. We will show that, at least in the case of k=1, the both of these inclusions are proper.   In a recent work by the author it was shown that k-ary inclusion-exclusion logic is equivalent with k-ary existential second order logic, ESO[k]. We will show that, on the level of sentences, it is possible to simulate inclusion atoms with exclusion atoms, and this way express ESO[k]-sentences by using only k-ary exclusion atoms. For this translation we also need to introduce a novel method for ""unifying"" the values of certain variables in a team. As a consequence, EXC[k] captures ESO[k] on the level of sentences, and we get a strict arity hierarchy for exclusion logic. It also follows that k-ary inclusion logic is strictly weaker than EXC[k].   Finally we will use similar techniques to formulate a translation from ESO[k] to k-ary inclusion logic with strict semantics. Consequently, for any arity fragment of inclusion logic, strict semantics is more expressive than lax semantics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Beyond knowing that: a new generation of epistemic logics,"Epistemic logic has become a major field of philosophical logic ever since the groundbreaking work by Hintikka (1962). Despite its various successful applications in theoretical computer science, AI, and game theory, the technical development of the field has been mainly focusing on the propositional part, i.e., the propositional modal logics of ""knowing that"". However, knowledge is expressed in everyday life by using various other locutions such as ""knowing whether"", ""knowing what"", ""knowing how"" and so on (knowing-wh hereafter). Such knowledge expressions are better captured in quantified epistemic logic, as was already discussed by Hintikka (1962) and his sequel works at length. This paper aims to draw the attention back again to such a fascinating but largely neglected topic. We first survey what Hintikka and others did in the literature of quantified epistemic logic, and then advocate a new quantifier-free approach to study the epistemic logics of knowing-wh, which we believe can balance expressivity and complexity, and capture the essential reasoning patterns about knowing-wh. We survey our recent line of work on the epistemic logics of ""knowing whether"", ""knowing what"" and ""knowing how"" to demonstrate the use of this new approach.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Utilizing Treewidth for Quantitative Reasoning on Epistemic Logic Programs,"Extending the popular Answer Set Programming (ASP) paradigm by introspective reasoning capacities has received increasing interest within the last years. Particular attention is given to the formalism of epistemic logic programs (ELPs) where standard rules are equipped with modal operators which allow to express conditions on literals for being known or possible, i.e., contained in all or some answer sets, respectively. ELPs thus deliver multiple collections of answer sets, known as world views. Employing ELPs for reasoning problems so far has mainly been restricted to standard decision problems (complexity analysis) and enumeration (development of systems) of world views. In this paper, we take a next step and contribute to epistemic logic programming in two ways: First, we establish quantitative reasoning for ELPs, where the acceptance of a certain set of literals depends on the number (proportion) of world views that are compatible with the set. Second, we present a novel system that is capable of efficiently solving the underlying counting problems required to answer such quantitative reasoning problems. Our system exploits the graph-based measure treewidth and works by iteratively finding and refining (graph) abstractions of an ELP program. On top of these abstractions, we apply dynamic programming that is combined with utilizing existing search-based solvers like (e)clingo for hard combinatorial subproblems that appear during solving. It turns out that our approach is competitive with existing systems that were introduced recently. This work is under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reasoning with Higher-Order Abstract Syntax in a Logical Framework,"Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Two Results on Separation Logic With Theory Reasoning,"Two results are presented concerning the entailment problem in Separation Logic with inductively defined predicate symbols and theory reasoning. First, we show that the entailment problem is undecidable for rules with bounded tree-width, if theory reasoning is considered. The result holds for a wide class of theories, even with a very low expressive power. For instance it applies to the natural numbers with the successor function, or with the usual order. Second, we show that every entailment problem can be reduced to an entailment problem containing no equality (neither in the formulas nor in the recursive rules defining the semantics of the predicate symbols).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Topological representation of intuitionistic and distributive abstract logics,"We continue work of our earlier paper (Lewitzka and Brunner: Minimally generated abstract logics, Logica Universalis 3(2), 2009), where abstract logics and particularly intuitionistic abstract logics are studied. Abstract logics can be topologized in a direct and natural way. This facilitates a topological study of classes of concrete logics whenever they are given in abstract form. Moreover, such a direct topological approach avoids the often complex algebraic and lattice-theoretic machinery usually applied to represent logics. Motivated by that point of view, we define in this paper the category of intuitionistic abstract logics with stable logic maps as morphisms, and the category of implicative spectral spaces with spectral maps as morphisms. We show the equivalence of these categories and conclude that the larger categories of distributive abstract logics and distributive sober spaces are equivalent, too.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
MAP Inference for Probabilistic Logic Programming,"In Probabilistic Logic Programming (PLP) the most commonly studied inference task is to compute the marginal probability of a query given a program. In this paper, we consider two other important tasks in the PLP setting: the Maximum-A-Posteriori (MAP) inference task, which determines the most likely values for a subset of the random variables given evidence on other variables, and the Most Probable Explanation (MPE) task, the instance of MAP where the query variables are the complement of the evidence variables. We present a novel algorithm, included in the PITA reasoner, which tackles these tasks by representing each problem as a Binary Decision Diagram and applying a dynamic programming procedure on it. We compare our algorithm with the version of ProbLog that admits annotated disjunctions and can perform MAP and MPE inference. Experiments on several synthetic datasets show that PITA outperforms ProbLog in many cases.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On Uniform Equivalence of Epistemic Logic Programs,"Epistemic Logic Programs (ELPs) extend Answer Set Programming (ASP) with epistemic negation and have received renewed interest in recent years. This led to the development of new research and efficient solving systems for ELPs. In practice, ELPs are often written in a modular way, where each module interacts with other modules by accepting sets of facts as input, and passing on sets of facts as output. An interesting question then presents itself: under which conditions can such a module be replaced by another one without changing the outcome, for any set of input facts? This problem is known as uniform equivalence, and has been studied extensively for ASP. For ELPs, however, such an investigation is, as of yet, missing. In this paper, we therefore propose a characterization of uniform equivalence that can be directly applied to the language of state-of-the-art ELP solvers. We also investigate the computational complexity of deciding uniform equivalence for two ELPs, and show that it is on the third level of the polynomial hierarchy.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Wijesekera-style constructive modal logics,"We define a family of propositional constructive modal logics corresponding each to a different classical modal system. The logics are defined in the style of Wijesekera's constructive modal logic, and are both proof-theoretically and semantically motivated. On the one hand, they correspond to the single-succedent restriction of standard sequent calculi for classical modal logics. On the other hand, they are obtained by incorporating the hereditariness of intuitionistic Kripke models into the classical satisfaction clauses for modal formulas. We show that, for the considered classical logics, the proof-theoretical and the semantical approach return the same constructive systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Decidability of predicate logics with team semantics,"We study the complexity of predicate logics based on team semantics. We show that the satisfiability problems of two-variable independence logic and inclusion logic are both NEXPTIME-complete. Furthermore, we show that the validity problem of two-variable dependence logic is undecidable, thereby solving an open problem from the team semantics literature. We also briefly analyse the complexity of the Bernays-Schnfinkel-Ramsey prefix classes of dependence logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Capturing k-ary Existential Second Order Logic with k-ary Inclusion-Exclusion Logic,"In this paper we analyze k-ary inclusion-exclusion logic, INEX[k], which is obtained by extending first order logic with k-ary inclusion and exclusion atoms. We show that every formula of INEX[k] can be expressed with a formula of k-ary existential second order logic, ESO[k]. Conversely, every formula of ESO[k] with at most k-ary free relation variables can be expressed with a formula of INEX[k]. From this it follows that, on the level of sentences, INEX[k] captures the expressive power of ESO[k].   We also introduce several useful operators that can be expressed in INEX[k]. In particular, we define inclusion and exclusion quantifiers and so-called term value preserving disjunction which is essential for the proofs of the main results in this paper. Furthermore, we present a novel method of relativization for team semantics and analyze the duality of inclusion and exclusion atoms.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Some problems with two axiomatizations of discussive logic,"Problems in two axiomatizations of Jakowski's discussive (or discursive) logic D2 are considered. A recent axiomatization of D2 and completeness proof relative to D2's intended semantics seems to be mistaken because some formulas valid according to the intended semantics turn out to be unprovable. Although no new axiomatization is offered, nor a repaired completeness proof given, the shortcomings identified here may be a step toward an improved axiomatization.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Cantor Derivative Logic in Topological Dynamics,"Topological semantics for modal logic based on the Cantor derivative operator gives rise to derivative logics, also referred to as d-logics. Unlike logics based on the topological closure operator, d-logics have not previously been studied in the framework of dynamic topological systems (DTSs), which are pairs (X,f) consisting of a topological space X equipped with a continuous function f : X -> X. We introduce the logics wK4C, K4C and GLC and show that they all have the finite Kripke model property and are sound and complete with respect to the d-semantics in this dynamical setting. We also prove a general result for the case where f is a homeomorphism, which yields soundness and completeness for the corresponding systems wK4H, K4H and GLH. Of special interest is GLC, which is the d-logic of all DTSs based on a scattered space. We use the completeness of GLC and the properties of scattered spaces to demonstrate the first sound and complete dynamic topological logic in the original trimodal language. In particular, we show that the version of DTL based on the class of scattered spaces is finitely axiomatisable over the original language, and that the natural axiomatisation is sound and complete.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Rethinking Defeasible Reasoning: A Scalable Approach,"Recent technological advances have led to unprecedented amounts of generated data that originate from the Web, sensor networks and social media. Analytics in terms of defeasible reasoning - for example for decision making - could provide richer knowledge of the underlying domain. Traditionally, defeasible reasoning has focused on complex knowledge structures over small to medium amounts of data, but recent research efforts have attempted to parallelize the reasoning process over theories with large numbers of facts. Such work has shown that traditional defeasible logics come with overheads that limit scalability. In this work, we design a new logic for defeasible reasoning, thus ensuring scalability by design. We establish several properties of the logic, including its relation to existing defeasible logics. Our experimental results indicate that our approach is indeed scalable and defeasible reasoning can be applied to billions of facts.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Flexible coinductive logic programming,"Recursive definitions of predicates are usually interpreted either inductively or coinductively. Recently, a more powerful approach has been proposed, called flexible coinduction, to express a variety of intermediate interpretations, necessary in some cases to get the correct meaning. We provide a detailed formal account of an extension of logic programming supporting flexible coinduction. Syntactically, programs are enriched by coclauses, clauses with a special meaning used to tune the interpretation of predicates. As usual, the declarative semantics can be expressed as a fixed point which, however, is not necessarily the least, nor the greatest one, but is determined by the coclauses. Correspondingly, the operational semantics is a combination of standard SLD resolution and coSLD resolution. We prove that the operational semantics is sound and complete with respect to declarative semantics restricted to finite comodels. This paper is under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Interleaving Logic and Counting,"Reasoning with quantifier expressions in natural language combines logical and arithmetical features, transcending strict divides between qualitative and quantitative. Our topic is this cooperation of styles as it occurs in common linguistic usage and its extension into the broader practice of natural language plus ""grassroots mathematics"".   We begin with a brief review of first-order logic with counting operators and cardinality comparisons. This system is known to be of high complexity, and drowns out finer aspects of the combination of logic and counting. We move to a small fragment that can represent numerical syllogisms and basic reasoning about comparative size: monadic first-order logic with counting. We provide normal forms that allow for axiomatization, determine which arithmetical notions can be defined on finite and on infinite models, and conversely, we discuss which logical notions can be defined out of purely arithmetical ones, and what sort of (non-)classical logics can be induced.   Next, we investigate a series of strengthenings, again using normal form methods. The monadic second-order version is close, in a precise sense, to additive Presburger Arithmetic, while versions with the natural device of tuple counting take us to Diophantine equations, making the logic undecidable. We also define a system that combines basic modal logic over binary accessibility relations with counting, needed to formulate ubiquitous reasoning patterns such as the Pigeonhole Principle.   We return to our starting point in natural language, confronting the architecture of our formal systems with linguistic quantifier vocabulary and syntax. We conclude with some general thoughts on yet further entanglements of logic and counting in formal systems, on rethinking the qualitative/quantitative divide, and on connecting our analysis to empirical findings in cognitive science.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
First-Order Game Logic and Modal Mu-Calculus,"This paper investigates first-order game logic and first-order modal mu-calculus, which extend their propositional modal logic counterparts with first-order modalities of interpreted effects such as variable assignments. Unlike in the propositional case, both logics are shown to have the same expressive power and their proof calculi to have the same deductive power. Both calculi are also mutually relatively complete.   In the presence of differential equations, corollaries obtain usable and complete translations between differential game logic, a logic for the deductive verification of hybrid games, and the differential mu-calculus, the modal mu-calculus for hybrid systems. The differential mu-calculus is complete with respect to first-order fixpoint logic and differential game logic is complete with respect to its ODE-free fragment.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Exploring the Landscape of Relational Syllogistic Logics,"This paper explores relational syllogistic logics, a family of logical systems related to reasoning about relations in extensions of the classical syllogistic. These are all decidable logical systems. We prove completeness theorems and complexity results for a natural subfamily of relational syllogistic logics, parametrized by constructors for terms and for sentences.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On the strongest three-valued paraconsistent logic contained in classical logic and its dual,"LP$^{\supset,\mathsf{F}}$ is a three-valued paraconsistent propositional logic which is essentially the same as J3. It has most properties that have been proposed as desirable properties of a reasonable paraconsistent propositional logic. However, it follows easily from already published results that there are exactly 8192 different three-valued paraconsistent propositional logics that have the properties concerned. In this paper, properties concerning the logical equivalence relation of a logic are used to distinguish LP$^{\supset,\mathsf{F}}$ from the others. As one of the bonuses of focussing on the logical equivalence relation, it is found that only 32 of the 8192 logics have a logical equivalence relation that satisfies the identity, annihilation, idempotent, and commutative laws for conjunction and disjunction. For most properties of LP$^{\supset,\mathsf{F}}$ that have been proposed as desirable properties of a reasonable paraconsistent propositional logic, its paracomplete analogue has a comparable property. In this paper, properties concerning the logical equivalence relation of a logic are also used to distinguish the paracomplete analogue of LP$^{\supset,\mathsf{F}}$ from the other three-valued paracomplete propositional logics with those comparable properties.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Verifying Tight Logic Programs with anthem and Vampire,"This paper continues the line of research aimed at investigating the relationship between logic programs and first-order theories. We extend the definition of program completion to programs with input and output in a subset of the input language of the ASP grounder gringo, study the relationship between stable models and completion in this context, and describe preliminary experiments with the use of two software tools, anthem and vampire, for verifying the correctness of programs with input and output. Proofs of theorems are based on a lemma that relates the semantics of programs studied in this paper to stable models of first-order formulas. Under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Canonical Proof nets for Classical Logic,"Proof nets provide abstract counterparts to sequent proofs modulo rule permutations; the idea being that if two proofs have the same underlying proof-net, they are in essence the same proof. Providing a convincing proof-net counterpart to proofs in the classical sequent calculus is thus an important step in understanding classical sequent calculus proofs. By convincing, we mean that (a) there should be a canonical function from sequent proofs to proof nets, (b) it should be possible to check the correctness of a net in polynomial time, (c) every correct net should be obtainable from a sequent calculus proof, and (d) there should be a cut-elimination procedure which preserves correctness. Previous attempts to give proof-net-like objects for propositional classical logic have failed at least one of the above conditions. In [23], the author presented a calculus of proof nets (expansion nets) satisfying (a) and (b); the paper defined a sequent calculus corresponding to expansion nets but gave no explicit demonstration of (c). That sequent calculus, called LK\ast in this paper, is a novel one-sided sequent calculus with both additively and multiplicatively formulated disjunction rules. In this paper (a self-contained extended version of [23]), we give a full proof of (c) for expansion nets with respect to LK\ast, and in addition give a cut-elimination procedure internal to expansion nets - this makes expansion nets the first notion of proof-net for classical logic satisfying all four criteria.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reconciling Rationality and Stochasticity: Rich Behavioral Models in Two-Player Games,"Two traditional paradigms are often used to describe the behavior of agents in multi-agent complex systems. In the first one, agents are considered to be fully rational and systems are seen as multi-player games. In the second one, agents are considered to be fully stochastic processes and the system itself is seen as a large stochastic process. From the standpoint of a particular agent - having to choose a strategy, the choice of the paradigm is crucial: the most adequate strategy depends on the assumptions made on the other agents.   In this paper, we focus on two-player games and their application to the automated synthesis of reliable controllers for reactive systems - a field at the crossroads between computer science and mathematics. In this setting, the reactive system to control is a player, and its environment is its opponent, usually assumed to be fully antagonistic or fully stochastic. We illustrate several recent developments aiming to breach this narrow taxonomy by providing formal concepts and mathematical frameworks to reason about richer behavioral models.   The interest of such models is not limited to reactive system synthesis but extends to other application fields of game theory. The goal of our contribution is to give a high-level presentation of key concepts and applications, aimed at a broad audience. To achieve this goal, we illustrate those rich behavioral models on a classical challenge of the everyday life: planning a journey in an uncertain environment.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Non-commutative linear logic fragments with sub-context-free complexity,"We present new descriptive complexity characterisations of classes REG (regular languages), LCFL (linear context-free languages) and CFL (context-free languages) as restrictions on inference rules, size of formulae and permitted connectives in the Lambek calculus; fragments of the intuitionistic non-commutative linear logic with direction-sensitive implication connectives. Our identification of the Lambek calculus fragments with proof complexity REG and LCFL is the first result of its kind. We further show the CFL complexity of one of the strictly `weakest' possible variants of the logic, admitting only a single inference rule. The proof thereof, moreover, is based on a direct translation between type-logical and formal grammar and structural induction on provable sequents; a simpler and more intuitive method than those employed in prior works. We thereby establish a clear conceptual utility of the Cut-elimination theorem for comparing formal grammar and sequent calculus, and identify the exact analogue of the Greibach Normal Form in Lambek grammar. We believe the result presented herein constitutes a first step toward a more extensive and richer characterisation of the interaction between computation and logic, as well as a finer-grained complexity separation of various sequent calculi.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Classical Predicative Logic-Enriched Type Theories,"A logic-enriched type theory (LTT) is a type theory extended with a primitive mechanism for forming and proving propositions. We construct two LTTs, named LTTO and LTTO*, which we claim correspond closely to the classical predicative systems of second order arithmetic ACAO and ACA. We justify this claim by translating each second-order system into the corresponding LTT, and proving that these translations are conservative. This is part of an ongoing research project to investigate how LTTs may be used to formalise different approaches to the foundations of mathematics.   The two LTTs we construct are subsystems of the logic-enriched type theory LTTW, which is intended to formalise the classical predicative foundation presented by Herman Weyl in his monograph Das Kontinuum. The system ACAO has also been claimed to correspond to Weyl's foundation. By casting ACAO and ACA as LTTs, we are able to compare them with LTTW. It is a consequence of the work in this paper that LTTW is strictly stronger than ACAO.   The conservativity proof makes use of a novel technique for proving one LTT conservative over another, involving defining an interpretation of the stronger system out of the expressions of the weaker. This technique should be applicable in a wide variety of different cases outside the present work.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Introduction to Cirquent Calculus and Abstract Resource Semantics,"This paper introduces a refinement of the sequent calculus approach called cirquent calculus. While in Gentzen-style proof trees sibling (or cousin, etc.) sequents are disjoint sequences of formulas, in cirquent calculus they are permitted to share elements. Explicitly allowing or disallowing shared resources and thus taking to a more subtle level the resource-awareness intuitions underlying substructural logics, cirquent calculus offers much greater flexibility and power than sequent calculus does. A need for substantially new deductive tools came with the birth of computability logic (see http://www.cis.upenn.edu/~giorgi/cl.html) - the semantically constructed formal theory of computational resources, which has stubbornly resisted any axiomatization attempts within the framework of traditional syntactic approaches. Cirquent calculus breaks the ice. Removing contraction from the full collection of its rules yields a sound and complete system for the basic fragment CL5 of computability logic. Doing the same in sequent calculus, on the other hand, throws out the baby with the bath water, resulting in the strictly weaker affine logic. An implied claim of computability logic is that it is CL5 rather than affine logic that adequately materializes the resource philosophy traditionally associated with the latter. To strengthen this claim, the paper further introduces an abstract resource semantics and shows the soundness and completeness of CL5 with respect to it.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Monadic Second-Order Logic with Arbitrary Monadic Predicates,"We study Monadic Second-Order Logic (MSO) over finite words, extended with (non-uniform arbitrary) monadic predicates. We show that it defines a class of languages that has algebraic, automata-theoretic and machine-independent characterizations. We consider the regularity question: given a language in this class, when is it regular? To answer this, we show a substitution property and the existence of a syntactical predicate.   We give three applications. The first two are to give very simple proofs that the Straubing Conjecture holds for all fragments of MSO with monadic predicates, and that the Crane Beach Conjecture holds for MSO with monadic predicates. The third is to show that it is decidable whether a language defined by an MSO formula with morphic predicates is regular.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Classical linear logic, cobordisms and categorical semantics of categorial grammars","We propose a categorial grammar based on classical multiplicative linear logic.   This can be seen as an extension of abstract categorial grammars (ACG) and is at least as expressive. However, constituents of {\it linear logic grammars (LLG)} are not abstract $$-terms, but simply tuples of words with labeled endpoints, we call them {\it multiwords}. At least, this gives a concrete and intuitive representation of ACG.   A key observation is that the class of multiwords has a fundamental algebraic structure. Namely, multiwords can be organized in a category, very similar to the category of topological cobordisms. This category is symmetric monoidal closed and compact closed and thus is a model of linear $$-calculus and classical linear logic. We think that this category is interesting on its own right. In particular, it might provide categorical representation for other formalisms.   On the other hand, many models of language semantics are based on commutative logic or, more generally, on symmetric monoidal closed categories. But the category of {\it word cobordisms} is a category of language elements, which is itself symmetric monoidal closed and independent of any grammar. Thus, it might prove useful in understanding language semantics as well.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Simple example of weak modal logic based on intuitionistic core,"In this paper we present simple example of propositional logic which has one modal operator and is based on intuitionistic core. This system is very weak in modal sense - e.g. rules of regularity or monotonicity do not hold. It has complete semantics composed of possible worlds equipped with neighborhoods and pre-order relation. We discuss certain restrictions imposed on those structures. Also, we present characterization of axiom 4 known from logic S4.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings Fourth International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the Fourth International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2013). The symposium took place in Borca di Cadore, Italy, from 29th to 31st of August 2013.    The proceedings of the symposium contain the abstracts of three invited talks and 17 papers that were accepted after a careful evaluation for presentation at the conference. The topics of the accepted papers range over a wide spectrum, including algorithmic and behavioral game theory, game semantics, formal languages and automata theory, modal and temporal logics, software verification, hybrid systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logical Varieties in Normative Reasoning,"Although conventional logical systems based on logical calculi have been successfully used in mathematics and beyond, they have definite limitations that restrict their application in many cases. For instance, the principal condition for any logical calculus is its consistency. At the same time, knowledge about large object domains (in science or in practice) is essentially inconsistent. Logical prevarieties and varieties were introduced to eliminate these limitations in a logically correct way. In this paper, the Logic of Reasonable Inferences is described. This logic has been applied successfully to model legal reasoning with inconsistent knowledge. It is demonstrated that this logic is a logical variety and properties of logical varieties related to legal reasoning are developed.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
About epistemic negation and world views in Epistemic Logic Programs,"In this paper we consider Epistemic Logic Programs, which extend Answer Set Programming (ASP) with ""epistemic operators"" and ""epistemic negation"", and a recent approach to the semantics of such programs in terms of World Views. We propose some observations on the existence and number of world views. We show how to exploit an extended ASP semantics in order to: (i) provide a characterization of world views, different from existing ones; (ii) query world views and query the whole set of world views.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Transformations of Logic Programs on Infinite Lists,"We consider an extension of logic programs, called -programs, that can be used to define predicates over infinite lists. -programs allow us to specify properties of the infinite behavior of reactive systems and, in general, properties of infinite sequences of events. The semantics of -programs is an extension of the perfect model semantics. We present variants of the familiar unfold/fold rules which can be used for transforming -programs. We show that these new rules are correct, that is, their application preserves the perfect model semantics. Then we outline a general methodology based on program transformation for verifying properties of -programs. We demonstrate the power of our transformation-based verification methodology by proving some properties of Buechi automata and -regular languages.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
RAESON: A Tool for Reasoning Tasks Driven by Interactive Visualization of Logical Structure,The paper presents a software tool for analysis and interactive engagement in various logical reasoning tasks. A first feature of the program consists in providing an interface for working with logic-specific repositories of formal knowledge. A second feature provides the means to intuitively visualize and interactively generate the underlying logical structure that propels customary logical reasoning tasks. Starting from this we argue that both aspects have didactic potential and can be integrated in teaching activities to provide an engaging learning experience.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Game-theoretic Interpretation of Intuitionistic Type Theory,"We present a game semantics for intuitionistic type theory. Specifically, we propose categories with families of a new variant of games and strategies for both extensional and intensional variants of the type theory with dependent function, dependent pair, and identity types as well as universes. Our games and strategies generalize the existing notion of games and strategies and achieve an interpretation of dependent types and the hierarchy of universes in an intuitive manner. We believe that it is a significant step towards a computational and intensional interpretation of the type theory.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Tableaux for multi-modal hybrid logic with binders, transitive relations and relation hierarchies","In a previous paper, a tableau calculus has been presented, which constitute a decision procedure for hybrid logic with the converse and global modalities and a restricted use of the binder. This work extends such a calculus to multi-modal logic with transitive relations and relation inclusion assertions.   The separate addition of either transitive relations or relation hierarchies to the considered decidable fragment of multi-modal hybrid logic can easily be shown to stay decidable, by resorting to results already proved in the literature. However, such results do not directly allow for concluding whether the logic including both features is still decidable. The existence of a terminating, sound and complete calculus for the considered logic proves that the addition of transitive relations and relation hierarchies to such an expressive decidable fragment of hybrid logic yields a decidable logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Logic that Captures $$P on Ordered Structures,"We extend the inflationary fixed-point logic, IFP, with a new kind of second-order quantifiers which have (poly-)logarithmic bounds. We prove that on ordered structures the new logic $\exists^{\log^}\text{IFP}$ captures the limited nondeterminism class $\text{P}$. In order to study its expressive power, we also design a new version of Ehrenfeucht-Frass game for this logic and show that our capturing result will not hold on the general case, i.e. on all the finite structures.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings Fifth International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the Fifth International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2014). The symposium took place in Verona, Italy, from 10th to 12th of September 2014.  The proceedings of the symposium contain the abstracts of three invited talks and 19 papers that were accepted after a careful evaluation for presentation at the conference. The topics of the accepted papers range over a wide spectrum, including algorithmic and behavioral game theory, game semantics, formal languages and automata theory, modal and temporal logics, software verification, hybrid systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Logics for Epistemic Actions: Completeness, Decidability, Expressivity","We consider dynamic versions of epistemic logic as formulated in Baltag and Moss ""Logics for epistemic programs"" (2004). That paper proposed a logical language (actually families of languages parameterized by action signatures) for dynamic epistemic logic. It had been shown that validity in the language is Pi-1-1-complete, so there are no recursively axiomatized complete logical systems for it. In contrast, this paper proves a weak completeness result for the fragment without action iteration, and a strong completeness result for the fragment without action iteration and common knowledge. Our work involves a detour into term rewriting theory. The argument uses modal filtration, and thus we obtain the finite model property and hence decidability. We also give a translation of our largest language into PDL, thereby obtaining a second proof of decidability. The paper closes with some results on expressive power. These are mostly concerned with comparing the action-iteration-free language with modal logic augmented by transitive closure operators. We answer a natural question about the languages we obtain by varying the action signature: we prove that a logical language with operators for private announcements is more expressive than one for public announcements.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Completeness Theorems for First-Order Logic Analysed in Constructive Type Theory (Extended Version),"We study various formulations of the completeness of first-order logic phrased in constructive type theory and mechanised in the Coq proof assistant. Specifically, we examine the completeness of variants of classical and intuitionistic natural deduction and sequent calculi with respect to model-theoretic, algebraic, and game-theoretic semantics. As completeness with respect to the standard model-theoretic semantics  la Tarski and Kripke is not readily constructive, we analyse connections of completeness theorems to Markov's Principle and Weak Knig's Lemma and discuss non-standard semantics admitting assumption-free completeness. We contribute a reusable Coq library for first-order logic containing all results covered in this paper.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Algebraic classifications for fragments of first-order logic and beyond,"Complexity and decidability of logics is a major research area involving a huge range of different logical systems. This calls for a unified and systematic approach for the field. We introduce a research program based on an algebraic approach to complexity classifications of fragments of first-order logic (FO) and beyond. Our base system GRA, or general relation algebra, is equiexpressive with FO. It resembles cylindric algebra but employs a finite signature with only seven different operators. We provide a comprehensive classification of the decidability and complexity of the systems obtained by limiting the allowed sets of operators. We also give algebraic characterizations of the best known decidable fragments of FO. Furthermore, to move beyond FO, we introduce the notion of a generalized operator and briefly study related systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logic + control: On program construction and verification,"This paper presents an example of formal reasoning about the semantics of a Prolog program of practical importance (the SAT solver of Howe and King). The program is treated as a definite clause logic program with added control. The logic program is constructed by means of stepwise refinement, hand in hand with its correctness and completeness proofs. The proofs are declarative - they do not refer to any operational semantics. Each step of the logic program construction follows a systematic approach to constructing programs which are provably correct and complete. We also prove that correctness and completeness of the logic program is preserved in the final Prolog program. Additionally, we prove termination, occur-check freedom and non-floundering.   Our example shows how dealing with ""logic"" and with ""control"" can be separated. Most of the proofs can be done at the ""logic"" level, abstracting from any operational semantics.   The example employs approximate specifications; they are crucial in simplifying reasoning about logic programs. It also shows that the paradigm of semantics-preserving program transformations may be not sufficient. We suggest considering transformations which preserve correctness and completeness with respect to an approximate specification.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Inconsistency Robustness in Logic Programs,"Inconsistency robustness is ""information system performance in the face of continually pervasive inconsistencies."" A fundamental principle of Inconsistency Robustness is to make contradictions explicit so that arguments for and against propositions can be formalized. This paper explores the role of Inconsistency Robustness in the history and theory of Logic Programs.   Robert Kowalski put forward a bold thesis: ""Looking back on our early discoveries, I value most the discovery that computation could be subsumed by deduction."" However, mathematical logic cannot always infer computational steps because computational systems make use of arbitration for determining which message is processed next by a recipient that is sent multiple messages concurrently. Since reception orders are in general indeterminate, they cannot be inferred from prior information by mathematical logic alone. Therefore mathematical logic cannot in general implement computation.   Over the course of history, the term ""Functional Program"" has grown more precise and technical as the field has matured. ""Logic Program"" should be on a similar trajectory. Accordingly, ""Logic Program"" should have a general precise characterization. In the fall of 1972, different characterizations of Logic Programs that have continued to this day:   * A Logic Program uses Horn-Clause syntax for forward and backward chaining   * Each computational step (according to Actor Model) of a Logic Program is deductively inferred (e.g. in Direct Logic).   The above examples are illustrative of how issues of inconsistency robustness have repeatedly arisen in Logic Programs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Defining Logical Systems via Algebraic Constraints on Proofs,"We present a comprehensive programme analysing the decomposition of proof systems for non-classical logics into proof systems for other logics, especially classical logic, using an algebra of constraints. That is, one recovers a proof system for a target logic by enriching a proof system for another, typically simpler, logic with an algebra of constraints that act as correctness conditions on the latter to capture the former; for example, one may use Boolean algebra to give constraints in a sequent calculus for classical propositional logic to produce a sequent calculus for intuitionistic propositional logic. The idea behind such forms of reduction is to obtain a tool for uniform and modular treatment of proof theory and provide a bridge between semantics logics and their proof theory. The article discusses the theoretical background of the project and provides several illustrations of its work in the field of intuitionistic and modal logics. The results include the following: a uniform treatment of modular and cut-free proof systems for a large class of propositional logics; a general criterion for a novel approach to soundness and completeness of a logic with respect to a model-theoretic semantics; and, a case study deriving a model-theoretic semantics from a proof-theoretic specification of a logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logic and linear algebra: an introduction,"We give an introduction to logic tailored for algebraists, explaining how proofs in linear logic can be viewed as algorithms for constructing morphisms in symmetric closed monoidal categories with additional structure. This is made explicit by showing how to represent proofs in linear logic as linear maps between vector spaces. The interesting part of this vector space semantics is based on the cofree cocommutative coalgebra of Sweedler.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
In the beginning was game semantics,"This article presents an overview of computability logic -- the game-semantically constructed logic of interactive computational tasks and resources. There is only one non-overview, technical section in it, devoted to a proof of the soundness of affine logic with respect to the semantics of computability logic. A comprehensive online source on the subject can be found at http://www.cis.upenn.edu/~giorgi/cl.html","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Towards Statistical Reasoning in Description Logics over Finite Domains (Full Version),"We present a probabilistic extension of the description logic $\mathcal{ALC}$ for reasoning about statistical knowledge. We consider conditional statements over proportions of the domain and are interested in the probabilistic-logical consequences of these proportions. After introducing some general reasoning problems and analyzing their properties, we present first algorithms and complexity results for reasoning in some fragments of Statistical $\mathcal{ALC}$.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Restricted Second-Order Logic for Non-deterministic Poly-Logarithmic Time,"We introduce a restricted second-order logic $\mathrm{SO}^{\mathit{plog}}$ for finite structures where second-order quantification ranges over relations of size at most poly-logarithmic in the size of the structure. We demonstrate the relevance of this logic and complexity class by several problems in database theory. We then prove a Fagin's style theorem showing that the Boolean queries which can be expressed in the existential fragment of $\mathrm{SO}^{\mathit{plog}}$ corresponds exactly to the class of decision problems that can be computed by a non-deterministic Turing machine with random access to the input in time $O((\log n)^k)$ for some $k \ge 0$, i.e., to the class of problems computable in non-deterministic poly-logarithmic time. It should be noted that unlike Fagin's theorem which proves that the existential fragment of second-order logic captures NP over arbitrary finite structures, our result only holds over ordered finite structures, since $\mathrm{SO}^{\mathit{plog}}$ is too weak as to define a total order of the domain. Nevertheless $\mathrm{SO}^{\mathit{plog}}$ provides natural levels of expressibility within poly-logarithmic space in a way which is closely related to how second-order logic provides natural levels of expressibility within polynomial space. Indeed, we show an exact correspondence between the quantifier prefix classes of $\mathrm{SO}^{\mathit{plog}}$ and the levels of the non-deterministic poly-logarithmic time hierarchy, analogous to the correspondence between the quantifier prefix classes of second-order logic and the polynomial-time hierarchy. Our work closely relates to the constant depth quasipolynomial size AND/OR circuits and corresponding restricted second-order logic defined by David A. Mix Barrington in 1992. We explore this relationship in detail.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Proof-theoretic Semantics for First-order Logic,"Sandqvist gave a proof-theoretic semantics (P-tS) for classical logic (CL) that explicates the meaning of the connectives without assuming bivalance. Later, he gave a semantics for intuitionistic propositional logic (IPL). While soundness in both cases is proved through standard techniques, the proof completeness for CL is complex and somewhat obscure, but clear and simple for IPL. Makinson gave a simplified proof of completeness for classical propositional logic (CPL) by directly relating the the P-tS to the logic's extant truth-functional semantics. In this paper, we give an elementary, constructive, and native -- in the sense that it does not presuppose the model-theoretic interpretation of classical logic -- proof of completeness the P-tS of CL using the techniques applies for IPL. Simultaneously, we give a proof of soundness and completeness for first-order intuitionistic logic (IL).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Quantum modal logic,"A modal logic based on quantum logic is formalized in its simplest possible form. Specifically, a relational semantics and a sequent calculus are provided, and the soundness and the completeness theorems connecting both notions are demonstrated. This framework is intended to serve as a basis for formalizing various modal logics over quantum logic, such as quantum alethic logic, quantum temporal logic, quantum epistemic logic, and quantum dynamic logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Applying Constraint Logic Programming to SQL Semantic Analysis,"This paper proposes the use of Constraint Logic Programming (CLP) to model SQL queries in a data-independent abstract layer by focusing on some semantic properties for signalling possible errors in such queries. First, we define a translation from SQL to Datalog, and from Datalog to CLP, so that solving this CLP program will give information about inconsistency, tautology, and possible simplifications. We use different constraint domains which are mapped to SQL types, and propose them to cooperate for improving accuracy. Our approach leverages a deductive system that includes SQL and Datalog, and we present an implementation in this system which is currently being tested in classroom, showing its advantages and differences with respect to other approaches, as well as some performance data. This paper is under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Characterization of Strongly Equivalent Logic Programs in Intermediate Logics,"The non-classical, nonmonotonic inference relation associated with the answer set semantics for logic programs gives rise to a relationship of 'strong equivalence' between logical programs that can be verified in 3-valued Goedel logic, G3, the strongest non-classical intermediate propositional logic (Lifschitz, Pearce and Valverde, 2001). In this paper we will show that KC (the logic obtained by adding axiom ~A v ~~A to intuitionistic logic), is the weakest intermediate logic for which strongly equivalent logic programs, in a language allowing negations, are logically equivalent.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings First Symposium on Games, Automata, Logic, and Formal Verification","This volume contains the Proceedings of the first Symposium on ""Games,  Automata, Logic, and Formal Verification (GandALF)"", held in Minori  (Amalfi coast), Italy, 17-18 June 2010. The symposium has been promoted  by a number of Italian computer scientists interested in game theory, mathematical logic, automata theory, and their applications to the  specification, design, and verification of complex systems. It covers a large spectrum of research topics, ranging from  theoretical aspects to concrete applications. Its aim is to provide a forum where people from different  areas, and possibly with a different background, can  successfully interact. The high-level international profile of the event is witnessed by the composition of the program committee  and by the final program.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logics of formal inconsistency arising from systems of fuzzy logic,"This paper proposes the meeting of fuzzy logic with paraconsistency in a very precise and foundational way. Specifically, in this paper we introduce expansions of the fuzzy logic MTL by means of primitive operators for consistency and inconsistency in the style of the so-called Logics of Formal Inconsistency (LFIs). The main novelty of the present approach is the definition of postulates for this type of operators over MTL-algebras, leading to the definition and axiomatization of a family of logics, expansions of MTL, whose degree-preserving counterpart are paraconsistent and moreover LFIs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Questions and dependency in intuitionistic logic,"In recent years, the logic of questions and dependencies has been investigated in the closely related frameworks of inquisitive logic and dependence logic. These investigations have assumed classical logic as the background logic of statements, and added formulas expressing questions and dependencies to this classical core. In this paper, we broaden the scope of these investigations by studying questions and dependency in the context of intuitionistic logic. We propose an intuitionistic team semantics, where teams are embedded within intuitionistic Kripke models. The associated logic is a conservative extension of intuitionistic logic with questions and dependence formulas. We establish a number of results about this logic, including a normal form result, a completeness result, and translations to classical inquisitive logic and modal dependence logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Multitask Kernel-based Learning with First-Order Logic Constraints,"In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a penalty term that enforces the constraints on both the supervised and unsupervised examples. Unfortunately, the penalty term is not convex and it can hinder the optimization process. However, it is possible to avoid poor solutions by using a two stage learning schema, in which the supervised examples are learned first and then the constraints are enforced.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A First-Order Logic for Reasoning about Knowledge and Probability,"We present a first-order probabilistic epistemic logic, which allows combining operators of knowledge and probability within a group of possibly infinitely many agents. The proposed framework is the first order extension of the logic of Fagin and Halpern from (J.ACM 41:340-367,1994). We define its syntax and semantics, and prove the strong completeness property of the corresponding axiomatic system.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Characterising Modal Definability of Team-Based Logics via the Universal Modality,"We study model and frame definability of various modal logics. Let ML(A+) denote the fragment of modal logic extended with the universal modality in which the universal modality occurs only positively. We show that a class of Kripke models is definable in ML(A+) if and only if the class is elementary and closed under disjoint unions and surjective bisimulations. We also characterise the definability of ML(A+) in the spirit of the well-known Goldblatt--Thomason theorem. We show that an elementary class F of Kripke frames is definable in ML(A+) if and only if F is closed under taking generated subframes and bounded morphic images, and reflects ultrafilter extensions and finitely generated subframes. In addition we study frame definability relative to finite transitive frames and give an analogous characterisation of ML(A+)-definability relative to finite transitive frames. Finally, we initiate the study of model and frame definability in team-based logics. We study (extended) modal dependence logic, (extended) modal inclusion logic, and modal team logic. We establish strict linear hierarchies with respect to model definability and frame definability, respectively. We show that, with respect to model and frame definability, the before mentioned team-based logics, except modal dependence logic, either coincide with ML(A+) or plain modal logic ML. Thus as a corollary we obtain model theoretic characterisation of model and frame definability for the team-based logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings of the 13th International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the 13th International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2022). The aim of GandALF 2022 symposium is to bring together researchers from academia and industry which are actively working in the fields of Games, Automata, Logics, and Formal Verification. The idea is to cover an ample spectrum of themes, ranging from theory to applications, and stimulate cross-fertilization.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings 12th International Symposium on Games, Automata, Logics, and Formal Verification","This volume contains the proceedings of the 12th International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2021). The aim of GandALF 2021 symposium is to bring together researchers from academia and industry which are actively working in the fields of Games, Automata, Logics, and Formal Verification. The idea is to cover an ample spectrum of themes, ranging from theory to applications, and stimulate cross-fertilization.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Ordinal Folding Index: A Computable Metric for Self-Referential Semantics,"The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that measures how many rounds of self-reference a statement, protocol or position must unfold before its truth or outcome stabilises. By turning this abstract 'fold-back' depth into a single ordinal number, OFI forges a direct link between areas that are usually studied in isolation: the closure stages of fixed-point logics, the time-to-win values of infinite parity games, and the ordinal progressions that calibrate the strength of formal theories. We prove that OFI refines all classical game-theoretic and logical metrics while remaining algorithmically enumerable, supply a polynomial-time approximation scheme on finite arenas, and show how the index coincides exactly with the length of the shortest winning strategy in the associated evaluation game. Alongside the theory we outline five open problems from the completeness of the computable-ordinal spectrum to the possibility of 'compressing' deep self-reference that chart a research programme at the intersection of computer-aided logic, algorithmic game theory and ordinal analysis. OFI thus invites game theorists and logicians alike to view infinite play, transfinite induction and reflective reasoning through a single, intuitive lens, opening common ground for techniques.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings Tenth International Symposium on Games, Automata, Logics, and Formal Verification","This volume contains the proceedings of the Tenth International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2019). The symposium took place in Bordeaux, France, from the 2nd to the 3rd of September 2010. The GandALF symposium was established by a group of Italian computer scientists interested in mathematical logic, automata theory, game theory, and their applications to the specification, design, and verification of complex systems. Its aim is to provide a forum where people from different areas, and possibly with different backgrounds, can fruitfully interact. GandALF has a truly international spirit, as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Classical linear logic, cobordisms and categorial grammars","We propose a categorial grammar based on classical multiplicative linear logic.   This can be seen as an extension of abstract categorial grammars (ACG) and is at least as expressive. However, constituents of {\it linear logic grammars (LLG)} are not abstract $$-terms, but simply tuples of words with labeled endpoints and supplied with specific {\it plugging instructions}: the sets of endpoints are subdivided into the {\it incoming} and the {\it outgoing} parts. We call such objects {\it word cobordisms}.   A key observation is that word cobordisms can be organized in a category, very similar to the familiar category of topological cobordisms. This category is symmetric monoidal closed and compact closed and thus is a model of linear $$-calculus and classical, as well as intuitionistic linear logic. This allows us using linear logic as a typing system for word cobordisms.   At least, this gives a concrete and intuitive representation of ACG.   We think, however, that the category of word cobordisms, which has a rich structure and is independent of any grammar, might be interesting on its own right.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings Eighth International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the Eighth International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2017). The symposium took place in Roma, Italy, from the 20th to the 22nd of September 2017. The GandALF symposium was established by a group of Italian computer scientists interested in mathematical logic, automata theory, game theory, and their applications to the specification, design, and verification of complex systems. Its aim is to provide a forum where people from different areas, and possibly with different backgrounds, can fruitfully interact. GandALF has a truly international spirit, as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Completeness of Tableau Calculi for Two-Dimensional Hybrid Logics,"Hybrid logic is one of the extensions of modal logic. The many-dimensional product of hybrid logic is called hybrid product logic (HPL). We construct a sound and complete tableau calculus for two-dimensional HPL. Also, we made a tableau calculus for hybrid dependent product logic (HdPL), where one dimension depends on the other. In addition, we add a special rule to the tableau calculus for HdPL and show that it is still sound and complete. All of them lack termination, however.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Effectiveness in RPL, with Applications to Continuous Logic","In this paper, we introduce a foundation for computable model theory of rational Pavelka logic (an extension of ukasiewicz logic) and continuous logic, and prove effective versions of some theorems in model theory. We show how to reduce continuous logic to rational Pavelka logic. We also define notions of computability and decidability of a model for logics with computable, but uncountable, set of truth values; show that provability degree of a formula w.r.t. a linear theory is computable, and use this to carry out an effective Henkin construction. Therefore, for any effectively given consistent linear theory in continuous logic, we effectively produce its decidable model. This is the best possible, since we show that the computable model theory of continuous logic is an extension of computable model theory of classical logic. We conclude with noting that the unique separable model of a separably categorical and computably axiomatizable theory (such as that of a probability space or an $L^p$ Banach lattice) is decidable.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Games on Graphs: From Logic and Automata to Algorithms,"The objective of this book is to give a comprehensive presentation of the research field concerned with infinite duration games on graphs. Historically, these game models appeared in the study of automata and logic, and they later became important for program verification and synthesis. They have many more applications, in particular some of the models investigated in this book were introduced and studied in neighbouring research communities such as optimisation, reinforcement learning, model theory, and set theory.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Combining Proofs for Description Logic and Concrete Domain Reasoning (Technical Report),"Logic-based approaches to AI have the advantage that their behavior can in principle be explained with the help of proofs of the computed consequences. For ontologies based on Description Logic (DL), we have put this advantage into practice by showing how proofs for consequences derived by DL reasoners can be computed and displayed in a user-friendly way. However, these methods are insufficient in applications where also numerical reasoning is relevant. The present paper considers proofs for DLs extended with concrete domains (CDs) based on the rational numbers, which leave reasoning tractable if integrated into the lightweight DL $\mathcal{E}\hspace{-0.1em}\mathcal{L}_\bot$. Since no implemented DL reasoner supports these CDs, we first develop reasoning procedures for them, and show how they can be combined with reasoning approaches for pure DLs, both for $\mathcal{E}\hspace{-0.1em}\mathcal{L}_\bot$ and the more expressive DL $\mathcal{ALC}$. These procedures are designed such that it is easy to extract proofs from them. We show how the extracted CD proofs can be combined with proofs on the DL side into integrated proofs that explain both the DL and the CD reasoning.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Syntactic Cut-Elimination for Intuitionistic Fuzzy Logic via Linear Nested Sequents,"This paper employs the linear nested sequent framework to design a new cut-free calculus LNIF for intuitionistic fuzzy logic--the first-order Gdel logic characterized by linear relational frames with constant domains. Linear nested sequents--which are nested sequents restricted to linear structures--prove to be a well-suited proof-theoretic formalism for intuitionistic fuzzy logic. We show that the calculus LNIF possesses highly desirable proof-theoretic properties such as invertibility of all rules, admissibility of structural rules, and syntactic cut-elimination.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Propositional Logics of Dependence,"In this paper, we study logics of dependence on the propositional level. We prove that several interesting propositional logics of dependence, including propositional dependence logic, propositional intuitionistic dependence logic as well as propositional inquisitive logic, are expressively complete and have disjunctive or conjunctive normal forms. We provide deduction systems and prove the completeness theorems for these logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Fuzzy Linguistic Logic Programming and its Applications,"The paper introduces fuzzy linguistic logic programming, which is a combination of fuzzy logic programming, introduced by P. Vojtas, and hedge algebras in order to facilitate the representation and reasoning on human knowledge expressed in natural languages. In fuzzy linguistic logic programming, truth values are linguistic ones, e.g., VeryTrue, VeryProbablyTrue, and LittleFalse, taken from a hedge algebra of a linguistic truth variable, and linguistic hedges (modifiers) can be used as unary connectives in formulae. This is motivated by the fact that humans reason mostly in terms of linguistic terms rather than in terms of numbers, and linguistic hedges are often used in natural languages to express different levels of emphasis. The paper presents: (i) the language of fuzzy linguistic logic programming; (ii) a declarative semantics in terms of Herbrand interpretations and models; (iii) a procedural semantics which directly manipulates linguistic terms to compute a lower bound to the truth value of a query, and proves its soundness; (iv) a fixpoint semantics of logic programs, and based on it, proves the completeness of the procedural semantics; (v) several applications of fuzzy linguistic logic programming; and (vi) an idea of implementing a system to execute fuzzy linguistic logic programs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Rules with parameters in modal logic II,"We analyze the computational complexity of admissibility and unifiability with parameters in transitive modal logics. The class of cluster-extensible (clx) logics was introduced in the first part of this series of papers. We completely classify the complexity of unifiability or inadmissibility in any clx logic as being complete for one of $^{\exp}_2$, NEXP, coNEXP, PSPACE, or $^p_2$. In addition to the main case where arbitrary parameters are allowed, we consider restricted problems with the number of parameters bounded by a constant, and the parameter-free case. Our upper bounds are specific to clx logics, but we also include similar results for logics of bounded depth and width. In contrast, our lower bounds are very general: they apply each to a class of all transitive logics whose frames allow occurrence of certain finite subframes. We also discuss the baseline problem of complexity of derivability: it is coNP-complete or PSPACE-complete for each clx logic. In particular, we prove PSPACE-hardness of derivability for a broad class of transitive logics that includes all logics with the disjunction property.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Relevant Reasoners in a Classical World,"We develop a framework for epistemic logic that combines relevant modal logic with classical propositional logic. In our framework the agent is modeled as reasoning in accordance with a relevant modal logic while the propositional fragment of our logics is classical. In order to achieve this feature, we modify the relational semantics for relevant modal logics so that validity in a model is defined as satisfaction throughout a set of designated states that, as far as propositional connectives are concerned, behave like classical possible worlds. The main technical result of the paper is a modular completeness theorem parametrized by the relevant modal logic formalizing the agent's reasoning.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Fractal Logic of Phi-adic Recursion,"Our central observation is that unbounded additive recurrence establishes a homomorphism between $\mathbb{N}$ and Modus Ponens in a constructive sense. By finding sums of nonconsecutive Fibonacci indices, each inference step corresponds to a geometric constraint whose verification requires $O(M(\log n))$ bit-operations. Logical entailment can be interpreted constructively as arc-closures under $$-scaling, offering a bridge between additive combinatorics, proof theory, and symbolic computation.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Human Conditional Reasoning in Answer Set Programming,"Given a conditional sentence ""P=>Q"" (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Differential Game Logic,"Differential game logic (dGL) is a logic for specifying and verifying properties of hybrid games, i.e. games that combine discrete, continuous, and adversarial dynamics. Unlike hybrid systems, hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives. The logic dGL can be used to study the existence of winning strategies for such hybrid games, i.e. ways of resolving the player's choices in some way so that he wins by achieving his objective for all choices of the opponent. Hybrid games are determined, i.e. from each state, one player has a winning strategy, yet computing their winning regions may take transfinitely many steps. The logic dGL, nevertheless, has a sound and complete axiomatization relative to any expressive logic. Separating axioms are identified that distinguish hybrid games from hybrid systems. Finally, dGL is proved to be strictly more expressive than the corresponding logic of hybrid systems by characterizing the expressiveness of both.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On the expressive power of inquisitive epistemic logic,"Inquisitive modal logic, InqML, in its epistemic incarnation, extends standard epistemic logic to capture not just the information that agents have, but also the questions that they are interested in. We use the natural notion of bisimulation equivalence in the setting of InqML, as introduced in [Ciardelli/Otto: JSL 2021], to characterise the expressiveness of InqML as the bisimulation invariant fragment of first-order logic over natural classes of two-sorted first-order structures that arise as relational encodings of inquisitive epistemic (S5-like) models. The non-elementary nature of these classes crucially requires non-classical model-theoretic methods for the analysis of first-order expressiveness, irrespective of whether we aim for characterisations in the sense of classical or of finite model theory.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The ubiquity of conservative translations,"We study the notion of conservative translation between logics introduced by Feitosa and D'Ottaviano. We show that classical propositional logic (CPC) is universal in the sense that every finitary consequence relation over a countable set of formulas can be conservatively translated into CPC. The translation is computable if the consequence relation is decidable. More generally, we show that one can take instead of CPC a broad class of logics (extensions of a certain fragment of full Lambek calculus FL) including most nonclassical logics studied in the literature, hence in a sense, (almost) any two reasonable deductive systems can be conservatively translated into each other. We also provide some counterexamples, in particular the paraconsistent logic LP is not universal.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
To Teach Modal Logic: An Opinionated Survey,"I aim to promote an alternative agenda for teaching modal logic chiefly inspired by the relationships between modal logic and philosophy. The guiding idea for this proposal is a reappraisal of the interest of modal logic in philosophy, which do not stem mainly from mathematical issues, but which is motivated by central problems of philosophy and language. I will point out some themes to start elaborating a guide for a more comprehensive approach to teach modal logic, and consider the contributions of dual-process theories in cognitive science, in order to explore a pedagogical framework for the proposed point of view.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Cookbook for Temporal Conceptual Data Modelling with Description Logics,"We design temporal description logics suitable for reasoning about temporal conceptual data models and investigate their computational complexity. Our formalisms are based on DL-Lite logics with three types of concept inclusions (ranging from atomic concept inclusions and disjointness to the full Booleans), as well as cardinality constraints and role inclusions. In the temporal dimension, they capture future and past temporal operators on concepts, flexible and rigid roles, the operators `always' and `some time' on roles, data assertions for particular moments of time and global concept inclusions. The logics are interpreted over the Cartesian products of object domains and the flow of time (Z,<), satisfying the constant domain assumption. We prove that the most expressive of our temporal description logics (which can capture lifespan cardinalities and either qualitative or quantitative evolution constraints) turn out to be undecidable. However, by omitting some of the temporal operators on concepts/roles or by restricting the form of concept inclusions we obtain logics whose complexity ranges between PSpace and NLogSpace. These positive results were obtained by reduction to various clausal fragments of propositional temporal logic, which opens a way to employ propositional or first-order temporal provers for reasoning about temporal data models.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Possibility Frames and Forcing for Modal Logic,"This paper develops the model theory of normal modal logics based on partial ""possibilities"" instead of total ""worlds,"" following Humberstone (1981) instead of Kripke (1963). Possibility semantics can be seen as extending to modal logic the semantics for classical logic used in weak forcing in set theory, or as semanticizing a negative translation of classical modal logic into intuitionistic modal logic. Thus, possibility frames are based on posets with accessibility relations, like intuitionistic modal frames, but with the constraint that the interpretation of every formula is a regular open set in the Alexandrov topology on the poset. The standard world frames for modal logic are the special case of possibility frames wherein the poset is discrete. We develop the beginnings of duality theory, definability/correspondence theory, and completeness theory for possibility frames.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Public Announcement Logic in HOL,"A shallow semantical embedding for public announcement logic with relativized common knowledge is presented. This embedding enables the first-time automation of this logic with off-the-shelf theorem provers for classical higher-order logic. It is demonstrated (i) how meta-theoretical studies can be automated this way, and (ii) how non-trivial reasoning in the target logic (public announcement logic), required e.g. to obtain a convincing encoding and automation of the wise men puzzle, can be realized. Key to the presented semantical embedding -- in contrast, e.g., to related work on the semantical embedding of normal modal logics -- is that evaluation domains are modeled explicitly and treated as additional parameter in the encodings of the constituents of the embedded target logic, while they were previously implicitly shared between meta logic and target logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Labeled Sequent Calculus and Countermodel Construction for Justification Logics,"Justification logics are modal-like logics that provide a framework for reasoning about justifications. This paper introduces labeled sequent calculi for justification logics, as well as for hybrid modal-justification logics. Using the method due to Sara Negri, we internalize the Kripke-style semantics of justification logics, known as Fitting models, within the syntax of the sequent calculus to produce labeled sequent calculus. We show that our labeled sequent calculi enjoy a weak subformula property, all of the rules are invertible and the structural rules (weakening and contraction) and cut are admissible. Finally soundness and completeness are established, and termination of proof search for some of the labeled systems are shown. We describe a procedure, for some of the labeled systems, which produces a derivation for valid sequents and a countermodel for non-valid sequents. We also show a model correspondence for justification logics in the context of labeled sequent calculus.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Designing Equilibria in Concurrent Games with Social Welfare and Temporal Logic Constraints,"In game theory, mechanism design is concerned with the design of incentives so that a desired outcome of the game can be achieved. In this paper, we explore the concept of equilibrium design, where incentives are designed to obtain a desirable equilibrium that satisfies a specific temporal logic property. Our study is based on a framework where system specifications are represented as temporal logic formulae, games as quantitative concurrent game structures, and players' goals as mean-payoff objectives. We consider system specifications given by LTL and GR(1) formulae, and show that designing incentives to ensure that a given temporal logic property is satisfied on some/every Nash equilibrium of the game can be achieved in PSPACE for LTL properties and in NP/P 2 for GR(1) specifications. We also examine the complexity of related decision and optimisation problems, such as optimality and uniqueness of solutions, as well as considering social welfare, and show that the complexities of these problems lie within the polynomial hierarchy. Equilibrium design can be used as an alternative solution to rational synthesis and verification problems for concurrent games with mean-payoff objectives when no solution exists or as a technique to repair concurrent games with undesirable Nash equilibria in an optimal way.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The modal logic of Reverse Mathematics,"The implication relationship between subsystems in Reverse Mathematics has an underlying logic, which can be used to deduce certain new Reverse Mathematics results from existing ones in a routine way. We use techniques of modal logic to formalize the logic of Reverse Mathematics into a system that we name s-logic. We argue that s-logic captures precisely the ""logical"" content of the implication and nonimplication relations between subsystems in Reverse Mathematics. We present a sound, complete, decidable, and compact tableau-style deductive system for s-logic, and explore in detail two fragments that are particularly relevant to Reverse Mathematics practice and automated theorem proving of Reverse Mathematics results.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Rules with parameters in modal logic I,"We study admissibility of inference rules and unification with parameters in transitive modal logics (extensions of K4), in particular we generalize various results on parameter-free admissibility and unification to the setting with parameters.   Specifically, we give a characterization of projective formulas generalizing Ghilardi's characterization in the parameter-free case, leading to new proofs of Rybakov's results that admissibility with parameters is decidable and unification is finitary for logics satisfying suitable frame extension properties (called cluster-extensible logics in this paper). We construct explicit bases of admissible rules with parameters for cluster-extensible logics, and give their semantic description. We show that in the case of finitely many parameters, these logics have independent bases of admissible rules, and determine which logics have finite bases.   As a sideline, we show that cluster-extensible logics have various nice properties: in particular, they are finitely axiomatizable, and have an exponential-size model property. We also give a rather general characterization of logics with directed (filtering) unification.   In the sequel, we will use the same machinery to investigate the computational complexity of admissibility and unification with parameters in cluster-extensible logics, and we will adapt the results to logics with unique top cluster (e.g., S4.2) and superintuitionistic logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The logic of interactive Turing reduction,"The paper gives a soundness and completeness proof for the implicative fragment of intuitionistic calculus with respect to the semantics of computability logic, which understands intuitionistic implication as interactive algorithmic reduction. This concept -- more precisely, the associated concept of reducibility -- is a generalization of Turing reducibility from the traditional, input/output sorts of problems to computational tasks of arbitrary degrees of interactivity. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Canonicity in power and modal logics of finite achronal width,"We develop a method for showing that various modal logics that are valid in their countably generated canonical Kripke frames must also be valid in their uncountably generated ones. This is applied to many systems, including the logics of finite width, and a broader class of multimodal logics of `finite achronal width' that are introduced here.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
State-based Modal Logics for Free Choice,"We study the mathematical properties of bilateral state-based modal logic (BSML), a modal logic employing state-based semantics (also known as team semantics), which has been used to account for free choice inferences and related linguistic phenomena. This logic extends classical modal logic with a nonemptiness atom which is true in a state if and only if the state is nonempty. We introduce two extensions of BSML and show that the extensions are expressively complete, and develop natural deduction axiomatizations for the three logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Determinacy of Context-Free Games,"We prove that the determinacy of Gale-Stewart games whose winning sets are accepted by real-time 1-counter Bchi automata is equivalent to the determinacy of (effective) analytic Gale-Stewart games which is known to be a large cardinal assumption. We show also that the determinacy of Wadge games between two players in charge of omega-languages accepted by 1-counter Bchi automata is equivalent to the (effective) analytic Wadge determinacy. Using some results of set theory we prove that one can effectively construct a 1-counter Bchi automaton A and a Bchi automaton B such that: (1) There exists a model of ZFC in which Player 2 has a winning strategy in the Wadge game W(L(A), L(B)); (2) There exists a model of ZFC in which the Wadge game W(L(A), L(B)) is not determined. Moreover these are the only two possibilities, i.e. there are no models of ZFC in which Player 1 has a winning strategy in the Wadge game W(L(A), L(B)).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"""Knowing value"" logic as a normal modal logic","Recent years witness a growing interest in nonstandard epistemic logics of ""knowing whether"", ""knowing what"", ""knowing how"", and so on. These logics are usually not normal, i.e., the standard axioms and reasoning rules for modal logic may be invalid. In this paper, we show that the conditional ""knowing value"" logic proposed by Wang and Fan \cite{WF13} can be viewed as a disguised normal modal logic by treating the negation of the Kv operator as a special diamond. Under this perspective, it turns out that the original first-order Kripke semantics can be greatly simplified by introducing a ternary relation $R_i^c$ in standard Kripke models, which associates one world with two $i$-accessible worlds that do not agree on the value of constant $c$. Under intuitive constraints, the modal logic based on such Kripke models is exactly the one studied by Wang and Fan (2013,2014}. Moreover, there is a very natural binary generalization of the ""knowing value"" diamond, which, surprisingly, does not increase the expressive power of the logic. The resulting logic with the binary diamond has a transparent normal modal system, which sharpens our understanding of the ""knowing value"" logic and simplifies some previously hard problems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Completeness Proof for A Regular Predicate Logic with Undefined Truth Value,"We provide a sound and complete proof system for an extension of Kleene's ternary logic to predicates. The concept of theory is extended with, for each function symbol, a formula that specifies when the function is defined. The notion of ""is defined"" is extended to terms and formulas via a straightforward recursive algorithm. The ""is defined"" formulas are constructed so that they themselves are always defined. The completeness proof relies on the Henkin construction. For each formula, precisely one of the formula, its negation, and the negation of its ""is defined"" formula is true on the constructed model. Many other ternary logics in the literature can be reduced to ours. Partial functions are ubiquitous in computer science and even in (in)equation solving at schools. Our work was motivated by an attempt to explain, precisely in terms of logic, typical informal methods of reasoning in such applications.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Kripke Models for Classical Logic,We introduce a notion of Kripke model for classical logic for which we constructively prove soundness and cut-free completeness. We discuss the novelty of the notion and its potential applications.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Statistical Epistemic Logic,"We introduce a modal logic for describing statistical knowledge, which we call statistical epistemic logic. We propose a Kripke model dealing with probability distributions and stochastic assignments, and show a stochastic semantics for the logic. To our knowledge, this is the first semantics for modal logic that can express the statistical knowledge dependent on non-deterministic inputs and the statistical significance of observed results. By using statistical epistemic logic, we express a notion of statistical secrecy with a confidence level. We also show that this logic is useful to formalize statistical hypothesis testing and differential privacy in a simple and abstract manner.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A logical basis for constructive systems,"The work is devoted to Computability Logic (CoL) -- the philosophical/mathematical platform and long-term project for redeveloping classical logic after replacing truth} by computability in its underlying semantics (see http://www.cis.upenn.edu/~giorgi/cl.html). This article elaborates some basic complexity theory for the CoL framework. Then it proves soundness and completeness for the deductive system CL12 with respect to the semantics of CoL, including the version of the latter based on polynomial time computability instead of computability-in-principle. CL12 is a sequent calculus system, where the meaning of a sequent intuitively can be characterized as ""the succedent is algorithmically reducible to the antecedent"", and where formulas are built from predicate letters, function letters, variables, constants, identity, negation, parallel and choice connectives, and blind and choice quantifiers. A case is made that CL12 is an adequate logical basis for constructive applied theories, including complexity-oriented ones.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Complete Logic for Database Abstract State Machines,"In database theory, the term $\textit{database transformation}$ was used to refer to a unifying treatment for computable queries and updates. Recently, it was shown that non-deterministic database transformations can be captured exactly by a variant of ASMs, the so-called Database Abstract State Machines (DB-ASMs). In this article we present a logic for DB-ASMs, extending the logic of Nanchen and Strk for ASMs. In particular, we develop a rigorous proof system for the logic for DB-ASMs, which is proven to be sound and complete. The most difficult challenge to be handled by the extension is a proper formalisation capturing non-determinism of database transformations and all its related features such as consistency, update sets or multisets associated with DB-ASM rules. As the database part of a state of database transformations is a finite structure and DB-ASMs are restricted by allowing quantifiers only over the database part of a state, we resolve this problem by taking update sets explicitly into the logic, i.e. by using an additional modal operator $[X]$, where $X$ is interpreted as an update set $$ generated by a DB-ASM rule. The DB-ASM logic provides a powerful verification tool to study properties of database transformations.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A logic for temporal conditionals and a solution to the Sea Battle Puzzle,"Temporal reasoning with conditionals is more complex than both classical temporal reasoning and reasoning with timeless conditionals, and can lead to some rather counter-intuitive conclusions. For instance, Aristotle's famous ""Sea Battle Tomorrow"" puzzle leads to a fatalistic conclusion: whether there will be a sea battle tomorrow or not, but that is necessarily the case now. We propose a branching-time logic LTC to formalise reasoning about temporal conditionals and provide that logic with adequate formal semantics. The logic LTC extends the Nexttime fragment of CTL*, with operators for model updates, restricting the domain to only future moments where antecedent is still possible to satisfy. We provide formal semantics for these operators that implements the restrictor interpretation of antecedents of temporalized conditionals, by suitably restricting the domain of discourse. As a motivating example, we demonstrate that a naturally formalised in our logic version of the `Sea Battle' argument renders it unsound, thereby providing a solution to the problem with fatalist conclusion that it entails, because its underlying reasoning per cases argument no longer applies when these cases are treated not as material implications but as temporal conditionals. On the technical side, we analyze the semantics of LTC and provide a series of reductions of LTC-formulae, first recursively eliminating the dynamic update operators and then the path quantifiers in such formulae. Using these reductions we obtain a sound and complete axiomatization for LTC, and reduce its decision problem to that of the modal logic KD.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Constructive Proof of Cut Elimination for a System of Full Second Order Logic,"In this paper we present a constructive proof of cut elimination for a system of full second order logic with the structural rules absorbed and using sets instead of sequences. The standard problem of the cutrank growth is avoided by using a new parameter for the induction, the cutweight. This technique can also be applied to first order logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Counting proofs in propositional logic,We give a procedure for counting the number of different proofs of a formula in various sorts of propositional logic. This number is either an integer (that may be 0 if the formula is not provable) or infinite.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Valuations in Gdel Logic, and the Euler Characteristic","Using the lattice-theoretic version of the Euler characteristic introduced by V. Klee and G.-C. Rota in the Sixties, we define the Euler characteristic of a formula in Gdel logic (over finitely or infinitely many truth-values). We then prove that the information encoded by the Euler characteristic is classical, i.e. coincides with the analogous notion defined over Boolean logic. Building on this, we define many-valued versions of the Euler characteristic of a formula $\varphi$, and prove that they indeed provide information about the logical status of $\varphi$ in Gdel logic. Specifically, our first main result shows that the many-valued Euler characteristics are invariants that separate many-valued tautologies from non-tautologies. Further, we offer an initial investigation of the linear structure of these generalised characteristics. Our second main result is that the collection of many-valued characteristics forms a linearly independent set in the real vector space of all valuations of Gdel logic over finitely many propositional variables.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Complexity of Probabilistic Justification Logic,Probabilistic justification logic is a modal logic with two kind of modalities: probability measures and explicit justification terms. We present a tableau procedure that can be used to decide the satisfiability problem for this logic in polynomial space. We show that this upper complexity bound is tight.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Survey of the Proof-Theoretic Foundations of Logic Programming,"Several formal systems, such as resolution and minimal model semantics, provide a framework for logic programming. In this paper, we will survey the use of structural proof theory as an alternative foundation. Researchers have been using this foundation for the past 35 years to elevate logic programming from its roots in first-order classical logic into higher-order versions of intuitionistic and linear logic. These more expressive logic programming languages allow for capturing stateful computations and rich forms of abstractions, including higher-order programming, modularity, and abstract data types. Term-level bindings are another kind of abstraction, and these are given an elegant and direct treatment within both proof theory and these extended logic programming languages. Logic programming has also inspired new results in proof theory, such as those involving polarity and focused proofs. These recent results provide a high-level means for presenting the differences between forward-chaining and backward-chaining style inferences. Anchoring logic programming in proof theory has also helped identify its connections and differences with functional programming, deductive databases, and model checking.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
An Improved Proof-Theoretic Compilation of Logic Programs,"In prior work, we showed that logic programming compilation can be given a proof-theoretic justification for generic abstract logic programming languages, and demonstrated this technique in the case of hereditary Harrop formulas and their linear variant. Compiled clauses were themselves logic formulas except for the presence of a second-order abstraction over the atomic goals matching their head. In this paper, we revisit our previous results into a more detailed and fully logical justification that does away with this spurious abstraction. We then refine the resulting technique to support well-moded programs efficiently.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings Third International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the Third International Symposium on Games, Automata, Logic and Formal Verification (GandALF), held in Naples (Italy) from September 6th to 8th, 2012.   GandALF was founded by a number of Italian computer scientists interested in mathematical logic, automata theory, game theory, and their applications to the specification, design, and verification of complex systems. Its aim is to provide a forum where people from different areas, and possibly with different backgrounds, can fruitfully interact. Even though the idea of the symposium emerged within the Italian research community, the event has a truly international nature, as witnessed by the composition of the conference committees and the programme.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Interdefinability of defeasible logic and logic programming under the well-founded semantics,"We provide a method of translating theories of Nute's defeasible logic into logic programs, and a corresponding translation in the opposite direction. Under certain natural restrictions, the conclusions of defeasible theories under the ambiguity propagating defeasible logic ADL correspond to those of the well-founded semantics for normal logic programs, and so it turns out that the two formalisms are closely related. Using the same translation of logic programs into defeasible theories, the semantics for the ambiguity blocking defeasible logic NDL can be seen as indirectly providing an ambiguity blocking semantics for logic programs. We also provide antimonotone operators for both ADL and NDL, each based on the Gelfond-Lifschitz (GL) operator for logic programs. For defeasible theories without defeaters or priorities on rules, the operator for ADL corresponds to the GL operator and so can be seen as partially capturing the consequences according to ADL. Similarly, the operator for NDL captures the consequences according to NDL, though in this case no restrictions on theories apply. Both operators can be used to define stable model semantics for defeasible theories.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
ECA-LP / ECA-RuleML: A Homogeneous Event-Condition-Action Logic Programming Language,"Event-driven reactive functionalities are an urgent need in nowadays distributed service-oriented applications and (Semantic) Web-based environments. An important problem to be addressed is how to correctly and efficiently capture and process the event-based behavioral, reactive logic represented as ECA rules in combination with other conditional decision logic which is represented as derivation rules. In this paper we elaborate on a homogeneous integration approach which combines derivation rules, reaction rules (ECA rules) and other rule types such as integrity constraint into the general framework of logic programming. The developed ECA-LP language provides expressive features such as ID-based updates with support for external and self-updates of the intensional and extensional knowledge, transac-tions including integrity testing and an event algebra to define and process complex events and actions based on a novel interval-based Event Calculus variant.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Datalog-Expressibility for Monadic and Guarded Second-Order Logic,"We characterise the sentences in Monadic Second-order Logic (MSO) that are over finite structures equivalent to a Datalog program, in terms of an existential pebble game. We also show that for every class C of finite structures that can be expressed in MSO and is closed under homomorphisms, and for all integers l,k, there exists a canonical Datalog program Pi of width (l,k) in the sense of Feder and Verdi. The same characterisations also hold for Guarded Second-order Logic (GSO), which properly extends MSO. To prove our results, we show that every class C in GSO whose complement is closed under homomorphisms is a finite union of constraint satisfaction problems (CSPs) of countably categorical structures. The intersection of MSO and Datalog is known to contain the class of nested monadically defined queries (Nemodeq); likewise, we show that the intersection of GSO and Datalog contains all problems that can be expressed by the more expressive language of nested guarded queries. Yet, by exploiting our results, we can show that neither of the two query languages can serve as a characterization, as we exhibit a query in the intersection of MSO and Datalog that is not expressible in nested guarded queries.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Deontic Action Logics: A Modular Algebraic Perspective,"In a seminal work, K. Segerberg introduced a deontic logic called DAL to investigate normative reasoning over actions. DAL marked the beginning of a new area of research in Deontic Logic by shifting the focus from deontic operators on propositions to deontic operators on actions. In this work, we revisit DAL and provide a complete algebraization for it. In our algebraization we introduce deontic action algebras -- algebraic structures consisting of a Boolean algebra for interpreting actions, a Boolean algebra for interpreting formulas, and two mappings from one Boolean algebra to the other interpreting the deontic concepts of permission and prohibition. We elaborate on how the framework underpinning deontic action algebras enables the derivation of different deontic action logics by removing or imposing additional conditions over either of the Boolean algebras. We leverage this flexibility to demonstrate how we can capture in this framework several logics in the DAL family. Furthermore, we introduce four variations of DAL by: (a) enriching the algebra of formulas with propositions on states, (b) adopting a Heyting algebra for state propositions, (c) adopting a Heyting algebra for actions, and (d) adopting Heyting algebras for both. We illustrate these new deontic action logics with examples and establish their algebraic completeness.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Valuations in Nilpotent Minimum Logic,"The Euler characteristic can be defined as a special kind of valuation on finite distributive lattices. This work begins with some brief consideration on the role of the Euler characteristic on NM algebras, the algebraic counterpart of Nilpotent Minimum logic. Then, we introduce a new valuation, a modified version of the Euler characteristic we call idempotent Euler characteristic. We show that the new valuation encodes information about the formul in NM propositional logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Transaction Logic with (Complex) Events,"This work deals with the problem of combining reactive features, such as the ability to respond to events and define complex events, with the execution of transactions over general Knowledge Bases (KBs).   With this as goal, we build on Transaction Logic (TR), a logic precisely designed to model and execute transactions in KBs defined by arbitrary logic theories. In it, transactions are written in a logic-programming style, by combining primitive update operations over a general KB, with the usual logic programming connectives and some additional connectives e.g. to express sequence of actions. While TR is a natural choice to deal with transactions, it remains the question whether TR can be used to express complex events, but also to deal simultaneously with the detection of complex events and the execution of transactions. In this paper we show that the former is possible while the latter is not. For that, we start by illustrating how TR can express complex events, and in particular, how SNOOP event expressions can be translated in the logic. Afterwards, we show why TR fails to deal with the two issues together, and to solve the intended problem propose Transaction Logic with Events, its syntax, model theory and executional semantics. The achieved solution is a non-monotonic extension of TR, which guarantees that every complex event detected in a transaction is necessarily responded.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On the proof complexity of logics of bounded branching,"We investigate the proof complexity of extended Frege (EF) systems for basic transitive modal logics (K4, S4, GL, ...) augmented with the bounded branching axioms $\mathbf{BB}_k$. First, we study feasibility of the disjunction property and more general extension rules in EF systems for these logics: we show that the corresponding decision problems reduce to total coNP search problems (or equivalently, disjoint NP pairs, in the binary case); more precisely, the decision problem for extension rules is equivalent to a certain special case of interpolation for the classical EF system. Next, we use this characterization to prove superpolynomial (or even exponential, with stronger hypotheses) separations between EF and substitution Frege (SF) systems for all transitive logics contained in $\mathbf{S4.2GrzBB_2}$ or $\mathbf{GL.2BB_2}$ under some assumptions weaker than $\mathrm{PSPACE \ne NP}$. We also prove analogous results for superintuitionistic logics: we characterize the decision complexity of multi-conclusion Visser's rules in EF systems for Gabbay--de Jongh logics $\mathbf T_k$, and we show conditional separations between EF and SF for all intermediate logics contained in $\mathbf{T_2 + KC}$.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Probabilistic Description Logic $\mathcal{BALC}$,"Description logics (DLs) are well-known knowledge representation formalisms focused on the representation of terminological knowledge. Due to their first-order semantics, these languages (in their classical form) are not suitable for representing and handling uncertainty. A probabilistic extension of a light-weight DL was recently proposed for dealing with certain knowledge occurring in uncertain contexts. In this paper, we continue that line of research by introducing the Bayesian extension \BALC of the propositionally closed DL \ALC. We present a tableau-based procedure for deciding consistency, and adapt it to solve other probabilistic, contextual, and general inferences in this logic. We also show that all these problems remain \ExpTime-complete, the same as reasoning in the underlying classical \ALC.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Simple Type Theory as Framework for Combining Logics,"Simple type theory is suited as framework for combining classical and non-classical logics. This claim is based on the observation that various prominent logics, including (quantified) multimodal logics and intuitionistic logics, can be elegantly  embedded in simple type theory. Furthermore, simple type theory is sufficiently expressive to model combinations of embedded logics and it has a well understood semantics. Off-the-shelf reasoning systems for simple type theory exist that can be uniformly employed for reasoning within and about combinations of logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Definite Descriptions in Intuitionist Positive Free Logic,"This paper presents rules of inference for a binary quantifier $I$ for the formalisation of sentences containing definite descriptions within intuitionist positive free logic. $I$ binds one variable and forms a formula from two formulas. $Ix[F, G]$ means `The $F$ is $G$'. The system is shown to have desirable proof-theoretic properties: it is proved that deductions in it can be brought into normal form. The discussion is rounded up by comparisons between the approach to the formalisation of definite descriptions recommended here and the more usual approach that uses a term-forming operator $$, where $xF$ means `the F'.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A New Game Equivalence and its Modal Logic,"We revisit the crucial issue of natural game equivalences, and semantics of game logics based on these. We present reasons for investigating finer concepts of game equivalence than equality of standard powers, though staying short of modal bisimulation. Concretely, we propose a more finegrained notion of equality of ""basic powers"" which record what players can force plus what they leave to others to do, a crucial feature of interaction. This notion is closer to game-theoretic strategic form, as we explain in detail, while remaining amenable to logical analysis. We determine the properties of basic powers via a new representation theorem, find a matching ""instantial neighborhood game logic"", and show how our analysis can be extended to a new game algebra and dynamic game logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings of the Seventh International Symposium on Games, Automata, Logics and Formal Verification","This volume contains the proceedings of the Seventh International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2016). The symposium took place in Catania, Italy, from the 14th to the 16th of September 2016. The proceedings of the symposium contain abstracts of the 3 invited talks and 21 full papers that were accepted after a careful evaluation for presentation at the conference. The topics of the accepted papers cover algorithmic game theory, automata theory, synthesis, formal verification, and dynamic, modal and temporal logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Games for Topological Fixpoint Logic,"Topological fixpoint logics are a family of logics that admits topological models and where   the fixpoint operators are defined with respect to the topological interpretations. Here we consider a topological fixpoint logic for relational structures based on Stone spaces,   where the fixpoint operators are interpreted via clopen sets.  We develop a game-theoretic semantics for this logic. First we introduce games characterising clopen  fixpoints of monotone operators on Stone spaces. These fixpoint games allow us to characterise the semantics for our topological fixpoint logic using a two-player graph game. Adequacy of this game is the main result of our paper. Finally, we define bisimulations for the topological structures under consideration and  use our game semantics to prove that the truth of a formula of our topological fixpoint logic is bisimulation-invariant.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Normalisation and Subformula Property for a System of Classical Logic with Tarski's Rule, and a Correction","This paper considers a formalisation of classical logic using general introduction rules and general elimination rules. It proposes a definition of `maximal formula', `segment' and `maximal segment' suitable to the system, and gives reduction procedures for them. It is then shown that deductions in the system convert into normal form, i.e. deductions that contain neither maximal formulas nor maximal segments, and that deductions in normal form satisfy the subformula property. Tarski's Rule is treated as a general introduction rule for implication. The general introduction rule for negation has a similar form. Maximal formulas with implication or negation as main operator require reduction procedures of a more intricate kind not present in normalisation for intuitionist logic. The Correction added to the end of the paper corrects an error: Theorem 2 is mistaken, and so is a corollary drawn from it as well as a corollary that was concluded by the same mistake. Luckily this does not affect the main result of the paper.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Non-Analytic Tableaux for Chellas's Conditional Logic CK and Lewis's Logic of Counterfactuals VC,"Priest has provided a simple tableau calculus for Chellas's conditional logic Ck. We provide rules which, when added to Priest's system, result in tableau calculi for Chellas's CK and Lewis's VC. Completeness of these tableaux, however, relies on the cut rule.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Contextual hypotheses and semantics of logic programs,"Logic programming has developed as a rich field, built over a logical substratum whose main constituent is a nonclassical form of negation, sometimes coexisting with classical negation. The field has seen the advent of a number of alternative semantics, with Kripke-Kleene semantics, the well-founded semantics, the stable model semantics, and the answer-set semantics standing out as the most successful. We show that all aforementioned semantics are particular cases of a generic semantics, in a framework where classical negation is the unique form of negation and where the literals in the bodies of the rules can be `marked' to indicate that they can be the targets of hypotheses. A particular semantics then amounts to choosing a particular marking scheme and choosing a particular set of hypotheses. When a literal belongs to the chosen set of hypotheses, all marked occurrences of that literal in the body of a rule are assumed to be true, whereas the occurrences of that literal that have not been marked in the body of the rule are to be derived in order to contribute to the firing of the rule. Hence the notion of hypothetical reasoning that is presented in this framework is not based on making global assumptions, but more subtly on making local, contextual assumptions, taking effect as indicated by the chosen marking scheme on the basis of the chosen set of hypotheses. Our approach offers a unified view on the various semantics proposed in logic programming, classical in that only classical negation is used, and links the semantics of logic programs to mechanisms that endow rule-based systems with the power to harness hypothetical reasoning.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Presentation of a Game Semantics for First-Order Propositional Logic,"Game semantics aim at describing the interactive behaviour of proofs by interpreting formulas as games on which proofs induce strategies. In this article, we introduce a game semantics for a fragment of first order propositional logic. One of the main difficulties that has to be faced when constructing such semantics is to make them precise by characterizing definable strategies - that is strategies which actually behave like a proof. This characterization is usually done by restricting to the model to strategies satisfying subtle combinatory conditions such as innocence, whose preservation under composition is often difficult to show. Here, we present an original methodology to achieve this task which requires to combine tools from game semantics, rewriting theory and categorical algebra. We introduce a diagrammatic presentation of definable strategies by the means of generators and relations: those strategies can be generated from a finite set of ``atomic'' strategies and that the equality between strategies generated in such a way admits a finite axiomatization. These generators satisfy laws which are a variation of bialgebras laws, thus bridging algebra and denotational semantics in a clean and unexpected way.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Least and Greatest Fixed Points in Linear Logic,"The first-order theory of MALL (multiplicative, additive linear logic) over only equalities is an interesting but weak logic since it cannot capture unbounded (infinite) behavior. Instead of accounting for unbounded behavior via the addition of the exponentials (! and ?), we add least and greatest fixed point operators. The resulting logic, which we call muMALL, satisfies two fundamental proof theoretic properties: we establish weak normalization for it, and we design a focused proof system that we prove complete. That second result provides a strong normal form for cut-free proof structures that can be used, for example, to help automate proof search. We show how these foundations can be applied to intuitionistic logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Propositional dynamic logic with Belnapian truth values,"We introduce BPDL, a combination of propositional dynamic logic PDL with the basic four-valued modal logic BK studied by Odintsov and Wansing (`Modal logics with Belnapian truth values', J. Appl. Non-Class. Log. 20, 279--301 (2010)). We modify the standard arguments based on canonical models and filtration to suit the four-valued context and prove weak completeness and decidability of BPDL.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
CP-logic: A Language of Causal Probabilistic Events and Its Relation to Logic Programming,"This papers develops a logical language for representing probabilistic causal laws. Our interest in such a language is twofold. First, it can be motivated as a fundamental study of the representation of causal knowledge. Causality has an inherent dynamic aspect, which has been studied at the semantical level by Shafer in his framework of probability trees. In such a dynamic context, where the evolution of a domain over time is considered, the idea of a causal law as something which guides this evolution is quite natural. In our formalization, a set of probabilistic causal laws can be used to represent a class of probability trees in a concise, flexible and modular way. In this way, our work extends Shafer's by offering a convenient logical representation for his semantical objects.   Second, this language also has relevance for the area of probabilistic logic programming. In particular, we prove that the formal semantics of a theory in our language can be equivalently defined as a probability distribution over the well-founded models of certain logic programs, rendering it formally quite similar to existing languages such as ICL or PRISM. Because we can motivate and explain our language in a completely self-contained way as a representation of probabilistic causal laws, this provides a new way of explaining the intuitions behind such probabilistic logic programs: we can say precisely which knowledge such a program expresses, in terms that are equally understandable by a non-logician. Moreover, we also obtain an additional piece of knowledge representation methodology for probabilistic logic programs, by showing how they can express probabilistic causal laws.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Nested Sequents for Provability Logic GLP,"We present a proof system for the provability logic GLP in the formalism of nested sequents and prove the cut elimination theorem for it. As an application, we obtain the reduction of GLP to its important fragment called J syntactically.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The intuitionistic fragment of computability logic at the propositional level,"This paper presents a soundness and completeness proof for propositional intuitionistic calculus with respect to the semantics of computability logic. The latter interprets formulas as interactive computational problems, formalized as games between a machine and its environment. Intuitionistic implication is understood as algorithmic reduction in the weakest possible -- and hence most natural -- sense, disjunction and conjunction as deterministic-choice combinations of problems (disjunction = machine's choice, conjunction = environment's choice), and ""absurd"" as a computational problem of universal strength. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Higher-order illative combinatory logic,"We show a model construction for a system of higher-order illative combinatory logic $\mathcal{I}_$, thus establishing its strong consistency. We also use a variant of this construction to provide a complete embedding of first-order intuitionistic predicate logic with second-order propositional quantifiers into the system $\mathcal{I}_0$ of Barendregt, Bunder and Dekkers, which gives a partial answer to a question posed by these authors.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Predicate Transformers and Linear Logic, yet another denotational model","In the refinement calculus, monotonic predicate transformers are used to model specifications for (imperative) programs. Together with a natural notion of simulation, they form a category enjoying many algebraic properties. We build on this structure to make predicate transformers into a de notational model of full linear logic: all the logical constructions have a natural interpretation in terms of predicate transformers (i.e. in terms of specifications). We then interpret proofs of a formula by a safety property for the corresponding specification.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
(Co)recursion in Logic Programming: Lazy vs Eager,"CoAlgebraic Logic Programming (CoALP) is a dialect of Logic Programming designed to bring a more precise compile-time and run-time analysis of termination and productivity for recursive and corecursive functions in Logic Programming. Its second goal is to introduce guarded lazy (co)recursion akin to functional theorem provers into logic programming. In this paper, we explain lazy features of CoALP, and compare them with the loop-analysis and eager execution in Coinductive Logic Programming (CoLP). We conclude by outlining the future directions in developing the guarded (co)recursion in logic programming.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents,"Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Martin Davis: An Overview of his Work in Logic, Computer Science, and Philosophy","In his autobiographic essay written in 1999, ``From logic to computer science and back'', Martin David Davis (3/8/1928--1/1/2023) indicated that he viewed himself as a logician \emph{and} a computer scientist. He expanded the essay in 2016 and expressed a new perspective through a changed title, ``My life as a logician''. He points out that logic was the unifying theme underlying his scientific career. Our paper attempts to provide a consistent vision that illuminates Davis' successive contributions leading to his landmark writings on computability, unsolvable problems, automated reasoning, as well as the history and philosophy of computing.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Equivalence and Conditional Independence in Atomic Sheaf Logic,"We propose a semantic foundation for logics for reasoning in settings that possess a distinction between equality of variables, a coarser equivalence of variables, and a notion of conditional independence between variables. We show that such relations can be modelled naturally in atomic sheaf toposes.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Logic Programming with Default, Weak and Strict Negations","This paper treats logic programming with three kinds of negation: default, weak and strict negations. A 3-valued logic model theory is discussed for logic programs with three kinds of negation. The procedure is constructed for negations so that a soundness of the procedure is guaranteed in terms of 3-valued logic model theory.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Characterising equilibrium logic and nested logic programs: Reductions and complexity,"Equilibrium logic is an approach to nonmonotonic reasoning that extends the stable-model and answer-set semantics for logic programs. In particular, it includes the general case of nested logic programs, where arbitrary Boolean combinations are permitted in heads and bodies of rules, as special kinds of theories. In this paper, we present polynomial reductions of the main reasoning tasks associated with equilibrium logic and nested logic programs into quantified propositional logic, an extension of classical propositional logic where quantifications over atomic formulas are permitted. We provide reductions not only for decision problems, but also for the central semantical concepts of equilibrium logic and nested logic programs. In particular, our encodings map a given decision problem into some formula such that the latter is valid precisely in case the former holds. The basic tasks we deal with here are the consistency problem, brave reasoning, and skeptical reasoning. Additionally, we also provide encodings for testing equivalence of theories or programs under different notions of equivalence, viz. ordinary, strong, and uniform equivalence. For all considered reasoning tasks, we analyse their computational complexity and give strict complexity bounds.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Complexity and expressivity of propositional dynamic logics with finitely many variables,"We investigate the complexity of satisfiability for finite-variable fragments of propositional dynamic logics. We consider three formalisms belonging to three representative complexity classes, broadly understood,---regular PDL, which is EXPTIME-complete, PDL with intersection, which is 2EXPTIME-complete, and PDL with parallel composition, which is undecidable. We show that, for each of these logics, the complexity of satisfiability remains unchanged even if we only allow as inputs formulas built solely out of propositional constants, i.e. without propositional variables. Moreover, we show that this is a consequence of the richness of the expressive power of variable-free fragments: for all the logics we consider, such fragments are as semantically expressive as entire logics. We conjecture that this is representative of PDL-style, as well as closely related, logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Separating the basic logics of the basic recurrences,"This paper shows that, even at the most basic level, the parallel, countable branching and uncountable branching recurrences of Computability Logic (see http://www.cis.upenn.edu/~giorgi/cl.html) validate different principles.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Integrating Belief Domains into Probabilistic Logic Programs,"Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Temporal Logic Programs with Variables,"In this note we consider the problem of introducing variables in temporal logic programs under the formalism of ""Temporal Equilibrium Logic"" (TEL), an extension of Answer Set Programming (ASP) for dealing with linear-time modal operators. To this aim, we provide a definition of a first-order version of TEL that shares the syntax of first-order Linear-time Temporal Logic (LTL) but has a different semantics, selecting some LTL models we call ""temporal stable models"". Then, we consider a subclass of theories (called ""splittable temporal logic programs"") that are close to usual logic programs but allowing a restricted use of temporal operators. In this setting, we provide a syntactic definition of ""safe variables"" that suffices to show the property of ""domain independence"" -- that is, addition of arbitrary elements in the universe does not vary the set of temporal stable models. Finally, we present a method for computing the derivable facts by constructing a non-temporal logic program with variables that is fed to a standard ASP grounder. The information provided by the grounder is then used to generate a subset of ground temporal rules which is equivalent to (and generally smaller than) the full program instantiation.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Strong Equivalence of Logic Programs with Ordered Disjunction: a Logical Perspective,"Logic Programs with Ordered Disjunction (LPODs) extend classical logic programs with the capability of expressing preferential disjunctions in the heads of program rules. The initial semantics of LPODs, although simple and quite intuitive, is not purely model-theoretic. A consequence of this is that certain properties of programs appear non-trivial to formalize in purely logical terms. An example of this state of affairs is the characterization of the notion of strong equivalence for LPODs. Although the results of Faber et al. (2008) are accurately developed, they fall short of characterizing strong equivalence of LPODs as logical equivalence in some specific logic. This comes in sharp contrast with the well-known characterization of strong equivalence for classical logic programs, which, as proved by Lifschitz et al. (2001), coincides with logical equivalence in the logic of here-and-there. In this paper we obtain a purely logical characterization of strong equivalence of LPODs as logical equivalence in a four-valued logic. Moreover, we provide a new proof of the coNP-completeness of strong equivalence for LPODs, which has an interest in its own right since it relies on the special structure of such programs. Our results are based on the recent logical semantics of LPODs introduced by Charalambidis et al. (2021), a fact which we believe indicates that this new semantics may prove to be a useful tool in the further study of LPODs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A uniform approach to logic programming semantics,"Part of the theory of logic programming and nonmonotonic reasoning concerns the study of fixed-point semantics for these paradigms. Several different semantics have been proposed during the last two decades, and some have been more successful and acknowledged than others. The rationales behind those various semantics have been manifold, depending on one's point of view, which may be that of a programmer or inspired by commonsense reasoning, and consequently the constructions which lead to these semantics are technically very diverse, and the exact relationships between them have not yet been fully understood. In this paper, we present a conceptually new method, based on level mappings, which allows to provide uniform characterizations of different semantics for logic programs. We will display our approach by giving new and uniform characterizations of some of the major semantics, more particular of the least model semantics for definite programs, of the Fitting semantics, and of the well-founded semantics. A novel characterization of the weakly perfect model semantics will also be provided.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Proceedings 11th International Symposium on Games, Automata, Logics, and Formal Verification","This volume contains the proceedings of the 11th International Symposium on Games, Automata, Logic and Formal Verification (GandALF 2020). The symposium took place as a fully online event on September 21-22, 2020. The GandALF symposium was established by a group of Italian computer scientists interested in mathematical logic, automata theory, game theory, and their applications to the specification, design, and verification of complex systems. Its aim is to provide a forum where people from different areas, and possibly with different backgrounds, can fruitfully interact. GandALF has a truly international spirit, as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Continuation-passing Style Models Complete for Intuitionistic Logic,"A class of models is presented, in the form of continuation monads polymorphic for first-order individuals, that is sound and complete for minimal intuitionistic predicate logic. The proofs of soundness and completeness are constructive and the computational content of their composition is, in particular, a $$-normalisation-by-evaluation program for simply typed lambda calculus with sum types. Although the inspiration comes from Danvy's type-directed partial evaluator for the same lambda calculus, the there essential use of delimited control operators (i.e. computational effects) is avoided. The role of polymorphism is crucial -- dropping it allows one to obtain a notion of model complete for classical predicate logic. The connection between ours and Kripke models is made through a strengthening of the Double-negation Shift schema.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"The taming of recurrences in computability logic through cirquent calculus, Part I","This paper constructs a cirquent calculus system and proves its soundness and completeness with respect to the semantics of computability logic (see http://www.cis.upenn.edu/~giorgi/cl.html). The logical vocabulary of the system consists of negation, parallel conjunction, parallel disjunction, branching recurrence, and branching corecurrence. The article is published in two parts, with (the present) Part I containing preliminaries and a soundness proof, and (the forthcoming) Part II containing a completeness proof.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Quantified Markov Logic Networks,"Markov Logic Networks (MLNs) are well-suited for expressing statistics such as ""with high probability a smoker knows another smoker"" but not for expressing statements such as ""there is a smoker who knows most other smokers"", which is necessary for modeling, e.g. influencers in social networks. To overcome this shortcoming, we study quantified MLNs which generalize MLNs by introducing statistical universal quantifiers, allowing to express also the latter type of statistics in a principled way. Our main technical contribution is to show that the standard reasoning tasks in quantified MLNs, maximum a posteriori and marginal inference, can be reduced to their respective MLN counterparts in polynomial time.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Transfer of semantics from argumentation frameworks to logic programming A preliminary report,"There are various interesting semantics' (extensions) designed for argumentation frameworks. They enable to assign a meaning, e.g., to odd-length cycles. Our main motivation is to transfer semantics' proposed by Baroni, Giacomin and Guida for argumetation frameworks with odd-length cycles to logic programs with odd-length cycles through default negation. The developed construction is even stronger. For a given logic program an argumentation framework is defined. The construction enables to transfer each semantics of the resulting argumentation framework to a semantics of the given logic program. Weak points of the construction are discussed and some future continuations of this approach are outlined.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"The taming of recurrences in computability logic through cirquent calculus, Part II","This paper constructs a cirquent calculus system and proves its soundness and completeness with respect to the semantics of computability logic (see http://www.cis.upenn.edu/~giorgi/cl.html). The logical vocabulary of the system consists of negation, parallel conjunction, parallel disjunction, branching recurrence, and branching corecurrence. The article is published in two parts, with (the previous) Part I containing preliminaries and a soundness proof, and (the present) Part II containing a completeness proof.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Modular Labelled Sequent Calculi for Abstract Separation Logics,"Abstract separation logics are a family of extensions of Hoare logic for reasoning about programs that manipulate resources such as memory locations. These logics are ""abstract"" because they are independent of any particular concrete resource model. Their assertion languages, called propositional abstract separation logics (PASLs), extend the logic of (Boolean) Bunched Implications (BBI) in various ways. In particular, these logics contain the connectives $*$ and $-\!*$, denoting the composition and extension of resources respectively.   This added expressive power comes at a price since the resulting logics are all undecidable. Given their wide applicability, even a semi-decision procedure for these logics is desirable. Although several PASLs and their relationships with BBI are discussed in the literature, the proof theory and automated reasoning for these logics were open problems solved by the conference version of this paper, which developed a modular proof theory for various PASLs using cut-free labelled sequent calculi. This paper non-trivially improves upon this previous work by giving a general framework of calculi on which any new axiom in the logic satisfying a certain form corresponds to an inference rule in our framework, and the completeness proof is generalised to consider such axioms.   Our base calculus handles Calcagno et al.'s original logic of separation algebras by adding sound rules for partial-determinism and cancellativity, while preserving cut-elimination. We then show that many important properties in separation logic, such as indivisible unit, disjointness, splittability, and cross-split, can be expressed in our general axiom form. Thus our framework offers inference rules and completeness for these properties for free. Finally, we show how our calculi reduce to calculi with global label substitutions, enabling more efficient implementation.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Programming in logic without Prolog,"Logic can be made useful for programming and for databases independently of logic programming. To be useful in this way, logic has to provide a mechanism for the definition of new functions and new relations on the basis of those given in the interpretation of a logical theory. We provide this mechanism by creating a compositional semantics on top of the classical semantics. In this approach verification of computational results relies on a correspondence between logic interpretations and a class definition in languages like Java or C++. The advantage of this approach is the combination of an expressive medium for the programmer with, in the case of C++, optimal use of computer resources.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
From rules to runs: A dynamic epistemic take on imperfect information games,"In the literature of game theory, the information sets of extensive form games have different interpretations, which may lead to confusions and paradoxical cases. We argue that the problem lies in the mix-up of two interpretations of the extensive form game structures: game rules or game runs which do not always coincide. In this paper, we try to separate and connect these two views by proposing a dynamic epistemic framework in which we can compute the runs step by step from the game rules plus the given assumptions of the players. We propose a modal logic to describe players' knowledge and its change during the plays, and provide a complete axiomatization. We also show that, under certain conditions, the mix-up of the rules and the runs is not harmful due to the structural similarity of the two.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
String diagrams for game theory,"This paper presents a monoidal category whose morphisms are games (in the sense of game theory, not game semantics) and an associated diagrammatic language. The two basic operations of a monoidal category, namely categorical composition and tensor product, correspond roughly to sequential and simultaneous composition of games. This leads to a compositional theory in which we can reason about properties of games in terms of corresponding properties of the component parts. In particular, we give a definition of Nash equilibrium which is recursive on the causal structure of the game.   The key technical idea in this paper is the use of continuation passing style for reasoning about the future consequences of players' choices, closely based on applications of selection functions in game theory. Additionally, the clean categorical foundation gives many opportunities for generalisation, for example to learning agents.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Non-iterative Modal Logics are Coalgebraic,"A modal logic is \emph{non-iterative} if it can be defined by axioms that do not nest modal operators, and \emph{rank-1} if additionally all propositional variables in axioms are in scope of a modal operator. It is known that every syntactically defined rank-1 modal logic can be equipped with a canonical coalgebraic semantics, ensuring soundness and strong completeness. In the present work, we extend this result to non-iterative modal logics, showing that every non-iterative modal logic can be equipped with a canonical coalgebraic semantics defined in terms of a copointed functor, again ensuring soundness and strong completeness via a canonical model construction. Like in the rank-1 case, the canonical coalgebraic semantics is equivalent to a neighbourhood semantics with suitable frame conditions, so the known strong completeness of non-iterative modal logics over neighbourhood semantics is implied. As an illustration of these results, we discuss deontic logics with factual detachment, which is captured by axioms that are non-iterative but not rank~1.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Representing First-Order Causal Theories by Logic Programs,"Nonmonotonic causal logic, introduced by Norman McCain and Hudson Turner, became a basis for the semantics of several expressive action languages. McCain's embedding of definite propositional causal theories into logic programming paved the way to the use of answer set solvers for answering queries about actions described in such languages. In this paper we extend this embedding to nondefinite theories and to first-order causal logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Complexity of finite-variable fragments of propositional modal logics of symmetric frames,"While finite-variable fragments of the propositional modal logic S5--complete with respect to reflexive, symmetric and transitive frames--are polynomial-time decidable, the restriction to finite-variable formulas for logics of reflexive and transitive frames yields fragments that remain ""intractable."" The role of the symmetry condition in this context has not been investigated. We show that symmetry either by itself or in combination with reflexivity produces logics that behave just like logics of reflexive and transitive frames, i.e. their finite-variable fragments remain intractable, namely PSPACE-hard. This raises the question of where exactly the borderline lies between modal logics whose finite-variable fragments are tractable and the rest.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Tableau-based decision procedure for the multi-agent epistemic logic with all coalitional operators for common and distributed knowledge,"We develop a conceptually clear, intuitive, and feasible decision procedure for testing satisfiability in the full multi-agent epistemic logic CMAEL(CD) with operators for common and distributed knowledge for all coalitions of agents mentioned in the language. To that end, we introduce Hintikka structures for CMAEL(CD) and prove that satisfiability in such structures is equivalent to satisfiability in standard models. Using that result, we design an incremental tableau-building procedure that eventually constructs a satisfying Hintikka structure for every satisfiable input set of formulae of CMAEL(CD) and closes for every unsatisfiable input set of formulae.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Blending margins: The modal logic K has nullary unification type,"We investigate properties of the formula $p \to \Box p$ in the basic modal logic K. We show that K satisfies an infinitary weaker variant of the rule of margins $\to \Box/ , \neg$, and as a consequence, we obtain various negative results about admissibility and unification in K. We describe a complete set of unifiers (i.e., substitutions making the formula provable) of $p \to \Box p$, and use it to establish that K has the worst possible unification type: nullary. In well-behaved transitive modal logics, admissibility and unification can be analyzed in terms of projective formulas, introduced by Ghilardi; in particular, projective formulas coincide for these logics with formulas that are admissibly saturated (i.e., derive all their multiple-conclusion admissible consequences) or exact (i.e., axiomatize a theory of a substitution). In contrast, we show that in K, the formula $p \to \Box p$ is admissibly saturated, but neither projective nor exact. All our results for K also apply to the basic description logic ALC.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Applications of Intuitionistic Logic in Answer Set Programming,"We present some applications of intermediate logics in the field of Answer Set Programming (ASP). A brief, but comprehensive introduction to the answer set semantics, intuitionistic and other intermediate logics is given. Some equivalence notions and their applications are discussed. Some results on intermediate logics are shown, and applied later to prove properties of answer sets. A characterization of answer sets for logic programs with nested expressions is provided in terms of intuitionistic provability, generalizing a recent result given by Pearce.   It is known that the answer set semantics for logic programs with nested expressions may select non-minimal models. Minimal models can be very important in some applications, therefore we studied them; in particular we obtain a characterization, in terms of intuitionistic logic, of answer sets which are also minimal models. We show that the logic G3 characterizes the notion of strong equivalence between programs under the semantic induced by these models. Finally we discuss possible applications and consequences of our results. They clearly state interesting links between ASP and intermediate logics, which might bring research in these two areas together.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The failure of cut-elimination in cyclic proof for first-order logic with inductive definitions,A cyclic proof system is a proof system whose proof figure is a tree with cycles. The cut-elimination in a proof system is fundamental. It is conjectured that the cut-elimination in the cyclic proof system for first-order logic with inductive definitions does not hold. This paper shows that the conjecture is correct by giving a sequent not provable without the cut rule but provable in the cyclic proof system.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
LQP: The Dynamic Logic of Quantum Information,"The main contribution of this paper is the introduction of a dynamic logic formalism for reasoning about information flow in composite quantum systems. This builds on our previous work on a complete quantum dynamic logic for single systems. Here we extend that work to a sound (but not necessarily complete) logic for composite systems, which brings together ideas from the quantum logic tradition with concepts from (dynamic) modal logic and from quantum computation. This Logic of Quantum Programs (LQP) is capable of expressing important features of quantum measurements and unitary evolutions of multi-partite states, as well as giving logical characterisations to various forms of entanglement (for example, the Bell states, the GHZ states etc.). We present a finitary syntax, a relational semantics and a sound proof system for this logic. As applications, we use our system to give formal correctness proofs for the Teleportation protocol and for a standard Quantum Secret Sharing protocol; a whole range of other quantum circuits and programs, including other well-known protocols (for example, superdense coding, entanglement swapping, logic-gate teleportation etc.), can be similarly verified using our logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Constraint Propagation for First-Order Logic and Inductive Definitions,"Constraint propagation is one of the basic forms of inference in many logic-based reasoning systems. In this paper, we investigate constraint propagation for first-order logic (FO), a suitable language to express a wide variety of constraints. We present an algorithm with polynomial-time data complexity for constraint propagation in the context of an FO theory and a finite structure. We show that constraint propagation in this manner can be represented by a datalog program and that the algorithm can be executed symbolically, i.e., independently of a structure. Next, we extend the algorithm to FO(ID), the extension of FO with inductive definitions. Finally, we discuss several applications.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Being correct is not enough: efficient verification using robust linear temporal logic,"While most approaches in formal methods address system correctness, ensuring robustness has remained a challenge. In this paper we present and study the logic rLTL which provides a means to formally reason about both correctness and robustness in system design. Furthermore, we identify a large fragment of rLTL for which the verification problem can be efficiently solved, i.e., verification can be done by using an automaton, recognizing the behaviors described by the rLTL formula $\varphi$, of size at most $\mathcal{O} \left( 3^{ |\varphi|} \right)$, where $|\varphi|$ is the length of $\varphi$. This result improves upon the previously known bound of $\mathcal{O}\left(5^{|\varphi|} \right)$ for rLTL verification and is closer to the LTL bound of $\mathcal{O}\left( 2^{|\varphi|} \right)$. The usefulness of this fragment is demonstrated by a number of case studies showing its practical significance in terms of expressiveness, the ability to describe robustness, and the fine-grained information that rLTL brings to the process of system verification. Moreover, these advantages come at a low computational overhead with respect to LTL verification.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Abstract argumentation and answer set programming: two faces of Nelson's logic,"In this work, we show that both logic programming and abstract argumentation frameworks can be interpreted in terms of Nelson's constructive logic N4. We do so by formalizing, in this logic, two principles that we call non-contradictory inference and strengthened closed world assumption: the first states that no belief can be held based on contradictory evidence while the latter forces both unknown and contradictory evidence to be regarded as false. Using these principles, both logic programming and abstract argumentation frameworks are translated into constructive logic in a modular way and using the object language. Logic programming implication and abstract argumentation supports become, in the translation, a new implication connective following the non-contradictory inference principle. Attacks are then represented by combining this new implication with strong negation. Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Fertile Steppe: Computability Logic and the decidability of one of its fragments,"The present work is devoted to Computability Logic (CoL), the young and volcanic research-project developed by Giorgi Japaridze. Our main goal is to provide the reader with a clear panoramic view of this vast new land, starting from its core knots and making our way towards the outer threads, in a somewhat three-dimensional, spacial gait. Furthermore, through the present work, we provide a tentative proof for the decidability of one of CoL's numerous axiomatisations, namely CL15. Thus, our expedition initially takes off for an aerial, perusal overview of this fertile steppe. The first chapter introduces CoL in a philosophical fashion, exposing and arguing its main key points. We then move over to unfold its semantics and syntax profiles, allowing the reader to become increasingly more familiar with this new environment. Landing on to the second chapter, we thoroughly introduce Cirquent Calculus, the new deductive system Japaridze has developed in order to axiomatise Computability Logic. Indeed, this new proof-system can also be a useful tool for many other logics. We then review each of the 17 axiomatisations found so far. The third chapter zooms-in on CL15, in order to come up with a possible solution to its open problem. We outline its soundness and completeness proofs; then provide some few deductive examples; and, finally, build a tentative proof of its decidability. Lastly, the fourth chapter focuses on the potential and actual applications of Computability Logic, both in arithmetic (clarithmetic) and in Artificial Intelligence systems (meaning knowledgebase and planning-and-action ones). We close our journey with some final remarks on the richness of this framework and, hence, the research-worthiness it entails.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Pebble-Relation Comonad in Finite Model Theory,"The pebbling comonad, introduced by Abramsky, Dawar and Wang, provides a categorical interpretation for the k-pebble games from finite model theory. The coKleisli category of the pebbling comonad specifies equivalences under different fragments and extensions of infinitary k-variable logic. Moreover, the coalgebras over this pebbling comonad characterise treewidth and correspond to tree decompositions. In this paper we introduce the pebble-relation comonad, which characterises pathwidth and whose coalgebras correspond to path decompositions. We further show that the existence of a coKleisli morphism in this comonad is equivalent to truth preservation in the restricted conjunction fragment of k-variable infinitary logic. We do this using Dalmau's pebble-relation game and an equivalent all-in-one pebble game. We then provide a similar treatment to the corresponding coKleisli isomorphisms via a bijective version of the all-in-one pebble game. Finally, we show as a consequence a new Lovsz-type theorem relating pathwidth to the restricted conjunction fragment of k-variable infinitary logic with counting quantifiers.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A comparison between two logical formalisms for rewriting,"Meseguer's rewriting logic and the rewriting logic CRWL are two well-known approaches to rewriting as logical deduction that, despite some clear similarities, were designed with different objectives. Here we study the relationships between them, both at a syntactic and at a semantic level. Even though it is not possible to establish an entailment system map between them, both can be naturally simulated in each other. Semantically, there is no embedding between the corresponding institutions. Along the way, the notions of entailment and satisfaction in Meseguer's rewriting logic are generalized. We also use the syntactic results to prove reflective properties of CRWL.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Termination Proofs for Logic Programs with Tabling,"Tabled logic programming is receiving increasing attention in the Logic Programming community. It avoids many of the shortcomings of SLD execution and provides a more flexible and often extremely efficient execution mechanism for logic programs. In particular, tabled execution of logic programs terminates more often than execution based on SLD-resolution. In this article, we introduce two notions of universal termination of logic programming with Tabling: quasi-termination and (the stronger notion of) LG-termination. We present sufficient conditions for these two notions of termination, namely quasi-acceptability and LG-acceptability, and we show that these conditions are also necessary in case the tabling is well-chosen. Starting from these conditions, we give modular termination proofs, i.e., proofs capable of combining termination proofs of separate programs to obtain termination proofs of combined programs. Finally, in the presence of mode information, we state sufficient conditions which form the basis for automatically proving termination in a constraint-based way.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Bimodal logics with a `weakly connected' component without the finite model property,"There are two known general results on the finite model property (fmp) of commutators [L,L'] (bimodal logics with commuting and confluent modalities). If L is finitely axiomatisable by modal formulas having universal Horn first-order correspondents, then both [L,K] and [L,S5] are determined by classes of frames that admit filtration, and so have the fmp. On the negative side, if both L and L' are determined by transitive frames and have frames of arbitrarily large depth, then [L,L'] does not have the fmp. In this paper we show that commutators with a `weakly connected' component often lack the fmp. Our results imply that the above positive result does not generalise to universally axiomatisable component logics, and even commutators without `transitive' components such as [K.3,K] can lack the fmp. We also generalise the above negative result to cases where one of the component logics has frames of depth one only, such as [S4.3,S5] and the decidable product logic S4.3xS5. We also show cases when already half of commutativity is enough to force infinite frames.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reasoning about proof and knowledge,"In previous work [Lewitzka, Log. J. IGPL 2017], we presented a hierarchy of classical modal systems, along with algebraic semantics, for the reasoning about intuitionistic truth, belief and knowledge. Deviating from Gdel's interpretation of IPC in S4, our modal systems contain IPC in the way established in [Lewitzka, J. Log. Comp. 2015]. The modal operator can be viewed as a predicate for intuitionistic truth, i.e. proof. Epistemic principles are partially adopted from Intuitionistic Epistemic Logic IEL [Artemov and Protopopescu, Rev. Symb. Log. 2016]. In the present paper, we show that the S5-style systems of our hierarchy correspond to an extended Brouwer-Heyting-Kolmogorov interpretation and are complete w.r.t. a relational semantics based on intuitionistic general frames. In this sense, our S5-style logics are adequate and complete systems for the reasoning about proof combined with belief or knowledge. The proposed relational semantics is a uniform framework in which also IEL can be modeled. Verification-based intuitionistic knowledge formalized in IEL turns out to be a special case of the kind of knowledge described by our S5-style systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"A Mixed Linear and Graded Logic: Proofs, Terms, and Models (with appendices)","Graded modal logics generalise standard modal logics via families of modalities indexed by an algebraic structure whose operations mediate between the different modalities. The graded ""of-course"" modality $!_r$ captures how many times a proposition is used and has an analogous interpretation to the of-course modality from linear logic; the of-course modality from linear logic can be modelled by a linear exponential comonad and graded of-course can be modelled by a graded linear exponential comonad. Benton showed in his seminal paper on Linear/Non-Linear logic that the of-course modality can be split into two modalities connecting intuitionistic logic with linear logic, forming a symmetric monoidal adjunction. Later, Fujii et al. demonstrated that every graded comonad can be decomposed into an adjunction and a `strict action'. We give a similar result to Benton, leveraging Fujii et al.'s decomposition, showing that graded modalities can be split into two modalities connecting a graded logic with a graded linear logic. We propose a sequent calculus, its proof theory and categorical model, and a natural deduction system which we show is isomorphic to the sequent calculus system. Interestingly, our system can also be understood as Linear/Non-Linear logic composed with an action that adds the grading, further illuminating the shared principles between linear logic and a class of graded modal logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Classical and Intuitionistic Subexponential Logics are Equally Expressive,"It is standard to regard the intuitionistic restriction of a classical logic as increasing the expressivity of the logic because the classical logic can be adequately represented in the intuitionistic logic by double-negation, while the other direction has no truth-preserving propositional encodings. We show here that subexponential logic, which is a family of substructural refinements of classical logic, each parametric over a preorder over the subexponential connectives, does not suffer from this asymmetry if the preorder is systematically modified as part of the encoding. Precisely, we show a bijection between synthetic (i.e., focused) partial sequent derivations modulo a given encoding. Particular instances of our encoding for particular subexponential preorders give rise to both known and novel adequacy theorems for substructural logics.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Proof complexity of intuitionistic implicational formulas,"We study implicational formulas in the context of proof complexity of intuitionistic propositional logic (IPC). On the one hand, we give an efficient transformation of tautologies to implicational tautologies that preserves the lengths of intuitionistic extended Frege (EF) or substitution Frege (SF) proofs up to a polynomial. On the other hand, EF proofs in the implicational fragment of IPC polynomially simulate full intuitionistic logic for implicational tautologies. The results also apply to other fragments of other superintuitionistic logics under certain conditions.   In particular, the exponential lower bounds on the length of intuitionistic EF proofs by Hrube \cite{hru:lbint}, generalized to exponential separation between EF and SF systems in superintuitionistic logics of unbounded branching by Jebek \cite{ej:sfef}, can be realized by implicational tautologies.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Cut Elimination for a Logic with Induction and Co-induction,"Proof search has been used to specify a wide range of computation systems. In order to build a framework for reasoning about such specifications, we make use of a sequent calculus involving induction and co-induction. These proof principles are based on a proof theoretic (rather than set-theoretic) notion of definition. Definitions are akin to logic programs, where the left and right rules for defined atoms allow one to view theories as ""closed"" or defining fixed points. The use of definitions and free equality makes it possible to reason intentionally about syntax. We add in a consistent way rules for pre and post fixed points, thus allowing the user to reason inductively and co-inductively about properties of computational system making full use of higher-order abstract syntax. Consistency is guaranteed via cut-elimination, where we give the first, to our knowledge, cut-elimination procedure in the presence of general inductive and co-inductive definitions.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
selp: A Single-Shot Epistemic Logic Program Solver,"Epistemic Logic Programs (ELPs) are an extension of Answer Set Programming (ASP) with epistemic operators that allow for a form of meta-reasoning, that is, reasoning over multiple possible worlds. Existing ELP solving approaches generally rely on making multiple calls to an ASP solver in order to evaluate the ELP. However, in this paper, we show that there also exists a direct translation from ELPs into non-ground ASP with bounded arity. The resulting ASP program can thus be solved in a single shot. We then implement this encoding method, using recently proposed techniques to handle large, non-ground ASP rules, into the prototype ELP solving system ""selp"", which we present in this paper. This solver exhibits competitive performance on a set of ELP benchmark instances. Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Provenance Analysis for Logic and Games,"A model checking computation checks whether a given logical sentence is true in a given finite structure. Provenance analysis abstracts from such a computation mathematical information on how the result depends on the atomic data that describe the structure. In database theory, provenance analysis by interpretations in commutative semirings has been rather succesful for positive query languages (such a unions of conjunctive queries, positive relational algebra, or datalog). However, it did not really offer an adequate treatment of negation or missing information.   Here we propose a new approach for the provenance analysis of logics with negation, such as first-order logic and fixed-point logics. It is closely related to a provenance analysis of the associated model-checking games, and based on new semirings of dual-indeterminate polynomials or dual-indeterminate formal power series. These are obtained by taking quotients of traditional provenance semirings by congruences that are generated by products of positive and negative provenance tokens. Beyond the use for model-checking problems in logics, provenance analysis of games is of independent interest. Provenance values in games provide detailed information about the number and properties of the strategies of the players, far beyond the question whether or not a player has a winning strategy from a given position.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Strong completeness of modal logics over 0-dimensional metric spaces,"We prove strong completeness results for some modal logics with the universal modality, with respect to their topological semantics over 0-dimensional dense-in-themselves metric spaces. We also use failure of compactness to show that, for some languages and spaces, no standard modal deductive system is strongly complete.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
An interactive semantics of logic programming,"We apply to logic programming some recently emerging ideas from the field of reduction-based communicating systems, with the aim of giving evidence of the hidden interactions and the coordination mechanisms that rule the operational machinery of such a programming paradigm. The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. Moreover, the compositionality of the tile semantics is guaranteed by standard results, as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property. An extension of the approach for handling constraint systems is also discussed.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Two Treatments of Definite Descriptions in Intuitionist Negative Free Logic,"Sentences containing definite descriptions, expressions of the form `The $F$', can be formalised using a binary quantifier $$ that forms a formula out of two predicates, where $x[F, G]$ is read as `The $F$ is $G$'. This is an innovation over the usual formalisation of definite descriptions with a term forming operator. The present paper compares the two approaches. After a brief overview of the system $\mathbf{INF}^$ of intuitionist negative free logic extended by such a quantifier, which was presented in \citep{kurbisiotaI}, $\mathbf{INF}^$ is first compared to a system of Tennant's and an axiomatic treatment of a term forming $$ operator within intuitionist negative free logic. Both systems are shown to be equivalent to the subsystem of $\mathbf{INF}^$ in which the $G$ of $x[F, G]$ is restricted to identity. $\mathbf{INF}^$ is then compared to an intuitionist version of a system of Lambert's which in addition to the term forming operator has an operator for predicate abstraction for indicating scope distinctions. The two systems will be shown to be equivalent through a translation between their respective languages. Advantages of the present approach over the alternatives are indicated in the discussion.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logical reduction of relations: from relational databases to Peirce's reduction thesis,"We study logical reduction (factorization) of relations into relations of lower arity by Boolean or relative products that come from applying conjunctions and existential quantifiers to predicates, i.e. by primitive positive formulas of predicate calculus. Our algebraic framework unifies natural joins and data dependencies of database theory and relational algebra of clone theory with the bond algebra of C.S. Peirce. We also offer new constructions of reductions, systematically study irreducible relations and reductions to them, and introduce a new characteristic of relations, ternarity, that measures their `complexity of relating' and allows to refine reduction results. In particular, we refine Peirce's controversial reduction thesis, and show that reducibility behavior is dramatically different on finite and infinite domains.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Abductive Reasoning in a Paraconsistent Framework,"We explore the problem of explaining observations starting from a classically inconsistent theory by adopting a paraconsistent framework. We consider two expansions of the well-known Belnap--Dunn paraconsistent four-valued logic $\mathsf{BD}$: $\mathsf{BD}_\circ$ introduces formulas of the form $\circ$ (the information on $$ is reliable), while $\mathsf{BD}_\triangle$ augments the language with $\triangle$'s (there is information that $$ is true). We define and motivate the notions of abduction problems and explanations in $\mathsf{BD}_\circ$ and $\mathsf{BD}_\triangle$ and show that they are not reducible to one another. We analyse the complexity of standard abductive reasoning tasks (solution recognition, solution existence, and relevance / necessity of hypotheses) in both logics. Finally, we show how to reduce abduction in $\mathsf{BD}_\circ$ and $\mathsf{BD}_\triangle$ to abduction in classical propositional logic, thereby enabling the reuse of existing abductive reasoning procedures.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Ceteris Paribus Structure in Logics of Game Forms,"The article introduces a ceteris paribus modal logic interpreted on the equivalence classes induced by sets of propositional atoms. This logic is used to embed two logics of agency and games, namely atemporal STIT and the coalition logic of propositional control (CL-PC). The embeddings highlight a common ceteris paribus structure underpinning the key modal operators of both logics, they clarify the relationship between STIT and CL-PC, and enable the transfer of complexity results to the ceteris paribus logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Game Theoretical Semantics for Logics of Nonsense,"Logics of non-sense allow a third truth value to express propositions that are \emph{nonsense}. These logics are ideal formalisms to understand how errors are handled in programs and how they propagate throughout the programs once they appear. In this paper, we give a Hintikkan game semantics for logics of non-sense and prove its correctness. We also discuss how a known solution method in game theory, the iterated elimination of strictly dominated strategies, relates to semantic games for logics of nonsense. Finally, we extend the logics of nonsense only by means of semantic games, developing a new logic of nonsense, and propose a new game semantics for Priest's Logic of Paradox.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On Modular Termination Proofs of General Logic Programs,"We propose a modular method for proving termination of general logic programs (i.e., logic programs with negation). It is based on the notion of acceptable programs, but it allows us to prove termination in a truly modular way. We consider programs consisting of a hierarchy of modules and supply a general result for proving termination by dealing with each module separately. For programs which are in a certain sense well-behaved, namely well-moded or well-typed programs, we derive both a simple verification technique and an iterative proof method. Some examples show how our system allows for greatly simplified proofs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Preferred Answer Sets for Ordered Logic Programs,"We extend answer set semantics to deal with inconsistent programs (containing classical negation), by finding a ``best'' answer set. Within the context of inconsistent programs, it is natural to have a partial order on rules, representing a preference for satisfying certain rules, possibly at the cost of violating less important ones. We show that such a rule order induces a natural order on extended answer sets, the minimal elements of which we call preferred answer sets. We characterize the expressiveness of the resulting semantics and show that it can simulate negation as failure, disjunction and some other formalisms such as logic programs with ordered disjunction. The approach is shown to be useful in several application areas, e.g. repairing database, where minimal repairs correspond to preferred answer sets.   To appear in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Symbolic Specialization of Rewriting Logic Theories with Presto,"This paper introduces Presto, a symbolic partial evaluator for Maude's rewriting logic theories that can improve system analysis and verification. In Presto, the automated optimization of a conditional rewrite theory R (whose rules define the concurrent transitions of a system) is achieved by partially evaluating, with respect to the rules of R, an underlying, companion equational logic theory E that specifies the algebraic structure of the system states of R. This can be particularly useful for specializing an overly general equational theory E whose operators may obey complex combinations of associativity, commutativity, and/or identity axioms, when being plugged into a host rewrite theory R as happens, for instance, in protocol analysis, where sophisticated equational theories for cryptography are used. Presto implements different unfolding operators that are based on folding variant narrowing (the symbolic engine of Maude's equational theories). When combined with an appropriate abstraction algorithm, they allow the specialization to be adapted to the theory termination behavior and bring significant improvement while ensuring strong correctness and termination of the specialization. We demonstrate the effectiveness of Presto in several examples of protocol analysis where it achieves a significant speed-up. Actually, the transformation provided by Presto may cut down an infinite folding variant narrowing space to a finite one, and moreover, some of the costly algebraic axioms and rule conditions may be eliminated as well. As far as we know, this is the first partial evaluator for Maude that respects the semantics of functional, logic, concurrent, and object-oriented computations. Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Tableau Proof Systems for Justification Logics,"In this paper we present tableau proof systems for various justification logics. We show that the tableau systems are sound and complete with respect to Mkrtychev models. In order to prove the completeness of the tableaux, we give a syntactic proof of cut elimination. We also show the subformula property for our tableaux.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Extended Lambek calculi and first-order linear logic,"First-order multiplicative intuitionistic linear logic (MILL1) can be seen as an extension of the Lambek calculus. In addition to the fragment of MILL1 which corresponds to the Lambek calculus (of Moot & Piazza 2001), I will show fragments of MILL1 which generate the multiple context-free languages and which correspond to the Displacement calculus of Morrilll e.a.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Integrating Cardinality Constraints into Constraint Logic Programming with Sets,"Formal reasoning about finite sets and cardinality is an important tool for many applications, including software verification, where very often one needs to reason about the size of a given data structure and not only about what its elements are. The Constraint Logic Programming tool {log} provides a decision procedure for deciding the satisfiability of formulas involving very general forms of finite sets, without cardinality. In this paper we adapt and integrate a decision procedure for a theory of finite sets with cardinality into {log}. The proposed solver is proved to be a decision procedure for its formulas. Besides, the new CLP instance is implemented as part of the {log} tool. In turn, the implementation uses Howe and King's Prolog SAT solver and Prolog's CLP(Q) library, as an integer linear programming solver. The empirical evaluation of this implementation based on +250 real verification conditions shows that it can be useful in practice.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Expressing Properties in Second and Third Order Logic: Hypercube Graphs and SATQBF,"It follows from the famous Fagin's theorem that all problems in NP are expressible in existential second-order logic (ESO), and vice versa. Indeed, there are well-known ESO characterizations of NP-complete problems such as 3-colorability, Hamiltonicity and clique. Furthermore, the ESO sentences that characterize those problems are simple and elegant. However, there are also NP problems that do not seem to possess equally simple and elegant ESO characterizations. In this work, we are mainly interested in this latter class of problems. In particular, we characterize in second-order logic the class of hypercube graphs and the classes SATQBF_k of satisfiable quantified Boolean formulae with k alternations of quantifiers. We also provide detailed descriptions of the strategies followed to obtain the corresponding nontrivial second-order sentences. Finally, we sketch a third-order logic sentence that defines the class SATQBF = \bigcup_{k \geq 1} SATQBF_k. The sub-formulae used in the construction of these complex second- and third-order logic sentences, are good candidates to form part of a library of formulae. Same as libraries of frequently used functions simplify the writing of complex computer programs, a library of formulae could potentially simplify the writing of complex second- and third-order queries, minimizing the probability of error.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Convergence, Continuity and Recurrence in Dynamic Epistemic Logic","The paper analyzes dynamic epistemic logic from a topological perspective. The main contribution consists of a framework in which dynamic epistemic logic satisfies the requirements for being a topological dynamical system thus interfacing discrete dynamic logics with continuous mappings of dynamical systems. The setting is based on a notion of logical convergence, demonstratively equivalent with convergence in Stone topology. Presented is a flexible, parametrized family of metrics inducing the latter, used as an analytical aid. We show maps induced by action model transformations continuous with respect to the Stone topology and present results on the recurrent behavior of said maps.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Computing with Logic as Operator Elimination: The ToyElim System,"A prototype system is described whose core functionality is, based on propositional logic, the elimination of second-order operators, such as Boolean quantifiers and operators for projection, forgetting and circumscription. This approach allows to express many representational and computational tasks in knowledge representation - for example computation of abductive explanations and models with respect to logic programming semantics - in a uniform operational system, backed by a uniform classical semantic framework.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reasoning on $\textit{DL-Lite}_{\cal R}$ with Defeasibility in ASP,"Reasoning on defeasible knowledge is a topic of interest in the area of description logics, as it is related to the need of representing exceptional instances in knowledge bases. In this direction, in our previous works we presented a framework for representing (contextualized) OWL RL knowledge bases with a notion of justified exceptions on defeasible axioms: reasoning in such framework is realized by a translation into ASP programs. The resulting reasoning process for OWL RL, however, introduces a complex encoding in order to capture reasoning on the negative information needed for reasoning on exceptions. In this paper, we apply the justified exception approach to knowledge bases in $\textit{DL-Lite}_{\cal R}$, i.e., the language underlying OWL QL. We provide a definition for $\textit{DL-Lite}_{\cal R}$ knowledge bases with defeasible axioms and study their semantic and computational properties. In particular, we study the effects of exceptions over unnamed individuals. The limited form of $\textit{DL-Lite}_{\cal R}$ axioms allows us to formulate a simpler ASP encoding, where reasoning on negative information is managed by direct rules. The resulting materialization method gives rise to a complete reasoning procedure for instance checking in $\textit{DL-Lite}_{\cal R}$ with defeasible axioms. Under consideration in Theory and Practice of Logic Programming (TPLP).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Existential Notation3 Logic,"In this paper, we delve into Notation3 Logic (N3), an extension of RDF, which empowers users to craft rules introducing fresh blank nodes to RDF graphs. This capability is pivotal in various applications such as ontology mapping, given the ubiquitous presence of blank nodes directly or in auxiliary constructs across the Web. However, the availability of fast N3 reasoners fully supporting blank node introduction remains limited. Conversely, engines like VLog or Nemo, though not explicitly designed for Semantic Web rule formats, cater to analogous constructs, namely existential rules.   We investigate the correlation between N3 rules featuring blank nodes in their heads and existential rules. We pinpoint a subset of N3 that seamlessly translates to existential rules and establish a mapping preserving the equivalence of N3 formulae. To showcase the potential benefits of this translation in N3 reasoning, we implement this mapping and compare the performance of N3 reasoners like EYE and cwm against VLog and Nemo, both on native N3 rules and their translated counterparts. Our findings reveal that existential rule reasoners excel in scenarios with abundant facts, while the EYE reasoner demonstrates exceptional speed in managing a high volume of dependent rules.   Additionally to the original conference version of this paper, we include all proofs of the theorems and introduce a new section dedicated to N3 lists featuring built-in functions and how they are implemented in existential rules. Adding lists to our translation/framework gives interesting insights on related design decisions influencing the standardization of N3.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Cut-free Completeness for Modular Hypersequent Calculi for Modal Logics K, T, and D","We investigate a recent proposal for modal hypersequent calculi. The interpretation of relational hypersequents incorporates an accessibility relation along the hypersequent. These systems give the same interpretation of hypersequents as Lellman's linear nested sequents, but were developed independently by Restall for S5 and extended to other normal modal logics by Parisi. The resulting systems obey Dosen's principle: the modal rules are the same across different modal logics. Different modal systems only differ in the presence or absence of external structural rules. With the exception of S5, the systems are modular in the sense that different structural rules capture different properties of the accessibility relation. We provide the first direct semantical cut-free completeness proofs for K, T, and D, and show how this method fails in the case of B and S4.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Observing Interventions: A logic for thinking about experiments,"This paper makes a first step towards a logic of learning from experiments. For this, we investigate formal frameworks for modeling the interaction of causal and (qualitative) epistemic reasoning. Crucial for our approach is the idea that the notion of an intervention can be used as a formal expression of a (real or hypothetical) experiment. In a first step we extend the well-known causal models with a simple Hintikka-style representation of the epistemic state of an agent. In the resulting setting, one can talk not only about the knowledge of an agent about the values of variables and how interventions affect them, but also about knowledge update. The resulting logic can model reasoning about thought experiments. However, it is unable to account for learning from experiments, which is clearly brought out by the fact that it validates the no learning principle for interventions. Therefore, in a second step, we implement a more complex notion of knowledge that allows an agent to observe (measure) certain variables when an experiment is carried out. This extended system does allow for learning from experiments. For all the proposed logical systems, we provide a sound and complete axiomatization.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Monadic Second Order Logic with Measure and Category Quantifiers,"We investigate the extension of Monadic Second Order logic, interpreted over infinite words and trees, with generalized ""for almost all"" quantifiers interpreted using the notions of Baire category and Lebesgue measure.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Complexity of the Infinitary Lambek Calculus with Kleene Star,"We consider the Lambek calculus, or non-commutative multiplicative intuitionistic linear logic, extended with iteration, or Kleene star, axiomatised by means of an $$-rule, and prove that the derivability problem in this calculus is $_1^0$-hard. This solves a problem left open by Buszkowski (2007), who obtained the same complexity bound for infinitary action logic, which additionally includes additive conjunction and disjunction. As a by-product, we prove that any context-free language without the empty word can be generated by a Lambek grammar with unique type assignment, without Lambek's non-emptiness restriction imposed (cf. Safiullin 2007).","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
TWAM: A Certifying Abstract Machine for Logic Programs,Type-preserving (or typed) compilation uses typing derivations to certify correctness properties of compilation. We have designed and implemented a type-preserving compiler for a simply-typed dialect of Prolog we call T-Prolog. The crux of our approach is a new certifying abstract machine which we call the Typed Warren Abstract Machine (TWAM). The TWAM has a dependent type system strong enough to specify the semantics of a logic program in the logical framework LF. We present a soundness metatheorem which constitutes a partial correctness guarantee: well-typed programs implement the logic program specified by their type. This metatheorem justifies our design and implementation of a certifying compiler from T-Prolog to TWAM.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Temporalized logics and automata for time granularity,"Suitable extensions of the monadic second-order theory of k successors have been proposed in the literature to capture the notion of time granularity. In this paper, we provide the monadic second-order theories of downward unbounded layered structures, which are infinitely refinable structures consisting of a coarsest domain and an infinite number of finer and finer domains, and of upward unbounded layered structures, which consist of a finest domain and an infinite number of coarser and coarser domains, with expressively complete and elementarily decidable temporal logic counterparts.   We obtain such a result in two steps. First, we define a new class of combined automata, called temporalized automata, which can be proved to be the automata-theoretic counterpart of temporalized logics, and show that relevant properties, such as closure under Boolean operations, decidability, and expressive equivalence with respect to temporal logics, transfer from component automata to temporalized ones. Then, we exploit the correspondence between temporalized logics and automata to reduce the task of finding the temporal logic counterparts of the given theories of time granularity to the easier one of finding temporalized automata counterparts of them.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Characterizations of Stable Model Semantics for Logic Programs with Arbitrary Constraint Atoms,"This paper studies the stable model semantics of logic programs with (abstract) constraint atoms and their properties. We introduce a succinct abstract representation of these constraint atoms in which a constraint atom is represented compactly. We show two applications. First, under this representation of constraint atoms, we generalize the Gelfond-Lifschitz transformation and apply it to define stable models (also called answer sets) for logic programs with arbitrary constraint atoms. The resulting semantics turns out to coincide with the one defined by Son et al., which is based on a fixpoint approach. One advantage of our approach is that it can be applied, in a natural way, to define stable models for disjunctive logic programs with constraint atoms, which may appear in the disjunctive head as well as in the body of a rule. As a result, our approach to the stable model semantics for logic programs with constraint atoms generalizes a number of previous approaches. Second, we show that our abstract representation of constraint atoms provides a means to characterize dependencies of atoms in a program with constraint atoms, so that some standard characterizations and properties relying on these dependencies in the past for logic programs with ordinary atoms can be extended to logic programs with constraint atoms.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logic programming: laxness and saturation,"A propositional logic program $P$ may be identified with a $P_fP_f$-coalgebra on the set of atomic propositions in the program. The corresponding $C(P_fP_f)$-coalgebra, where $C(P_fP_f)$ is the cofree comonad on $P_fP_f$, describes derivations by resolution. That correspondence has been developed to model first-order programs in two ways, with lax semantics and saturated semantics, based on locally ordered categories and right Kan extensions respectively. We unify the two approaches, exhibiting them as complementary rather than competing, reflecting the theorem-proving and proof-search aspects of logic programming. While maintaining that unity, we further refine lax semantics to give finitary models of logic programs with existential variables, and to develop a precise semantic relationship between variables in logic programming and worlds in local state.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Some Common Mistakes in the Teaching and Textbooks of Modal Logic,"We discuss four common mistakes in the teaching and textbooks of modal logic. The first one is missing the axiom $\Diamond\varphi\leftrightarrow\neg\Box\neg\varphi$, when choosing $\Diamond$ as the primitive modal operator, misunderstanding that $\Box$ and $\Diamond$ are symmetric. The second one is forgetting to make the set of formulas for filtration closed under subformulas, when proving the finite model property through filtration, neglecting that $\Box\varphi$ and $\Diamond\varphi$ may be abbreviations of formulas. The third one is giving wrong definitions of canonical relations in minimal canonical models that are unmatched with the primitive modal operators. The final one is misunderstanding the rule of necessitation, without knowing its distinction from the rule of modus ponens. To better understand the rule of necessitation, we summarize six ways of defining deductive consequence in modal logic: omitted definition, classical definition, ternary definition, reduced definition, bounded definition, and deflationary definition, and show that the last three definitions are equivalent to each other.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Binary Quantifier for Definite Descriptions in Intuitionist Negative Free Logic: Natural Deduction and Normalisation,"This paper presents a way of formalising definite descriptions with a binary quantifier $$, where $x[F, G]$ is read as `The $F$ is $G$'. Introduction and elimination rules for $$ in a system of intuitionist negative free logic are formulated. Procedures for removing maximal formulas of the form $x[F, G]$ are given, and it is shown that deductions in the system can be brought into normal form.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Game for Counting Logic Formula Size and an Application to Linear Orders,"Ehrenfeucht-Frass (EF) games are a basic tool in finite model theory for proving definability lower bounds, with many applications in complexity theory and related areas. They have been applied to study various logics, giving insights on quantifier rank and other logical complexity measures. In this paper, we present an EF game to capture formula size in counting logic with a bounded number of variables. The game combines games introduced previously for counting logic quantifier rank due to Immerman and Lander, and for first-order formula size due to Adler and Immerman, and Hella and Vnnen. The game is used to prove the main result of the paper, an extension of a formula size lower bound of Grohe and Schweikardt for distinguishing linear orders, from 3-variable first-order logic to 3-variable counting logic. As far as we know, this is the first formula size lower bound for counting logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Automating Reasoning with Standpoint Logic via Nested Sequents,"Standpoint logic is a recently proposed formalism in the context of knowledge integration, which advocates a multi-perspective approach permitting reasoning with a selection of diverse and possibly conflicting standpoints rather than forcing their unification. In this paper, we introduce nested sequent calculi for propositional standpoint logics--proof systems that manipulate trees whose nodes are multisets of formulae--and show how to automate standpoint reasoning by means of non-deterministic proof-search algorithms. To obtain worst-case complexity-optimal proof-search, we introduce a novel technique in the context of nested sequents, referred to as ""coloring,"" which consists of taking a formula as input, guessing a certain coloring of its subformulae, and then running proof-search in a nested sequent calculus on the colored input. Our technique lets us decide the validity of standpoint formulae in CoNP since proof-search only produces a partial proof relative to each permitted coloring of the input. We show how all partial proofs can be fused together to construct a complete proof when the input is valid, and how certain partial proofs can be transformed into a counter-model when the input is invalid. These ""certificates"" (i.e. proofs and counter-models) serve as explanations of the (in)validity of the input.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
PDL as a Multi-Agent Strategy Logic,"Propositional Dynamic Logic or PDL was invented as a logic for reasoning about regular programming constructs. We propose a new perspective on PDL as a multi-agent strategic logic (MASL). This logic for strategic reasoning has group strategies as first class citizens, and brings game logic closer to standard modal logic. We demonstrate that MASL can express key notions of game theory, social choice theory and voting theory in a natural way, we give a sound and complete proof system for MASL, and we show that MASL encodes coalition logic. Next, we extend the language to epistemic multi-agent strategic logic (EMASL), we give examples of what it can express, we propose to use it for posing new questions in epistemic social choice theory, and we give a calculus for reasoning about a natural class of epistemic game models. We end by listing avenues for future research and by tracing connections to a number of other logics for reasoning about strategies.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Parameterized Dynamic Logic -- Towards A Cyclic Logical Framework for General Program Specification and Verification,"We present a theory of parameterized dynamic logic, namely DLp, for specifying and reasoning about a rich set of program models based on their transitional behaviours. Different from most dynamic logics that deal with regular expressions or a particular type of formalisms, DLp introduces a type of labels called ""program configurations"" as explicit program status for symbolic executions, allowing programs and formulas to be of arbitrary forms according to interested domains. This characteristic empowers dynamic logical formulas with a direct support of symbolic-execution-based reasoning, while still maintaining reasoning based on syntactic structures in traditional dynamic logics through a rule-lifting process. We propose a proof system and build a cyclic preproof structure special for DLp, which guarantees the soundness of infinite proof trees induced by symbolically executing programs with explicit/implicit loop structures. The soundness of DLp is formally analyzed and proved. DLp provides a flexible verification framework based on the theories of dynamic logics. It helps reduce the burden of developing different dynamic-logic theories for different programs, and save the additional transformations in the derivations of non-compositional programs. We give some examples of instantiations of DLp in particular domains, showing the potential and advantages of using DLp in practical usage.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Theory of Hypergames on Graphs for Synthesizing Dynamic Cyber Defense with Deception,"In this chapter, we present an approach using formal methods to synthesize reactive defense strategy in a cyber network, equipped with a set of decoy systems. We first generalize formal graphical security models--attack graphs--to incorporate defender's countermeasures in a game-theoretic model, called an attack-defend game on graph. This game captures the dynamic interactions between the defender and the attacker and their defense/attack objectives in formal logic. Then, we introduce a class of hypergames to model asymmetric information created by decoys in the attacker-defender interactions. Given qualitative security specifications in formal logic, we show that the solution concepts from hypergames and reactive synthesis in formal methods can be extended to synthesize effective dynamic defense strategy using cyber deception. The strategy takes the advantages of the misperception of the attacker to ensure security specification is satisfied, which may not be satisfiable when the information is symmetric.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
From a Constraint Logic Programming Language to a Formal Verification Tool,"{log} (read 'setlog') was born as a Constraint Logic Programming (CLP) language where sets and binary relations are first-class citizens, thus fostering set programming. Internally, {log} is a constraint satisfiability solver implementing decision procedures for several fragments of set theory. Hence, {log} can be used as a declarative, set, logic programming language and as an automated theorem prover for set theory. Over time {log} has been extended with some components integrated to the satisfiability solver thus providing a formal verification environment. In this paper we make a comprehensive presentation of this environment which includes a language for the description of state machines based on set theory, an interactive environment for the execution of functional scenarios over state machines, a generator of verification conditions for state machines, automated verification of state machines, and test case generation. State machines are both, programs and specifications; exactly the same code works as a program and as its specification. In this way, with a few additions, a CLP language turned into a seamlessly integrated programming and automated proof system.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Reasoning on Multi-Relational Contextual Hierarchies via Answer Set Programming with Algebraic Measures,"Dealing with context dependent knowledge has led to different formalizations of the notion of context. Among them is the Contextualized Knowledge Repository (CKR) framework, which is rooted in description logics but links on the reasoning side strongly to logic programs and Answer Set Programming (ASP) in particular. The CKR framework caters for reasoning with defeasible axioms and exceptions in contexts, which was extended to knowledge inheritance across contexts in a coverage (specificity) hierarchy. However, the approach supports only this single type of contextual relation and the reasoning procedures work only for restricted hierarchies, due to non-trivial issues with model preference under exceptions. In this paper, we overcome these limitations and present a generalization of CKR hierarchies to multiple contextual relations, along with their interpretation of defeasible axioms and preference. To support reasoning, we use ASP with algebraic measures, which is a recent extension of ASP with weighted formulas over semirings that allows one to associate quantities with interpretations depending on the truth values of propositional atoms. Notably, we show that for a relevant fragment of CKR hierarchies with multiple contextual relations, query answering can be realized with the popular asprin framework. The algebraic measures approach is more powerful and enables e.g. reasoning with epistemic queries over CKRs, which opens interesting perspectives for the use of quantitative ASP extensions in other applications.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Parity Games and Automata for Game Logic (Extended Version),"Parikh's game logic is a PDL-like fixpoint logic interpreted on monotone neighbourhood frames that represent the strategic power of players in determined two-player games. Game logic translates into a fragment of the monotone $$-calculus, which in turn is expressively equivalent to monotone modal automata. Parity games and automata are important tools for dealing with the combinatorial complexity of nested fixpoints in modal fixpoint logics, such as the modal $$-calculus. In this paper, we (1) discuss the semantics a of game logic over neighbourhood structures in terms of parity games, and (2) use these games to obtain an automata-theoretic characterisation of the fragment of the monotone $$-calculus that corresponds to game logic. Our proof makes extensive use of structures that we call syntax graphs that combine the ease-of-use of syntax trees of formulas with the flexibility and succinctness of automata. They are essentially a graph-based view of the alternating tree automata that were introduced by Wilke in the study of modal $$-calculus.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
NP Datalog: a Logic Language for Expressing NP Search and Optimization Problems,"This paper presents a logic language for expressing NP search and optimization problems. Specifically, first a language obtained by extending (positive) Datalog with intuitive and efficient constructs (namely, stratified negation, constraints and exclusive disjunction) is introduced. Next, a further restricted language only using a restricted form of disjunction to define (non-deterministically) subsets (or partitions) of relations is investigated. This language, called NP Datalog, captures the power of Datalog with unstratified negation in expressing search and optimization problems. A system prototype implementing NP Datalog is presented. The system translates NP Datalog queries into OPL programs which are executed by the ILOG OPL Development Studio. Our proposal combines easy formulation of problems, expressed by means of a declarative logic language, with the efficiency of the ILOG System. Several experiments show the effectiveness of this approach.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Logic Modelling,"This is a reflection on the author's experience in teaching logic at the graduate level in a computer science department. The main lesson is that model building and the process of modelling must be placed at the centre stage of logic teaching. Furthermore, effective use must be supported with adequate tools. Finally, logic is the methodology underlying many applications, it is hence paramount to pass on its principles, methods and concepts to computer science audiences.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
L-Recursion and a new Logic for Logarithmic Space,"We extend first-order logic with counting by a new operator that allows it to formalise a limited form of recursion which can be evaluated in logarithmic space. The resulting logic LREC has a data complexity in LOGSPACE, and it defines LOGSPACE-complete problems like deterministic reachability and Boolean formula evaluation. We prove that LREC is strictly more expressive than deterministic transitive closure logic with counting and incomparable in expressive power with symmetric transitive closure logic STC and transitive closure logic (with or without counting). LREC is strictly contained in fixed-point logic with counting FPC. We also study an extension LREC= of LREC that has nicer closure properties and is more expressive than both LREC and STC, but is still contained in FPC and has a data complexity in LOGSPACE. Our main results are that LREC captures LOGSPACE on the class of directed trees and that LREC= captures LOGSPACE on the class of interval graphs.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Defeasible Reasoning via Datalog$^\neg$,"We address the problem of compiling defeasible theories to Datalog$^\neg$ programs. We prove the correctness of this compilation, for the defeasible logic $DL(\partial_{||})$, but the techniques we use apply to many other defeasible logics. Structural properties of $DL(\partial_{||})$ are identified that support efficient implementation and/or approximation of the conclusions of defeasible theories in the logic, compared with other defeasible logics. We also use previously well-studied structural properties of logic programs to adapt to incomplete Datalog$^\neg$ implementations.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Termination Prediction for General Logic Programs,"We present a heuristic framework for attacking the undecidable termination problem of logic programs, as an alternative to current termination/non-termination proof approaches. We introduce an idea of termination prediction, which predicts termination of a logic program in case that neither a termination nor a non-termination proof is applicable. We establish a necessary and sufficient characterization of infinite (generalized) SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an algorithm that predicts termination of general logic programs with arbitrary non-floundering queries. We have implemented a termination prediction tool and obtained quite satisfactory experimental results. Except for five programs which break the experiment time limit, our prediction is 100% correct for all 296 benchmark programs of the Termination Competition 2007, of which eighteen programs cannot be proved by any of the existing state-of-the-art analyzers like AProVE07, NTI, Polytool and TALP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
"Minimal modal logics, constructive modal logics and their relations","We present a family of minimal modal logics (namely, modal logics based on minimal propositional logic) corresponding each to a different classical modal logic. The minimal modal logics are defined based on their classical counterparts in two distinct ways: (1) via embedding into fusions of classical modal logics through a natural extension of the Gdel-Johansson translation of minimal logic into modal logic S4; (2) via extension to modal logics of the multi- vs. single-succedent correspondence of sequent calculi for classical and minimal logic. We show that, despite being mutually independent, the two methods turn out to be equivalent for a wide class of modal systems. Moreover, we compare the resulting minimal version of K with the constructive modal logic CK studied in the literature, displaying tight relations among the two systems. Based on these relations, we also define a constructive correspondent for each minimal system, thus obtaining a family of constructive modal logics which includes CK as well as other constructive modal logics studied in the literature.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The temporal logic of two-dimensional Minkowski spacetime with slower-than-light accessibility is decidable,"We work primarily with the Kripke frame consisting of two-dimensional Minkowski spacetime with the irreflexive accessibility relation 'can reach with a slower-than-light signal'. We show that in the basic temporal language, the set of validities over this frame is decidable. We then refine this to PSPACE-complete. In both cases the same result for the corresponding reflexive frame follows immediately. With a little more work we obtain PSPACE-completeness for the validities of the Halpern-Shoham logic of intervals on the real line with two different combinations of modalities.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Composing Programs in a Rewriting Logic for Declarative Programming,"Constructor-Based Conditional Rewriting Logic is a general framework for integrating first-order functional and logic programming which gives an algebraic semantics for non-deterministic functional-logic programs. In the context of this formalism, we introduce a simple notion of program module as an open program which can be extended together with several mechanisms to combine them. These mechanisms are based on a reduced set of operations. However, the high expressiveness of these operations enable us to model typical constructs for program modularization like hiding, export/import, genericity/instantiation, and inheritance in a simple way. We also deal with the semantic aspects of the proposal by introducing an immediate consequence operator, and studying several alternative semantics for a program module, based on this operator, in the line of logic programming: the operator itself, its least fixpoint (the least model of the module), the set of its pre-fixpoints (term models of the module), and some other variations in order to find a compositional and fully abstract semantics wrt the set of operations and a natural notion of observability.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Symmetric Circuits for Rank Logic,"Fixed-point logic with rank (FPR) is an extension of fixed-point logic with counting (FPC) with operators for computing the rank of a matrix over a finite field. The expressive power of FPR properly extends that of FPC and is contained in PTime, but not known to be properly contained. We give a circuit characterization for FPR in terms of families of symmetric circuits with rank gates, along the lines of that for FPC given by [Anderson and Dawar 2017]. This requires the development of a broad framework of circuits in which the individual gates compute functions that are not symmetric (i.e., invariant under all permutations of their inputs). In the case of FPC, the proof of equivalence of circuits and logic rests heavily on the assumption that individual gates compute such symmetric functions and so novel techniques are required to make this work for FPR.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
On systematic construction of correct logic programs,"Partial correctness of imperative or functional programming divides in logic programming into two notions. Correctness means that all answers of the program are compatible with the specification. Completeness means that the program produces all the answers required by the specifications. We also consider semi-completeness -- completeness for those queries for which the program does not diverge. This paper presents an approach to systematically construct provably correct and semi-complete logic programs, for a given specification. Normal programs are considered, under Kunen's 3-valued completion semantics (of negation as finite failure) and the well-founded semantics (of negation as possibly infinite failure). The approach is declarative, it abstracts from details of operational semantics, like e.g.\ the form of the selected literals (``procedure calls'') during the computation. The proposed method is simple, and can be used (maybe informally) in actual everyday programming.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The EPFL Logic Synthesis Libraries,"We present a collection of modular open source C++ libraries for the development of logic synthesis applications. These libraries can be used to develop applications for the design of classical and emerging technologies, as well as for the implementation of quantum compilers. All libraries are well documented and well tested. Furthermore, being header-only, the libraries can be readily used as core components in complex logic synthesis systems.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
PSPACE Reasoning for Graded Modal Logics,"We present a PSPACE algorithm that decides satisfiability of the graded modal logic Gr(K_R)---a natural extension of propositional modal logic K_R by counting expressions---which plays an important role in the area of knowledge representation. The algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem. Thus, we exactly fix the complexity of the problem and refute an ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \cap I)), which augments Gr(K_R) with inverse relations and intersection of accessibility relations. This establishes a kind of ``theoretical benchmark'' that all algorithmic approaches can be measured against.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Differential Hybrid Games,"This article introduces differential hybrid games, which combine differential games with hybrid games. In both kinds of games, two players interact with continuous dynamics. The difference is that hybrid games also provide all the features of hybrid systems and discrete games, but only deterministic differential equations. Differential games, instead, provide differential equations with continuous-time game input by both players, but not the luxury of hybrid games, such as mode switches and discrete-time or alternating adversarial interaction. This article augments differential game logic with modalities for the combined dynamics of differential hybrid games. It shows how hybrid games subsume differential games and introduces differential game invariants and differential game variants for proving properties of differential games inductively.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
CLP(H): Constraint Logic Programming for Hedges,"CLP(H) is an instantiation of the general constraint logic programming scheme with the constraint domain of hedges. Hedges are finite sequences of unranked terms, built over variadic function symbols and three kinds of variables: for terms, for hedges, and for function symbols. Constraints involve equations between unranked terms and atoms for regular hedge language membership. We study algebraic semantics of CLP(H) programs, define a sound, terminating, and incomplete constraint solver, investigate two fragments of constraints for which the solver returns a complete set of solutions, and describe classes of programs that generate such constraints.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Logical Characterization of the Preferred Models of Logic Programs with Ordered Disjunction,"Logic Programs with Ordered Disjunction (LPODs) extend classical logic programs with the capability of expressing alternatives with decreasing degrees of preference in the heads of program rules. Despite the fact that the operational meaning of ordered disjunction is clear, there exists an important open issue regarding its semantics. In particular, there does not exist a purely model-theoretic approach for determining the most preferred models of an LPOD. At present, the selection of the most preferred models is performed using a technique that is not based exclusively on the models of the program and in certain cases produces counterintuitive results. We provide a novel, model-theoretic semantics for LPODs, which uses an additional truth value in order to identify the most preferred models of a program. We demonstrate that the proposed approach overcomes the shortcomings of the traditional semantics of LPODs. Moreover, the new approach can be used to define the semantics of a natural class of logic programs that can have both ordered and classical disjunctions in the heads of clauses. This allows programs that can express not only strict levels of preferences but also alternatives that are equally preferred. This work is under consideration for acceptance in TPLP.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Neural Logic Reasoning,"Recent years have witnessed the success of deep neural networks in many research areas. The fundamental idea behind the design of most neural networks is to learn similarity patterns from data for prediction and inference, which lacks the ability of cognitive reasoning. However, the concrete ability of reasoning is critical to many theoretical and practical problems. On the other hand, traditional symbolic reasoning methods do well in making logical inference, but they are mostly hard rule-based reasoning, which limits their generalization ability to different tasks since difference tasks may require different rules. Both reasoning and generalization ability are important for prediction tasks such as recommender systems, where reasoning provides strong connection between user history and target items for accurate prediction, and generalization helps the model to draw a robust user portrait over noisy inputs.   In this paper, we propose Logic-Integrated Neural Network (LINN) to integrate the power of deep learning and logic reasoning. LINN is a dynamic neural architecture that builds the computational graph according to input logical expressions. It learns basic logical operations such as AND, OR, NOT as neural modules, and conducts propositional logical reasoning through the network for inference. Experiments on theoretical task show that LINN achieves significant performance on solving logical equations and variables. Furthermore, we test our approach on the practical task of recommendation by formulating the task into a logical inference problem. Experiments show that LINN significantly outperforms state-of-the-art recommendation models in Top-K recommendation, which verifies the potential of LINN in practice.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Representation results for defeasible logic,"The importance of transformations and normal forms in logic programming, and generally in computer science, is well documented. This paper investigates transformations and normal forms in the context of Defeasible Logic, a simple but efficient formalism for nonmonotonic reasoning based on rules and priorities. The transformations described in this paper have two main benefits: on one hand they can be used as a theoretical tool that leads to a deeper understanding of the formalism, and on the other hand they have been used in the development of an efficient implementation of defeasible logic.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Towards Interactive Logic Programming,"Linear logic programming uses provability as the basis for computation. In the operational semantics based on provability, executing the additive-conjunctive goal $G_1 \& G_2$ from a program $P$ simply terminates with a success if both $G_1$ and $G_2$ are solvable from $P$. This is an unsatisfactory situation, as a central action of \& -- the action of choosing either $G_1$ or $G_2$ by the user -- is missing in this semantics.   We propose to modify the operational semantics above to allow for more active participation from the user. We illustrate our idea via muProlog, an extension of Prolog with additive goals.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
The Logic of XACML - Extended,We study the international standard XACML 3.0 for describing security access control policy in a compositional way. Our main contribution is to derive a logic that precisely captures the idea behind the standard and to formally define the semantics of the policy combining algorithms of XACML. To guard against modelling artefacts we provide an alternative way of characterizing the policy combining algorithms and we formally prove the equivalence of these approaches. This allows us to pinpoint the shortcoming of previous approaches to formalization based either on Belnap logic or on D-algebra.,"cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
A Refinement Calculus for Logic Programs,"Existing refinement calculi provide frameworks for the stepwise development of imperative programs from specifications. This paper presents a refinement calculus for deriving logic programs. The calculus contains a wide-spectrum logic programming language, including executable constructs such as sequential conjunction, disjunction, and existential quantification, as well as specification constructs such as general predicates, assumptions and universal quantification. A declarative semantics is defined for this wide-spectrum language based on executions. Executions are partial functions from states to states, where a state is represented as a set of bindings. The semantics is used to define the meaning of programs and specifications, including parameters and recursion. To complete the calculus, a notion of correctness-preserving refinement over programs in the wide-spectrum language is defined and refinement laws for developing programs are introduced. The refinement calculus is illustrated using example derivations and prototype tool support is discussed.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
Productive Corecursion in Logic Programming,"Logic Programming is a Turing complete language. As a consequence, designing algorithms that decide termination and non-termination of programs or decide inductive/coinductive soundness of formulae is a challenging task. For example, the existing state-of-the-art algorithms can only semi-decide coinductive soundness of queries in logic programming for regular formulae. Another, less famous, but equally fundamental and important undecidable property is productivity. If a derivation is infinite and coinductively sound, we may ask whether the computed answer it determines actually computes an infinite formula. If it does, the infinite computation is productive. This intuition was first expressed under the name of computations at infinity in the 80s. In modern days of the Internet and stream processing, its importance lies in connection to infinite data structure processing.   Recently, an algorithm was presented that semi-decides a weaker property -- of productivity of logic programs. A logic program is productive if it can give rise to productive derivations. In this paper we strengthen these recent results. We propose a method that semi-decides productivity of individual derivations for regular formulae. Thus we at last give an algorithmic counterpart to the notion of productivity of derivations in logic programming. This is the first algorithmic solution to the problem since it was raised more than 30 years ago. We also present an implementation of this algorithm.","cat:cs.LO AND (logic OR philosophy OR reasoning OR ""game theory"")",0
CRISPR SWAPnDROP -- A multifunctional system for genome editing and large-scale interspecies gene transfer,"The need for diverse chromosomal modifications in biotechnology, synthetic biology and basic research requires the development of new technologies. With CRISPR SWAPnDROP, we extend the limits of genome editing to large-scale in-vivo DNA transfer between bacterial species. Its modular platform approach facilitates species specific adaptation to confer genome editing in various species. In this study, we show the implementation of the CRISPR SWAPnDROP concept for the model organism Escherichia coli and the currently fastest growing and biotechnologically relevant organism Vibrio natriegens. We demonstrate the excision, transfer and integration of 151kb chromosomal DNA between E. coli strains and from E. coli to V. natriegens without size-limiting intermediate DNA extraction. With the transfer of the E. coli MG1655 wild type lac operon, we establish a functional lactose and galactose degradation pathway in V. natriegens to extend its biotechnological spectrum. We also transfer the E. coli DH5alpha lac operon and make V. natriegens capable of alpha-complementation - a step towards an ultra-fast cloning strain. Furthermore, CRISPR SWAPnDROP is designed to be the swiss army knife of genome engineering. Its spectrum of application comprises scarless, marker-free, iterative and parallel insertions and deletions, genome rearrangements, as well as gene transfer between strains and across species. The modular character facilitates DNA library applications and the recycling of standardized parts. Its novel multi-color scarless co-selection system significantly improves editing efficiency to 92% for single edits and 83% for quadruple edits and provides visual quality controls throughout the assembly and editing process.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
"Gene and RNA Editing: Methods, Enabling Technologies, Applications, and Future Directions","Gene and RNA editing methods, technologies, and applications are emerging as innovative forms of therapy and medicine, offering more efficient implementation compared to traditional pharmaceutical treatments. Current trends emphasize the urgent need for advanced methods and technologies to detect public health threats, including diseases and viral agents. Gene and RNA editing techniques enhance the ability to identify, modify, and ameliorate the effects of genetic diseases, disorders, and disabilities. Viral detection and identification methods present numerous opportunities for enabling technologies, such as CRISPR, applicable to both RNA and gene editing through the use of specific Cas proteins. This article explores the distinctions and benefits of RNA and gene editing processes, emphasizing their contributions to the future of medical treatment. CRISPR technology, particularly its adaptation via the Cas13 protein for RNA editing, is a significant advancement in gene editing. The article will delve into RNA and gene editing methodologies, focusing on techniques that alter and modify genetic coding. A-to-I and C-to-U editing are currently the most predominant methods of RNA modification. CRISPR stands out as the most cost-effective and customizable technology for both RNA and gene editing. Unlike permanent changes induced by cutting an individual's DNA genetic code, RNA editing offers temporary modifications by altering nucleoside bases in RNA strands, which can then attach to DNA strands as temporary modifiers.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Prokaryotic genome editing based on the subtype I-B-Svi CRISPR-Cas system,"Type I CRISPR-Cas systems are the most common among six types of CRISPR-Cas systems, however, non-self-targeting genome editing based on a single Cas3 of type I CRISPR-Cas systems has not been reported. Here, we present the subtype I-B-Svi CRISPR-Cas system (with three confirmed CRISPRs and a cas gene cluster) and genome editing based on this system found in Streptomyces virginiae IBL14. Importantly, like the animal-derived bacterial protein SpCas9 (1368 amino-acids), the single, compact, non-animal-derived bacterial protein SviCas3 (771 amino-acids) can also direct template-based microbial genome editing through the target cell's own homology-directed repair system, which breaks the view that the genome editing based on type I CRISPR-Cas systems requires a full Cascade. Notably, no off-target changes or indel-formation were detected in the analysis of potential off-target sites. This discovery broadens our understanding of the diversity of type I CRISPR-Cas systems and will facilitate new developments in genome editing tools.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Investigating the genomic background of CRISPR-Cas genomes for CRISPR-based antimicrobials,"CRISPR-Cas systems are an adaptive immunity that protects prokaryotes against foreign genetic elements. Genetic templates acquired during past infection events enable DNA-interacting enzymes to recognize foreign DNA for destruction. Due to the programmability and specificity of these genetic templates, CRISPR-Cas systems are potential alternative antibiotics that can be engineered to self-target antimicrobial resistance genes on the chromosome or plasmid. However, several fundamental questions remain to repurpose these tools against drug-resistant bacteria. For endogenous CRISPR-Cas self-targeting, antimicrobial resistance genes and functional CRISPR-Cas systems have to co-occur in the target cell. Furthermore, these tools have to outplay DNA repair pathways that respond to the nuclease activities of Cas proteins, even for exogenous CRISPR-Cas delivery. Here, we conduct a comprehensive survey of CRISPR-Cas genomes. First, we address the co-occurrence of CRISPR-Cas systems and antimicrobial resistance genes in the CRISPR-Cas genomes. We show that the average number of these genes varies greatly by the CRISPR-Cas type, and some CRISPR-Cas types (IE and IIIA) have over 20 genes per genome. Next, we investigate the DNA repair pathways of these CRISPR-Cas genomes, revealing that the diversity and frequency of these pathways differ by the CRISPR-Cas type. The interplay between CRISPR-Cas systems and DNA repair pathways is essential for the acquisition of new spacers in CRISPR arrays. We conduct simulation studies to demonstrate that the efficiency of these DNA repair pathways may be inferred from the time-series patterns in the RNA structure of CRISPR repeats. This bioinformatic survey of CRISPR-Cas genomes elucidates the necessity to consider multifaceted interactions between different genes and systems to design effective CRISPR-based antimicrobials.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
CRISPR: Ensemble Model,"Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results suggest that our method can be used to design sgRNAs with high sensitivity and specificity, even for new genes or cells. This could have important implications for the clinical use of CRISPR, as it would allow researchers to design more effective and safer treatments for a variety of diseases.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Template-based eukaryotic genome editing directed by SviCas3,"RNA-guided gene editing based on the CRISPR-Cas system is currently the most effective genome editing technique. Here, we report that the SviCas3 from the subtype I-B-Svi Cas system in Streptomyces virginiae IBL14 is an RNA-guided and DNA-guided DNA endonuclease suitable for the HDR-directed gene and/or base editing of eukaryotic cell genomes. The genome editing efficiency of SviCas3 guided by DNA is no less than that of SviCas3 guided by RNA. In particular, t-DNA, as a template and a guide, does not require a proto-spacer-adjacent motif, demonstrating that CRISPR, as the basis for crRNA design, is not required for the SviCas3-mediated gene and base editing. This discovery will broaden our understanding of enzyme diversity in CRISPR-Cas systems, will provide important tools for the creation and modification of living things and the treatment of human genetic diseases, and will usher in a new era of DNA-guided gene editing and base editing.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
pgMAP: a pipeline to enable guide RNA read mapping from dual-targeting CRISPR screens,"We developed pgMAP, an analysis pipeline to map gRNA sequencing reads from dual-targeting CRISPR screens. pgMAP output includes a dual gRNA read counts table and quality control metrics including the proportion of correctly-paired reads and CRISPR library sequencing coverage across all time points and samples. pgMAP is implemented using Snakemake and is available open-source under the MIT license at https://github.com/fredhutch/pgmap_pipeline.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Data-Driven Prediction of CRISPR-Based Transcription Regulation for Programmable Control of Metabolic Flux,"Multiplex and multi-directional control of metabolic pathways is crucial for metabolic engineering to improve product yield of fuels, chemicals, and pharmaceuticals. To achieve this goal, artificial transcriptional regulators such as CRISPR-based transcription regulators have been developed to specifically activate or repress genes of interest. Here, we found that by deploying guide RNAs to target on DNA sites at different locations of genetic cassettes, we could use just one synthetic CRISPR-based transcriptional regulator to simultaneously activate and repress gene expressions. By using the pairwise datasets of guide RNAs and gene expressions, we developed a data-driven predictive model to rationally design this system for fine-tuning expression of target genes. We demonstrated that this system could achieve programmable control of metabolic fluxes when using yeast to produce versatile chemicals. We anticipate that this master CRISPR-based transcription regulator will be a valuable addition to the synthetic biology toolkit for metabolic engineering, speeding up the design-build-test cycle in industrial biomanufacturing as well as generating new biological insights on the fates of eukaryotic cells.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
scGHSOM: Hierarchical clustering and visualization of single-cell and CRISPR data using growing hierarchical SOM,"High-dimensional single-cell data poses significant challenges in identifying underlying biological patterns due to the complexity and heterogeneity of cellular states. We propose a comprehensive gene-cell dependency visualization via unsupervised clustering, Growing Hierarchical Self-Organizing Map (GHSOM), specifically designed for analyzing high-dimensional single-cell data like single-cell sequencing and CRISPR screens. GHSOM is applied to cluster samples in a hierarchical structure such that the self-growth structure of clusters satisfies the required variations between and within. We propose a novel Significant Attributes Identification Algorithm to identify features that distinguish clusters. This algorithm pinpoints attributes with minimal variation within a cluster but substantial variation between clusters. These key attributes can then be used for targeted data retrieval and downstream analysis. Furthermore, we present two innovative visualization tools: Cluster Feature Map and Cluster Distribution Map. The Cluster Feature Map highlights the distribution of specific features across the hierarchical structure of GHSOM clusters. This allows for rapid visual assessment of cluster uniqueness based on chosen features. The Cluster Distribution Map depicts leaf clusters as circles on the GHSOM grid, with circle size reflecting cluster data size and color customizable to visualize features like cell type or other attributes. We apply our analysis to three single-cell datasets and one CRISPR dataset (cell-gene database) and evaluate clustering methods with internal and external CH and ARI scores. GHSOM performs well, being the best performer in internal evaluation (CH=4.2). In external evaluation, GHSOM has the third-best performance of all methods.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Curated loci prime editing (cliPE) for accessible multiplexed assays of variant effect (MAVEs),"Multiplexed assays of variant effect (MAVEs) perform simultaneous characterization of many variants. Prime editing has been recently adopted for introducing many variants in their native genomic contexts. However, robust protocols and standards are limited, preventing widespread uptake. Herein, we describe curated loci prime editing (cliPE) which is an accessible, low-cost experimental pipeline to perform MAVEs using prime editing of a target gene, as well as a companion Shiny app (pegRNA Designer) to rapidly and easily design user-specific MAVE libraries.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
CRISPR/Cas9 For Photoactivated Localization Microscopy (PALM),"We demonstrate that endonuclease deficient Clustered Regularly Interspaced Short Palindromic Repeats CRISPR-associated Cas9 protein (dCas9) fused to the photo-convertible fluorescence protein monomeric mEos3.1 (dCas9-mEos3) can be used to resolve sub-diffraction limited features of repetitive gene elements, thus providing a new route to investigate high-order chromatin organization at these sites.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Modeling variable guide efficiency in pooled CRISPR screens with ContrastiveVI+,"Genetic screens mediated via CRISPR-Cas9 combined with high-content readouts have emerged as powerful tools for biological discovery. However, computational analyses of these screens come with additional challenges beyond those found with standard scRNA-seq analyses. For example, perturbation-induced variations of interest may be subtle and masked by other dominant source of variation shared with controls, and variable guide efficiency results in some cells not undergoing genetic perturbation despite expressing a guide RNA. While a number of methods have been developed to address the former problem by explicitly disentangling perturbation-induced variations from those shared with controls, less attention has been paid to the latter problem of noisy perturbation labels. To address this issue, here we propose ContrastiveVI+, a generative modeling framework that both disentangles perturbation-induced from non-perturbation-related variations while also inferring whether cells truly underwent genomic edits. Applied to three large-scale Perturb-seq datasets, we find that ContrastiveVI+ better recovers known perturbation-induced variations compared to previous methods while successfully identifying cells that escaped the functional consequences of guide RNA expression. An open-source implementation of our model is available at \url{https://github.com/insitro/contrastive_vi_plus}.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Multimodal Modeling of CRISPR-Cas12 Activity Using Foundation Models and Chromatin Accessibility Data,"Predicting guide RNA (gRNA) activity is critical for effective CRISPR-Cas12 genome editing but remains challenging due to limited data, variation across protospacer adjacent motifs (PAMs-short sequence requirements for Cas binding), and reliance on large-scale training. We investigate whether pre-trained biological foundation model originally trained on transcriptomic data can improve gRNA activity estimation even without domain-specific pre-training. Using embeddings from existing RNA foundation model as input to lightweight regressor, we show substantial gains over traditional baselines. We also integrate chromatin accessibility data to capture regulatory context, improving performance further. Our results highlight the effectiveness of pre-trained foundation models and chromatin accessibility data for gRNA activity prediction.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models,"Predicting phenotypes with complex genetic bases based on a small, interpretable set of variant features remains a challenging task. Conventionally, data-driven approaches are utilized for this task, yet the high dimensional nature of genotype data makes the analysis and prediction difficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and their success in processing complex biomedical concepts, we set to examine the ability of LLMs in feature selection and engineering for tabular genotype data, with a novel knowledge-driven framework. We develop FREEFORM, Free-flow Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling, designed with chain-of-thought and ensembling principles, to select and engineer features with the intrinsic knowledge of LLMs. Evaluated on two distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing loss, we find this framework outperforms several data-driven methods, particularly on low-shot regimes. FREEFORM is available as open-source framework at GitHub: https://github.com/PennShenLab/FREEFORM.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Systematic identification of abundant A-to-I editing sites in the human transcriptome,"RNA editing by members of the double-stranded RNA-specific ADAR family leads to site-specific conversion of adenosine to inosine (A-to-I) in precursor messenger RNAs. Editing by ADARs is believed to occur in all metazoa, and is essential for mammalian development. Currently, only a limited number of human ADAR substrates are known, while indirect evidence suggests a substantial fraction of all pre-mRNAs being affected. Here we describe a computational search for ADAR editing sites in the human transcriptome, using millions of available expressed sequences. 12,723 A-to-I editing sites were mapped in 1,637 different genes, with an estimated accuracy of 95%, raising the number of known editing sites by two orders of magnitude. We experimentally validated our method by verifying the occurrence of editing in 26 novel substrates. A-to-I editing in humans primarily occurs in non-coding regions of the RNA, typically in Alu repeats. Analysis of the large set of editing sites indicates the role of editing in controlling dsRNA stability.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
The Triplet Genetic Code had a Doublet Predecessor,"Information theoretic analysis of genetic languages indicates that the naturally occurring 20 amino acids and the triplet genetic code arose by duplication of 10 amino acids of class-II and a doublet genetic code having codons NNY and anticodons $\overleftarrow{\rm GNN}$. Evidence for this scenario is presented based on the properties of aminoacyl-tRNA synthetases, amino acids and nucleotide bases.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
From In Silico to In Vitro: A Comprehensive Guide to Validating Bioinformatics Findings,"The integration of bioinformatics predictions and experimental validation plays a pivotal role in advancing biological research, from understanding molecular mechanisms to developing therapeutic strategies. Bioinformatics tools and methods offer powerful means for predicting gene functions, protein interactions, and regulatory networks, but these predictions must be validated through experimental approaches to ensure their biological relevance. This review explores the various methods and technologies used for experimental validation, including gene expression analysis, protein-protein interaction verification, and pathway validation. We also discuss the challenges involved in translating computational predictions to experimental settings and highlight the importance of collaboration between bioinformatics and experimental research. Finally, emerging technologies, such as CRISPR gene editing, next-generation sequencing, and artificial intelligence, are shaping the future of bioinformatics validation and driving more accurate and efficient biological discoveries.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Genetic Testing for Complex Diseases: a Simulation Study Perspective,"It is widely recognized nowadays that complex diseases are caused by, amongst the others, multiple genetic factors. The recent advent of genome-wide association study (GWA) has triggered a wave of research aimed at discovering genetic factors underlying common complex diseases. While the number of reported susceptible genetic variants is increasing steadily, the application of such findings into diseases prognosis for the general population is still unclear, and there are doubts about whether the size of the contribution by such factors is significant. In this respect, some recent simulation-based studies have shed more light to the prospect of genetic tests. In this report, we discuss several aspects of simulation-based studies: their parameters, their assumptions, and the information they provide.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Random Forest as a Tumour Genetic Marker Extractor,"Finding tumour genetic markers is essential to biomedicine due to their relevance for cancer detection and therapy development. In this paper, we explore a recently released dataset of chromosome rearrangements in 2,586 cancer patients, where different sorts of alterations have been detected. Using a Random Forest classifier, we evaluate the relevance of several features (some directly available in the original data, some engineered by us) related to chromosome rearrangements. This evaluation results in a set of potential tumour genetic markers, some of which are validated in the bibliography, while others are potentially novel.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
A Simpler Explanation to BAK1 Gene Variation in Aortic and Blood Tissues,"The explanation is that in aortic tissue (both diseased and nondiseased) a BAK1 pseudogene is expressed; while in the matching blood samples the actual BAK1 gene is expressed. This explanation was reached after we realized that BAK1 has two edited copies in human genome. These copies are probably BAK1 pseudogenes. One copy belongs to chromosome 11 (NG_005599.3) and the other to chromosome 20 (NC_000850.5). The first copy has frameshifts which means that probably it does not express any functional protein; by other hand, the chromosome 20 copy has no frameshifts and what is more important contains all the reported polymorphisms.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
"Matrix genetics, part 2: the degeneracy of the genetic code and the octave algebra with two quasi-real units (the genetic octave Yin-Yang-algebra)","Algebraic properties of the genetic code are analyzed. The investigations of the genetic code on the basis of matrix approaches (""matrix genetics"") are described. The degeneracy of the vertebrate mitochondria genetic code is reflected in the black-and-white mosaic of the (8*8)-matrix of 64 triplets, 20 amino acids and stop-signals. This mosaic genetic matrix is connected with the matrix form of presentation of the special 8-dimensional Yin-Yang-algebra and of its particular 4-dimensional case. The special algorithm, which is based on features of genetic molecules, exists to transform the mosaic genomatrix into the matrices of these algebras. Two new numeric systems are defined by these 8-dimensional and 4-dimensional algebras: genetic Yin-Yang-octaves and genetic tetrions. Their comparison with quaternions by Hamilton is presented. Elements of new ""genovector calculation"" and ideas of ""genetic mechanics"" are discussed. These algebras are considered as models of the genetic code and as its possible pre-code basis. They are related with binary oppositions of the Yin-Yang type and they give new opportunities to investigate evolution of the genetic code. The revealed fact of the relation between the genetic code and these genetic algebras is discussed in connection with the idea by Pythagoras: ""All things are numbers"". Simultaneously these genetic algebras can be utilized as the algebras of genetic operators in biological organisms. The described results are related with the problem of algebraization of bioinformatics. They take attention to the question: what is life from the viewpoint of algebra?","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Tackling the dimensions in imaging genetics with CLUB-PLS,"A major challenge in imaging genetics and similar fields is to link high-dimensional data in one domain, e.g., genetic data, to high dimensional data in a second domain, e.g., brain imaging data. The standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes. That entails executing one genome-wide association study (GWAS) for each pre-defined imaging measure. Although this approach has been tremendously successful, one shortcoming is that phenotypes must be pre-defined. Consequently, effects that are not confined to pre-selected regions of interest or that reflect larger brain-wide patterns can easily be missed. In this work we introduce a Partial Least Squares (PLS)-based framework, which we term Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in both domains as well as with large sample sizes. One key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in both domains. We applied CLUB-PLS to investigating the genetic basis of surface area and cortical thickness in a sample of 33,000 subjects from the UK Biobank. We found 107 genome-wide significant locus-phenotype pairs that are linked to 386 different genes. We found that a vast majority of these loci could be technically validated at a high rate: using classic GWAS or Genome-Wide Inferred Statistics (GWIS) we found that 85 locus-phenotype pairs exceeded the genome-wide suggestive (P<1e-05) threshold.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Critical Transitions In a Model of a Genetic Regulatory System,"We consider a model for substrate-depletion oscillations in genetic systems, based on a stochastic differential equation with a slowly evolving external signal. We show the existence of critical transitions in the system. We apply two methods to numerically test the synthetic time series generated by the system for early indicators of critical transitions: a detrended fluctuation analysis method, and a novel method based on topological data analysis (persistence diagrams).","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Integration of Gene Expression Data and Methylation Reveals Genetic Networks for Glioblastoma,"Motivation: The consistent amount of different types of omics data requires novel methods of analysis and data integration. In this work we describe Regression2Net, a computational approach to analyse gene expression and methylation profiles via regression analysis and network-based techniques.   Results: We identified 284 and 447 unique candidate genes potentially associated to the Glioblastoma pathology from two networks inferred from mixed genetic datasets. In-depth biological analysis of these networks reveals genes that are related to energy metabolism, cell cycle control (AATF), immune system response and several types of cancer. Importantly, we observed significant over- representation of cancer related pathways including glioma especially in the methylation network. This confirms the strong link between methylation and glioblastomas. Potential glioma suppressor genes ACCN3 and ACCN4 linked to NBPF1 neuroblastoma breakpoint family have been identified in our expression network. Numerous ABC transporter genes (ABCA1, ABCB1) present in the expression network suggest drug resistance of glioblastoma tumors.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Genoogle: an indexed and parallelized search engine for similar DNA sequences,"The search for similar genetic sequences is one of the main bioinformatics tasks. The genetic sequences data banks are growing exponentially and the searching techniques that use linear time are not capable to do the search in the required time anymore. Another problem is that the clock speed of the modern processors are not growing as it did before, instead, the processing capacity is growing with the addiction of more processing cores and the techniques which does not use parallel computing does not have benefits from these extra cores. This work aims to use data indexing techniques to reduce the searching process computation cost united with the parallelization of the searching techniques to use the computational capacity of the multi core processors. To verify the viability of using these two techniques simultaneously, a software which uses parallelization techniques with inverted indexes was developed.   Experiments were executed to analyze the performance gain when parallelism is utilized, the search time gain, and also the quality of the results when it compared with others searching tools. The results of these experiments were promising, the parallelism gain overcame the expected speedup, the searching time was 20 times faster than the parallelized NCBI BLAST, and the searching results showed a good quality when compared with this tool.   The software source code is available at https://github.com/felipealbrecht/Genoogle .","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Active learning for efficient discovery of optimal gene combinations in the combinatorial perturbation space,"The advancement of novel combinatorial CRISPR screening technologies enables the identification of synergistic gene combinations on a large scale. This is crucial for developing novel and effective combination therapies, but the combinatorial space makes exhaustive experimentation infeasible. We introduce NAIAD, an active learning framework that efficiently discovers optimal gene pairs capable of driving cells toward desired cellular phenotypes. NAIAD leverages single-gene perturbation effects and adaptive gene embeddings that scale with the training data size, mitigating overfitting in small-sample learning while capturing complex gene interactions as more data is collected. Evaluated on four CRISPR combinatorial perturbation datasets totaling over 350,000 genetic interactions, NAIAD, trained on small datasets, outperforms existing models by up to 40\% relative to the second-best. NAIAD's recommendation system prioritizes gene pairs with the maximum predicted effects, resulting in the highest marginal gain in each AI-experiment round and accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD framework (https://github.com/NeptuneBio/NAIAD) improves the identification of novel, effective gene combinations, enabling more efficient CRISPR library design and offering promising applications in genomics research and therapeutic development.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Analysis of Extremely Obese Individuals Using Deep Learning Stacked Autoencoders and Genome-Wide Genetic Data,"The aetiology of polygenic obesity is multifactorial, which indicates that life-style and environmental factors may influence multiples genes to aggravate this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have been associated with BMI. However, identified loci only explain a small proportion of the variation ob-served for this phenotype. The linear nature of genome wide association studies (GWAS) used to identify associations between genetic variants and the phenotype have had limited success in explaining the heritability variation of BMI and shown low predictive capacity in classification studies. GWAS ignores the epistatic interactions that less significant variants have on the phenotypic outcome. In this paper we utilise a novel deep learning-based methodology to reduce the high dimensional space in GWAS and find epistatic interactions between SNPs for classification purposes. SNPs were filtered based on the effects associations have with BMI. Since Bonferroni adjustment for multiple testing is highly conservative, an important proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore, only SNPs with p-values < 1x10-2 were considered for subsequent epistasis analysis using stacked auto encoders (SAE). This allows the nonlinearity present in SNP-SNP interactions to be discovered through progressively smaller hidden layer units and to initialise a multi-layer feedforward artificial neural network (ANN) classifier. The classifier is fine-tuned to classify extremely obese and non-obese individuals. The best results were obtained with 2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936, Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566, Logloss=0.476864, AUC=0.85178 and MSE=0.156315).","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Securing the Language of Life: Inheritable Watermarks from DNA Language Models to Proteins,"DNA language models have revolutionized our ability to understand and design DNA sequences--the fundamental language of life--with unprecedented precision, enabling transformative applications in therapeutics, synthetic biology, and gene editing. However, this capability also poses substantial dual-use risks, including the potential for creating pathogens, viruses, and even bioweapons. To address these biosecurity challenges, we introduce two innovative watermarking techniques to reliably track the designed DNA: DNAMark and CentralMark. DNAMark employs synonymous codon substitutions to embed watermarks in DNA sequences while preserving the original function. CentralMark further advances this by creating inheritable watermarks that transfer from DNA to translated proteins, leveraging protein embeddings to ensure detection across the central dogma. Both methods utilize semantic embeddings to generate watermark logits, enhancing robustness against natural mutations, synthesis errors, and adversarial attacks. Evaluated on our therapeutic DNA benchmark, DNAMark and CentralMark achieve F1 detection scores above 0.85 under various conditions, while maintaining over 60% sequence similarity to ground truth and degeneracy scores below 15%. A case study on the CRISPR-Cas9 system underscores CentralMark's utility in real-world settings. This work establishes a vital framework for securing DNA language models, balancing innovation with accountability to mitigate biosecurity risks.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Probabilistic Regulatory Networks: Modeling Genetic Networks,"We describe here the new concept of $$-Homomorphisms of Probabilistic Regulatory Gene Networks(PRN). The $$-homomorphisms are special mappings between two probabilistic networks, that consider the algebraic action of the iteration of functions and the probabilistic dynamic of the two networks. It is proved here that the class of PRN, together with the homomorphisms, form a category with products and coproducts. Projections are special homomorphisms, induced by invariant subnetworks. Here, it is proved that an $$-homomorphism for 0 <$$< 1 produces simultaneous Markov Chains in both networks, that permit to introduce the concepts of $$-isomorphism of Markov Chains, and similar networks.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Season combinatorial intervention predictions with Salt & Peper,"Interventions play a pivotal role in the study of complex biological systems. In drug discovery, genetic interventions (such as CRISPR base editing) have become central to both identifying potential therapeutic targets and understanding a drug's mechanism of action. With the advancement of CRISPR and the proliferation of genome-scale analyses such as transcriptomics, a new challenge is to navigate the vast combinatorial space of concurrent genetic interventions. Addressing this, our work concentrates on estimating the effects of pairwise genetic combinations on the cellular transcriptome. We introduce two novel contributions: Salt, a biologically-inspired baseline that posits the mostly additive nature of combination effects, and Peper, a deep learning model that extends Salt's additive assumption to achieve unprecedented accuracy. Our comprehensive comparison against existing state-of-the-art methods, grounded in diverse metrics, and our out-of-distribution analysis highlight the limitations of current models in realistic settings. This analysis underscores the necessity for improved modelling techniques and data acquisition strategies, paving the way for more effective exploration of genetic intervention effects.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Validating GWAS Findings through Reverse Engineering of Contingency Tables,"Reproducibility in genome-wide association studies (GWAS) is crucial for ensuring reliable genomic research outcomes. However, limited access to original genomic datasets (mainly due to privacy concerns) prevents researchers from reproducing experiments to validate results. In this paper, we propose a novel method for GWAS reproducibility validation that detects unintentional errors without the need for dataset sharing. Our approach leverages p-values from GWAS outcome reports to estimate contingency tables for each single nucleotide polymorphism (SNP) and calculates the Hamming distance between the minor allele frequencies (MAFs) derived from these contingency tables and publicly available phenotype-specific MAF data. By comparing the average Hamming distance, we validate results that fall within a trusted threshold as reliable, while flagging those that exceed the threshold for further inspection. This approach not only allows researchers to validate the correctness of GWAS findings of other researchers, but it also provides a self-check step for the researchers before they publish their findings. We evaluate our approach using three real-life SNP datasets from OpenSNP, showing its ability to detect unintentional errors effectively, even when small errors occur, such as 1\% of SNPs being reported incorrectly. This novel validation technique offers a promising solution to the GWAS reproducibility challenge, balancing the need for rigorous validation with the imperative of protecting sensitive genomic data, thereby enhancing trust and accuracy in genetic research.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
From Genotype to Phenotype: polygenic prediction of complex human traits,"Decoding the genome confers the capability to predict characteristics of the organism(phenotype) from DNA (genotype). We describe the present status and future prospects of genomic prediction of complex traits in humans. Some highly heritable complex phenotypes such as height and other quantitative traits can already be predicted with reasonable accuracy from DNA alone. For many diseases, including important common conditions such as coronary artery disease, breast cancer, type I and II diabetes, individuals with outlier polygenic scores (e.g., top few percent) have been shown to have 5 or even 10 times higher risk than average. Several psychiatric conditions such as schizophrenia and autism also fall into this category. We discuss related topics such as the genetic architecture of complex traits, sibling validation of polygenic scores, and applications to adult health, in vitro fertilization (embryo selection), and genetic engineering.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Applications of Machine Learning Methods to Quantifying Phenotypic Traits that Distinguish the Wild Type from the Mutant Arabidopsis Thaliana Seedlings during Root Gravitropism,"Post-genomic research deals with challenging problems in screening genomes of organisms for particular functions or potential for being the targets of genetic engineering for desirable biological features. 'Phenotyping' of wild type and mutants is a time-consuming and costly effort by many individuals. This article is a preliminary progress report in research on large-scale automation of phenotyping steps (imaging, informatics and data analysis) needed to study plant gene-proteins networks that influence growth and development of plants. Our results undermine the significance of phenotypic traits that are implicit in patterns of dynamics in plant root response to sudden changes of its environmental conditions, such as sudden re-orientation of the root tip against the gravity vector. Including dynamic features besides the common morphological ones has paid off in design of robust and accurate machine learning methods to automate a typical phenotyping scenario, i.e. to distinguish the wild type from the mutants.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Bivariate Causal Discovery and its Applications to Gene Expression and Imaging Data Analysis,"The mainstream of research in genetics, epigenetics and imaging data analysis focuses on statistical association or exploring statistical dependence between variables. Despite their significant progresses in genetic research, understanding the etiology and mechanism of complex phenotypes remains elusive. Using association analysis as a major analytical platform for the complex data analysis is a key issue that hampers the theoretic development of genomic science and its application in practice. Causal inference is an essential component for the discovery of mechanical relationships among complex phenotypes. Many researchers suggest making the transition from association to causation. Despite its fundamental role in science, engineering and biomedicine, the traditional methods for causal inference require at least three variables. However, quantitative genetic analysis such as QTL, eQTL, mQTL, and genomic-imaging data analysis requires exploring the causal relationships between two variables. This paper will focus on bivariate causal discovery. We will introduce independence of cause and mechanism (ICM) as a basic principle for causal inference, algorithmic information theory and additive noise model (ANM) as major tools for bivariate causal discovery. Large-scale simulations will be performed to evaluate the feasibility of the ANM for bivariate causal discovery. To further evaluate their performance for causal inference, the ANM will be applied to the construction of gene regulatory networks. Also, the ANM will be applied to trait-imaging data analysis to illustrate three scenarios: presence of both causation and association, presence of association while absence of causation, and presence of causation, while lack of association between two variables.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Characteristics of transposable element exonization within human and mouse,"Insertion of transposed elements within mammalian genes is thought to be an important contributor to mammalian evolution and speciation. Insertion of transposed elements into introns can lead to their activation as alternatively spliced cassette exons, an event called exonization. Elucidation of the evolutionary constraints that have shaped fixation of transposed elements within human and mouse protein coding genes and subsequent exonization is important for understanding of how the exonization process has affected transcriptome and proteome complexities. Here we show that exonization of transposed elements is biased towards the beginning of the coding sequence in both human and mouse genes. Analysis of single nucleotide polymorphisms (SNPs) revealed that exonization of transposed elements can be population-specific, implying that exonizations may enhance divergence and lead to speciation. SNP density analysis revealed differences between Alu and other transposed elements. Finally, we identified cases of primate-specific Alu elements that depend on RNA editing for their exonization. These results shed light on TE fixation and the exonization process within human and mouse genes.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Host immune response driving SARS-CoV-2 evolution,"The transmission and evolution of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) are of paramount importance to the controlling and combating of coronavirus disease 2019 (COVID-19) pandemic. Currently, near 15,000 SARS-CoV-2 single mutations have been recorded, having a great ramification to the development of diagnostics, vaccines, antibody therapies, and drugs. However, little is known about SARS-CoV-2 evolutionary characteristics and general trend. In this work, we present a comprehensive genotyping analysis of existing SARS-CoV-2 mutations. We reveal that host immune response via APOBEC and ADAR gene editing gives rise to near 65\% of recorded mutations. Additionally, we show that children under age five and the elderly may be at high risk from COVID-19 because of their overreacting to the viral infection. Moreover, we uncover that populations of Oceania and Africa react significantly more intensively to SARS-CoV-2 infection than those of Europe and Asia, which may explain why African Americans were shown to be at increased risk of dying from COVID-19, in addition to their high risk of getting sick from COVID-19 caused by systemic health and social inequities. Finally, our study indicates that for two viral genome sequences of the same origin, their evolution order may be determined from the ratio of mutation type C$>$T over T$>$C.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Quantifying selection in immune receptor repertoires,"The efficient recognition of pathogens by the adaptive immune system relies on the diversity of receptors displayed at the surface of immune cells. T-cell receptor diversity results from an initial random DNA editing process, called VDJ recombination, followed by functional selection of cells according to the interaction of their surface receptors with self and foreign antigenic peptides. To quantify the effect of selection on the highly variable elements of the receptor, we apply a probabilistic maximum likelihood approach to the analysis of high-throughput sequence data from the $$-chain of human T-cell receptors. We quantify selection factors for V and J gene choice, and for the length and amino-acid composition of the variable region. Our approach is necessary to disentangle the effects of selection from biases inherent in the recombination process. Inferred selection factors differ little between donors, or between naive and memory repertoires. The number of sequences shared between donors is well-predicted by the model, indicating a purely stochastic origin of such ""public"" sequences. We find a significant correlation between biases induced by VDJ recombination and our inferred selection factors, together with a reduction of diversity during selection. Both effects suggest that natural selection acting on the recombination process has anticipated the selection pressures experienced during somatic evolution.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Are we far from correctly inferring gene interaction networks with Lasso?,"Detecting the interactions of genetic compounds like genes, SNPs, proteins, metabolites, etc. can potentially unravel the mechanisms behind complex traits and common genetic disorders. Several methods have been taken into consideration for the analysis of different types of genetic data, regression being one of the most widely adopted. Without any doubt, a common data type is represented by gene expression profiles, from which gene regulatory networks have been inferred with different approaches. In this work we review nine penalised regression methods applied to microarray data to infer the topology of the network of interactions. We evaluate each method with respect to the complexity of biological data. We analyse the limitations of each of them in order to suggest a number of precautions that should be considered to make their predictions more significant and reliable.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes,"Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
An explanation of unexpected Hoxd expressions in mutant mice,"The Hox gene collinearity enigma has often been approached using models based on biomolecular mechanisms. The biophysical model, is an alternative approach, speculating that collinearity is caused by physical forces pulling the Hox clusters from a territory where they are inactive to a distinct spatial domain where they are activated in a step by step manner.   Hox gene translocations have recently been observed in support of the biophysical model. Furthermore, genetic engineering experiments, performed in embryonic mice, gave rise to some unexpected mutant expressions that biomolecular models could not predict. In several cases when anterior Hoxd genes are deleted, the expression of the genes whose expression is probed in the mutants are impossible to anticipate. On the contrary, the biophysical model offers convincing explanation.   All these experimental results support the idea of physical forces being responsible for Hox gene collinearity. In order to test the validity of the various models further, certain experiment involving gene deletions are proposed. The biophysical and biomolecular models predict different results for these experiments, hence the expected outcome will confirm or question the validity of these models.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
A framework for cost-constrained genome rearrangement under Double Cut and Join,"The study of genome rearrangement has many flavours, but they all are somehow tied to edit distances on variations of a multi-graph called the breakpoint graph. We study a weighted 2-break distance on Eulerian 2-edge-colored multi-graphs, which generalizes weighted versions of several Double Cut and Join problems, including those on genomes with unequal gene content. We affirm the connection between cycle decompositions and edit scenarios first discovered with the Sorting By Reversals problem. Using this we show that the problem of finding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored multi-graph - with a general cost function for 2-breaks - can be solved by decomposing the problem into independent instances on simple alternating cycles. For breakpoint graphs, and a more constrained cost function, based on coloring the vertices, we give a polynomial-time algorithm for finding a parsimonious 2-break scenario of minimum cost, while showing that finding a non-parsimonious 2-break scenario of minimum cost is NP-Hard.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Machine Learning Based Multimodal Neuroimaging Genomics Dementia Score for Predicting Future Conversion to Alzheimer's Disease,"Background: The increasing availability of databases containing both magnetic resonance imaging (MRI) and genetic data allows researchers to utilize multimodal data to better understand the characteristics of dementia of Alzheimer's type (DAT). Objective: The goal of this study was to develop and analyze novel biomarkers that can help predict the development and progression of DAT. Methods: We used feature selection and ensemble learning classifier to develop an image/genotype-based DAT score that represents a subject's likelihood of developing DAT in the future. Three feature types were used: MRI only, genetic only, and combined multimodal data. We used a novel data stratification method to better represent different stages of DAT. Using a pre-defined 0.5 threshold on DAT scores, we predicted whether or not a subject would develop DAT in the future. Results: Our results on Alzheimer's Disease Neuroimaging Initiative (ADNI) database showed that dementia scores using genetic data could better predict future DAT progression for currently normal control subjects (Accuracy=0.857) compared to MRI (Accuracy=0.143), while MRI can better characterize subjects with stable mild cognitive impairment (Accuracy=0.614) compared to genetics (Accuracy=0.356). Combining MRI and genetic data showed improved classification performance in the remaining stratified groups. Conclusion: MRI and genetic data can contribute to DAT prediction in different ways. MRI data reflects anatomical changes in the brain, while genetic data can detect the risk of DAT progression prior to the symptomatic onset. Combining information from multimodal data in the right way can improve prediction performance.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences,"Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
"Evolutionary Algorithms: Concepts, Designs, and Applications in Bioinformatics: Evolutionary Algorithms for Bioinformatics","Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in the early 1970s, the study of evolutionary algorithm has emerged as a popular research field (Civicioglu & Besdok, 2013). Researchers from various scientific and engineering disciplines have been digging into this field, exploring the unique power of evolutionary algorithms (Hadka & Reed, 2013). Many applications have been successfully proposed in the past twenty years. For example, mechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization (Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice, Moretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration (Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, & Torresen, 2010), and nuclear reactor core design (Sacco, Henderson, Rios-Coelho, Ali, & Pereira, 2009). In particular, its function optimization capability was highlighted (Goldberg & Richardson, 1987) because of its high adaptability to different function landscapes, to which we cannot apply traditional optimization techniques (Wong, Leung, & Wong, 2009). Here we review the applications of evolutionary algorithms in bioinformatics.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
PCA and K-Means decipher genome,"In this paper, we aim to give a tutorial for undergraduate students studying statistical methods and/or bioinformatics. The students will learn how data visualization can help in genomic sequence analysis. Students start with a fragment of genetic text of a bacterial genome and analyze its structure. By means of principal component analysis they ``discover'' that the information in the genome is encoded by non-overlapping triplets. Next, they learn how to find gene positions. This exercise on PCA and K-Means clustering enables active study of the basic bioinformatics notions. Appendix 1 contains program listings that go along with this exercise. Appendix 2 includes 2D PCA plots of triplet usage in moving frame for a series of bacterial genomes from GC-poor to GC-rich ones. Animated 3D PCA plots are attached as separate gif files. Topology (cluster structure) and geometry (mutual positions of clusters) of these plots depends clearly on GC-content.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Machine Learning-Based Prediction of Key Genes Correlated to the Subretinal Lesion Severity in a Mouse Model of Age-Related Macular Degeneration,"Age-related macular degeneration (AMD) is a major cause of blindness in older adults, severely affecting vision and quality of life. Despite advances in understanding AMD, the molecular factors driving the severity of subretinal scarring (fibrosis) remain elusive, hampering the development of effective therapies. This study introduces a machine learning-based framework to predict key genes that are strongly correlated with lesion severity and to identify potential therapeutic targets to prevent subretinal fibrosis in AMD. Using an original RNA sequencing (RNA-seq) dataset from the diseased retinas of JR5558 mice, we developed a novel and specific feature engineering technique, including pathway-based dimensionality reduction and gene-based feature expansion, to enhance prediction accuracy. Two iterative experiments were conducted by leveraging Ridge and ElasticNet regression models to assess biological relevance and gene impact. The results highlight the biological significance of several key genes and demonstrate the framework's effectiveness in identifying novel therapeutic targets. The key findings provide valuable insights for advancing drug discovery efforts and improving treatment strategies for AMD, with the potential to enhance patient outcomes by targeting the underlying genetic mechanisms of subretinal lesion development.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
OncoEnrichR: cancer-dedicated gene set interpretation,"Genome-scale screening experiments in cancer produce long lists of candidate genes that require extensive interpretation for biological insight and prioritization for follow-up studies. Interrogation of gene lists frequently represents a significant and time-consuming undertaking, in which experimental biologists typically combine results from a variety of bioinformatics resources in an attempt to portray and understand cancer relevance. As a means to simplify and strengthen the support for this endeavor, we have developed oncoEnrichR, a flexible bioinformatics tool that allows cancer researchers to comprehensively interrogate a given gene list along multiple facets of cancer relevance. oncoEnrichR differs from general gene set analysis frameworks through the integration of an extensive set of prior knowledge specifically relevant for cancer, including ranked gene-tumor type associations, literature-supported proto-oncogene and tumor suppressor gene annotations, target druggability data, regulatory interactions, synthetic lethality predictions, as well as prognostic associations, gene aberrations, and co-expression patterns across tumor types. The software produces a structured and user-friendly analysis report as its main output, where versions of all underlying data resources are explicitly logged, the latter being a critical component for reproducible science. We demonstrate the usefulness of oncoEnrichR through interrogation of two candidate lists from proteomic and CRISPR screens. oncoEnrichR is freely available as a web-based workflow hosted by the Galaxy platform (https://oncotools.elixir.no), and can also be accessed as a stand-alone R package (https://github.com/sigven/oncoEnrichR).","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
inPHAP: Interactive visualization of genotype and phased haplotype data,"Background: To understand individual genomes it is necessary to look at the variations that lead to changes in phenotype and possibly to disease. However, genotype information alone is often not sufficient and additional knowledge regarding the phase of the variation is needed to make correct interpretations. Interactive visualizations, that allow the user to explore the data in various ways, can be of great assistance in the process of making well informed decisions. But, currently there is a lack for visualizations that are able to deal with phased haplotype data. Results: We present inPHAP, an interactive visualization tool for genotype and phased haplotype data. inPHAP features a variety of interaction possibilities such as zooming, sorting, filtering and aggregation of rows in order to explore patterns hidden in large genetic data sets. As a proof of concept, we apply inPHAP to the phased haplotype data set of Phase 1 of the 1000 Genomes Project. Thereby, inPHAP's ability to show genetic variations on the population as well as on the individuals level is demonstrated for several disease related loci. Conclusions: As of today, inPHAP is the only visual analytical tool that allows the user to explore unphased and phased haplotype data interactively. Due to its highly scalable design, inPHAP can be applied to large datasets with up to 100 GB of data, enabling users to visualize even large scale input data. inPHAP closes the gap between common visualization tools for unphased genotype data and introduces several new features, such as the visualization of phased data.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Machine Learning Applications for Therapeutic Tasks with Genomics Data,"Thanks to the increasing availability of genomics and other biomedical data, many machine learning approaches have been proposed for a wide range of therapeutic discovery and development tasks. In this survey, we review the literature on machine learning applications for genomics through the lens of therapeutic development. We investigate the interplay among genomics, compounds, proteins, electronic health records (EHR), cellular images, and clinical texts. We identify twenty-two machine learning in genomics applications across the entire therapeutics pipeline, from discovering novel targets, personalized medicine, developing gene-editing tools all the way to clinical trials and post-market studies. We also pinpoint seven important challenges in this field with opportunities for expansion and impact. This survey overviews recent research at the intersection of machine learning, genomics, and therapeutic development.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
GenomeFingerprinter and universal genome fingerprint analysis for systematic comparative genomics,"How to compare whole genome sequences at large scale has not been achieved via conventional methods based on pair-wisely base-to-base comparison; nevertheless, no attention was paid to handle in-one-sitting a number of genomes crossing genetic category (chromosome, plasmid, and phage) with farther divergences (much less or no homologous) over large size ranges (from Kbp to Mbp). We created a new method, GenomeFingerprinter, to unambiguously produce three-dimensional coordinates from a sequence, followed by one three-dimensional plot and six two-dimensional trajectory projections to illustrate whole genome fingerprints. We further developed a set of concepts and tools and thereby established a new method, universal genome fingerprint analysis. We demonstrated their applications through case studies on over a hundred of genome sequences. Particularly, we defined the total genetic component configuration (TGCC) (i.e., chromosome, plasmid, and phage) for describing a strain as a system, and the universal genome fingerprint map (UGFM) of TGCC for differentiating a strain as a universal system, as well as the systematic comparative genomics (SCG) for comparing in-one-sitting a number of genomes crossing genetic category in diverse strains. By using UGFM, UGFM-TGCC, and UGFM-TGCC-SCG, we compared a number of genome sequences with farther divergences (chromosome, plasmid, and phage; bacterium, archaeal bacterium, and virus) over large size ranges (6Kbp~5Mbp), giving new insights into critical problematic issues in microbial genomics in the post-genomic era. This paper provided a new method for rapidly computing, geometrically visualizing, and intuitively comparing genome sequences at fingerprint level, and hence established a new method of universal genome fingerprint analysis for systematic comparative genomics.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Cancer classification and pathway discovery using non-negative matrix factorization,"Extracting genetic information from a full range of sequencing data is important for understanding diseases. We propose a novel method to effectively explore the landscape of genetic mutations and aggregate them to predict cancer type. We used multinomial logistic regression, nonsmooth non-negative matrix factorization (nsNMF), and support vector machine (SVM) to utilize the full range of sequencing data, aiming at better aggregating genetic mutations and improving their power in predicting cancer types. Specifically, we introduced a classifier to distinguish cancer types using somatic mutations obtained from whole-exome sequencing data. Mutations were identified from multiple cancers and scored using SIFT, PP2, and CADD, and grouped at the individual gene level. The nsNMF was then applied to reduce dimensionality and to obtain coefficient and basis matrices. A feature matrix was derived from the obtained matrices to train a classifier for cancer type classification with the SVM model. We have demonstrated that the classifier was able to distinguish the cancer types with reasonable accuracy. In five-fold cross-validations using mutation counts as features, the average prediction accuracy was 77.1% (SEM=0.1%), significantly outperforming baselines and outperforming models using mutation scores as features. Using the factor matrices derived from the nsNMF, we identified multiple genes and pathways that are significantly associated with each cancer type. This study presents a generic and complete pipeline to study the associations between somatic mutations and cancers. The discovered genes and pathways associated with each cancer type can lead to biological insights. The proposed method can be adapted to other studies for disease classification and pathway discovery.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Comprehensive Detection of Genes Causing a Phenotype using Phenotype Sequencing and Pathway Analysis,"Discovering all the genetic causes of a phenotype is an important goal in functional genomics. In this paper we combine an experimental design for multiple independent detections of the genetic causes of a phenotype, with a high-throughput sequencing analysis that maximizes sensitivity for comprehensively identifying them. Testing this approach on a set of 24 mutant strains generated for a metabolic phenotype with many known genetic causes, we show that this pathway-based phenotype sequencing analysis greatly improves sensitivity of detection compared with previous methods, and reveals a wide range of pathways that can cause this phenotype. We demonstrate our approach on a metabolic re-engineering phenotype, the PEP/OAA metabolic node in E. coli, which is crucial to a substantial number of metabolic pathways and under renewed interest for biofuel research. Out of 2157 mutations in these strains, pathway-phenoseq discriminated just five gene groups (12 genes) as statistically significant causes of the phenotype. Experimentally, these five gene groups, and the next two high-scoring pathway-phenoseq groups, either have a clear connection to the PEP metabolite level or offer an alternative path of producing oxaloacetate (OAA), and thus clearly explain the phenotype. These high-scoring gene groups also show strong evidence of positive selection pressure, compared with strictly neutral selection in the rest of the genome.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
DCJVis: visualization of genome rearrangements using DCJ operations,"The {\em double-cut-and-join} (DCJ) operation, introduced by Yancopoulos \emph{et al.}, allows minimum edit distance to be computed by modeling all possible classical rearrangement operations, such as inversions, fusions, fissions, translocations, and transpositions, in linear-time between two genomes. However, there is lack of visualization tool that can effectively present DCJ operations that will help biologists to use DCJ operation. In this paper, a new visualization program is introduced, DCJVis, to create a diagram of each DCJ operation necessary to transform between the genomes of two distinct organisms by describing a possible sequence of genome graphs based on the selected gene adjacency on the source genome for the DCJ operation. Our program is the first visualization tool for DCJ operations using circular layout. Specifically, the genomes of \textit{Saccharomyces cerevisiae} and \textit{Candida albicans} are used to demonstrate the functionality of this program and provide an example of the type of problem this program can solve for biologists.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Data navigation on the ENCODE portal,"Spanning two decades, the Encyclopaedia of DNA Elements (ENCODE) is a collaborative research project that aims to identify all the functional elements in the human and mouse genomes. To best serve the scientific community, all data generated by the consortium is shared through a web-portal (https://www.encodeproject.org/) with no access restrictions. The fourth and final phase of the project added a diverse set of new samples (including those associated with human disease), and a wide range of new assays aimed at detection, characterization and validation of functional genomic elements. The ENCODE data portal hosts results from over 23,000 functional genomics experiments, over 800 functional elements characterization experiments (including in vivo transgenic enhancer assays, reporter assays and CRISPR screens) along with over 60,000 results of computational and integrative analyses (including imputations, predictions and genome annotations). The ENCODE Data Coordination Center (DCC) is responsible for development and maintenance of the data portal, along with the implementation and utilisation of the ENCODE uniform processing pipelines to generate uniformly processed data. Here we report recent updates to the data portal. Specifically, we have completely redesigned the home page, improved search interface, added several new pages to highlight collections of biologically related data (deeply profiled cell lines, immune cells, Alzheimer's Disease, RNA-Protein interactions, degron matrix and a matrix of experiments organised by human donors), added single-cell experiments, and enhanced the cart interface for visualisation and download of user-selected datasets.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Combining exome and gene expression datasets in one graphical model of disease to empower the discovery of disease mechanisms,"Identifying genes associated with complex human diseases is one of the main challenges of human genetics and computational medicine. To answer this question, millions of genetic variants get screened to identify a few of importance. To increase the power of identifying genes associated with diseases and to account for other potential sources of protein function aberrations, we propose a novel factor-graph based model, where much of the biological knowledge is incorporated through factors and priors. Our extensive simulations show that our method has superior sensitivity and precision compared to variant-aggregating and differential expression methods. Our integrative approach was able to identify important genes in breast cancer, identifying genes that had coding aberrations in some patients and regulatory abnormalities in others, emphasizing the importance of data integration to explain the disease in a larger number of patients.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Cell lineage tracing using nuclease barcoding,"Lineage tracing, the determination and mapping of progeny arising from single cells, is an important approach enabling the elucidation of mechanisms underlying diverse biological processes ranging from development to disease. We developed a dynamic sequence-based barcode for lineage tracing and have demonstrated its performance in C. elegans, a model organism whose lineage tree is well established. The strategy we use creates lineage trees based upon the introduction of specific mutations into cells and the propagation of these mutations to daughter cells at each cell division. We present an experimental proof of concept along with a corresponding simulation and analytical model for deeper understanding of the coding capacity of the system. By introducing mutations in a predictable manner using CRISPR/Cas9, our technology will enable more complete investigations of cellular processes.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Exemplar or Matching: Modeling DCJ Problems with Unequal Content Genome Data,"The edit distance under the DCJ model can be computed in linear time for genomes with equal content or with Indels. But it becomes NP-Hard in the presence of duplications, a problem largely unsolved especially when Indels are considered. In this paper, we compare two mainstream methods to deal with duplications and associate them with Indels: one by deletion, namely DCJ-Indel-Exemplar distance; versus the other by gene matching, namely DCJ-Indel-Matching distance. We design branch-and-bound algorithms with set of optimization methods to compute exact distances for both. Furthermore, median problems are discussed in alignment with both of these distance methods, which are to find a median genome that minimizes distances between itself and three given genomes. Lin-Kernighan (LK) heuristic is leveraged and powered up by sub-graph decomposition and search space reduction technologies to handle median computation. A wide range of experiments are conducted on synthetic data sets and real data sets to show pros and cons of these two distance metrics per se, as well as putting them in the median computation scenario.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Probabilistic analysis of the human transcriptome with side information,"Understanding functional organization of genetic information is a major challenge in modern biology. Following the initial publication of the human genome sequence in 2001, advances in high-throughput measurement technologies and efficient sharing of research material through community databases have opened up new views to the study of living organisms and the structure of life. In this thesis, novel computational strategies have been developed to investigate a key functional layer of genetic information, the human transcriptome, which regulates the function of living cells through protein synthesis. The key contributions of the thesis are general exploratory tools for high-throughput data analysis that have provided new insights to cell-biological networks, cancer mechanisms and other aspects of genome function.   A central challenge in functional genomics is that high-dimensional genomic observations are associated with high levels of complex and largely unknown sources of variation. By combining statistical evidence across multiple measurement sources and the wealth of background information in genomic data repositories it has been possible to solve some the uncertainties associated with individual observations and to identify functional mechanisms that could not be detected based on individual measurement sources. Statistical learning and probabilistic models provide a natural framework for such modeling tasks. Open source implementations of the key methodological contributions have been released to facilitate further adoption of the developed methods by the research community.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Iterative Learning for Reference-Guided DNA Sequence Assembly from Short Reads: Algorithms and Limits of Performance,"Recent emergence of next-generation DNA sequencing technology has enabled acquisition of genetic information at unprecedented scales. In order to determine the genetic blueprint of an organism, sequencing platforms typically employ so-called shotgun sequencing strategy to oversample the target genome with a library of relatively short overlapping reads. The order of nucleotides in the reads is determined by processing the acquired noisy signals generated by the sequencing instrument. Assembly of a genome from potentially erroneous short reads is a computationally daunting task even in the scenario where a reference genome exists. Errors and gaps in the reference, and perfect repeat regions in the target, further render the assembly challenging and cause inaccuracies. In this paper, we formulate the reference-guided sequence assembly problem as the inference of the genome sequence on a bipartite graph and solve it using a message-passing algorithm. The proposed algorithm can be interpreted as the well-known classical belief propagation scheme under a certain prior. Unlike existing state-of-the-art methods, the proposed algorithm combines the information provided by the reads without needing to know reliability of the short reads (so-called quality scores). Relation of the message-passing algorithm to a provably convergent power iteration scheme is discussed. To evaluate and benchmark the performance of the proposed technique, we find an analytical expression for the probability of error of a genie-aided maximum a posteriori (MAP) decision scheme. Results on both simulated and experimental data demonstrate that the proposed message-passing algorithm outperforms commonly used state-of-the-art tools, and it nearly achieves the performance of the aforementioned MAP decision scheme.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Deep Learning Classification of Polygenic Obesity using Genome Wide Association Study SNPs,"In this paper, association results from genome-wide association studies (GWAS) are combined with a deep learning framework to test the predictive capacity of statistically significant single nucleotide polymorphism (SNPs) associated with obesity phenotype. Our approach demonstrates the potential of deep learning as a powerful framework for GWAS analysis that can capture information about SNPs and the important interactions between them. Basic statistical methods and techniques for the analysis of genetic SNP data from population-based genome-wide studies have been considered. Statistical association testing between individual SNPs and obesity was conducted under an additive model using logistic regression. Four subsets of loci after quality-control (QC) and association analysis were selected: P-values lower than 1x10-5 (5 SNPs), 1x10-4 (32 SNPs), 1x10-3 (248 SNPs) and 1x10-2 (2465 SNPs). A deep learning classifier is initialised using these sets of SNPs and fine-tuned to classify obese and non-obese observations. Using a deep learning classifier model and genetic variants with P-value < 1x10-2 (2465 SNPs) it was possible to obtain results (SE=0.9604, SP=0.9712, Gini=0.9817, LogLoss=0.1150, AUC=0.9908 and MSE=0.0300). As the P-value increased, an evident deterioration in performance was observed. Results demonstrate that single SNP analysis fails to capture the cumulative effect of less significant variants and their overall contribution to the outcome in disease prediction, which is captured using a deep learning framework.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Analysis of Microarray Data using Artificial Intelligence Based Techniques,"Microarray is one of the essential technologies used by the biologist to measure genome-wide expression levels of genes in a particular organism under some particular conditions or stimuli. As microarrays technologies have become more prevalent, the challenges of analyzing these data for getting better insight about biological processes have essentially increased. Due to availability of artificial intelligence based sophisticated computational techniques, such as artificial neural networks, fuzzy logic, genetic algorithms, and many other nature-inspired algorithms, it is possible to analyse microarray gene expression data in more better way. Here, we reviewed artificial intelligence based techniques for the analysis of microarray gene expression data. Further, challenges in the field and future work direction have also been suggested.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Algorithms for Large-scale Whole Genome Association Analysis,"In order to associate complex traits with genetic polymorphisms, genome-wide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms. When handling these datasets, which exceed the main memory of contemporary computers, one faces two distinct challenges: 1) Millions of polymorphisms come at the cost of hundreds of Gigabytes of genotype data, which can only be kept in secondary storage; 2) the relatedness of the test population is represented by a covariance matrix, which, for large populations, can only fit in the combined main memory of a distributed architecture. In this paper, we present solutions for both challenges: The genotype data is streamed from and to secondary storage using a double buffering technique, while the covariance matrix is kept across the main memory of a distributed memory system. We show that these methods sustain high-performance and allow the analysis of enormous dataset","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
The transmission sense of information,"Biologists rely heavily on the language of information, coding, and transmission that is commonplace in the field of information theory as developed by Claude Shannon, but there is open debate about whether such language is anything more than facile metaphor. Philosophers of biology have argued that when biologists talk about information in genes and in evolution, they are not talking about the sort of information that Shannon's theory addresses. First, philosophers have suggested that Shannon theory is only useful for developing a shallow notion of correlation, the so-called ""causal sense"" of information. Second they typically argue that in genetics and evolutionary biology, information language is used in a ""semantic sense,"" whereas semantics are deliberately omitted from Shannon theory. Neither critique is well-founded. Here we propose an alternative to the causal and semantic senses of information: a transmission sense of information, in which an object X conveys information if the function of X is to reduce, by virtue of its sequence properties, uncertainty on the part of an agent who observes X. The transmission sense not only captures much of what biologists intend when they talk about information in genes, but also brings Shannon's theory back to the fore. By taking the viewpoint of a communications engineer and focusing on the decision problem of how information is to be packaged for transport, this approach resolves several problems that have plagued the information concept in biology, and highlights a number of important features of the way that information is encoded, stored, and transmitted as genetic sequence.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Spaced seeds improve k-mer-based metagenomic classification,"Metagenomics is a powerful approach to study genetic content of environmental samples that has been strongly promoted by NGS technologies. To cope with massive data involved in modern metagenomic projects, recent tools [4, 39] rely on the analysis of k-mers shared between the read to be classified and sampled reference genomes. Within this general framework, we show in this work that spaced seeds provide a significant improvement of classification accuracy as opposed to traditional contiguous k-mers. We support this thesis through a series a different computational experiments, including simulations of large-scale metagenomic projects. Scripts and programs used in this study, as well as supplementary material, are available from http://github.com/gregorykucherov/spaced-seeds-for-metagenomics.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Fast computation of the principal components of genotype matrices in Julia,"Finding the largest few principal components of a matrix of genetic data is a common task in genome-wide association studies (GWASs), both for dimensionality reduction and for identifying unwanted factors of variation. We describe a simple random matrix model for matrices that arise in GWASs, showing that the singular values have a bulk behavior that obeys a Marchenko-Pastur distributed with a handful of large outliers. We also implement Golub-Kahan-Lanczos (GKL) bidiagonalization in the Julia programming language, providing thick restarting and a choice between full and partial reorthogonalization strategies to control numerical roundoff. Our implementation of GKL bidiagonalization is up to 36 times faster than software tools used commonly in genomics data analysis for computing principal components, such as EIGENSOFT and FlashPCA, which use dense LAPACK routines and randomized subspace iteration respectively.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
A genome-scale deep learning model to predict gene expression changes of genetic perturbations from multiplex biological networks,"Systematic characterization of biological effects to genetic perturbation is essential to the application of molecular biology and biomedicine. However, the experimental exhaustion of genetic perturbations on the genome-wide scale is challenging. Here, we show that TranscriptionNet, a deep learning model that integrates multiple biological networks to systematically predict transcriptional profiles to three types of genetic perturbations based on transcriptional profiles induced by genetic perturbations in the L1000 project: RNA interference (RNAi), clustered regularly interspaced short palindromic repeat (CRISPR) and overexpression (OE). TranscriptionNet performs better than existing approaches in predicting inducible gene expression changes for all three types of genetic perturbations. TranscriptionNet can predict transcriptional profiles for all genes in existing biological networks and increases perturbational gene expression changes for each type of genetic perturbation from a few thousand to 26,945 genes. TranscriptionNet demonstrates strong generalization ability when comparing predicted and true gene expression changes on different external tasks. Overall, TranscriptionNet can systemically predict transcriptional consequences induced by perturbing genes on a genome-wide scale and thus holds promise to systemically detect gene function and enhance drug development and target discovery.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
High Performance Solutions for Big-data GWAS,"In order to associate complex traits with genetic polymorphisms, genome-wide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms. When handling these datasets, which exceed the main memory of contemporary computers, one faces two distinct challenges: 1) Millions of polymorphisms and thousands of phenotypes come at the cost of hundreds of gigabytes of data, which can only be kept in secondary storage; 2) the relatedness of the test population is represented by a relationship matrix, which, for large populations, can only fit in the combined main memory of a distributed architecture. In this paper, by using distributed resources such as Cloud or clusters, we address both challenges: The genotype and phenotype data is streamed from secondary storage using a double buffer- ing technique, while the relationship matrix is kept across the main memory of a distributed memory system. With the help of these solutions, we develop separate algorithms for studies involving only one or a multitude of traits. We show that these algorithms sustain high-performance and allow the analysis of enormous datasets.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Prediction of a Gene Regulatory Network from Gene Expression Profiles With Linear Regression and Pearson Correlation Coefficient,"Reconstruction of gene regulatory networks is the process of identifying gene dependency from gene expression profile through some computation techniques. In our human body, though all cells pose similar genetic material but the activation state may vary. This variation in the activation of genes helps researchers to understand more about the function of the cells. Researchers get insight about diseases like mental illness, infectious disease, cancer disease and heart disease from microarray technology, etc. In this study, a cancer-specific gene regulatory network has been constructed using a simple and novel machine learning approach. In First Step, linear regression algorithm provided us the significant genes those expressed themselves differently. Next, regulatory relationships between the identified genes has been computed using Pearson correlation coefficient. Finally, the obtained results have been validated with the available databases and literatures. We can identify the hub genes and can be targeted for the cancer diagnosis.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
-cyc: A Reference-free SNP Discovery Application using Parallel Graph Search,"Motivation: Working with a large number of genomes simultaneously is of great interest in genetic population and comparative genomics research. Bubbles discovery in multi-genomes coloured de bruijn graph for de novo genome assembly is a problem that can be translated to cycles enumeration in graph theory. Cycle enumerations algorithms in big and complex de Bruijn graphs are time consuming. Specialised fast algorithms for efficient bubble search are needed for coloured de bruijn graph variant calling applications. In coloured de Bruijn graphs, bubble paths coverages are used in downstream variants calling analysis. Results: In this paper, we introduce a fast parallel graph search for different K-mer cycle sizes. Coloured path coverages are used for SNP prediction. The graph search method uses a combined multi-node and multi-core design to speeds up cycles enumeration. The search algorithm uses an index extracted from the raw assembly of a coloured de Bruijn graph stored in a hash table. The index is distributed across different CPU-cores, in a shared memory HPC compute node, to build undirected subgraphs then search independently and simultaneously specific cycle sizes. This same index can also be split between several HPC compute nodes to take advantage of as many CPU-cores available to the user. The local neighbourhood parallel search approach reduces the graph's complexity and facilitate cycles search of a multi-colour de Bruijn graph. The search algorithm is incorporated into $$-cyc application and tested on a number of Schizosaccharomyces Pombe genomes. Availability: $$-cyc is an open-source software available at www.github.com/2kplus2P","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
RNA-GPT: Multimodal Generative System for RNA Sequence Understanding,"RNAs are essential molecules that carry genetic information vital for life, with profound implications for drug development and biotechnology. Despite this importance, RNA research is often hindered by the vast literature available on the topic. To streamline this process, we introduce RNA-GPT, a multi-modal RNA chat model designed to simplify RNA discovery by leveraging extensive RNA literature. RNA-GPT integrates RNA sequence encoders with linear projection layers and state-of-the-art large language models (LLMs) for precise representation alignment, enabling it to process user-uploaded RNA sequences and deliver concise, accurate responses. Built on a scalable training pipeline, RNA-GPT utilizes RNA-QA, an automated system that gathers RNA annotations from RNACentral using a divide-and-conquer approach with GPT-4o and latent Dirichlet allocation (LDA) to efficiently handle large datasets and generate instruction-tuning samples. Our experiments indicate that RNA-GPT effectively addresses complex RNA queries, thereby facilitating RNA research. Additionally, we present RNA-QA, a dataset of 407,616 RNA samples for modality alignment and instruction tuning, further advancing the potential of RNA research tools.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
BOOST: A fast approach to detecting gene-gene interactions in genome-wide case-control studies,"Gene-gene interactions have long been recognized to be fundamentally important to understand genetic causes of complex disease traits. At present, identifying gene-gene interactions from genome-wide case-control studies is computationally and methodologically challenging. In this paper, we introduce a simple but powerful method, named `BOolean Operation based Screening and Testing'(BOOST). To discover unknown gene-gene interactions that underlie complex diseases, BOOST allows examining all pairwise interactions in genome-wide case-control studies in a remarkably fast manner. We have carried out interaction analyses on seven data sets from the Wellcome Trust Case Control Consortium (WTCCC). Each analysis took less than 60 hours on a standard 3.0 GHz desktop with 4G memory running Windows XP system. The interaction patterns identified from the type 1 diabetes data set display significant difference from those identified from the rheumatoid arthritis data set, while both data sets share a very similar hit region in the WTCCC report. BOOST has also identified many undiscovered interactions between genes in the major histocompatibility complex (MHC) region in the type 1 diabetes data set. In the coming era of large-scale interaction mapping in genome-wide case-control studies, our method can serve as a computationally and statistically useful tool.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Multi-omics Prediction from High-content Cellular Imaging with Deep Learning,"High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics - a deep learning approach that predicts multi-omics in a cell population directly from high-content images of cells stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cells (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves significantly better performance in predicting transcriptomics and proteomics measurements directly from cell images than predictions based on the mean observed training set abundance. We observed significant predictability of abundances for 4927 (18.72%; 95% CI: 6.52%, 35.52%) and 3521 (13.38%; 95% CI: 4.10%, 32.21%) transcripts out of 26137 in M1 and M2-stimulated macrophages respectively and for 422 (8.46%; 95% CI: 0.58%, 25.83%) and 697 (13.98%; 95% CI: 2.41%, 32.83%) proteins out of 4986 in M1 and M2-stimulated macrophages respectively. Our results show that some transcript and protein abundances are predictable from cell imaging and that cell imaging may potentially, in some settings and depending on the mechanisms of interest and desired performance threshold, even be a scalable and resource-efficient substitute for multi-omics measurements.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Classification of large DNA methylation datasets for identifying cancer drivers,"DNA methylation is a well-studied genetic modification crucial to regulate the functioning of the genome. Its alterations play an important role in tumorigenesis and tumor-suppression. Thus, studying DNA methylation data may help biomarker discovery in cancer. Since public data on DNA methylation become abundant, and considering the high number of methylated sites (features) present in the genome, it is important to have a method for efficiently processing such large datasets. Relying on big data technologies, we propose BIGBIOCL an algorithm that can apply supervised classification methods to datasets with hundreds of thousands of features. It is designed for the extraction of alternative and equivalent classification models through iterative deletion of selected features. We run experiments on DNA methylation datasets extracted from The Cancer Genome Atlas, focusing on three tumor types: breast, kidney, and thyroid carcinomas. We perform classifications extracting several methylated sites and their associated genes with accurate performance. Results suggest that BIGBIOCL can perform hundreds of classification iterations on hundreds of thousands of features in few hours. Moreover, we compare the performance of our method with other state-of-the-art classifiers and with a wide-spread DNA methylation analysis method based on network analysis. Finally, we are able to efficiently compute multiple alternative classification models and extract, from DNA-methylation large datasets, a set of candidate genes to be further investigated to determine their active role in cancer. BIGBIOCL, results of experiments, and a guide to carry on new experiments are freely available on GitHub.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
CLEVER: Clique-Enumerating Variant Finder,"Next-generation sequencing techniques have facilitated a large scale analysis of human genetic variation. Despite the advances in sequencing speeds, the computational discovery of structural variants is not yet standard. It is likely that many variants have remained undiscovered in most sequenced individuals. Here we present a novel internal segment size based approach, which organizes all, including also concordant reads into a read alignment graph where max-cliques represent maximal contradiction-free groups of alignments. A specifically engineered algorithm then enumerates all max-cliques and statistically evaluates them for their potential to reflect insertions or deletions (indels). For the first time in the literature, we compare a large range of state-of-the-art approaches using simulated Illumina reads from a fully annotated genome and present various relevant performance statistics. We achieve superior performance rates in particular on indels of sizes 20--100, which have been exposed as a current major challenge in the SV discovery literature and where prior insert size based approaches have limitations. In that size range, we outperform even split read aligners. We achieve good results also on real data where we make a substantial amount of correct predictions as the only tool, which complement the predictions of split-read aligners. CLEVER is open source (GPL) and available from http://clever-sv.googlecode.com.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
ARACNE: An Algorithm for the Reconstruction of Gene Regulatory Networks in a Mammalian Cellular Context,"Background: Elucidating gene regulatory networks is crucial for understanding normal cell physiology and complex pathologic phenotypes. Existing computational methods for the genome-wide ``reverse engineering'' of such networks have been successful only for lower eukaryotes with simple genomes. Here we present ARACNE, a novel algorithm, using microarray expression profiles, specifically designed to scale up to the complexity of regulatory networks in mammalian cells, yet general enough to address a wider range of network deconvolution problems. This method uses an information theoretic approach to eliminate the majority of indirect interactions inferred by co-expression methods.   Results: We prove that ARACNE reconstructs the network exactly (asymptotically) if the effect of loops in the network topology is negligible, and we show that the algorithm works well in practice, even in the presence of numerous loops and complex topologies. We assess ARACNE's ability to reconstruct transcriptional regulatory networks using both a realistic synthetic dataset and a microarray dataset from human B cells. On synthetic datasets ARACNE achieves very low error rates and outperforms established methods, such as Relevance Networks and Bayesian Networks. Application to the deconvolution of genetic networks in human B cells demonstrates ARACNE's ability to infer validated transcriptional targets of the c MYC proto-oncogene. We also study the effects of mis estimation of mutual information on network reconstruction, and show that algorithms based on mutual information ranking are more resilient to estimation errors.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Conditional Network Analysis Identifies Candidate Regulator Genes in Human B Cells,"Cellular phenotypes are determined by the dynamical activity of networks of co-regulated genes. Elucidating such networks is crucial for the understanding of normal cell physiology as well as for the dissection of complex pathologic phenotypes. Existing methods for such ""reverse engineering"" of genetic networks from microarray expression data have been successful only in prokaryotes (E. coli) and lower eukaryotes (S. cerevisiae) with relatively simple genomes. Additionally, they have mostly attempted to reconstruct average properties about the network connectivity without capturing the highly conditional nature of the interactions. In this paper we extend the ARACNE algorithm, which we recently introduced and successfully applied to the reconstruction of whole-genome transcriptional networks from mammalian cells, precisely to link the existence of specific network structures to the expression or lack thereof of specific regulator genes. This is accomplished by analyzing thousands of alternative network topologies generated by constraining the data set on the presence or absence of putative regulator genes. By considering interactions that are consistently supported across several such constraints, we identify many transcriptional interactions that would not have been detectable by the original method. By selecting genes that produce statistically significant changes in network topology, we identify novel candidate regulator genes. Further analysis shows that transcription factors, kinases, phosphatases, and other gene families known to effect biochemical interactions, are significantly overrepresented among the set of candidate regulator genes identified in silico, indirectly supporting the validity of the approach.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
sc-OTGM: Single-Cell Perturbation Modeling by Solving Optimal Mass Transport on the Manifold of Gaussian Mixtures,"Influenced by breakthroughs in LLMs, single-cell foundation models are emerging. While these models show successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a GMM as its prior distribution and distinguish between distinct cell populations by learning their respective marginal PDFs. It uses a Hit-and-Run Markov chain sampler to determine the OT plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Maximizing Protein Translation Rate in the Ribosome Flow Model: the Homogeneous Case,"Gene translation is the process in which intracellular macro-molecules, called ribosomes, decode genetic information in the mRNA chain into the corresponding proteins. Gene translation includes several steps. During the elongation step, ribosomes move along the mRNA in a sequential manner and link amino-acids together in the corresponding order to produce the proteins.   The homogeneous ribosome flow model(HRFM) is a deterministic computational model for translation-elongation under the assumption of constant elongation rates along the mRNA chain. The HRFM is described by a set of n first-order nonlinear ordinary differential equations, where n represents the number of sites along the mRNA chain. The HRFM also includes two positive parameters: ribosomal initiation rate and the (constant) elongation rate. In this paper, we show that the steady-state translation rate in the HRFM is a concave function of its parameters. This means that the problem of determining the parameter values that maximize the translation rate is relatively simple. Our results may contribute to a better understanding of the mechanisms and evolution of translation-elongation. We demonstrate this by using the theoretical results to estimate the initiation rate in M. musculus embryonic stem cell. The underlying assumption is that evolution optimized the translation mechanism.   For the infinite-dimensional HRFM, we derive a closed-form solution to the problem of determining the initiation and transition rates that maximize the protein translation rate. We show that these expressions provide good approximations for the optimal values in the n-dimensional HRFM already for relatively small values of n. These results may have applications for synthetic biology where an important problem is to re-engineer genomic systems in order to maximize the protein production rate.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning,"How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.   Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.","cat:q-bio.GN AND (CRISPR OR ""gene editing"" OR ""genetic engineering"")",0
COVID-19 is linked to changes in the time-space dimension of human mobility,"Socio-economic constructs and urban topology are crucial drivers of human mobility patterns. During the coronavirus disease 2019 pandemic, these patterns were reshaped in their components: the spatial dimension represented by the daily travelled distance, and the temporal dimension expressed as the synchronization time of commuting routines. Here, leveraging location-based data from de-identified mobile phone users, we observed that, during lockdowns restrictions, the decrease of spatial mobility is interwoven with the emergence of asynchronous mobility dynamics. The lifting of restriction in urban mobility allowed a faster recovery of the spatial dimension compared with the temporal one. Moreover, the recovery in mobility was different depending on urbanization levels and economic stratification. In rural and low-income areas, the spatial mobility dimension suffered a more considerable disruption when compared with urbanized and high-income areas. In contrast, the temporal dimension was more affected in urbanized and high-income areas than in rural and low-income areas.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Metropolitan Scale and Longitudinal Dataset of Anonymized Human Mobility Trajectories,"Modeling and predicting human mobility trajectories in urban areas is an essential task for various applications. The recent availability of large-scale human movement data collected from mobile devices have enabled the development of complex human mobility prediction models. However, human mobility prediction methods are often trained and tested on different datasets, due to the lack of open-source large-scale human mobility datasets amid privacy concerns, posing a challenge towards conducting fair performance comparisons between methods. To this end, we created an open-source, anonymized, metropolitan scale, and longitudinal (90 days) dataset of 100,000 individuals' human mobility trajectories, using mobile phone location data. The location pings are spatially and temporally discretized, and the metropolitan area is undisclosed to protect users' privacy. The 90-day period is composed of 75 days of business-as-usual and 15 days during an emergency. To promote the use of the dataset, we will host a human mobility prediction data challenge (`HuMob Challenge 2023') using the human mobility dataset, which will be held in conjunction with ACM SIGSPATIAL 2023.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding individual human mobility patterns,"Despite their importance for urban planning, traffic forecasting, and the spread of biological and mobile viruses, our understanding of the basic laws governing human motion remains limited thanks to the lack of tools to monitor the time resolved location of individuals. Here we study the trajectory of 100,000 anonymized mobile phone users whose position is tracked for a six month period. We find that in contrast with the random trajectories predicted by the prevailing Levy flight and random walk models, human trajectories show a high degree of temporal and spatial regularity, each individual being characterized by a time independent characteristic length scale and a significant probability to return to a few highly frequented locations. After correcting for differences in travel distances and the inherent anisotropy of each trajectory, the individual travel patterns collapse into a single spatial probability distribution, indicating that despite the diversity of their travel history, humans follow simple reproducible patterns. This inherent similarity in travel patterns could impact all phenomena driven by human mobility, from epidemic prevention to emergency response, urban planning and agent based modeling.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Identifying Human Mobility Patterns using Smart Card Data,"Human mobility is subject to collective dynamics that are the outcome of numerous individual choices. Smart card data which originated as a means of facilitating automated fare collections has emerged as an invaluable source for analyzing human mobility patterns. A variety of clustering and segmentation techniques has been adopted and adapted for applications ranging from passenger demand market segmentation to the analysis of urban activity locations. In this paper we provide a systematic review of the state-of-the-art on clustering public transport users based on their temporal or spatial-temporal characteristics as well as studies that use the patter to characterize individual stations, lines or urban areas. Furthermore, a critical review of the literature reveals an important distinction between studies focusing on the intra-personal variability of travel patterns versus those concerned with the inter-personal variability of travel patterns. We synthesize the key analysis approaches and based on which identify and outline the following directions for further research: (i) predictions of passenger travel patterns; (ii) decision support for service planning and policy evaluation; (iii) enhanced geographical characterization of users' travel patterns; (iv) from demand analytics towards behavioral analytics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A visit generation process for human mobility random graphs with location-specific latent-variables: from land use to travel demand,"This research introduces a mathematical framework to comprehending human mobility patterns, integrating mathematical modeling and economic analysis. The study focuses on latent-variable networks, investigating the dynamics of human mobility using stochastic models. By examining actual origin-destination data, the research reveals scaling relations and uncovers the economic implications of mobility patterns, such as the income elasticity of travel demand. The mathematical analysis commences with the development of a stochastic model based on inhomogeneous random graphs to construct a visitation model with multipurpose drivers for travel demand. A directed multigraph with weighted edges is considered, incorporating trip costs and labels to represent factors like distance traveled and travel time. The study gains insights into the structural properties and dynamic correlations of human mobility networks, to derive analytical and computational solutions for key network metrics, including scale-free behavior of the strength and degree distribution, together with the estimation of assortativity and clustering coefficient. Additionally, the model's validity is assessed through a real-world case study of the New York metropolitan area. The analysis of this data exposes clear scaling relations in commuting patterns, confirming theoretical predictions and validating the efficacy of the mathematical model. The model further explains a series of scaling behaviors in origin-destination flows among areas of a region, successfully reproducing statistical regularities observed in real-world cases using extensive human mobility datasets. In particular, the model's application to estimating income elasticity of travel demand bears significant implications for urban and transport economics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Characteristics of human mobility patterns revealed by high-frequency cell-phone position data,"Human mobility is an important characteristic of human behavior, but since tracking personalized position to high temporal and spatial resolution is difficult, most studies on human mobility patterns rely largely on mathematical models. Seminal models which assume frequently visited locations tend to be re-visited, reproduce a wide range of statistical features including collective mobility fluxes and numerous scaling laws. However, these models cannot be verified at a time-scale relevant to our daily travel patterns as most available data do not provide the necessary temporal resolution. In this work, we re-examined human mobility mechanisms via comprehensive cell-phone position data recorded at a high frequency up to every second. We found that the next location visited by users is not their most frequently visited ones in many cases. Instead, individuals exhibit origin-dependent, path-preferential patterns in their short time-scale mobility. These behaviors are prominent when the temporal resolution of the data is high, and are thus overlooked in most previous studies. Incorporating measured quantities from our high frequency data into conventional human mobility models shows contradictory statistical results. We finally revealed that the individual preferential transition mechanism characterized by the first-order Markov process can quantitatively reproduce the observed travel patterns at both individual and population levels at all relevant time-scales.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
How mobility patterns drive disease spread: A case study using public transit passenger card travel data,"Outbreaks of infectious diseases present a global threat to human health and are considered a major health-care challenge. One major driver for the rapid spatial spread of diseases is human mobility. In particular, the travel patterns of individuals determine their spreading potential to a great extent. These travel behaviors can be captured and modelled using novel location-based data sources, e.g., smart travel cards, social media, etc. Previous studies have shown that individuals who cannot be characterized by their most frequently visited locations spread diseases farther and faster; however, these studies are based on GPS data and mobile call records which have position uncertainty and do not capture explicit contacts. It is unclear if the same conclusions hold for large scale real-world transport networks. In this paper, we investigate how mobility patterns impact disease spread in a large-scale public transit network of empirical data traces. In contrast to previous findings, our results reveal that individuals with mobility patterns characterized by their most frequently visited locations and who typically travel large distances pose the highest spreading risk.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Social media and mobility landscape: uncovering spatial patterns of urban human mobility with multi source data,"In this paper, we present a three-step methodological framework, including location identification, bias modification, and out-of-sample validation, so as to promote human mobility analysis with social media data. More specifically, we propose ways of identifying personal activity-specific places and commuting patterns in Beijing, China, based on Weibo (China's Twitter) check-in records, as well as modifying sample bias of check-in data with population synthesis technique. An independent citywide travel logistic survey is used as the benchmark for validating the results. Obvious differences are discerned from Weibo users' and survey respondents' activity-mobility patterns, while there is a large variation of population representativeness between data from the two sources. After bias modification, the similarity coefficient between commuting distance distributions of Weibo data and survey observations increases substantially from 23% to 63%. Synthetic data proves to be a satisfactory cost-effective alternative source of mobility information. The proposed framework can inform many applications related to human mobility, ranging from transportation, through urban planning to transport emission modelling.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exploring Human Mobility Patterns Based on Location Information of US Flights,"A range of early studies have been conducted to illustrate human mobility patterns using different tracking data, such as dollar notes, cell phones and taxicabs. Here, we explore human mobility patterns based on massive tracking data of US flights. Both topological and geometric properties are examined in detail. We found that topological properties, such as traffic volume (between airports) and degree of connectivity (of individual airports), including both in- and outdegrees, follow a power law distribution but not a geometric property like travel lengths. The travel lengths exhibit an exponential distribution rather than a power law with an exponential cutoff as previous studies illustrated. We further simulated human mobility on the established topologies of airports with various moving behaviors and found that the mobility patterns are mainly attributed to the underlying binary topology of airports and have little to do with other factors, such as moving behaviors and geometric distances. Apart from the above findings, this study adopts the head/tail division rule, which is regularity behind any heavy-tailed distribution for extracting individual airports. The adoption of this rule for data processing constitutes another major contribution of this paper.   Keywords: scaling of geographic space, head/tail division rule, power law, geographic information, agent-based simulations","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
On the use of human mobility proxy for the modeling of epidemics,"Human mobility is a key component of large-scale spatial-transmission models of infectious diseases. Correctly modeling and quantifying human mobility is critical for improving epidemic control policies, but may be hindered by incomplete data in some regions of the world. Here we explore the opportunity of using proxy data or models for individual mobility to describe commuting movements and predict the diffusion of infectious disease. We consider three European countries and the corresponding commuting networks at different resolution scales obtained from official census surveys, from proxy data for human mobility extracted from mobile phone call records, and from the radiation model calibrated with census data. Metapopulation models defined on the three countries and integrating the different mobility layers are compared in terms of epidemic observables. We show that commuting networks from mobile phone data well capture the empirical commuting patterns, accounting for more than 87% of the total fluxes. The distributions of commuting fluxes per link from both sources of data - mobile phones and census - are similar and highly correlated, however a systematic overestimation of commuting traffic in the mobile phone data is observed. This leads to epidemics that spread faster than on census commuting networks, however preserving the order of infection of newly infected locations. Match in the epidemic invasion pattern is sensitive to initial conditions: the radiation model shows higher accuracy with respect to mobile phone data when the seed is central in the network, while the mobile phone proxy performs better for epidemics seeded in peripheral locations. Results suggest that different proxies can be used to approximate commuting patterns across different resolution scales in spatial epidemic simulations, in light of the desired accuracy in the epidemic outcome under study.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
How Many Equations of Motion Describe a Moving Human?,"A human is a thing that moves in space. Like all things that move in space, we can in principle use differential equations to describe their motion as a set of functions that maps time to position (and velocity, acceleration, and so on). With inanimate objects, we can reliably predict their trajectories by using differential equations that account for up to the second-order time derivative of their position, as is commonly done in analytical mechanics. With animate objects, though, and with humans, in particular, we do not know the cardinality of the set of equations that define their trajectory. We may be tempted to think, for example, that by reason of their complexity in cognition or behaviour as compared to, say, a rock, then the motion of humans requires a more complex description than the one generally used to describe the motion of physical systems. In this paper, we examine a real-world dataset on human mobility and consider the information that is added by each (computed, but denoised) additional time derivative, and find the maximum order of derivatives of the position that, for that particular dataset, cannot be expressed as a linear transformation of the previous. In this manner, we identify the dimensionality of a minimal model that correctly describes the observed trajectories. We find that every higher-order derivative after the acceleration is linearly dependent upon one of the previous time-derivatives. This measure is robust against noise and the choice for differentiation techniques that we use to compute the time-derivatives numerically as a function of the measured position. This result imposes empirical constraints on the possible sets of differential equations that can be used to describe the kinematics of a moving human.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
On the importance of trip destination for modeling individual human mobility patterns,"Getting insights on human mobility patterns and being able to reproduce them accurately is of the utmost importance in a wide range of applications from public health, to transport and urban planning. Still the relationship between the effort individuals will invest in a trip and its purpose importance is not taken into account in the individual mobility models that can be found in the recent literature. Here, we address this issue by introducing a model hypothesizing a relation between the importance of a trip and the distance traveled. In most practical cases, quantifying such importance is undoable. We overcome this difficulty by focusing on shopping trips (for which we have empirical data) and by taking the price of items as a proxy. Our model is able to reproduce the long-tailed distribution in travel distances empirically observed and to explain the scaling relationship between distance traveled and item value found in the data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Inferring human mobility using communication patterns,"Understanding the patterns of mobility of individuals is crucial for a number of reasons, from city planning to disaster management. There are two common ways of quantifying the amount of travel between locations: by direct observations that often involve privacy issues, e.g., tracking mobile phone locations, or by estimations from models. Typically, such models build on accurate knowledge of the population size at each location. However, when this information is not readily available, their applicability is rather limited. As mobile phones are ubiquitous, our aim is to investigate if mobility patterns can be inferred from aggregated mobile phone call data alone. Using data released by Orange for Ivory Coast, we show that human mobility is well predicted by a simple model based on the frequency of mobile phone calls between two locations and their geographical distance. We argue that the strength of the model comes from directly incorporating the social dimension of mobility. Furthermore, as only aggregated call data is required, the model helps to avoid potential privacy problems.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multiscale mobility patterns and the restriction of human movement,"From the perspective of human mobility, the COVID-19 pandemic constituted a natural experiment of enormous reach in space and time. Here, we analyse the inherent multiple scales of human mobility using Facebook Movement Maps collected before and during the first UK lockdown. First, we obtain the pre-lockdown UK mobility graph, and employ multiscale community detection to extract, in an unsupervised manner, a set of robust partitions into flow communities at different levels of coarseness. The partitions so obtained capture intrinsic mobility scales with better coverage than NUTS regions, which suffer from mismatches between human mobility and administrative divisions. Furthermore, the flow communities in the fine scale partition match well the UK Travel to Work Areas (TTWAs) but also capture mobility patterns beyond commuting to work. We also examine the evolution of mobility under lockdown, and show that mobility first reverted towards fine scale flow communities already found in the pre-lockdown data, and then expanded back towards coarser flow communities as restrictions were lifted. The improved coverage induced by lockdown is well captured by a linear decay shock model, which allows us to quantify regional differences both in the strength of the effect and the recovery time from the lockdown shock.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Ring aggregation pattern of Human Travel Trips,"Although a lot of attentions have been paid to human mobility, the relationship between travel pattern with city structure is still unclear. Here we probe into this relationship by analyzing the metro passenger trip data.There are two unprecedented findings. One, from the average view a linear law exists between the individual's travel distance with his original distance to city center. The mechanism underlying is a travel pattern we called ""ring aggregation"", i.e., the daily movement of city passengers is just aggregating to a ring with roughly equal distance to city center.Interestingly, for the round trips the daily travel pattern can be regarded as a switching between the home ring at outer area with the office ring at the inner area. Second, this linear law and ring aggregation pattern seems to be an exclusive characteristic of the metro system. It can not be found in short distance transportation modes, such as bicycle and taxi, neither as multiple transportation modes. This means the ring aggregation pattern is a token of the relationship between travel pattern with city structure in the large scale space.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Diversity of individual mobility patterns and emergence of aggregated scaling laws,"Uncovering human mobility patterns is of fundamental importance to the understanding of epidemic spreading, urban transportation and other socioeconomic dynamics embodying spatiality and human travel. According to the direct travel diaries of volunteers, we show the absence of scaling properties in the displacement distribution at the individual level,while the aggregated displacement distribution follows a power law with an exponential cutoff. Given the constraint on total travelling cost, this aggregated scaling law can be analytically predicted by the mixture nature of human travel under the principle of maximum entropy. A direct corollary of such theory is that the displacement distribution of a single mode of transportation should follow an exponential law, which also gets supportive evidences in known data. We thus conclude that the travelling cost shapes the displacement distribution at the aggregated level.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Universal Predictability of Mobility Patterns in Cities,"Despite the long history of modelling human mobility, we continue to lack a highly accurate approach with low data requirements for predicting mobility patterns in cities. Here, we present a population-weighted opportunities model without any adjustable parameters to capture the underlying driving force accounting for human mobility patterns at the city scale. We use various mobility data collected from a number of cities with different characteristics to demonstrate the predictive power of our model. We find that insofar as the spatial distribution of population is available, our model offers universal prediction of mobility patterns in good agreement with real observations, including distance distribution, destination travel constraints and flux. In contrast, the models that succeed in modelling mobility patterns in countries are not applicable in cities, which suggests that there is a diversity of human mobility at different spatial scales. Our model has potential applications in many fields relevant to mobility behaviour in cities, without relying on previous mobility measurements.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Post-pandemic mobility patterns in London,"Understanding human mobility is crucial for urban and transport studies in cities. People's daily activities provide valuable insight, such as where people live, work, shop, leisure or eat during midday or after-work hours. However, such activities are changed due to travel behaviours after COVID-19 in cities. This study examines the mobility patterns captured from mobile phone apps to explore the behavioural patterns established since the COVID-19 lockdowns triggered a series of changes in urban environments.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Explaining the Power-law Distribution of Human Mobility Through Transportation Modality Decomposition,"Human mobility has been empirically observed to exhibit Levy flight characteristics and behaviour with power-law distributed jump size. The fundamental mechanisms behind this behaviour has not yet been fully explained. In this paper, we analyze urban human mobility and we propose to explain the Levy walk behaviour observed in human mobility patterns by decomposing them into different classes according to the different transportation modes, such as Walk/Run, Bicycle, Train/Subway or Car/Taxi/Bus. Our analysis is based on two real-life GPS datasets containing approximately 10 and 20 million GPS samples with transportation mode information. We show that human mobility can be modelled as a mixture of different transportation modes, and that these single movement patterns can be approximated by a lognormal distribution rather than a power-law distribution. Then, we demonstrate that the mixture of the decomposed lognormal flight distributions associated with each modality is a power-law distribution, providing an explanation to the emergence of Levy Walk patterns that characterize human mobility patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility in the Metaverse,"The metaverse promises a shift in the way humans interact with each other, and with their digital and physical environments. The lack of geographical boundaries and travel costs in the metaverse prompts us to ask if the fundamental laws that govern human mobility in the physical world apply. We collected data on avatar movements, along with their network mobility extracted from NFT purchases. We find that despite the absence of commuting costs, an individuals inclination to explore new locations diminishes over time, limiting movement to a small fraction of the metaverse. We also find a lack of correlation between land prices and visitation, a deviation from the patterns characterizing the physical world. Finally, we identify the scaling laws that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility. Our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics, rather than the nature of space and cost of movement.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding the Impact of the COVID-19 Pandemic on Transportation-related Behaviors with Human Mobility Data,"The constrained outbreak of COVID-19 in Mainland China has recently been regarded as a successful example of fighting this highly contagious virus. Both the short period (in about three months) of transmission and the sub-exponential increase of confirmed cases in Mainland China have proved that the Chinese authorities took effective epidemic prevention measures, such as case isolation, travel restrictions, closing recreational venues, and banning public gatherings. These measures can, of course, effectively control the spread of the COVID-19 pandemic. Meanwhile, they may dramatically change the human mobility patterns, such as the daily transportation-related behaviors of the public. To better understand the impact of COVID-19 on transportation-related behaviors and to provide more targeted anti-epidemic measures, we use the huge amount of human mobility data collected from Baidu Maps, a widely-used Web mapping service in China, to look into the detail reaction of the people there during the pandemic. To be specific, we conduct data-driven analysis on transportation-related behaviors during the pandemic from the perspectives of 1) means of transportation, 2) type of visited venues, 3) check-in time of venues, 4) preference on ""origin-destination"" distance, and 5) ""origin-transportation-destination"" patterns. For each topic, we also give our specific insights and policy-making suggestions. Given that the COVID-19 pandemic is still spreading in more than 200 countries and territories worldwide, infecting millions of people, the insights and suggestions provided here may help fight COVID-19.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Using coarse GPS data to quantify city-scale transportation system resilience to extreme events,"This article proposes a method to quantitatively measure the resilience of transportation systems using GPS data from taxis. The granularity of the GPS data necessary for this analysis is relatively coarse; it only requires coordinates for the beginning and end of trips, the metered distance, and the total travel time. The method works by computing the historical distribution of pace (normalized travel times) between various regions of a city and measuring the pace deviations during an unusual event. This method is applied to a dataset of nearly 700 million taxi trips in New York City, which is used to analyze the transportation infrastructure resilience to Hurricane Sandy. The analysis indicates that Hurricane Sandy impacted traffic conditions for more than five days, and caused a peak delay of two minutes per mile. Practically, it identifies that the evacuation caused only minor disruptions, but significant delays were encountered during the post-disaster reentry process. Since the implementation of this method is very efficient, it could potentially be used as an online monitoring tool, representing a first step toward quantifying city scale resilience with coarse GPS data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The spectral dimension of human mobility,"Human mobility patterns are surprisingly structured. In spite of many hard to model factors, such as climate, culture, and socioeconomic opportunities, aggregate migration rates obey a universal, parameter-free, `radiation' model. Recent work has further shown that the detailed spectral decomposition of these flows -- defined as the number of individuals that visit a given location with frequency $f$ from a distance $r$ away -- also obeys simple rules, namely, scaling as a universal inverse square law in the combination, $rf$. However, this surprising regularity, derived on general grounds, has not been explained through microscopic mechanisms of individual behavior. Here we confirm this by analyzing large-scale cell phone datasets from three distinct regions and show that a direct consequence of this scaling law is that the average `travel energy' spent by visitors to a given location is constant across space, a finding reminiscent of the well-known travel budget hypothesis of human movement. The attractivity of different locations, which we define by the total number of visits to that location, also admits non-trivial, spatially-clustered structure. The observed pattern is consistent with the well-known central place theory in urban geography, as well as with the notion of Weber optimality in spatial economy, hinting to a collective human capacity of optimizing recurrent movements. We close by proposing a simple, microscopic human mobility model which simultaneously captures all our empirical findings. Our results have relevance for transportation, urban planning, geography, and other disciplines in which a deeper understanding of aggregate human mobility is key.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Deep Generative Model for Human Mobility Behavior,"Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we present MobilityGen, a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales. By linking behavioral attributes with environmental context, MobilityGen reproduces key patterns such as scaling laws for location visits, activity time allocation, and the coupled evolution of travel mode and destination choices. It reflects spatio-temporal variability and generates diverse, plausible, and novel mobility patterns consistent with the built environment. Beyond standard validation, MobilityGen yields insights not attainable with earlier models, including how access to urban space varies across travel modes and how co-presence dynamics shape social exposure and segregation. Our work establishes a new framework for mobility simulation, paving the way for fine-grained, data-driven studies of human behavior and its societal implications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Predictability of Irregular Human Mobility,"Understanding human mobility is critical for decision support in areas from urban planning to infectious diseases control. Prior work has focused on tracking daily logs of outdoor mobility without considering relevant context, which contain a mixture of regular and irregular human movement for a range of purposes, and thus diverse effects on the dynamics have been ignored. This study aims to focus on irregular human movement of different meta-populations with various purposes. We propose approaches to estimate the predictability of mobility in different contexts. With our survey data from international and domestic visitors to Australia, we found that the travel patterns of Europeans visiting for holidays are less predictable than those visiting for education, while East Asian visitors show the opposite patterns, ie, more predictable for holidays than for education. Domestic residents from the most populous Australian states exhibit the most unpredictable patterns, while visitors from less populated states show the highest predictable movement.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mesoscopic structure and social aspects of human mobility,"The individual movements of large numbers of people are important in many contexts, from urban planning to disease spreading. Datasets that capture human mobility are now available and many interesting features have been discovered, including the ultra-slow spatial growth of individual mobility. However, the detailed substructures and spatiotemporal flows of mobility - the sets and sequences of visited locations - have not been well studied. We show that individual mobility is dominated by small groups of frequently visited, dynamically close locations, forming primary ""habitats"" capturing typical daily activity, along with subsidiary habitats representing additional travel. These habitats do not correspond to typical contexts such as home or work. The temporal evolution of mobility within habitats, which constitutes most motion, is universal across habitats and exhibits scaling patterns both distinct from all previous observations and unpredicted by current models. The delay to enter subsidiary habitats is a primary factor in the spatiotemporal growth of human travel. Interestingly, habitats correlate with non-mobility dynamics such as communication activity, implying that habitats may influence processes such as information spreading and revealing new connections between human mobility and social networks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
An Investigation of Intra-Urban Mobility Pattern of Taxi Passengers,"The study of human mobility patterns is of both theoretical and practical values in many aspects. For long-distance travels, a few research endeavors have shown that the displacements of human travels follow the power-law distribution. However, controversies remain in the issue of the scaling law of human mobility in intra-urban areas. In this work we focus on the mobility pattern of taxi passengers by examining five datasets of the three metropolitans of New York, Dalian and Nanjing. Through statistical analysis, we find that the lognormal distribution with a power-law tail can best approximate both the displacement and the duration time of taxi trips, as well as the vacant time of taxicabs, in all the examined cities. The universality of scaling law of human mobility is subsequently discussed, in accordance with the data analytics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Gender Patterns of Human Mobility in Colombia: Reexamining Ravenstein's Laws of Migration,"Public stakeholders implement several policies and regulations to tackle gender gaps, fostering the change in the cultural constructs associated with gender. One way to quantify if such changes elicit gender equality is by studying mobility. In this work, we study the daily mobility patterns of women and men occurring in Medelln (Colombia) in two years: 2005 and 2017. Specifically, we focus on the spatiotemporal differences in the travels and find that purpose of travel and occupation characterise each gender differently. We show that women tend to make shorter trips, corroborating Ravenstein's Laws of Migration. Our results indicate that urban mobility in Colombia seems to behave in agreement with the ""archetypal"" case studied by Ravenstein.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding Human Mobility from Twitter,"Understanding human mobility is crucial for a broad range of applications from disease prediction to communication networks. Most efforts on studying human mobility have so far used private and low resolution data, such as call data records. Here, we propose Twitter as a proxy for human mobility, as it relies on publicly available data and provides high resolution positioning when users opt to geotag their tweets with their current location. We analyse a Twitter dataset with more than six million geotagged tweets posted in Australia, and we demonstrate that Twitter can be a reliable source for studying human mobility patterns. Our analysis shows that geotagged tweets can capture rich features of human mobility, such as the diversity of movement orbits among individuals and of movements within and between cities. We also find that short and long-distance movers both spend most of their time in large metropolitan areas, in contrast with intermediate-distance movers movements, reflecting the impact of different modes of travel. Our study provides solid evidence that Twitter can indeed be a useful proxy for tracking and predicting human movement.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying relation between mobility patterns and socioeconomic status of dockless sharing-bike users,"Bikes are among the healthiest, greenest, and most affordable means of transportation for a better future city, but mobility patterns of riders with different income were rarely studied due to limitations on collecting data. Newly emergent dockless bike-sharing platforms that record detailed information regarding each trip provide us a unique opportunity. Attribute to its better usage flexibility and accessibility, dockless bike-sharing platforms are booming over the past a few years worldwide and reviving the riding fashion in cities. In this work, by exploiting massive riding records in two megacities from a dockless bike-sharing platform, we reveal that individual mobility patterns, including radius of gyration and average travel distance, are similar among users with different income, which indicates that human beings all follow similar physical rules. However, collective mobility patterns, including average range and diversity of visitation, and commuting directions, all exhibit different behaviors and spatial patterns across income categories. Hotspot locations that attract more cycling activities are quite different over groups, and locations where users reside are of a low user ratio for both higher and lower income groups. Lower income groups are inclined to visit less flourishing locations, and commute towards the direction to the city center in both cities, and of a smaller mobility diversity in Beijing but a larger diversity in Shanghai. In addition, differences on mobility patterns among socioeconomic categories are more evident in Beijing than in Shanghai. Our findings would be helpful on designing better promotion strategies for dockless bike-sharing platforms and towards the transition to a more sustainable green transportation.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Influence of sociodemographic characteristics on human mobility,"Human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns. The growing accessibility to ICT information from portable digital media has recently opened the possibility of exploring human behavior at high spatio-temporal resolutions. Mobile phone records, geolocated tweets, check-ins from Foursquare or geotagged photos, have contributed to this purpose at different scales, from cities to countries, in different world areas. Many previous works lacked, however, details on the individuals' attributes such as age or gender. In this work, we analyze credit-card records from Barcelona and Madrid and by examining the geolocated credit-card transactions of individuals living in the two provinces, we find that the mobility patterns vary according to gender, age and occupation. Differences in distance traveled and travel purpose are observed between younger and older people, but, curiously, either between males and females of similar age. While mobility displays some generic features, here we show that sociodemographic characteristics play a relevant role and must be taken into account for mobility and epidemiological modelization.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Modeling collective human mobility: Understanding exponential law of intra-urban movement,"It is very important to understand urban mobility patterns because most trips are concentrated in urban areas. In the paper, a new model is proposed to model collective human mobility in urban areas. The model can be applied to predict individual flows not only in intra-city but also in countries or a larger range. Based on the model, it can be concluded that the exponential law of distance distribution is attributed to decreasing exponentially of average density of human travel demands. Since the distribution of human travel demands only depends on urban planning, population distribution, regional functions and so on, it illustrates that these inherent properties of cities are impetus to drive collective human movements.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The scaling of human mobility by taxis is exponential,"As a significant factor in urban planning, traffic forecasting and prediction of epidemics, modeling patterns of human mobility draws intensive attention from researchers for decades. Power-law distribution and its variations are observed from quite a few real-world human mobility datasets such as the movements of banking notes, trackings of cell phone users' locations and trajectories of vehicles. In this paper, we build models for 20 million trajectories with fine granularity collected from more than 10 thousand taxis in Beijing. In contrast to most models observed in human mobility data, the taxis' traveling displacements in urban areas tend to follow an exponential distribution instead of a power-law. Similarly, the elapsed time can also be well approximated by an exponential distribution. Worth mentioning, analysis of the interevent time indicates the bursty nature of human mobility, similar to many other human activities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Geo-located Twitter as the proxy for global mobility patterns,"In the advent of a pervasive presence of location sharing services researchers gained an unprecedented access to the direct records of human activity in space and time. This paper analyses geo-located Twitter messages in order to uncover global patterns of human mobility. Based on a dataset of almost a billion tweets recorded in 2012 we estimate volumes of international travelers in respect to their country of residence. We examine mobility profiles of different nations looking at the characteristics such as mobility rate, radius of gyration, diversity of destinations and a balance of the inflows and outflows. The temporal patterns disclose the universal seasons of increased international mobility and the peculiar national nature of overseen travels. Our analysis of the community structure of the Twitter mobility network, obtained with the iterative network partitioning, reveals spatially cohesive regions that follow the regional division of the world. Finally, we validate our result with the global tourism statistics and mobility models provided by other authors, and argue that Twitter is a viable source to understand and quantify global mobility patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The Impact of Social Segregation on Human Mobility in Developing and Urbanized Regions,"This study leverages mobile phone data to analyze human mobility patterns in developing countries, especially in comparison to more industrialized countries. Developing regions, such as the Ivory Coast, are marked by a number of factors that may influence mobility, such as less infrastructural coverage and maturity, less economic resources and stability, and in some cases, more cultural and language-based diversity. By comparing mobile phone data collected from the Ivory Coast to similar data collected in Portugal, we are able to highlight both qualitative and quantitative differences in mobility patterns - such as differences in likelihood to travel, as well as in the time required to travel - that are relevant to consideration on policy, infrastructure, and economic development. Our study illustrates how cultural and linguistic diversity in developing regions (such as Ivory Coast) can present challenges to mobility models that perform well and were conceptualized in less culturally diverse regions. Finally, we address these challenges by proposing novel techniques to assess the strength of borders in a regional partitioning scheme and to quantify the impact of border strength on mobility model accuracy.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Flow descriptors of human mobility networks,"Mobile phone data has enabled the timely and fine-grained study human mobility. Call Detail Records, generated at call events, allow building descriptions of mobility at different resolutions and with different spatial, temporal and social granularity. Individual trajectories are the basis for long-term observation of mobility patterns and identify factors of human dynamics. Here we propose a systematic analysis to characterize mobility network flows and topology and assess their impact into individual traces. Discrete flow-based descriptors are used to classify and understand human mobility patterns at multiple scales. This framework is suitable to assess urban planning, optimize transportation, measure the impact of external events and conditions, monitor internal dynamics and profile users according to their movement patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multiple gravity laws for human mobility within cities,"The gravity model of human mobility has successfully described the deterrence of travels with distance in urban mobility patterns. While a broad spectrum of deterrence was found across different cities, yet it is not empirically clear if movement patterns in a single city could also have a spectrum of distance exponents denoting a varying deterrence depending on the origin and destination regions in the city. By analyzing the travel data in the twelve most populated cities of the United States of America, we empirically find that the distance exponent governing the deterrence of travels significantly varies within a city depending on the traffic volumes of the origin and destination regions. Despite the diverse traffic landscape of the cities analyzed, a common pattern is observed for the distance exponents; the exponent value tends to be higher between regions with larger traffic volumes, while it tends to be lower between regions with smaller traffic volumes. This indicates that our method indeed reveals the hidden diversity of gravity laws that would be overlooked otherwise.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Predicting human mobility through the assimilation of social media traces into mobility models,"Predicting human mobility flows at different spatial scales is challenged by the heterogeneity of individual trajectories and the multi-scale nature of transportation networks. As vast amounts of digital traces of human behaviour become available, an opportunity arises to improve mobility models by integrating into them proxy data on mobility collected by a variety of digital platforms and location-aware services. Here we propose a hybrid model of human mobility that integrates a large-scale publicly available dataset from a popular photo-sharing system with the classical gravity model, under a stacked regression procedure. We validate the performance and generalizability of our approach using two ground-truth datasets on air travel and daily commuting in the United States: using two different cross-validation schemes we show that the hybrid model affords enhanced mobility prediction at both spatial scales.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A survey on Human Mobility and its applications,"Human Mobility has attracted attentions from different fields of studies such as epidemic modeling, traffic engineering, traffic prediction and urban planning. In this survey we review major characteristics of human mobility studies including from trajectory-based studies to studies using graph and network theory. In trajectory-based studies statistical measures such as jump length distribution and radius of gyration are analyzed in order to investigate how people move in their daily life, and if it is possible to model this individual movements and make prediction based on them. Using graph in mobility studies, helps to investigate the dynamic behavior of the system, such as diffusion and flow in the network and makes it easier to estimate how much one part of the network influences another by using metrics like centrality measures. We aim to study population flow in transportation networks using mobility data to derive models and patterns, and to develop new applications in predicting phenomena such as congestion. Human Mobility studies with the new generation of mobility data provided by cellular phone networks, arise new challenges such as data storing, data representation, data analysis and computation complexity. A comparative review of different data types used in current tools and applications of Human Mobility studies leads us to new approaches for dealing with mentioned challenges.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The Scales of Human Mobility,"There is a contradiction at the heart of our current understanding of individual and collective mobility patterns. On one hand, a highly influential stream of literature on human mobility driven by analyses of massive empirical datasets finds that human movements show no evidence of characteristic spatial scales. There, human mobility is described as scale-free. On the other hand, in geography, the concept of scale, referring to meaningful levels of description from individual buildings through neighborhoods, cities, regions, and countries, is central for the description of various aspects of human behavior such as socio-economic interactions, or political and cultural dynamics. Here, we resolve this apparent paradox by showing that day-to-day human mobility does indeed contain meaningful scales, corresponding to spatial containers restricting mobility behavior. The scale-free results arise from aggregating displacements across containers. We present a simple model, which given a person's trajectory, infers their neighborhoods, cities, and so on, as well as the sizes of these geographical containers. We find that the containers characterizing the trajectories of more than 700,000 individuals do indeed have typical sizes. We show that our model generates highly realistic trajectories without overfitting and provides a new lens through which to understand the differences in mobility behaviour across countries, gender groups, and urban-rural areas.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A universal opportunity model for human mobility,"Predicting human mobility between locations has practical applications in transportation science, spatial economics, sociology and many other fields. For more than 100 years, many human mobility prediction models have been proposed, among which the gravity model analogous to Newton's law of gravitation is widely used. Another classical model is the intervening opportunity (IO) model, which indicates that an individual selecting a destination is related to both the destination's opportunities and the intervening opportunities between the origin and the destination. The IO model established from the perspective of individual selection behavior has recently triggered the establishment of many new IO class models. Although these IO class models can achieve accurate prediction at specific spatiotemporal scales, an IO class model that can describe an individual's destination selection behavior at different spatiotemporal scales is still lacking. Here, we develop a universal opportunity model that considers two human behavioral tendencies: one is the exploratory tendency, and the other is the cautious tendency. Our model establishes a new framework in IO class models and covers the classical radiation model and opportunity priority selection model. Furthermore, we use various mobility data to demonstrate our model's predictive ability. The results show that our model can better predict human mobility than previous IO class models. Moreover, this model can help us better understand the underlying mechanism of the individual's destination selection behavior in different types of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The distorting lens of human mobility data,"The description of complex human mobility patterns is at the core of many important applications ranging from urbanism and transportation to epidemics containment. Data about collective human movements, once scarce, has become widely available thanks to new sources such as Phone CDR, GPS devices, or Smartphone apps. Nevertheless, it is still common to rely on a single dataset by implicitly assuming that it is a valid instance of universal dynamics, regardless of factors such as data gathering and processing techniques. Here, we test such an overarching assumption on an unprecedented scale by comparing human mobility datasets obtained from 7 different data-sources, tracing over 500 millions individuals in 145 countries. We report wide quantifiable differences in the resulting mobility networks and, in particular, in the displacement distribution previously thought to be universal. These variations -- that do not necessarily imply that the human mobility is not universal -- also impact processes taking place on these networks, as we show for the specific case of epidemic spreading. Our results point to the crucial need for disclosing the data processing and, overall, to follow good practices to ensure the robustness and the reproducibility of the results.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility in a Continuum Approach,"Human mobility is investigated using a continuum approach that allows to calculate the probability to observe a trip to anyarbitrary region, and the fluxes between any two regions. The considered description offers a general and unified framework, in which previously proposed mobility models like the gravity model, the intervening opportunities model, and the recently introduced radiation model are naturally resulting as special cases. A new form of radiation model is derived and its validity is investigated using observational data offered by commuting trips obtained from the United States census data set, and the mobility fluxesextracted from mobile phone data collected in a western European country. The new modeling paradigm offered by this description suggests that the complex topological features observed in large mobility and transportation networks may be the result of a simple stochastic process taking place on an inhomogeneous landscape.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A stochastic model of randomly accelerated walkers for human mobility,"The recent availability of large databases allows to study macroscopic properties of many complex systems. However, inferring a model from a fit of empirical data without any knowledge of the dynamics might lead to erroneous interpretations [6]. We illustrate this in the case of human mobility [1-3] and foraging human patterns [4] where empirical long-tailed distributions of jump sizes have been associated to scale-free super-diffusive random walks called Lvy flights [5]. Here, we introduce a new class of accelerated random walks where the velocity changes due to acceleration kicks at random times, which combined with a peaked distribution of travel times [7], displays a jump length distribution that could easily be misinterpreted as a truncated power law, but that is not governed by large fluctuations. This stochastic model allows us to explain empirical observations about the movements of 780,000 private vehicles in Italy, and more generally, to get a deeper quantitative understanding of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Biases in human mobility data impact epidemic modeling,"Large-scale human mobility data is a key resource in data-driven policy making and across many scientific fields. Most recently, mobility data was extensively used during the COVID-19 pandemic to study the effects of governmental policies and to inform epidemic models. Large-scale mobility is often measured using digital tools such as mobile phones. However, it remains an open question how truthfully these digital proxies represent the actual travel behavior of the general population. Here, we examine mobility datasets from multiple countries and identify two fundamentally different types of bias caused by unequal access to, and unequal usage of mobile phones. We introduce the concept of data generation bias, a previously overlooked type of bias, which is present when the amount of data that an individual produces influences their representation in the dataset. We find evidence for data generation bias in all examined datasets in that high-wealth individuals are overrepresented, with the richest 20% contributing over 50% of all recorded trips, substantially skewing the datasets. This inequality is consequential, as we find mobility patterns of different wealth groups to be structurally different, where the mobility networks of high-wealth users are denser and contain more long-range connections. To mitigate the skew, we present a framework to debias data and show how simple techniques can be used to increase representativeness. Using our approach we show how biases can severely impact outcomes of dynamic processes such as epidemic simulations, where biased data incorrectly estimates the severity and speed of disease transmission. Overall, we show that a failure to account for biases can have detrimental effects on the results of studies and urge researchers and practitioners to account for data-fairness in all future studies of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mitigating biases in big mobility data: a case study of monitoring large-scale transit systems,"Big mobility datasets (BMD) have shown many advantages in studying human mobility and evaluating the performance of transportation systems. However, the quality of BMD remains poorly understood. This study evaluates biases in BMD and develops mitigation methods. Using Google and Apple mobility data as examples, this study compares them with benchmark data from governmental agencies. Spatio-temporal discrepancies between BMD and benchmark are observed and their impacts on transportation applications are investigated, emphasizing the urgent need to address these biases to prevent misguided policymaking. This study further proposes and tests a bias mitigation method. It is shown that the mitigated BMD could generate valuable insights into large-scale public transit systems across 100+ US counties, revealing regional disparities of the recovery of transit systems from the COVID-19. This study underscores the importance of caution when using BMD in transportation research and presents effective mitigation strategies that would benefit practitioners.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility: Models and Applications,"Recent years have witnessed an explosion of extensive geolocated datasets related to human movement, enabling scientists to quantitatively study individual and collective mobility patterns, and to generate models that can capture and reproduce the spatiotemporal structures and regularities in human trajectories. The study of human mobility is especially important for applications such as estimating migratory flows, traffic forecasting, urban planning, and epidemic modeling. In this survey, we review the approaches developed to reproduce various mobility patterns, with the main focus on recent developments. This review can be used both as an introduction to the fundamental modeling principles of human mobility, and as a collection of technical methods applicable to specific mobility-related problems. The review organizes the subject by differentiating between individual and population mobility and also between short-range and long-range mobility. Throughout the text the description of the theory is intertwined with real-world applications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Data driven discovery of human mobility models,"Human mobility is a fundamental aspect of social behavior, with broad applications in transportation, urban planning, and epidemic modeling. However, for decades new mathematical formulas to model mobility phenomena have been scarce and usually discovered by analogy to physical processes, such as the gravity model and the radiation model. These sporadic discoveries are often thought to rely on intuition and luck in fitting empirical data. Here, we propose a systematic approach that leverages symbolic regression to automatically discover interpretable models from human mobility data. Our approach finds several well-known formulas, such as the distance decay effect and classical gravity models, as well as previously unknown ones, such as an exponential-power-law decay that can be explained by the maximum entropy principle. By relaxing the constraints on the complexity of model expressions, we further show how key variables of human mobility are progressively incorporated into the model, making this framework a powerful tool for revealing the underlying mathematical structures of complex social phenomena directly from observational data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Measures of Human Mobility Using Mobile Phone Records Enhanced with GIS Data,"In the past decade, large scale mobile phone data have become available for the study of human movement patterns. These data hold an immense promise for understanding human behavior on a vast scale, and with a precision and accuracy never before possible with censuses, surveys or other existing data collection techniques. There is already a significant body of literature that has made key inroads into understanding human mobility using this exciting new data source, and there have been several different measures of mobility used. However, existing mobile phone based mobility measures are inconsistent, inaccurate, and confounded with social characteristics of local context. New measures would best be developed immediately as they will influence future studies of mobility using mobile phone data. In this article, we do exactly this. We discuss problems with existing mobile phone based measures of mobility and describe new methods for measuring mobility that address these concerns. Our measures of mobility, which incorporate both mobile phone records and detailed GIS data, are designed to address the spatial nature of human mobility, to remain independent of social characteristics of context, and to be comparable across geographic regions and time. We also contribute a discussion of the variety of uses for these new measures in developing a better understanding of how human mobility influences micro-level human behaviors and well-being, and macro-level social organization and change.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The role of parsimonious models in addressing mobility challenges,"Mobility is a complex phenomenon encompassing diverse transportation modes, infrastructure elements, and human behaviors. Tackling the persistent challenges of congestion, pollution, and accessibility requires a range of modeling approaches to optimize these systems. While AI offers transformative potential, it should not be the sole solution. Parsimonious models remain crucial in generating innovative concepts and tools, and fostering collaborative efforts among researchers, policymakers, and industry stakeholders.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility in Epidemic Modeling,"Human mobility forms the backbone of contact patterns through which infectious diseases propagate, fundamentally shaping the spatio-temporal dynamics of epidemics and pandemics. While traditional models are often based on the assumption that all individuals have the same probability of infecting every other individual in the population, a so-called random homogeneous mixing, they struggle to capture the complex and heterogeneous nature of real-world human interactions. Recent advancements in data-driven methodologies and computational capabilities have unlocked the potential of integrating high-resolution human mobility data into epidemic modeling, significantly improving the accuracy, timeliness, and applicability of epidemic risk assessment, contact tracing, and intervention strategies. This review provides a comprehensive synthesis of the current landscape in human mobility-informed epidemic modeling. We explore diverse sources and representations of human mobility data, and then examine the behavioral and structural roles of mobility and contact in shaping disease transmission dynamics. Furthermore, the review spans a wide range of epidemic modeling approaches, ranging from classical compartmental models to network-based, agent-based, and machine learning models. And we also discuss how mobility integration enhances risk management and response strategies during epidemics. By synthesizing these insights, the review can serve as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while charting clear directions for future research.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
On the Inability of Markov Models to Capture Criticality in Human Mobility,"We examine the non-Markovian nature of human mobility by exposing the inability of Markov models to capture criticality in human mobility. In particular, the assumed Markovian nature of mobility was used to establish a theoretical upper bound on the predictability of human mobility (expressed as a minimum error probability limit), based on temporally correlated entropy. Since its inception, this bound has been widely used and empirically validated using Markov chains. We show that recurrent-neural architectures can achieve significantly higher predictability, surpassing this widely used upper bound. In order to explain this anomaly, we shed light on several underlying assumptions in previous research works that has resulted in this bias. By evaluating the mobility predictability on real-world datasets, we show that human mobility exhibits scale-invariant long-range correlations, bearing similarity to a power-law decay. This is in contrast to the initial assumption that human mobility follows an exponential decay. This assumption of exponential decay coupled with Lempel-Ziv compression in computing Fano's inequality has led to an inaccurate estimation of the predictability upper bound. We show that this approach inflates the entropy, consequently lowering the upper bound on human mobility predictability. We finally highlight that this approach tends to overlook long-range correlations in human mobility. This explains why recurrent-neural architectures that are designed to handle long-range structural correlations surpass the previously computed upper bound on mobility predictability.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Modeling the scaling properties of human mobility,"While the fat tailed jump size and the waiting time distributions characterizing individual human trajectories strongly suggest the relevance of the continuous time random walk (CTRW) models of human mobility, no one seriously believes that human traces are truly random. Given the importance of human mobility, from epidemic modeling to traffic prediction and urban planning, we need quantitative models that can account for the statistical characteristics of individual human trajectories. Here we use empirical data on human mobility, captured by mobile phone traces, to show that the predictions of the CTRW models are in systematic conflict with the empirical results. We introduce two principles that govern human trajectories, allowing us to build a statistically self-consistent microscopic model for individual human mobility. The model not only accounts for the empirically observed scaling laws but also allows us to analytically predict most of the pertinent scaling exponents.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Unravelling the Spatial Properties of Individual Mobility Patterns using Longitudinal Travel Data,"The analysis of longitudinal travel data enables investigating how mobility patterns vary across the population and identify the spatial properties thereof. The objective of this study is to identify the extent to which users explore different parts of the network as well as identify distinctive user groups in terms of the spatial extent of their mobility patterns. To this end, we propose two means for representing spatial mobility profiles and clustering travellers accordingly. We represent users patterns in terms of zonal visiting frequency profiles and grid-cells spatial extent heatmaps. We apply the proposed analysis to a large-scale multi-modal mobility data set from the public transport system in Stockholm, Sweden. We unravel three clusters - locals, commuters and explorers - that best describe the zonal visiting frequency and show that their composition varies considerably across users' place of residence and related demographics. We also identify 18 clusters of visiting spatial extent which form four groups that follow similar shapes of travel extent yet oriented in different directions. The approach proposed and applied in this study could be applied for any longitudinal individual travel demand data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Future Directions in Human Mobility Science,"We provide a brief review of human mobility science and present three key areas where we expect to see substantial advancements. We start from the mind and discuss the need to better understand how spatial cognition shapes mobility patterns. We then move to societies and argue the importance of better understanding new forms of transportation. We conclude by discussing how algorithms shape mobility behaviour and provide useful tools for modellers. Finally, we discuss how progress in these research directions may help us address some of the challenges our society faces today.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Identifying highly influential travellers for spreading disease on a public transport system,"The recent outbreak of a novel coronavirus and its rapid spread underlines the importance of understanding human mobility. Enclosed spaces, such as public transport vehicles (e.g. buses and trains), offer a suitable environment for infections to spread widely and quickly. Investigating the movement patterns and the physical encounters of individuals on public transit systems is thus critical to understand the drivers of infectious disease outbreaks. For instance previous work has explored the impact of recurring patterns inherent in human mobility on disease spread, but has not considered other dimensions such as the distance travelled or the number of encounters. Here, we consider multiple mobility dimensions simultaneously to uncover critical information for the design of effective intervention strategies. We use one month of citywide smart card travel data collected in Sydney, Australia to classify bus passengers along three dimensions, namely the degree of exploration, the distance travelled and the number of encounters. Additionally, we simulate disease spread on the transport network and trace the infection paths. We investigate in detail the transmissions between the classified groups while varying the infection probability and the suspension time of pathogens. Our results show that characterizing individuals along multiple dimensions simultaneously uncovers a complex infection interplay between the different groups of passengers, that would remain hidden when considering only a single dimension. We also identify groups that are more influential than others given specific disease characteristics, which can guide containment and vaccination efforts.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Decoupling geographical constraints from human mobility,"Driven by access to large volumes of movement data, the study of human mobility has grown rapidly over the past decades. The field has shown that human mobility is scale-free, proposed models to generate scale-free moving distance distributions, and explained how the scale-free distribution arises. It has not, however, explicitly addressed how mobility is structured by geographical constraints. How mobility relates to the outlines of landmasses, lakes, and rivers; by the placement of buildings, roadways, and cities. Based on millions of moves, we show how separating the effect of geography from mobility choices, reveals a power law spanning five orders of magnitude. To do so, we incorporate geography via the `pair distribution function' that encapsulates the structure of locations on which mobility occurs. Showing how the spatial distribution of human settlements shapes human mobility, our approach bridges the gap between distance- and opportunity-based models of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Trackintel: An open-source Python library for human mobility analysis,"Over the past decade, scientific studies have used the growing availability of large tracking datasets to enhance our understanding of human mobility behavior. However, so far data processing pipelines for the varying data collection methods are not standardized and consequently limit the reproducibility, comparability, and transferability of methods and results in quantitative human mobility analysis. This paper presents Trackintel, an open-source Python library for human mobility analysis. Trackintel is built on a standard data model for human mobility used in transport planning that is compatible with different types of tracking data. We introduce the main functionalities of the library that covers the full life-cycle of human mobility analysis, including processing steps according to the conceptual data model, read and write interfaces, as well as analysis functions (e.g., data quality assessment, travel mode prediction, and location labeling). We showcase the effectiveness of the Trackintel library through a case study with four different tracking datasets. Trackintel can serve as an essential tool to standardize mobility data analysis and increase the transparency and comparability of novel research on human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Spatial-temporal Analysis of COVID-19's Impact on Human Mobility: the Case of the United States,"COVID-19 has been affecting every aspect of societal life including human mobility since December, 2019. In this paper, we study the impact of COVID-19 on human mobility patterns at the state level within the United States. From the temporal perspective, we find that the change of mobility patterns does not necessarily correlate with government policies and guidelines, but is more related to people's awareness of the pandemic, which is reflected by the search data from Google Trends. Our results show that it takes on average 14 days for the mobility patterns to adjust to the new situation. From the spatial perspective, we conduct a state-level network analysis and clustering using the mobility data from Multiscale Dynamic Human Mobility Flow Dataset. As a result, we find that 1) states in the same cluster have shorter geographical distances; 2) a 14-day delay again is found between the time when the largest number of clusters appears and the peak of Coronavirus-related search queries on Google Trends; and 3) a major reduction in other network flow properties, namely degree, closeness, and betweenness, of all states from the week of March 2 to the week of April 6 (the week of the largest number of clusters).","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exact solution of gyration radius of individual's trajectory for a simplified human mobility model,"Gyration radius of individual's trajectory plays a key role in quantifying human mobility patterns. Of particular interests, empirical analyses suggest that the growth of gyration radius is slow versus time except the very early stage and may eventually arrive to a steady value. However, up to now, the underlying mechanism leading to such a possibly steady value has not been well understood. In this Letter, we propose a simplified human mobility model to simulate individual's daily travel with three sequential activities: commuting to workplace, going to do leisure activities and returning home. With the assumption that individual has constant travel speed and inferior limit of time at home and work, we prove that the daily moving area of an individual is an ellipse, and finally get an exact solution of the gyration radius. The analytical solution well captures the empirical observation reported in [M. C. Gonz`alez et al., Nature, 453 (2008) 779]. We also find that, in spite of the heterogeneous displacement distribution in the population level, individuals in our model have characteristic displacements, indicating a completely different mechanism to the one proposed by Song et al. [Nat. Phys. 6 (2010) 818].","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Dynamic predictability and spatio-temporal contexts in human mobility,"Human travelling behaviours are markedly regular, to a large extent, predictable, and mostly driven by biological necessities (\eg sleeping, eating) and social constructs (\eg school schedules, synchronisation of labour). Not surprisingly, such predictability is influenced by an array of factors ranging in scale from individual (\eg preference, choices) and social (\eg household, groups) all the way to global scale (\eg mobility restrictions in a pandemic). In this work, we explore how spatio-temporal patterns in individual-level mobility, which we refer to as \emph{predictability states}, carry a large degree of information regarding the nature of the regularities in mobility. Our findings indicate the existence of contextual and activity signatures in predictability states, pointing towards the potential for more sophisticated, data-driven approaches to short-term, higher-order mobility predictions beyond frequentist/probabilistic methods.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Cascading Walks Model for Human Mobility Patterns,"Uncovering the mechanism behind the scaling law in human trajectories is of fundamental significance in understanding many spatio-temporal phenomena. In combination of the exploration and the preferential returns, we propose a simple dynamical model mainly based on the cascading processes to capture the human mobility patterns. By the numerical simulations and analytical studies, we show more than five statistical characters that are well consistent with the empirical observations, including several type of scaling anomalies, and the ultraslow diffusion property, implying the cascading processes associated with the other two mechanisms are indeed a key in the understanding of human mobility activities. Moreover, both of the diverse individual mobility and aggregated scaling move-lengths, bridging the micro and macro patterns in human mobility. Our model provides deeper understandings on the emergence of human mobility patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Effects of Levy Flights Mobility Pattern on Epidemic Spreading under Limited Energy Constraint,"Recently, many empirical studies uncovered that animal foraging, migration and human traveling obey Levy flights with an exponent around -2. Inspired by the deluge of H1N1 this year, in this paper, the effects of Levy flights' mobility pattern on epidemic spreading is studied from a network perspective. We construct a spatial weighted network which possesses Levy flight spatial property under a restriction of total energy. The energy restriction is represented by the limitation of total travel distance within a certain time period of an individual. We find that the exponent -2 is the epidemic threshold of SIS spreading dynamics. Moreover, at the threshold the speed of epidemics spreading is highest. The results are helpful for the understanding of the effect of mobility pattern on epidemic spreading.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility patterns at the smallest scales,"We present a study on human mobility at small spatial scales. Differently from large scale mobility, recently studied through dollar-bill tracking and mobile phone data sets within one big country or continent, we report Brownian features of human mobility at smaller scales. In particular, the scaling exponents found at the smallest scales is typically close to one-half, differently from the larger values for the exponent characterizing mobility at larger scales. We carefully analyze $12$-month data of the Eduroam database within the Portuguese university of Minho. A full procedure is introduced with the aim of properly characterizing the human mobility within the network of access points composing the wireless system of the university. In particular, measures of flux are introduced for estimating a distance between access points. This distance is typically non-euclidean, since the spatial constraints at such small scales distort the continuum space on which human mobility occurs. Since two different exponents are found depending on the scale human motion takes place, we raise the question at which scale the transition from Brownian to non-Brownian motion takes place. In this context, we discuss how the numerical approach can be extended to larger scales, using the full Eduroam in Europe and in Asia, for uncovering the transition between both dynamical regimes.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Switching exploration modes in human mobility,"Recent advances in human mobility research have revealed consistent pairwise characteristics in movement behavior, yet existing mobility models often overlook the spatial and topological structure of mobility networks. By analyzing millions of devices' anonymized cell phone trajectories, we uncover a distinct modular organization within these networks, demonstrating that movements within spatial modules differ significantly from those between modules. This finding challenges the conventional assumption of uniform mobility dynamics and underscores the influence of heterogeneous environments on human movement. Inspired by switching behaviors in animal movement patterns, we introduce a novel ""switch mechanism"" to differentiate movement modes, allowing our model to accurately reproduce both the modular structures of trajectory networks and spatial mobility patterns. Our results provide new insights into the dynamics of human mobility and its impact on network formation, with broad applications in traffic prediction, disease transmission modeling, and urban planning. Beyond advancing the theoretical and practical understanding of mobility networks, this work opens new avenues for understanding societal dynamics at large.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A universal law in human mobility,"The intrinsic factor that drives the human movement remains unclear for decades. While our observations from intra-urban and inter-urban trips both demonstrate a universal law in human mobility. Be specific, the probability from one location to another is inversely proportional to the number of population living in locations which are closer than the destination. A simple rank-based model is then presented, which is parameterless but predicts human flows with a convincing fidelity. Besides, comparison with other models shows that our model is more stable and fundamental at different spatial scales by implying the strong correlation between human mobility and social relationship.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Universal properties of multimodal human mobility: a statistical physics point of view,"The statistical properties of human mobility have been studied in the framework of complex systems physics. Taking advantage from the new datasets made available by the information and communication technologies, the distributions of mobility path lengths and of trip duration have been considered to discover the fingerprints of complexity characters, but the role of the different transportation means on the statistical properties of urban mobility has not been studied in deep. In this paper we cope with the problem of pointing out the existence of universal features for different type of individual mobility: pedestrian, cycling and vehicular urban mobility. In particular, we propose the use of travel time as universal 'energy' for the mobility and we define a simple survival model that explains the travel time distribution of the different types of mobility. the analysis is performed in the metropolitan area of Bologna (Italy), where GPS datasets were available on individual trips using different transport means. Our results could suggest how to plan the different transportation networks to realize a multimodal mobility compatibly with the citizens propensities to use the different transport means.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
On the regularity of human mobility patterns at times of a pandemic,"The study of human mobility patterns is a crucially important research field for its impact on several socio-economic aspects and, in particular, the measure of regularity patters of human mobility can provide a across-the-board view of many social distancing variables in epidemics such as: human movement trends, physical interpersonal distances and population density. We will show that the notion of information entropy is also strongly related to demographic and economic trends by the use and analysis of real-time data. In the present research paper we address three different problems. First, we provide an evidence-based analytical approach which relates the human mobility patterns, social distancing attitudes and population density, with entropic measures which depict for erraticity of human contact behaviors. Second, we investigate the correlations between the aggregated mobility and entropic measures versus five external economic indicators. Finally,we show how entropic measures represents a useful tool for testing the limitations of typical assumptions in epidemiological and mobility models.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A stochastic agent-based model to evaluate COVID-19 transmission influenced by human mobility,"The COVID-19 pandemic has created an urgent need for mathematical models that can project epidemic trends and evaluate the effectiveness of mitigation strategies. To forecast the transmission of COVID-19, a major challenge is the accurate assessment of the multi-scale human mobility and how they impact the infection through close contacts. By combining the stochastic agent-based modeling strategy and hierarchical structures of spatial containers corresponding to the notion of places in geography, this study proposes a novel model, Mob-Cov, to study the impact of human traveling behaviour and individual health conditions on the disease outbreak and the probability of zero COVID in the population. Specifically, individuals perform power-law type of local movements within a container and global transport between different-level containers. Frequent short movements inside a small-level container (e.g. a road or a county) and a large population size influence the local crowdedness of people, which accelerates the infection and regional transmission. Travels between large-level containers (e.g. cities and nations) facilitate global spread and outbreak. Moreover, dynamic infection and recovery in the population are able to drive the bifurcation of the system to a ""zero-COVID"" state or a ""live with COVID"" state, depending on the mobility patterns, population number and health conditions. Reducing total population and local people accumulation as well as restricting global travels help achieve zero-COVID. In summary, the Mob-Cov model considers more realistic human mobility in a wide range of spatial scales, and has been designed with equal emphasis on performance, low simulation cost, accuracy, ease of use and flexibility. It is a useful tool for researchers and politicians to investigate the pandemic dynamics and plan actions against the disease.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multiscale Dynamic Human Mobility Flow Dataset in the U.S. during the COVID-19 Epidemic,"Understanding dynamic human mobility changes and spatial interaction patterns at different geographic scales is crucial for assessing the impacts of non-pharmaceutical interventions (such as stay-at-home orders) during the COVID-19 pandemic. In this data descriptor, we introduce a regularly-updated multiscale dynamic human mobility flow dataset across the United States, with data starting from March 1st, 2020. By analyzing millions of anonymous mobile phone users' visits to various places provided by SafeGraph, the daily and weekly dynamic origin-to-destination (O-D) population flows are computed, aggregated, and inferred at three geographic scales: census tract, county, and state. There is high correlation between our mobility flow dataset and openly available data sources, which shows the reliability of the produced data. Such a high spatiotemporal resolution human mobility flow dataset at different geographic scales over time may help monitor epidemic spreading dynamics, inform public health policy, and deepen our understanding of human behavior changes under the unprecedented public health crisis. This up-to-date O-D flow open data can support many other social sensing and transportation applications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Correlations and Scaling Laws in Human Mobility,"Human mobility patterns deeply affect the dynamics of many social systems. In this paper, we empirically analyze the real-world human movements based GPS records, and observe rich scaling properties in the temporal-spatial patterns as well as an abnormal transition in the speed-displacement patterns. We notice that the displacements at the population level show significant positive correlation, indicating a cascade-like nature in human movements. Furthermore, our analysis at the individual level finds that the displacement distributions of users with strong correlation of displacements are closer to power laws, implying a relationship between the positive correlation of the series of displacements and the form of an individual's displacement distribution. These findings from our empirical analysis show a factor directly relevant to the origin of the scaling properties in human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Non-Markovian Character in Human Mobility: Online and Offline,"The dynamics of human mobility characterizes the trajectories humans follow during their daily activities and is the foundation of processes from epidemic spreading to traffic prediction and information recommendation. In this paper, we investigate a massive data set of human activity including both online behavior of browsing websites and offline one of visiting towers based mobile terminations. The non-Markovian character observed from both online and offline cases is suggested by the scaling law in the distribution of dwelling time at individual and collective levels, respectively. Furthermore, we argue that the lower entropy and higher predictability in human mobility for both online and offline cases may origin from this non-Markovian character. However, the distributions of individual entropy and predictability show the different degrees of non-Markovian character from online to offline cases. To accounting for non-Markovian character in human mobility, we introduce a protype model with three basic ingredients, \emph{preferential return, inertial effect, and exploration} to reproduce the dynamic process of online and offline human mobility. In comparison with standard and biased random walk models with assumption of Markov process, the proposed model is able to obtain characters much closer to these empirical observations.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility during COVID-19 in the Context of Mild Social Distancing: Implications for Technological Interventions,"The COVID-19 pandemic has brought both tangible and intangible damage to our society. Many researchers studied about its societal impacts in the countries that had implemented strong social distancing measures such as stay-at-home orders. Among them, human mobility has been studied extensively due to its importance in flattening the curve. However, mobility has not been actively studied in the context of mild social distancing. Insufficient understanding of human mobility in diverse contexts might provide limited implications for any technological interventions to alleviate the situation. To this end, we collected a dataset consisting of more than 1M daily smart device users in the third-largest city of South Korea, which has implemented mild social distancing policies. We analyze how COVID-19 shaped human mobility in the city from geographical, socio-economic, and socio-political perspectives. We also examine mobility changes for points of interest and special occasions such as transportation stations and the case of legislative elections. We identify a typology of populations through these analyses as a means to provide design implications for technological interventions. This paper contributes to social sciences through in-depth analyses of human mobility and to the CSCW community with new design challenges and potential implications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
On the Dynamics of Human Proximity for Data Diffusion in Ad-Hoc Networks,"We report on a data-driven investigation aimed at understanding the dynamics of message spreading in a real-world dynamical network of human proximity. We use data collected by means of a proximity-sensing network of wearable sensors that we deployed at three different social gatherings, simultaneously involving several hundred individuals. We simulate a message spreading process over the recorded proximity network, focusing on both the topological and the temporal properties. We show that by using an appropriate technique to deal with the temporal heterogeneity of proximity events, a universal statistical pattern emerges for the delivery times of messages, robust across all the data sets. Our results are useful to set constraints for generic processes of data dissemination, as well as to validate established models of human mobility and proximity that are frequently used to simulate realistic behaviors.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Assessing Impacts of Abnormal Events on Travel Patterns Leveraging Passively Collected Trajectory Data,"Travel patterns can be impacted by abnormal events. Assessing the impacts has important implications for relief operations and improving preparedness or planning for future events. Conventionally, the assessment is done followed by data collection from post-event surveys, which are economically costly, suffering low-response rate, time-consuming and usually delayed for months (or even years) after an event, leading to inefficient and unreliable assessment and creating obstacles for relief organizations to reach people in need. Penetration of smartphones and services enabled by them continuously generate large amount of trajectory data (e.g., Call Records Data, App-based data), containing trajectories of massive users. These trajectory data are passively and timely collected and without additional cost and contain information of travel patterns of the massive number of individuals in a region for a prolonged time period (e.g., months to years). We propose a framework to assessing the impacts on travel patterns using these data. Utilizing the passively collected trajectory data, the proposed framework seeks to capturing and understanding the full spectrum of travel pattern changes, which helps to assess who, when and how people in a certain area were impacted. The proposed framework is applied to a mobile phone trajectory dataset containing about half-year trajectories of a million anonymous users to assess the impacts of Hurricane Harvey (the second-costliest hurricane in US history). The results are validated and show that the proposed framework can provide a comprehensive assessment of impacts of Harvey on travel patterns, which could guide the response to and the recovery from the impacts.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying Human Mobility Perturbation and Resilience in Natural Disasters,"Human mobility is influenced by environmental change and natural disasters. Researchers have used trip distance distribution, radius of gyration of movements, and individuals' visited locations to understand and capture human mobility patterns and trajectories. However, our knowledge of human movements during natural disasters is limited owing to both a lack of empirical data and the low precision of available data. Here, we studied human mobility using high-resolution movement data from individuals in New York City during and for several days after Hurricane Sandy in 2012. We found the human movements followed truncated power-law distributions during and after Hurricane Sandy, although the  value was noticeably larger during the first 24 hours after the storm struck. Also, we examined two parameters: the center of mass and the radius of gyration of each individual's movements. We found that their values during perturbation states and steady states are highly correlated, suggesting human mobility data obtained in steady states can possibly predict the perturbation state. Our results demonstrate that human movement trajectories experienced significant perturbations during hurricanes, but also exhibited high resilience. We expect the study will stimulate future research on the perturbation and inherent resilience of human mobility under the influence of natural disasters. For example, mobility patterns in coastal urban areas could be examined as tropical cyclones approach, gain or dissipate in strength, and as the path of the storm changes. Understanding nuances of human mobility under the influence of disasters will enable more effective evacuation, emergency response planning and development of strategies and policies to reduce fatality, injury, and economic loss.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding Predictability and Exploration in Human Mobility,"Predictive models for human mobility have important applications in many fields such as traffic control, ubiquitous computing and contextual advertisement. The predictive performance of models in literature varies quite broadly, from as high as 93% to as low as under 40%. In this work we investigate which factors influence the accuracy of next-place prediction, using a high-precision location dataset of more than 400 users for periods between 3 months and one year. We show that it is easier to achieve high accuracy when predicting the time-bin location than when predicting the next place. Moreover we demonstrate how the temporal and spatial resolution of the data can have strong influence on the accuracy of prediction. Finally we uncover that the exploration of new locations is an important factor in human mobility, and we measure that on average 20-25% of transitions are to new places, and approx. 70% of locations are visited only once. We discuss how these mechanisms are important factors limiting our ability to predict human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Evidence for a Conserved Quantity in Human Mobility,"Recent seminal works on human mobility have shown that individuals constantly exploit a small set of repeatedly visited locations. A concurrent literature has emphasized the explorative nature of human behavior, showing that the number of visited places grows steadily over time. How to reconcile these seemingly contradicting facts remains an open question. Here, we analyze high-resolution multi-year traces of $\sim$40,000 individuals from 4 datasets and show that this tension vanishes when the long-term evolution of mobility patterns is considered. We reveal that mobility patterns evolve significantly yet smoothly, and that the number of familiar locations an individual visits at any point is a conserved quantity with a typical size of $\sim$25 locations. We use this finding to improve state-of-the-art modeling of human mobility. Furthermore, shifting the attention from aggregated quantities to individual behavior, we show that the size of an individual's set of preferred locations correlates with the number of her social interactions. This result suggests a connection between the conserved quantity we identify, which as we show can not be understood purely on the basis of time constraints, and the `Dunbar number' describing a cognitive upper limit to an individual's number of social relations. We anticipate that our work will spark further research linking the study of Human Mobility and the Cognitive and Behavioral Sciences.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility and COVID-19 initial dynamics,"Mobility data at EU scale can help understand the dynamics of the pandemic and possibly limit the impact of future waves. Still, since a reliable and consistent method to measure the evolution of contagion at international level is missing, a systematic analysis of the relationship between human mobility and virus spread has never been conducted. A notable exceptions are France and Italy, for which data on excess deaths, an indirect indicator which is generally considered to be less affected by national and regional assumptions, are available at department and municipality level, respectively. Using this information together with anonymised and aggregated mobile data, this study shows that mobility alone can explain up to 92% of the initial spread in these two EU countries, while it has a slow decay effect after lockdown measures, meaning that mobility restrictions seem to have effectively contribute to save lives. It also emerges that internal mobility is more important than mobility across provinces and that the typical lagged positive effect of reduced human mobility on reducing excess deaths is around 14-20 days. An analogous analysis relative to Spain, for which an IgG SARS-Cov-2 antibody screening study at province level is used instead of excess deaths statistics, confirms the findings. The same approach adopted in this study can be easily extended to other European countries, as soon as reliable data on the spreading of the virus at a suitable level of granularity will be available. Looking at past data, relative to the initial phase of the outbreak in EU Member States, this study shows in which extent the spreading of the virus and human mobility are connected.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Where Would I Go Next? Large Language Models as Human Mobility Predictors,"Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the prediction target. Additionally, we design context-inclusive prompts that enable LLMs to generate more accurate predictions. Comprehensive evaluations of our method reveal that LLM-Mob excels in providing accurate and interpretable predictions, highlighting the untapped potential of LLMs in advancing human mobility prediction techniques. We posit that our research marks a significant paradigm shift in human mobility modelling, transitioning from building complex domain-specific models to harnessing general-purpose LLMs that yield accurate predictions through language instructions. The code for this work is available at https://github.com/xlwang233/LLM-Mob.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding Changes in Travel Patterns during the COVID-19 Outbreak in the Three Major Metropolitan Areas of Japan,"Unlike the lockdown measures taken in some countries or cities, the Japanese government declared a ""State of Emergency"" (SOE) under which people were only requested to reduce their contact with other people by at least 70 %, while some local governments also implemented their own mobility-reduction measures that had no legal basis. The effects of these measures are still unclear. Thus, in this study, we investigate changes in travel patterns in response to the COVID-19 outbreak and related policy measures in Japan using longitudinal aggregated mobile phone data. Specifically, we consider daily travel patterns as networks and analyze their structural changes by applying a framework for analyzing temporal networks used in network science. The cluster analysis with the network similarity measures across different dates showed that there are six main types of mobility patterns in the three major metropolitan areas of Japan: (I) weekends and holidays prior to the COVID-19 outbreak, (II) weekdays prior to the COVID-19 outbreak, (III) weekends and holidays before and after the SOE, (IV) weekdays before and after the SOE, (V) weekends and holidays during the SOE, and (VI) weekdays during the SOE. It was also found that travel patterns might have started to change from March 2020, when most schools were closed, and that the mobility patterns after the SOE returned to those prior to the SOE. Interestingly, we found that after the lifting of the SOE, travel patterns remained similar to those during the SOE for a few days, suggesting the possibility that self-restraint continued after the lifting of the SOE. Moreover, in the case of the Nagoya metropolitan area, we found that people voluntarily changed their travel patterns when the number of cases increased.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Examining Travel Patterns and Characteristics in a Bikesharing Network and Implications for Data-Driven Decision Supports: Case Study in the Washington DC Area,"Bikesharing has gradually become one adopted sustainable transportation mode recent years to bring us many social, environmental, economic, and health-related benefits and rewards. There is increased research toward better understanding of bikesharing systems (BSS) in urban environments. However, our comprehension remains incomplete on the patterns and characteristics of BSS. In this paper, aiming to help improving sustainability in multimodal transportation through BSS, we perform a systematic data analysis to examine underlying patterns and characteristics of the system dynamics in a bikeshare network and to acquire implications of the patterns and characteristics for decision making. As a case study, we use trip history data from the Capital Bikeshare system in the Washington DC area and some additional data sources. The study covers seven important aspects of bikeshare transportation systems, which are respectively trip demand and flow, operating activities, use and idle times, trip purpose, origin-destination flows, mobility, and safety. For these aspects, by using appropriate statistical methods and geographic techniques, we investigate travel patterns and characteristics of BSS from data to evaluate the qualitative and quantitative impacts of the inputs from key stakeholders on main measures of effectiveness such as trip costs, mobility, safety, quality of service, and operational efficiency, where key stakeholders include road users, system operators, and city. We also disclose some new patterns and characteristics of BSS to advance the knowledge on travel behaviors. Finally, we briefly summarize our findings and discuss the implications of the patterns and characteristics for data-driven decision supports from the relations between BSS and key stakeholders for promoting bikeshare utilization and transforming urban transportation to be more sustainable.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Using Location-Based Social Networks to Validate Human Mobility and Relationships Models,"We propose to use social networking data to validate mobility models for pervasive mobile ad-hoc networks (MANETs) and delay tolerant networks (DTNs). The Random Waypoint (RWP) and Erdos-Renyi (ER) models have been a popular choice among researchers for generating mobility traces of nodes and relationships between them. Not only RWP and ER are useful in evaluating networking protocols in a simulation environment, but they are also used for theoretical analysis of such dynamic networks. However, it has been observed that neither relationships among people nor their movements are random. Instead, human movements frequently contain repeated patterns and friendship is bounded by distance. We used social networking site Gowalla to collect, create and validate models of human mobility and relationships for analysis and evaluations of applications in opportunistic networks such as sensor networks and transportation models in civil engineering. In doing so, we hope to provide more human-like movements and social relationship models to researchers to study problems in complex and mobile networks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Global multi-layer network of human mobility,"Recent availability of geo-localized data capturing individual human activity together with the statistical data on international migration opened up unprecedented opportunities for a study on global mobility. In this paper we consider it from the perspective of a multi-layer complex network, built using a combination of three datasets: Twitter, Flickr and official migration data. Those datasets provide different but equally important insights on the global mobility: while the first two highlight short-term visits of people from one country to another, the last one - migration - shows the long-term mobility perspective, when people relocate for good. And the main purpose of the paper is to emphasize importance of this multi-layer approach capturing both aspects of human mobility at the same time. So we start from a comparative study of the network layers, comparing short- and long- term mobility through the statistical properties of the corresponding networks, such as the parameters of their degree centrality distributions or parameters of the corresponding gravity model being fit to the network. We also focus on the differences in country ranking by their short- and long-term attractiveness, discussing the most noticeable outliers. Finally, we apply this multi-layered human mobility network to infer the structure of the global society through a community detection approach and demonstrate that consideration of mobility from a multi-layer perspective can reveal important global spatial patterns in a way more consistent with other available relevant sources of international connections, in comparison to the spatial structure inferred from each network layer taken separately.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exploring universal patterns in human home-work commuting from mobile phone data,"Home-work commuting has always attracted significant research attention because of its impact on human mobility. One of the key assumptions in this domain of study is the universal uniformity of commute times. However, a true comparison of commute patterns has often been hindered by the intrinsic differences in data collection methods, which make observation from different countries potentially biased and unreliable. In the present work, we approach this problem through the use of mobile phone call detail records (CDRs), which offers a consistent method for investigating mobility patterns in wholly different parts of the world. We apply our analysis to a broad range of datasets, at both the country and city scale. Additionally, we compare these results with those obtained from vehicle GPS traces in Milan. While different regions have some unique commute time characteristics, we show that the home-work time distributions and average values within a single region are indeed largely independent of commute distance or country (Portugal, Ivory Coast, and Boston)--despite substantial spatial and infrastructural differences. Furthermore, a comparative analysis demonstrates that such distance-independence holds true only if we consider multimodal commute behaviors--as consistent with previous studies. In car-only (Milan GPS traces) and car-heavy (Saudi Arabia) commute datasets, we see that commute time is indeed influenced by commute distance.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Towards a Statistical Physics of Human Mobility,"In this paper, we extend some ideas of statistical physics to describe the properties of human mobility. From a physical point of view, we consider the statistical empirical laws of private cars mobility, taking advantage of a GPS database which contains a sampling of the individual trajectories of 2% of the whole vehicle population in an Italian region. Our aim is to discover possible ""universal laws"" that can be related to the dynamical cognitive features of individuals. Analyzing the empirical trip length distribution we study if the travel time can be used as universal cost function in a mesoscopic model of mobility. We discuss the implications of the elapsed times distribution between successive trips that shows an underlying Benford's law, and we study the rank distribution of the average visitation frequency to understand how people organize their daily agenda. We also propose simple stochastic models to suggest possible explanations of the empirical observations and we compare our results with analogous results on statistical properties of human mobility presented in the literature.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Universal expansion of human mobility across urban scales,"Human mobility is a fundamental process underpinning socioeconomic life and urban structure. Classic theories, such as egocentric activity spaces and central place theory, provide crucial insights into specific facets of movement, like home-centricity and hierarchical spatial organization. However, identifying universal characteristics or an underlying principle that quantitatively links these disparate perspectives has remained a challenge. Here, we reveal such a connection by analyzing the spatial structure of individual daily mobility trajectories using network-based modules. We discover a universal scaling law: the spatial extent (radius) of these mobility modules expands sublinearly with increasing distance from home, a pattern consistent across three orders of magnitude. Furthermore, we demonstrate that these modules precisely map onto the nested hierarchy of urban systems, corresponding to local, city-level, and regional scales as distance from home increases. These findings deepen our understanding of human mobility dynamics and demonstrate the profound connection between classical urban theory, human geography, and mobility studies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility is well described by closed-form gravity-like models learned automatically from data,"Modeling of human mobility is critical to address questions in urban planning and transportation, as well as global challenges in sustainability, public health, and economic development. However, our understanding and ability to model mobility flows within and between urban areas are still incomplete. At one end of the modeling spectrum we have simple so-called gravity models, which are easy to interpret and provide modestly accurate predictions of mobility flows. At the other end, we have complex machine learning and deep learning models, with tens of features and thousands of parameters, which predict mobility more accurately than gravity models at the cost of not being interpretable and not providing insight on human behavior. Here, we show that simple machine-learned, closed-form models of mobility are able to predict mobility flows more accurately, overall, than either gravity or complex machine and deep learning models. At the same time, these models are simple and gravity-like, and can be interpreted in terms similar to standard gravity models. Furthermore, these models work for different datasets and at different scales, suggesting that they may capture the fundamental universal features of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Universal underpinning of human mobility in the real world and cyberspace,"Human movements in the real world and in cyberspace affect not only dynamical processes such as epidemic spreading and information diffusion but also social and economical activities such as urban planning and personalized recommendation in online shopping. Despite recent efforts in characterizing and modeling human behaviors in both the real and cyber worlds, the fundamental dynamics underlying human mobility have not been well understood. We develop a minimal, memory-based random walk model in limited space for reproducing, with a single parameter, the key statistical behaviors characterizing human movements in both spaces. The model is validated using big data from mobile phone and online commerce, suggesting memory-based random walk dynamics as the universal underpinning for human mobility, regardless of whether it occurs in the real world or in cyberspace.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban Energy Flux: Human Mobility as a Predictor for Spatial Changes,"As a key energy challenge, we urgently require a better understanding of how growing urban populations interact with municipal energy systems and the resulting impact on energy demand across city neighborhoods, which are dense hubs of both consumer population and CO2 emissions. Currently, the physical characteristics of urban infrastructure are the main determinants in predictive modeling of the demand side of energy in our rapidly growing urban areas; overlooking influence related to fluctuating human activities. Here, we show how applying intra-urban human mobility as an indicator for interactions of the population with local energy systems can be translated into spatial imprints to predict the spatial distribution of energy use in urban settings. Our findings establish human mobility as an important element in explaining the spatial structure underlying urban energy flux and demonstrate the utility of a human mobility driven approach for predicting future urban energy demand with implications for CO2 emission strategies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multi-scale spatio-temporal analysis of human mobility,"The recent availability of digital traces generated by phone calls and online logins has significantly increased the scientific understanding of human mobility. Until now, however, limited data resolution and coverage have hindered a coherent description of human displacements across different spatial and temporal scales. Here, we characterise mobility behaviour across several orders of magnitude by analysing ~850 individuals' digital traces sampled every ~16 seconds for 25 months with ~10 meters spatial resolution. We show that the distributions of distances and waiting times between consecutive locations are best described by log-normal distributions and that natural time-scales emerge from the regularity of human mobility. We point out that log-normal distributions also characterise the patterns of discovery of new places, implying that they are not a simple consequence of the routine of modern life.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Relating land use and human intra-city mobility,"Understanding human mobility patterns -- how people move in their everyday lives -- is an interdisciplinary research field. It is a question with roots back to the 19th century that has been dramatically revitalized with the recent increase in data availability. Models of human mobility often take the population distribution as a starting point. Another, sometimes more accurate, data source is land-use maps. In this paper, we discuss how the intra-city movement patterns, and consequently population distribution, can be predicted from such data sources. As a link between land use and mobility, we show that the purposes of people's trips are strongly correlated with the land use of the trip's origin and destination. We calibrate, validate and discuss our model using survey data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Two-step dimensionality reduction of human mobility data: From potential landscapes to spatiotemporal insights,"Understanding the spatiotemporal patterns of human mobility is crucial for addressing societal challenges, such as epidemic control and urban transportation optimization. Despite advancements in data collection, the complexity and scale of mobility data continue to pose significant analytical challenges. Existing methods often result in losing location-specific details and fail to fully capture the intricacies of human movement. This study proposes a two-step dimensionality reduction framework to overcome existing limitations. First, we construct a potential landscape of human flow from origin-destination (OD) matrices using combinatorial Hodge theory, preserving essential spatial and structural information while enabling an intuitive visualization of flow patterns. Second, we apply principal component analysis (PCA) to the potential landscape, systematically identifying major spatiotemporal patterns. By implementing this two-step reduction method, we reveal significant shifts during a pandemic, characterized by an overall declines in mobility and stark contrasts between weekdays and holidays. These findings underscore the effectiveness of our framework in uncovering complex mobility patterns and provide valuable insights into urban planning and public health interventions.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Uncovering the socioeconomic facets of human mobility,"Given the rapid recent trend of urbanization, a better understanding of how urban infrastructure mediates socioeconomic interactions and economic systems is of vital importance. While the accessibility of location-enabled devices as well as large-scale datasets of human activities, has fueled significant advances in our understanding, there is little agreement on the linkage between socioeconomic status and its influence on movement patterns, in particular, the role of inequality. Here, we analyze a heavily aggregated and anonymized summary of global mobility and investigate the relationships between socioeconomic status and mobility across a hundred cities in the US and Brazil. We uncover two types of relationships, finding either a clear connection or little-to-no interdependencies. The former tend to be characterized by low levels of public transportation usage, inequitable access to basic amenities and services, and segregated clusters of communities in terms of income, with the latter class showing the opposite trends. Our findings provide useful lessons in designing urban habitats that serve the larger interests of all inhabitants irrespective of their economic status.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A tale of many cities: universal patterns in human urban mobility,"The advent of geographic online social networks such as Foursquare, where users voluntarily signal their current location, opens the door to powerful studies on human movement. In particular the fine granularity of the location data, with GPS accuracy down to 10 meters, and the worldwide scale of Foursquare adoption are unprecedented. In this paper we study urban mobility patterns of people in several metropolitan cities around the globe by analyzing a large set of Foursquare users. Surprisingly, while there are variations in human movement in different cities, our analysis shows that those are predominantly due to different distributions of places across different urban environments. Moreover, a universal law for human mobility is identified, which isolates as a key component the rank-distance, factoring in the number of places between origin and destination, rather than pure physical distance, as considered in some previous works. Building on our findings, we also show how a rank-based movement model accurately captures real human movements in different cities. Our results shed new light on the driving factors of urban human mobility, with potential applications for urban planning, location-based advertisement and even social studies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Explore Spatiotemporal and Demographic Characteristics of Human Mobility via Twitter: A Case Study of Chicago,"Characterizing human mobility patterns is essential for understanding human behaviors and the interactions with socioeconomic and natural environment. With the continuing advancement of location and Web 2.0 technologies, location-based social media (LBSM) have been gaining widespread popularity in the past few years. With an access to locations of users, profiles and the contents of the social media posts, the LBSM data provided a novel modality of data source for human mobility study. By exploiting the explicit location footprints and mining the latent demographic information implied in the LBSM data, the purpose of this paper is to investigate the spatiotemporal characteristics of human mobility with a particular focus on the impact of demography. We first collect geo-tagged Twitter feeds posted in the conterminous United States area, and organize the collection of feeds using the concept of space-time trajectory corresponding to each Twitter user. Commonly human mobility measures, including detected home and activity centers, are derived for each user trajectory. We then select a subset of Twitter users that have detected home locations in the city of Chicago as a case study, and apply name analysis to the names provided in user profiles to learn the implicit demographic information of Twitter users, including race/ethnicity, gender and age. Finally we explore the spatiotemporal distribution and mobility characteristics of Chicago Twitter users, and investigate the demographic impact by comparing the differences across three demographic dimensions (race/ethnicity, gender and age). We found that, although the human mobility measures of different demographic groups generally follow the generic laws (e.g., power law distribution), the demographic information, particular the race/ethnicity group, significantly affects the urban human mobility patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A statistical framework for measuring the temporal stability of human mobility patterns,"Despite the growing popularity of human mobility studies that collect GPS location data, the problem of determining the minimum required length of GPS monitoring has not been addressed in the current statistical literature. In this paper we tackle this problem by laying out a theoretical framework for assessing the temporal stability of human mobility based on GPS location data. We define several measures of the temporal dynamics of human spatiotemporal trajectories based on the average velocity process, and on activity distributions in a spatial observation window. We demonstrate the use of our methods with data that comprise the GPS locations of 185 individuals over the course of 18 months. Our empirical results suggest that GPS monitoring should be performed over periods of time that are significantly longer than what has been previously suggested. Furthermore, we argue that GPS study designs should take into account demographic groups.   KEYWORDS: Density estimation; global positioning systems (GPS); human mobility; spatiotemporal trajectories; temporal dynamics","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Assessing the interplay between human mobility and mosquito borne diseases in urban environments,"Urbanization drives the epidemiology of infectious diseases to many threats and new challenges. In this research, we study the interplay between human mobility and dengue outbreaks in the complex urban environment of the city-state of Singapore. We integrate both stylized and mobile phone data-driven mobility patterns in an agent-based transmission model in which humans and mosquitoes are represented as agents that go through the epidemic states of dengue. We monitor with numerical simulations the system-level response to the epidemic by comparing our results with the observed cases reported during the 2013 and 2014 outbreaks. Our results show that human mobility is a major factor in the spread of vector-borne diseases such as dengue even on the short scale corresponding to intra-city distances. We finally discuss the advantages and the limits of mobile phone data and potential alternatives for assessing valuable mobility patterns for modeling vector-borne diseases outbreaks in cities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Assess the impacts of human mobility change on COVID-19 dynamics in Arizona, U.S.: a modeling study incorporating Google Community Mobility Reports","In June 2020, Arizona, U.S., emerged as one of the world's worst coronavirus disease 2019(COVID-19) spots after the stay-at-home order was lifted in the middle of May. However, with the decisions to reimpose restrictions, the number of COVID-19 cases has been declining, and Arizona is considered to be a good model in slowing the epidemic. In this paper, we aimed to examine the COVID-19 situation in Arizona and assess the impact of human mobility change. We constructed the mobility integrated metapopulation susceptible-infectious-removed model and fitted to publicly available datasets on COVID-19 cases and mobility changes in Arizona. Our simulations showed that by reducing human mobility, the peak time was delayed, and the final size of the epidemic was decreased in all three regions. Our analysis suggests that rapid and effective decision making is crucial to control human mobility and, therefore, COVID-19 epidemics. Until a vaccine is available, reimplementations of mobility restrictions in response to the increase of new COVID-19 cases might need to be considered in Arizona and beyond.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Defining the urban ""local"" with low dimensional manifolds of human mobility networks","Urban science has largely relied on universal models, rendering the heterogeneous and locally specific nature of cities effectively invisible. Here we introduce a topological framework that defines and detects localities in human mobility networks. We empirically demonstrate that these human mobility network localities are rigorous geometric entities that map directly to geographic localities, revealing that human mobility networks lie on manifolds of dimension <=5. This representation provides a compact theoretical foundation for spatial embedding and enables efficient applications to facility location and propagation modeling. Our approach reconciles local heterogeneity with universal representation, offering a new pathway toward a more comprehensive urban science.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Clustering patterns connecting COVID-19 dynamics and Human mobility using optimal transport,"Social distancing and stay-at-home are among the few measures that are known to be effective in checking the spread of a pandemic such as COVID-19 in a given population. The patterns of dependency between such measures and their effects on disease incidence may vary dynamically and across different populations. We described a new computational framework to measure and compare the temporal relationships between human mobility and new cases of COVID-19 across more than 150 cities of the United States with relatively high incidence of the disease. We used a novel application of Optimal Transport for computing the distance between the normalized patterns induced by bivariate time series for each pair of cities. Thus, we identified 10 clusters of cities with similar temporal dependencies, and computed the Wasserstein barycenter to describe the overall dynamic pattern for each cluster. Finally, we used city-specific socioeconomic covariates to analyze the composition of each cluster.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Data-driven generation of spatio-temporal routines in human mobility,"The generation of realistic spatio-temporal trajectories of human mobility is of fundamental importance in a wide range of applications, such as the developing of protocols for mobile ad-hoc networks or what-if analysis in urban ecosystems. Current generative algorithms fail in accurately reproducing the individuals' recurrent schedules and at the same time in accounting for the possibility that individuals may break the routine during periods of variable duration. In this article we present DITRAS (DIary-based TRAjectory Simulator), a framework to simulate the spatio-temporal patterns of human mobility. DITRAS operates in two steps: the generation of a mobility diary and the translation of the mobility diary into a mobility trajectory. We propose a data-driven algorithm which constructs a diary generator from real data, capturing the tendency of individuals to follow or break their routine. We also propose a trajectory generator based on the concept of preferential exploration and preferential return. We instantiate DITRAS with the proposed diary and trajectory generators and compare the resulting algorithm with real data and synthetic data produced by other generative algorithms, built by instantiating DITRAS with several combinations of diary and trajectory generators. We show that the proposed algorithm reproduces the statistical properties of real trajectories in the most accurate way, making a step forward the understanding of the origin of the spatio-temporal patterns of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Non-Compulsory Measures Sufficiently Reduced Human Mobility in Tokyo during the COVID-19 Epidemic,"While large scale mobility data has become a popular tool to monitor the mobility patterns during the COVID-19 pandemic, the impacts of non-compulsory measures in Tokyo, Japan on human mobility patterns has been under-studied. Here, we analyze the temporal changes in human mobility behavior, social contact rates, and their correlations with the transmissibility of COVID-19, using mobility data collected from more than 200K anonymized mobile phone users in Tokyo. The analysis concludes that by April 15th (1 week into state of emergency), human mobility behavior decreased by around 50%, resulting in a 70% reduction of social contacts in Tokyo, showing the effectiveness of non-compulsory measures. Furthermore, the reduction in data-driven human mobility metrics showed correlation with the decrease in estimated effective reproduction number of COVID-19 in Tokyo. Such empirical insights could inform policy makers on deciding sufficient levels of mobility reduction to contain the disease.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Schedule-based Analysis of Transmission Risk in Public Transportation Systems,"Airborne diseases, including COVID-19, raise the question of transmission risk in public transportation systems. However, quantitative analysis of the effectiveness of transmission risk mitigation methods in public transportation is lacking. The paper develops a transmission risk modeling framework based on the Wells-Riley model using as inputs transit operating characteristics, schedule, Origin-Destination (OD) demand, and virus characteristics. The model is sensitive to various factors that operators can control, as well as external factors that may be subject of broader policy decisions (e.g. mask wearing). The model is utilized to assess transmission risk as a function of OD flows, planned operations, and factors such as mask-wearing, ventilation, and infection rates. Using actual data from the Massachusetts Bay Transportation Authority (MBTA) Red Line, the paper explores the transmission risk under different infection rate scenarios, both in magnitude and spatial characteristics. The paper assesses the combined impact from viral load related factors and passenger load factors. Increasing frequency can mitigate transmission risk, but cannot fully compensate for increases in infection rates. Imbalanced passenger distribution on different cars of a train is shown to increase the overall system-wide infection probability. Spatial infection rate patterns should also be taken into account during policymaking as it is shown to impact transmission risk. For lines with branches, demand distribution among the branches is important and headway allocation adjustment among branches to balance the load on trains to different branches can help reduce risk.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Coupling Human Mobility and Social Ties,"Studies using massive, passively data collected from communication technologies have revealed many ubiquitous aspects of social networks, helping us understand and model social media, information diffusion, and organizational dynamics. More recently, these data have come tagged with geographic information, enabling studies of human mobility patterns and the science of cities. We combine these two pursuits and uncover reproducible mobility patterns amongst social contacts. First, we introduce measures of mobility similarity and predictability and measure them for populations of users in three large urban areas. We find individuals' visitations patterns are far more similar to and predictable by social contacts than strangers and that these measures are positively correlated with tie strength. Unsupervised clustering of hourly variations in mobility similarity identifies three categories of social ties and suggests geography is an important feature to contextualize social relationships. We find that the composition of a user's ego network in terms of the type of contacts they keep is correlated with mobility behavior. Finally, we extend a popular mobility model to include movement choices based on social contacts and compare it's ability to reproduce empirical measurements with two additional models of mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Real-world Meeting Points for Shared Demand-Responsive Transportation Systems,"While conventional shared demand-responsive transportation (SDRT) systems mostly operate on a door-to-door policy, the usage of meeting points for the pick-up and drop-off of user groups can offer several advantages, like fewer stops and less total travelled mileage. Moreover, it offers the possibility to select only feasible and well-defined locations where a safe (de-)boarding is possible. This paper presents a three-step workflow for solving the SDRT problem with meeting points (SDRT-MP). Firstly, the customers are clustered into similar groups, then meeting (and divergence) points are determined for each cluster. Finally, a parallel neighbourhood search algorithm is applied to create the vehicle routes. Further, a simulation with realistic pick-up and drop-off locations based on map data is performed in order to demonstrate the impact of using meeting points for SDRT systems in contrast to the door-to-door service. Although the average passenger travel time is higher due to enhanced walking and waiting times, the experiment highlights a reduction of operator resources required to serve all customers.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Differences in the spatial landscape of urban mobility: gender and socioeconomic perspectives,"Many of our routines and activities are linked to our ability to move; be it commuting to work, shopping for groceries, or meeting friends. Yet, factors that limit the individuals' ability to fully realise their mobility needs will ultimately affect the opportunities they can have access to (e.g., cultural activities, professional interactions). One important aspect frequently overlooked in human mobility studies is how gender-centred issues can amplify other sources of mobility disadvantages (e.g., socioeconomic inequalities), unevenly affecting the pool of opportunities men and women have access to. In this work, we leverage on a combination of computational, statistical, and information-theoretical approaches to investigate the existence of systematic discrepancies in the mobility diversity (i.e., the diversity of travel destinations) of (1) men and women from different socioeconomic backgrounds, and (2) work and non-work travels. Our analysis is based on datasets containing multiple instances of large-scale, official, travel surveys carried out in three major metropolitan areas in South America: Medelln and Bogot in Colombia, and So Paulo in Brazil. Our results indicate the presence of general discrepancies in the urban mobility diversities related to the gender and socioeconomic characteristics of the individuals. Lastly, this paper sheds new light on the possible origins of gender-level human mobility inequalities, contributing to the general understanding of disaggregated patterns in human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Digital breadcrumbs: Detecting urban mobility patterns and transport mode choices from cellphone networks,"Many modern and growing cities are facing declines in public transport usage, with few efficient methods to explain why. In this article, we show that urban mobility patterns and transport mode choices can be derived from cellphone call detail records coupled with public transport data recorded from smart cards. Specifically, we present new data mining approaches to determine the spatial and temporal variability of public and private transportation usage and transport mode preferences across Singapore. Our results, which were validated by Singapore's quadriennial Household Interview Travel Survey (HITS), revealed that there are 3.5 (HITS: 3.5 million) million and 4.3 (HITS: 4.4 million) million inter-district passengers by public and private transport, respectively. Along with classifying which transportation connections are weak or underserved, the analysis shows that the mode share of public transport use increases from 38 percent in the morning to 44 percent around mid-day and 52 percent in the evening.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility and Predictability enriched by Social Phenomena Information (extended abstract),"The information collected by mobile phone operators can be considered as the most detailed information on human mobility across a large part of the population. The study of the dynamics of human mobility using the collected geolocations of users, and applying it to predict future users' locations, has been an active field of research in recent years. In this work, we study the extent to which social phenomena are reflected in mobile phone data, focusing in particular in the cases of urban commute and major sports events. We illustrate how these events are reflected in the data, and show how information about the events can be used to improve predictability in a simple model for a mobile phone user's location.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility and Predictability enriched by Social Phenomena Information,"The massive amounts of geolocation data collected from mobile phone records has sparked an ongoing effort to understand and predict the mobility patterns of human beings. In this work, we study the extent to which social phenomena are reflected in mobile phone data, focusing in particular in the cases of urban commute and major sports events. We illustrate how these events are reflected in the data, and show how information about the events can be used to improve predictability in a simple model for a mobile phone user's location.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Investigating Bimodal Clustering in Human Mobility,"We apply a simple clustering algorithm to a large dataset of cellular telecommunication records, reducing the complexity of mobile phone users' full trajectories and allowing for simple statistics to characterize their properties. For the case of two clusters, we quantify how clustered human mobility is, how much of a user's spatial dispersion is due to motion between clusters, and how spatially and temporally separated clusters are from one another.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human migration patterns in large scale spatial with the resume data,"Researches on the human mobility have made great progress in many aspects, but the long-term and long-distance migration behavior is lack of in-depth and extensive research because of the difficult in accessing to household data. In this paper, we use the resume data to discover the human migration behavior on the large scale scope. It is found that the asymmetry in the flow structure which reflects the influence of population competition is caused by the difference of attractiveness among cities. This flow structure can be approximately described by the gravity model of spatial economics. Besides, the value of scaling exponent of distance function in the gravity model is less than the value of short-term travel behavior. It means that, compared with the short-term travel behavior, the long-term human migration behavior is less sensitive. Moreover, the scaling coefficients of each variable in the gravity model are investigated. The result shows that the economic level is a mainly factor on the migration.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Human Mobility Networks Manifest Dissimilar Resilience Characteristics at Macroscopic, Substructure, and Microscopic Scales","Human mobility networks can reveal insights into resilience phenomena, such as population response to, impacts on, and recovery from crises. The majority of human mobility network resilience characterizations, however, focus mainly on macroscopic network properties; little is known about variation in measured resilience characteristics (i.e., the extent of impact and recovery duration) across macroscopic, substructure (motif), and microscopic mobility scales. To address this gap, in this study, we examine the human mobility network in eight parishes in Louisiana (USA) impacted by the 2021 Hurricane Ida. We constructed human mobility networks using location-based data and examined three sets of measures: (1) macroscopic measures, such as network density, giant component size, and modularity; (2) substructure measures, such motif distribution; and (3) microscopic mobility measures, such as the radius of gyration and average travel distance. To determine the extent of impact and duration of recovery, for each measure, we established the baseline values and examined the fluctuation of measures during the perturbation caused by Hurricane Ida. The results reveal the variation of impact extent and recovery duration obtained from different sets of measures at different scales. Macroscopic measures, such as giant components, tend to recover more quickly than substructure and microscopic measures. In fact, microscopic measures tend to recover more slowly than measures in other scales. These findings suggest that resilience characteristics in human mobility networks are scale-variant, and thus, a single measure at a particular scale may not be representative of the perturbation impacts and recovery duration in the network as a whole. These results spotlight the need to use measures at different scales to properly characterize resilience in human mobility networks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A Simple Walk Model for Reproducing Power Laws in Human mobility,"Identifying statistical patterns characterizing human trajectories is crucial for public health, traffic engineering, city planning, and epidemic modeling. Recent developments in global positioning systems and mobile phone networks have enabled the collection of substantial information on human movement. Analyses of these data have revealed various power laws in the temporal and spatial statistical patterns of human mobility. For example, jump size and waiting time distributions follow power laws. Zipf's law was also established for the frequency of visits to each location and rank. Relationship $S(t)\sim t^$ exists between time t and the number of sites visited up to that time t. Recently, a universal law of visitation for human mobility was established. Specifically, the number of people per unit area $(r,f)$, who reside at distance r from a particular location and visit that location f times in a given period, is inversely proportional to the square of rf, i.e., $(r,f) \propto (rf)^{-2}$ holds. The exploration and preferential return (EPR) model and its improved versions have been proposed to reproduce the above scaling laws. However, some rules that follow the power law are preinstalled in the EPR model. We propose a simple walking model to generate movements toward and away from a target via a single mechanism by relaxing the concept of approaching a target. Our model can reproduce the abovementioned power laws and some of the rules used in the EPR model are generated. These results provide a new perspective on why or how the scaling laws observed in human mobility behavior arise.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The Effect of Recency to Human Mobility,"In recent years, we have seen scientists attempt to model and explain human dynamics and, in particular, human movement. Many aspects of our complex life are affected by human movements such as disease spread and epidemics modeling, city planning, wireless network development, and disaster relief, to name a few. Given the myriad of applications it is clear that a complete understanding of how people move in space can lead to huge benefits to our society. In most of the recent works, scientists have focused on the idea that people movements are biased towards frequently-visited locations. According to them, human movement is based on an exploration/exploitation dichotomy in which individuals choose new locations (exploration) or return to frequently-visited locations (exploitation). In this work, we focus on the concept of recency. We propose a model in which exploitation in human movement also considers recently-visited locations and not solely frequently-visited locations. We test our hypothesis against different empirical data of human mobility and show that our proposed model is able to better explain the human trajectories in these datasets.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Toward a General Understanding of the Scaling Laws in Human and Animal Mobility,"Recent research highlighted the scaling property of human and animal mobility. An interesting issue is that the exponents of scaling law for animals and humans in different situations are quite different. This paper proposes a general optimization model, a random walker following scaling laws (whose traveling distances in each step obey a power law distribution with exponent ) tries to diversify its visiting places under a given total traveling distance with a home-return probability. The results show that different optimal exponents in between 1 and 2 can emerge naturally. Therefore, the scaling property of human and animal mobility can be understood in our framework where the discrepancy of the scaling law exponents is due to the home-return constraint under the maximization of the visiting places diversity.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban Dynamics Through the Lens of Human Mobility,"The urban spatial structure represents the distribution of public and private spaces in cities and how people move within them. While it usually evolves slowly, it can change fast during large-scale emergency events, as well as due to urban renewal in rapidly developing countries. This work presents an approach to delineate such urban dynamics in quasi-real-time through a human mobility metric, the mobility centrality index $KS$. As a case study, we tracked the urban dynamics of eleven Spanish cities during the COVID-19 pandemic. Results revealed that their structures became more monocentric during the lockdown in the first wave, but kept their regular spatial structures during the second wave. To provide a more comprehensive understanding of mobility from home, we also introduce a dimensionless metric, $KS_{HBT}$, which measures the extent of home-based travel and provides statistical insights into the transmission of COVID-19. By utilizing individual mobility data, our metrics enable the detection of changes in the urban spatial structure.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Optimization strategies of human mobility during the COVID-19 pandemic: A review,"The impact of the ongoing COVID-19 pandemic is being felt in all spheres of our lives -- cutting across the boundaries of nation, wealth, religions or race. From the time of the first detection of infection among the public, the virus spread though almost all the countries in the world in a short period of time. With humans as the carrier of the virus, the spreading process necessarily depends on the their mobility after being infected. Not only in the primary spreading process, but also in the subsequent spreading of the mutant variants, human mobility plays a central role in the dynamics. Therefore, on one hand travel restrictions of varying degree were imposed and are still being imposed, by various countries both nationally and internationally. On the other hand, these restrictions have severe fall outs in businesses and livelihood in general. Therefore, it is an optimization process, exercised on a global scale, with multiple changing variables. Here we review the techniques and their effects on optimization or proposed optimizations of human mobility in different scales, carried out by data driven, machine learning and model approaches.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Novel approaches to urban science problems: human mobility description by physical analogy of electric circuit network based on GPS data,"Human mobility in an urban area is complicated; the origins, destinations, and transport methods of each person differ. The quantitative description of urban human mobility has recently attracted the attention of researchers, and it highly related to urban science problems. Herein, combined with physics inspiration, we introduce a revised electric circuit model (RECM) in which moving people are regarded as charged particles and analogical concepts of electromagnetism such as human conductivity and human potential enable us to capture the characteristics of urban human mobility. We introduce the unit system, ensure the uniqueness of the calculation result, and reduce the computation cost of the algorithm to 1/10000 compared with the original ECM, making the model more universal and easier to use. We compared features including human conductivity and potential between different cities in Japan to show our improvement of the universality and the application range of the model. Furthermore, based on inspiration of physics, we propose a route generation model (RGM) to simulate a human flow pattern that automatically determines suitable routes between a given origin and destination as a source and sink, respectively. These discoveries are expected to lead to new approaches to the solution of urban science problems.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Evaluating resilience in urban transportation systems for sustainability: A systems-based Bayesian network model,"This paper proposes a hierarchical Bayesian network model (BNM) to quantitatively evaluate the resilience of urban transportation infrastructure. Based on systemic thinkings and sustainability perspectives, we investigate the long-term resilience of the road transportation systems in four cities of China from 1998 to 2017, namely Beijing, Tianjin, Shanghai, and Chongqing, respectively. The model takes into account the factors involved in stages of design, construction, operation, management, and innovation of urban road transportation, which collected from multi-source data platforms. We test the model with the forward inference, sensitivity analysis, and backward inference. The result shows that the overall resilience of all four cities' transportation infrastructure is within a moderate range with values between 50% to 60%. Although they all have an ever-increasing economic level, Beijing and Tianjin demonstrate a clear ""V"" shape in the long-term transportation resilience, which indicates a strong multi-dimensional, dynamic, and non-linear characteristic in resilience-economic coupling effect. Additionally, the results obtained from the sensitivity analysis and backward inference suggest that urban decision-makers should pay more attention to the capabilities of quick rebuilding and making changes to cope with future disturbance. As an exploratory study, this study clarifies the concepts of long-term multi-dimensional resilience and specific hazard-related resilience and provides an effective decision-support tool for stakeholders when building sustainable infrastructure.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Do Human Mobility Network Analyses Produced from Different Location-based Data Sources Yield Similar Results across Scales?,"The burgeoning availability of sensing technology and location-based data is driving the expansion of analysis of human mobility networks in science and engineering research, as well as in epidemic forecasting and mitigation, urban planning, traffic engineering, emergency response, and business development. However, studies employ datasets provided by different location-based data providers, and the extent to which the human mobility measures and results obtained from different datasets are comparable is not known. To address this gap, in this study, we examined three prominent location-based data sources: Spectus, X-Mode, and Veraset to analyze human mobility networks across metropolitan areas at different scales: global, sub-structure, and microscopic. Dissimilar results were obtained from the three datasets, suggesting the sensitivity of network models and measures to datasets. This finding has important implications for building generalized theories of human mobility and urban dynamics based on different datasets. The findings also highlighted the need for ground-truthed human movement datasets to serve as the benchmark for testing the representativeness of human mobility datasets. Researchers and decision-makers across different fields of science and technology should recognize the sensitivity of human mobility results to dataset choice and develop procedures for ground-truthing the selected datasets in terms of representativeness of data points and transferability of results.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Evaluation of Dedicated Lanes for Automated vehicles at Roundabouts with Various Flow Patterns,"Autonomous vehicles (AVs) are about to be used in transportation systems in the near future. To increase the level of safety and throughput of these vehicles, dedicated lanes for AVs have been suggested in past studies as exclusive mobility infrastructure for these types of vehicles. Although these lanes can bring obvious advantages in a transportation network, overall performance of these lanes, especially in urban areas and in micro level sight, is not clear. This study aims to examine the efficiency of dedicated lanes for AVs in roundabout with an unbalanced traffic flow pattern. Four factors of travel time, delay time, speed of vehicles, and queue of vehicles have been selected as variables of traffic performance at the roundabout. Two microscopic traffic simulation software, AIMSUN and SIDRA Intersection, were used to examine the impact of the AV dedicated lanes at the roundabout. This study shows that the effects of an imbalanced traffic pattern in a roundabout are higher when the penetration rate of AVs is lower and also dedicated lanes in roundabout's leg may improve traffic performance indicators when the penetration rate of AVs is higher, but this improvement is not significant.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Emergence of Urban Heat Traps from the Intersection of Human Mobility and Heat Hazard Exposure in Cities,"Understanding the relationship between spatial structures of cities and environmental hazard exposures (such as urban heat) is essential for urban health and sustainability planning. However, a critical knowledge gap exists in terms of the extent to which socio-spatial networks shaped by human mobility exacerbate or alleviate urban heat exposures of populations in cities. In this study, we utilize location-based data to construct human mobility networks in twenty metropolitan areas in the U.S. The human mobility networks are analyzed in conjunction with the urban heat characteristics of spatial areas. We identify areas with high and low urban heat exposure and evaluate visitation patterns of populations residing in high and low urban heat areas to other spatial areas with similar and dissimilar urban heat exposure. The results reveal the presence of urban heat traps in the majority of the studied metropolitan areas in which populations residing in high heat exposure areas primarily visit areas with high heat exposure. The results also show a small percentage of human mobility to produce urban heat escalate (visitations from low heat areas to high heat areas) and heat escapes (movements from high heat areas to low heat areas). The findings from this study provide a better understanding of urban heat exposure in cities based on patterns of human mobility. These finding contribute to a broader understanding of the intersection of human network dynamics and environmental hazard exposures in cities to inform more integrated urban design and planning to promote health and sustainability.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Characterizing Community Formation in Response to Extreme Weather Events through Human Mobility Networks,"Community formation in socio-spatial human networks is one of the important mechanisms for mitigating hazard impacts of extreme weather events. Research is scarce regarding latent network characteristics shaping community formation in human mobility networks during natural disasters. Here, we examined human mobility networks in Harris County, Texas, in the context of the managed power outage forced by 2021 Winter Storm Uri to detect communities and to evaluate latent characteristics in those communities. We examined three characteristics in the communities formed within human mobility networks: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The results show that population movements were shaped by socio-demographic homophily, heterophilic hazard exposure, and social connectedness strength. Our results also indicate that a community encompassing more high-impact areas would motivate population movements to areas with weaker social connectedness. Our findings reveal important characteristics shaping community formation in human mobility networks in hazard response. Specific to managed power outages, formed communities are spatially co-located, underscoring a best management practice to avoid prolonged power outages among areas within communities, thus improving hazard exposure heterophily. The findings have implications for power utility operators to account for the characteristics of socio-spatial human networks when determining the patterns of managed power outages.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Interdependence and Predictability of Human Mobility and Social Interactions,"Previous studies have shown that human movement is predictable to a certain extent at different geographic scales. Existing prediction techniques exploit only the past history of the person taken into consideration as input of the predictors. In this paper, we show that by means of multivariate nonlinear time series prediction techniques it is possible to increase the forecasting accuracy by considering movements of friends, people, or more in general entities, with correlated mobility patterns (i.e., characterised by high mutual information) as inputs. Finally, we evaluate the proposed techniques on the Nokia Mobile Data Challenge and Cabspotting datasets.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Impact of Distance on Epidemiological Dynamics in Human Connection Network with Mobility,"The spread of infectious diseases is often influenced by human mobility across different geographical regions. Although numerous studies have investigated how diseases like SARS and COVID-19 spread from China to various global locations, there remains a gap in understanding how the movement of individuals contributes to disease transmission on a more personal or human-to-human level. Typically, researchers have employed the concept of metapopulation movement to analyze how diseases move from one location to another. This paper shifts focus to the dynamics of disease transmission, incorporating the critical factor of distance between an infected person and a healthy individual during human movement. The study delves into the impact of distance on various parameters of epidemiological dynamics throughout human mobility. Mathematical expressions for important epidemiological metrics, such as the basic reproduction number ($R_0$) and the critical infection rate ($_{critical}$), are derived in relation to the distance between individuals. The results indicate that the proposed model closely aligns with observed patterns of COVID-19 spread based on the analysis done on the available datasets.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Revealing travel patterns and city structure with taxi trip data,"Detecting regional spatial structures based on spatial interactions is crucial in applications ranging from urban planning to traffic control. In the big data era, various movement trajectories are available for studying spatial structures. This research uses large scale Shanghai taxi trip data extracted from GPS-enabled taxi trajectories to reveal traffic flow patterns and urban structure of the city. Using the network science methods, 15 temporally stable regions reflecting the scope of people's daily travels are found using community detection method on the network built from short trips, which represent residents' daily intra-urban travels and exhibit a clear pattern. In each region, taxi traffic flows are dominated by a few 'hubs' and 'hubs' in suburbs impact more trips than 'hubs' in urban areas. Land use conditions in urban regions are different from those in suburban areas. Additionally, 'hubs' in urban area associate with office buildings and commercial areas more, whereas residential land use is more common in suburban hubs. The taxi flow structures and land uses reveal the polycentric and layered concentric structure of Shanghai. Finally, according to the temporal variations of taxi flows and the diversity levels of taxi trip lengths, we explore the total taxi traffic properties of each region and proved the city structure we find. External trips across regions also take large proportion of the total traffic in each region, especially in suburbs. The results could help transportation policy making and shed light on the way to reveal urban structures with big data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Scaling laws of human mobility persist during extreme floods,"Although a number of studies have investigated human mobility patterns during natural hazards, mechanistic models that capture mobility dynamics under large-scale perturbations, such as extreme floods, remain scarce. Leveraging mobile phone data and building upon recent insights into universal mobility patterns, we assess whether the general structure of population flows persists during the extreme floods that struck Emilia-Romagna, Italy, in 2023. Our analysis reveals that the relationship between visitor density, distance, and visitation frequency remains robust even under extreme flooding conditions. To disentangle the effects of distance and visitation frequency, we define two aggregated visitor densities: the marginal density over frequency and the aggregated density over distance. We find that the marginal density over frequency exhibits a time-invariant power-law exponent, indicating resilience to flooding disturbances. In contrast, the aggregated density over distance displays more complex behavior: an exponential decay over biweekly periods and a power-law decay over a monthly interval. We propose that the observed power law emerges from the superposition of exponential distributions across shorter timescales. These findings provide new insights into human mobility scaling laws under extreme perturbations, highlighting the robustness of visitation patterns and suggesting avenues for improved mechanistic modeling during natural disasters.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Assessing reliable human mobility patterns from higher-order memory in mobile communications,"Understanding how people move within a geographic area, e.g. a city, a country or the whole world, is fundamental in several applications, from predicting the spatio-temporal evolution of an epidemics to inferring migration patterns. Mobile phone records provide an excellent proxy of human mobility, showing that movements exhibit a high level of memory. However, the precise role of memory in widely adopted proxies of mobility, as mobile phone records, is unknown. Here we use 560 millions of call detail records from Senegal to show that standard Markovian approaches, including higher-order ones, fail in capturing real mobility patterns and introduce spurious movements never observed in reality. We introduce an adaptive memory-driven approach to overcome such issues. At variance with Markovian models, it is able to realistically model conditional waiting times, i.e. the probability to stay in a specific area depending on individual's historical movements. Our results demonstrate that in standard mobility models the individuals tend to diffuse faster than what observed in reality, whereas the predictions of the adaptive memory approach significantly agree with observations. We show that, as a consequence, the incidence and the geographic spread of a disease could be inadequately estimated when standard approaches are used, with crucial implications on resources deployment and policy making during an epidemic outbreak.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A severe local flood and social events show a similar impact on human mobility,"While a social event, such as a concert or a food festival, is a common experience to people, a natural disaster is experienced by a fewer individuals. The ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon, but devastating natural events on society, such as floods. Based on this idea, we present a comparison - in terms of human mobility -, between an extreme local flood that occurred in 2017 in Switzerland, and social events which took place in the same region, in the weeks before and after the inundation. Using mobile phone location data, we show that the severe local flood and social events have a similar impact on human mobility, both at the national scale and at a local scale. At the national level, we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place. At the local level, instead, we detected the anomalies (in time series) in the number of people travelling each road and railway, and we found that the distributions of anomalies, and of their clusters, are comparable between the flood and the social events. Hence, our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility. The proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and, in general, as complementary approaches for the analyses of complex systems.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Age-specific contacts and travel patterns in the spatial spread of 2009 H1N1 influenza pandemic,"Confirmed cases during the early stage of the 2009 H1N1 pdm in various countries showed an age shift between importations and local transmission cases, with adults mainly responsible for seeding unaffected regions and children most frequently driving community outbreaks. We introduce a multi-host stochastic metapopulation model with two age classes to analytically address the role of a heterogeneously mixing population and its associated non-homogeneous travel behaviors on the risk of a major epidemic. We inform the model with statistics on demography, mixing and travel behavior for Europe and Mexico, and calibrate it to the 2009 H1N1 pdm early outbreak. We varied model parameters to explore the invasion conditions under different scenarios. We derive the expression for the global invasion potential of the epidemic that depends on disease transmissibility, transportation network and mobility features, demographic profile and mixing pattern. Highly assortative mixing favor the spatial containment of the epidemic, this effect being contrasted by an increase in the social activity of adults vs. children. Heterogeneity of the mobility network topology and traffic flows strongly favor the disease invasion, as also a larger fraction of children traveling. Variations in the demography and mixing habits across countries lead to heterogeneous outbreak situations. Results are compatible with the H1N1 spatial spread observed. The work illustrates the importance of age-dependent mixing profiles and mobility features in the study of the conditions for the spatial invasion of an emerging influenza pandemic. Its results allow the immediate assessment of the risk of a major epidemic for a specific scenario upon availability of data, and the evaluation of the effectiveness of public health interventions targeting specific age groups, their interactions and mobility behaviors.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Inferring urban polycentricity from the variability in human mobility patterns,"The polycentric city model has gained popularity in spatial planning policy, since it is believed to overcome some of the problems often present in monocentric metropolises, ranging from congestion to difficult accessibility to jobs and services. However, the concept 'polycentric city' has a fuzzy definition and as a result, the extent to which a city is polycentric cannot be easily determined. Here, we leverage the fine spatio-temporal resolution of smart travel card data to infer urban polycentricity by examining how a city departs from a well-defined monocentric model. In particular, we analyse the human movements that arise as a result of sophisticated forms of urban structure by introducing a novel probabilistic approach which captures the complexity of these human movements. We focus on London (UK) and Seoul (South Korea) as our two case studies, and we specifically find evidence that London displays a higher degree of monocentricity than Seoul, suggesting that Seoul is likely to be more polycentric than London.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The limits of human mobility traces to predict the spread of COVID-19,"Mobile phone data have been widely used to model the spread of COVID-19, however, quantifying and comparing their predictive value across different settings is challenging. Their quality is affected by various factors and their relationship with epidemiological indicators varies over time. Here we adopt a model-free approach based on transfer entropy to quantify the relationship between mobile phone-derived mobility metrics and COVID-19 cases and deaths in more than 200 European subnational regions. We found that past knowledge of mobility does not provide statistically significant information on COVID-19 cases or deaths in most of the regions. In the remaining ones, measures of contact rates were often more informative than movements in predicting the spread of the disease, while the most predictive metrics between mid-range and short-range movements depended on the region considered. We finally identify geographic and demographic factors, such as users' coverage and commuting patterns, that can help determine the best metric for predicting disease incidence in a particular location. Our approach provides epidemiologists and public health officials with a general framework to evaluate the usefulness of human mobility data in responding to epidemics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Unraveling the origin of exponential law in intra-urban human mobility,"The vast majority of travel takes place within cities. Recently, new data has become available which allows for the discovery of urban mobility patterns which differ from established results about long distance travel. Specifically, the latest evidence increasingly points to exponential trip length distributions, contrary to the scaling laws observed on larger scales. In this paper, in order to explore the origin of the exponential law, we propose a new model which can predict individual flows in urban areas better. Based on the model, we explain the exponential law of intra-urban mobility as a result of the exponential decrease in average population density in urban areas. Indeed, both empirical and analytical results indicate that the trip length and the population density share the same exponential decaying rate.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Fractional diffusion emulates a human mobility network during a simulated disease outbreak,"From footpaths to flight routes, human mobility networks facilitate the spread of communicable diseases. Control and elimination efforts depend on characterizing these networks in terms of connections and flux rates of individuals between contact nodes. In some cases, transport can be parameterized with gravity-type models or approximated by a diffusive random walk. As a alternative, we have isolated intranational commercial air traffic as a case study for the utility of non-diffusive, heavy-tailed transport models. We implemented new stochastic simulations of a prototypical influenza-like infection, focusing on the dense, highly-connected United States air travel network. We show that mobility on this network can be described mainly by a power law, in agreement with previous studies. Remarkably, we find that the global evolution of an outbreak on this network is accurately reproduced by a two-parameter space-fractional diffusion equation, such that those parameters are determined by the air travel network.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding Human Mobility Flows from Aggregated Mobile Phone Data,"In this paper we deal with the study of travel flows and patterns of people in large populated areas. Information about the movements of people is extracted from coarse-grained aggregated cellular network data without tracking mobile devices individually. Mobile phone data are provided by the Italian telecommunication company TIM and consist of density profiles (i.e. the spatial distribution) of people in a given area at various instants of time. By computing a suitable approximation of the Wasserstein distance between two consecutive density profiles, we are able to extract the main directions followed by people, i.e. to understand how the mass of people distribute in space and time. The main applications of the proposed technique are the monitoring of daily flows of commuters, the organization of large events, and, more in general, the traffic management and control.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Balancing Mobility Behaviors to avoid Global epidemics from Local Outbreaks,"Human interactions and mobility shape epidemic dynamics by facilitating disease outbreaks and their spatial spread across regions. Traditional models often isolate commuting and random mobility as separate behaviors, focusing either on short, recurrent trips or on random, exploratory movements. Here, we propose a unified formalism that allows a smooth transition between commuting and exploratory behavior based on travel and return probabilities. We derive an analytical expression for the epidemic threshold, revealing a non-monotonic dependence on recurrence rates: while recurrence tends to lower the threshold by increasing agent concentration in high-contact hubs, it counterintuitively raises the invasion threshold in low-mobility scenarios, suggesting that allowing recurrence may foster local outbreaks while suppressing global epidemics. These results provide a comprehensive understanding of the interplay between human mobility patterns and epidemic spread, with implications for containment strategies in structured populations.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A Linear Dynamical Perspective on Epidemiology: Interplay Between Early COVID-19 Outbreak and Human Mobility,"This paper investigates the impact of human activity and mobility (HAM) in the spreading dynamics of an epidemic. Specifically, it explores the interconnections between HAM and its effect on the early spread of the COVID-19 virus. During the early stages of the pandemic, effective reproduction numbers exhibited a high correlation with human mobility patterns, leading to a hypothesis that the HAM system can be studied as a coupled system with disease spread dynamics. This study applies the generalized Koopman framework with control inputs to determine the nonlinear disease spread dynamics and the input-output characteristics as a locally linear controlled dynamical system. The approach solely relies on the snapshots of spatiotemporal data and does not require any knowledge of the system's physical laws. We exploit the Koopman operator framework by utilizing the Hankel Dynamic Mode Decomposition with Control (HDMDc) algorithm to obtain a linear disease spread model incorporating human mobility as a control input. The study demonstrated that the proposed methodology could capture the impact of local mobility on the early dynamics of the ongoing global pandemic. The obtained locally linear model can accurately forecast the number of new infections for various prediction windows ranging from two to four weeks. The study corroborates a leader-follower relationship between mobility and disease spread dynamics. In addition, the effect of delay embedding in the HDMDc algorithm is also investigated and reported. A case study was performed using COVID infection data from Florida, US, and HAM data extracted from Google community mobility data report.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Homesick Lvy walk: A mobility model having Ichi-go Ichi-e and scale-free properties of human encounters,"In recent years, mobility models have been reconsidered based on findings by analyzing some big datasets collected by GPS sensors, cellphone call records, and Geotagging. To understand the fundamental statistical properties of the frequency of serendipitous human encounters, we conducted experiments to collect long-term data on human contact using short-range wireless communication devices which many people frequently carry in daily life. By analyzing the data we showed that the majority of human encounters occur once-in-an-experimental-period: they are Ichi-go Ichi-e. We also found that the remaining more frequent encounters obey a power-law distribution: they are scale-free. To theoretically find the origin of these properties, we introduced as a minimal human mobility model, Homesick Lvy walk, where the walker stochastically selects moving long distances as well as Lvy walk or returning back home. Using numerical simulations and a simple mean-field theory, we offer a theoretical explanation for the properties to validate the mobility model. The proposed model is helpful for evaluating long-term performance of routing protocols in delay tolerant networks and mobile opportunistic networks better since some utility-based protocols select nodes with frequent encounters for message transfer.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Assessing the Interplay between travel patterns and SARS-CoV-2 outbreak in realistic urban setting,"The dense social contact networks and high mobility in congested urban areas facilitate the rapid transmission of infectious diseases. Typical mechanistic epidemiological models are either based on uniform mixing with ad-hoc contact processes or need real-time or archived population mobility data to simulate the social networks. However, the rapid and global transmission of the novel coronavirus (SARS-CoV-2) has led to unprecedented lockdowns at global and regional scales, leaving the archived datasets to limited use. While it is often hypothesized that population density is a significant driver in disease propagation, the disparate disease trajectories and infection rates exhibited by the different cities with comparable densities require a high-resolution description of the disease and its drivers. In this study, we explore the impact of the creation of containment zones on travel patterns within the city. Further, we use a dynamical network-based infectious disease model to understand the key drivers of disease spread at sub-kilometer scales demonstrated in the city of Ahmedabad, India, which has been classified as a SARS-CoV-2 hotspot. We find that in addition to the contact network and population density, road connectivity patterns and ease of transit are strongly correlated with the rate of transmission of the disease. Given the limited access to real-time traffic data during lockdowns, we generate road connectivity networks using open-source imageries and travel patterns from open-source surveys and government reports. Within the proposed framework, we then analyze the relative merits of social distancing, enforced lockdowns, and enhanced testing and quarantining mitigating the disease spread.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Natural human mobility patterns and spatial spread of infectious diseases,"We investigate a model for spatial epidemics explicitly taking into account bi-directional movements between base and destination locations on individual mobility networks. We provide a systematic analysis of generic dynamical features of the model on regular and complex metapopulation network topologies and show that significant dynamical differences exist to ordinary reaction-diffusion and effective force of infection models. On a lattice we calculate an expression for the velocity of the propagating epidemic front and find that in contrast to the diffusive systems, our model predicts a saturation of the velocity with increasing traveling rate. Furthermore, we show that a fully stochastic system exhibits a novel threshold for attack ratio of an outbreak absent in diffusion and force of infection models. These insights not only capture natural features of human mobility relevant for the geographical epidemic spread, they may serve as a starting point for modeling important dynamical processes in human and animal epidemiology, population ecology, biology and evolution.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Scaling of variations in traveling distances and times of taxi routes,"The importance of understanding human mobility patterns has led many studies to examine their spatial-temporal scaling laws. These studies mainly reveal that human travel can be highly non-homogeneous with power-law scaling distributions of distances and times. However, investigating and quantifying the extent of variability in time and space when traveling the same air distance has not been addressed so far. Using taxi data from five large cities, we focus on several novel measures of distance and time to explore the spatio-temporal variations of taxi travel routes relative to their typical routes during peak and nonpeak periods. To compare all trips using a single measure, we calculate the distributions of the ratios between actual travel distances and the average travel distance as well as between actual travel times and the average travel time for all origin destinations (OD) during peak and nonpeak periods. In this way, we measure the scaling of the distribution of all single trip paths with respect to their mean trip path. Our results surprisingly demonstrate very broad distributions for both the distance ratio and time ratio, characterized by a long-tail power-law distribution. Moreover, all analyzed cities have larger exponents in peak hours than in nonpeak hours. We suggest that the interesting results of shorter trip lengths and times, characterized by larger exponents during rush hours, are due to the higher availability of travelers in rush hours compared to non-rush hours...","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Data Bias in Human Mobility is a Universal Phenomenon but is Highly Location-specific,"Large-scale human mobility datasets play increasingly critical roles in many algorithmic systems, business processes and policy decisions. Unfortunately there has been little focus on understanding bias and other fundamental shortcomings of the datasets and how they impact downstream analyses and prediction tasks. In this work, we study `data production', quantifying not only whether individuals are represented in big digital datasets, but also how they are represented in terms of how much data they produce. We study GPS mobility data collected from anonymized smartphones for ten major US cities and find that data points can be more unequally distributed between users than wealth. We build models to predict the number of data points we can expect to be produced by the composition of demographic groups living in census tracts, and find strong effects of wealth, ethnicity, and education on data production. While we find that bias is a universal phenomenon, occurring in all cities, we further find that each city suffers from its own manifestation of it, and that location-specific models are required to model bias for each city. This work raises serious questions about general approaches to debias human mobility data and urges further research.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Contrasting social and non-social sources of predictability in human mobility,"Social structures influence a variety of human behaviors including mobility patterns, but the extent to which one individual's movements can predict another's remains an open question. Further, latent information about an individual's mobility can be present in the mobility patterns of both social and non-social ties, a distinction that has not yet been addressed. Here we develop a ""colocation"" network to distinguish the mobility patterns of an ego's social ties from those of non-social colocators, individuals not socially connected to the ego but who nevertheless arrive at a location at the same time as the ego. We apply entropy and predictability measures to analyse and bound the predictive information of an individual's mobility pattern and the flow of that information from their top social ties and from their non-social colocators. While social ties generically provide more information than non-social colocators, we find that significant information is present in the aggregation of non-social colocators: 3-7 colocators can provide as much predictive information as the top social tie, and colocators can replace up to 85% of the predictive information about an ego, compared with social ties that can replace up to 94% of the ego's predictability. The presence of predictive information among non-social colocators raises privacy concerns: given the increasing availability of real-time mobility traces from smartphones, individuals sharing data may be providing actionable information not just about their own movements but the movements of others whose data are absent, both known and unknown individuals.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Towards Understanding the Impact of Human Mobility on Police Allocation,"Motivated by recent findings that human mobility is proxy for crime behavior in big cities and that there is a superlinear relationship between the people's movement and crime, this article aims to evaluate the impact of how these findings influence police allocation. More precisely, we shed light on the differences between an allocation strategy, in which the resources are distributed by clusters of floating population, and conventional allocation strategies, in which the police resources are distributed by an Administrative Area (typically based on resident population). We observed a substantial difference in the distributions of police resources allocated following these strategies, what evidences the imprecision of conventional police allocation methods.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Characterization of delay propagation in the US air transportation network,"Complex networks provide a suitable framework to characterize air traffic. Previous works described the world air transport network as a graph where direct flights are edges and commercial airports are vertices. In this work, we focus instead on the properties of flight delays in the US air transportation network. We analyze flight performance data in 2010 and study the topological structure of the network as well as the aircraft rotation. The properties of flight delays, including the distribution of total delays, the dependence on the day of the week and the hour-by-hour evolution within each day, are characterized paying special attention to flights accumulating delays longer than 12 hours. We find that the distributions are robust to changes in takeoff or landing operations, different moments of the year or even different airports in the contiguous states. However, airports in remote areas (Hawaii, Alaska, Puerto Rico) can show peculiar distributions biased toward long delays. Additionally, we show that long delayed flights have an important dependence on the destination airport.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Destination Choice Game: A Spatial Interaction Theory on Human Mobility,"With remarkable significance in migration prediction, global disease mitigation, urban planning and many others, an arresting challenge is to predict human mobility fluxes between any two locations. A number of methods have been proposed against the above challenge, including the gravity model, the intervening opportunity model, the radiation model, the population-weighted opportunity model, and so on. Despite their theoretical elegance, all models ignored an intuitive and important ingredient in individual decision about where to go, that is, the possible congestion on the way and the possible crowding in the destination. Here we propose a microscopic mechanism underlying mobility decisions, named destination choice game (DCG), which takes into account the crowding effects resulted from spatial interactions among individuals. In comparison with the state-of-the-art models, the present one shows more accurate prediction on mobility fluxes across wide scales from intracity trips to intercity travels, and further to internal migrations. The well-known gravity model is proved to be the equilibrium solution of a degenerated DCG neglecting the crowding effects in the destinations.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Vector-borne epidemics driven by human mobility,"Vector-borne epidemics are the result of the combination of different factors such as the crossed contagions between humans and vectors, their demographic distribution and human mobility among others. The current availability of information about the former ingredients demands their incorporation to current mathematical models for vector-borne disease transmission. Here, relying on metapopulation dynamics, we propose a framework whose results are in fair agreement with those obtained from mechanistic simulations. This framework allows us to derive an expression of the epidemic threshold capturing with high accuracy the conditions leading to the onset of epidemics. Driven by these insights, we obtain a prevalence indicator to rank the patches according to the risk of being affected by a vector-borne disease. We illustrate the utility of this epidemic risk indicator by reproducing the spatial distribution Dengue cases reported in the city of Santiago de Cali (Colombia) from 2015 to 2016.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Scikit-mobility: a Python library for the analysis, generation and risk assessment of mobility data","The last decade has witnessed the emergence of massive mobility data sets, such as tracks generated by GPS devices, call detail records, and geo-tagged posts from social media platforms. These data sets have fostered a vast scientific production on various applications of mobility analysis, ranging from computational epidemiology to urban planning and transportation engineering. A strand of literature addresses data cleaning issues related to raw spatiotemporal trajectories, while the second line of research focuses on discovering the statistical ""laws"" that govern human movements. A significant effort has also been put on designing algorithms to generate synthetic trajectories able to reproduce, realistically, the laws of human mobility. Last but not least, a line of research addresses the crucial problem of privacy, proposing techniques to perform the re-identification of individuals in a database. A view on state of the art cannot avoid noticing that there is no statistical software that can support scientists and practitioners with all the aspects mentioned above of mobility data analysis. In this paper, we propose scikit-mobility, a Python library that has the ambition of providing an environment to reproduce existing research, analyze mobility data, and simulate human mobility habits. scikit-mobility is efficient and easy to use as it extends pandas, a popular Python library for data analysis. Moreover, scikit-mobility provides the user with many functionalities, from visualizing trajectories to generating synthetic data, from analyzing statistical patterns to assessing the privacy risk related to the analysis of mobility data sets.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
OpenStreetCab: Exploiting Taxi Mobility Patterns in New York City to Reduce Commuter Costs,"The rise of Uber as the global alternative taxi operator has attracted a lot of interest recently. Aside from the media headlines which discuss the new phenomenon, e.g. on how it has disrupted the traditional transportation industry, policy makers, economists, citizens and scientists have engaged in a discussion that is centred around the means to integrate the new generation of the sharing economy services in urban ecosystems. In this work, we aim to shed new light on the discussion, by taking advantage of a publicly available longitudinal dataset that describes the mobility of yellow taxis in New York City. In addition to movement, this data contains information on the fares paid by the taxi customers for each trip. As a result we are given the opportunity to provide a first head to head comparison between the iconic yellow taxi and its modern competitor, Uber, in one of the world's largest metropolitan centres. We identify situations when Uber X, the cheapest version of the Uber taxi service, tends to be more expensive than yellow taxis for the same journey. We also demonstrate how Uber's economic model effectively takes advantage of well known patterns in human movement. Finally, we take our analysis a step further by proposing a new mobile application that compares taxi prices in the city to facilitate traveller's taxi choices, hoping to ultimately to lead to a reduction of commuter costs. Our study provides a case on how big datasets that become public can improve urban services for consumers by offering the opportunity for transparency in economic sectors that lack up to date regulations.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Most Western African migrants remain local and travel short distances,"Migration patterns are complex and context-dependent, with the distances migrants travel varying greatly depending on socio-economic and demographic factors. While global migration studies often focus on Western countries, there is a crucial gap in our understanding of migration dynamics within the African continent, particularly in West Africa. Using data from over 60,000 individuals from eight West African countries, this study examines the determinants of migration distance in the region. Our analysis reveals a bimodal distribution of migration distances: while most migrants travel locally within a hundred km, a smaller yet significant portion undertakes long-distance journeys, often exceeding 3,000 km. Socio-economic factors such as employment status, marital status and level of education play a decisive role in determining migration distances. Unemployed migrants, for instance, travel substantially farther (1,467 km on average) than their employed counterparts (295 km). Furthermore, we find that conflict-induced migration is particularly variable, with migrants fleeing violence often undertaking longer and riskier journeys. Our findings highlight the importance of considering both local and long-distance migration in policy decisions and support systems, as well as the need for a comprehensive understanding of migration in non-Western contexts. This study contributes to the broader discourse on human mobility by providing new insights into migration patterns in Western Africa, which in turn has implications for global migration research and policy development.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Changes in air quality and human mobility in the U.S. during the COVID-19 pandemic,"The first goal of this study is to quantify the magnitude and spatial variability of air quality changes in the US during the COVID-19 pandemic. We focus on two federally regulated pollutants, nitrogen dioxide (NO2), and fine particulate matter (PM2.5). Observed concentrations at all available ground monitoring sites (240 and 480 for NO2 and PM2.5, respectively) were compared between April 2020 and April of the prior five years, 2015-2019, as the baseline. Large statistically significant decreases in NO2 concentrations were found at more than 65% of the monitoring sites, with an average drop of 2 ppb when compared to the mean of the previous five years. The same patterns are confirmed by satellite-derived NO2 column totals from NASA OMI. PM2.5 concentrations from the ground monitoring sites, however, were more likely to be higher. The second goal of this study is to explain the different responses of the two pollutants during the COVID-19 pandemic. The hypothesis put forward is that the shelter-in-place measures affected peoples' driving patterns most dramatically, thus passenger vehicle NO2 emissions were reduced. Commercial vehicles and electricity demand for all purposes remained relatively unchanged, thus PM2.5 concentrations did not drop significantly. To establish a correlation between the observed NO2 changes and the extent to which people were sheltering in place, we use a mobility index, which was produced and made public by Descartes Labs. This mobility index aggregates cell phone usage at the county level to capture changes in human movement over time. We found a strong correlation between the observed decreases in NO2 concentrations and decreases in human mobility. By contrast, no discernible pattern was detected between mobility and PM2.5 concentrations changes, suggesting that decreases in personal-vehicle traffic alone may not be effective at reducing PM2.5 pollution.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Spatiotemporal Detection of Unusual Human Population Behavior Using Mobile Phone Data,"With the aim to contribute to humanitarian response to disasters and violent events, scientists have proposed the development of analytical tools that could identify emergency events in real-time, using mobile phone data. The assumption is that dramatic and discrete changes in behavior, measured with mobile phone data, will indicate extreme events. In this study, we propose an efficient system for spatiotemporal detection of behavioral anomalies from mobile phone data and compare sites with behavioral anomalies to an extensive database of emergency and non-emergency events in Rwanda. Our methodology successfully captures anomalous behavioral patterns associated with a broad range of events, from religious and official holidays to earthquakes, floods, violence against civilians and protests. Our results suggest that human behavioral responses to extreme events are complex and multi-dimensional, including extreme increases and decreases in both calling and movement behaviors. We also find significant temporal and spatial variance in responses to extreme events. Our behavioral anomaly detection system and extensive discussion of results are a significant contribution to the long-term project of creating an effective real-time event detection system with mobile phone data and we discuss the implications of our findings for future research to this end.   KEYWORDS: Big data, call detail record, emergency events, human mobility","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The relationship between human mobility and viral transmissibility during the COVID-19 epidemics in Italy,"In 2020, countries affected by the COVID-19 pandemic implemented various non-pharmaceutical interventions to contrast the spread of the virus and its impact on their healthcare systems and economies. Using Italian data at different geographic scales, we investigate the relationship between human mobility, which subsumes many facets of the population's response to the changing situation, and the spread of COVID-19. Leveraging mobile phone data from February through September 2020, we find a striking relationship between the decrease in mobility flows and the net reproduction number. We find that the time needed to switch off mobility and bring the net reproduction number below the critical threshold of 1 is about one week. Moreover, we observe a strong relationship between the number of days spent above such threshold before the lockdown-induced drop in mobility flows and the total number of infections per 100k inhabitants. Estimating the statistical effect of mobility flows on the net reproduction number over time, we document a 2-week lag positive association, strong in March and April, and weaker but still significant in June. Our study demonstrates the value of big mobility data to monitor the epidemic and inform control interventions during its unfolding.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Impact of initial outbreak locations on transmission risk of infectious diseases in an intra-urban area,"Infectious diseases usually originate from a specific location within a city. Due to the heterogenous distribution of population and public facilities, and the structural heterogeneity of human mobility network embedded in space, infectious diseases break out at different locations would cause different transmission risk and control difficulty. This study aims to investigate the impact of initial outbreak locations on the risk of spatiotemporal transmission and reveal the driving force behind high-risk outbreak locations. First, integrating mobile phone location data, we built a SLIR (susceptible-latent-infectious-removed)-based meta-population model to simulate the spreading process of an infectious disease (i.e., COVID-19) across fine-grained intra-urban regions (i.e., 649 communities of Shenzhen City, China). Based on the simulation model, we evaluated the transmission risk caused by different initial outbreak locations by proposing three indexes including the number of infected cases (CaseNum), the number of affected regions (RegionNum), and the spatial diffusion range (SpatialRange). Finally, we investigated the contribution of different influential factors to the transmission risk via machine learning models. Results indicates that different initial outbreak locations would cause similar CaseNum but different RegionNum and SpatialRange. To avoid the epidemic spread quickly to more regions, it is necessary to prevent epidemic breaking out in locations with high population-mobility flow density. While to avoid epidemic spread to larger spatial range, remote regions with long daily trip distance of residents need attention. Those findings can help understand the transmission risk and driving force of initial outbreak locations within cities and make precise prevention and control strategies in advance.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Statistical Analysis of the Properties of Geometric Network with Node Mobility,"The movement changes the underlying spatial representation of the participated mobile objects or nodes. In real world scenario, such mobile nodes can be part of any biological network, transportation network, social network, human interaction, etc. The change in the geometry leads to the change in various desirable properties of real-world networks especially in human interaction networks. In real life, human movement is concerned for better lifestyle where they form their new connections due to the geographical changes. Therefore, in this paper, we design a model for geometric networks with mobile nodes (GNMN) and conduct a comprehensive statistical analysis of their properties. We analyze the effect of node mobility by evaluating key network metrics such as connectivity, node degree distribution, second hop neighbors, and centrality measures. Through extensive simulations, we observe significant variations in the behavior of geometric networks with mobile nodes.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
New parameter-free mobility model: Opportunity priority selection model,"Predicting human mobility patterns has many practical applications in urban planning, traffic engineering, infectious disease epidemiology, emergency management and location-based services. Developing a universal model capable of accurately predicting the mobility fluxes between locations is a fundamental and challenging problem in regional economics and transportation science. Here, we propose a new parameter-free model named opportunity priority selection model as an alternative in human mobility prediction. The basic assumption of the model is that an individual will select destination locations that present higher opportunity benefits than the location opportunities of the origin and the intervening opportunities between the origin and destination. We use real mobility data collected from a number of cities and countries to demonstrate the predictive ability of this simple model. The results show that the new model offers universal predictions of intracity and intercity mobility patterns that are consistent with real observations, thus suggesting that the proposed model better captures the mechanism underlying human mobility than previous models.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility Trends during the COVID-19 Pandemic in the United States,"In March of this year, COVID-19 was declared a pandemic and it continues to threaten public health. This global health crisis imposes limitations on daily movements, which have deteriorated every sector in our society. Understanding public reactions to the virus and the non-pharmaceutical interventions should be of great help to fight COVID-19 in a strategic way. We aim to provide tangible evidence of the human mobility trends by comparing the day-by-day variations across the U.S. Large-scale public mobility at an aggregated level is observed by leveraging mobile device location data and the measures related to social distancing. Our study captures spatial and temporal heterogeneity as well as the sociodemographic variations regarding the pandemic propagation and the non-pharmaceutical interventions. All mobility metrics adapted capture decreased public movements after the national emergency declaration. The population staying home has increased in all states and becomes more stable after the stay-at-home order with a smaller range of fluctuation. There exists overall mobility heterogeneity between the income or population density groups. The public had been taking active responses, voluntarily staying home more, to the in-state confirmed cases while the stay-at-home orders stabilize the variations. The study suggests that the public mobility trends conform with the government message urging to stay home. We anticipate our data-driven analysis offers integrated perspectives and serves as evidence to raise public awareness and, consequently, reinforce the importance of social distancing while assisting policymakers.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Identifying macroscopic features in foreign visitor travel pathways,"Human travel patterns are commonly studied as networks in which the points of departure and destination are encoded as nodes and the travel frequency between two points is recorded as a weighted edge. However, because travelers often visit multiple destinations, which constitute pathways, an analysis incorporating pathway statistics is expected to be more informative over an approach based solely on pairwise frequencies. Hence, in this study, we apply a higher-order network representation framework to identify characteristic travel patterns from foreign visitor pathways in Japan. We expect that the results herein are mainly useful for marketing research in the tourism industry.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Intertemporal Community Detection in Human Mobility Networks,"We introduce a community detection method that finds clusters in network time-series by introducing an algorithm that finds significantly interconnected nodes across time. These connections are either increasing, decreasing, or constant over time. Significance of nodal connectivity within a set is judged using the Weighted Configuration Null Model at each time-point, then a novel significance-testing scheme is used to assess connectivity at all time points and the direction of its time-trend. We apply this method to bikeshare networks in New York City and Chicago and taxicab pickups and dropoffs in New York to find and illustrate patterns in human mobility in urban zones. Results show stark geographical patterns in clusters that are growing and declining in relative usage across time and potentially elucidate latent economic or demographic trends.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Diagnosing the performance of human mobility models at small spatial scales using volunteered geographic information,"Accurate modelling of local population movement patterns is a core contemporary concern for urban policymakers, affecting both the short term deployment of public transport resources and the longer term planning of transport infrastructure. Yet, while macro-level population movement models (such as the gravity and radiation models) are well developed, micro-level alternatives are in much shorter supply, with most macro-models known to perform badly in smaller geographic confines. In this paper we take a first step to remedying this deficit, by leveraging two novel datasets to analyse where and why macro-level models of human mobility break down at small scales. In particular, we use an anonymised aggregate dataset from a major mobility app and combine this with freely available data from OpenStreetMap concerning land-use composition of different areas around the county of Oxfordshire in the United Kingdom. We show where different models fail, and make the case for a new modelling strategy which moves beyond rough heuristics such as distance and population size towards a detailed, granular understanding of the opportunities presented in different areas of the city.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multilayer networks characterize human-mobility patterns by industry sector for the 2021 Texas winter storm,"Understanding human mobility during disastrous events is crucial for emergency planning and disaster management. Here, we develop a methodology involving the construction of time-varying, multilayer networks in which edges encode observed movements between spatial regions (census tracts) and network layers encode different movement categories according to industry sectors (e.g., visitations to schools, hospitals, and grocery stores). This approach provides a rich characterization of human mobility, thereby complementing studies examining the risk-aversion activities of evacuation and sheltering in place. Focusing on the 2021 Texas winter storm as a case study which led to many casualties, we find that people largely reduced their movements to ambulatory healthcare services, restaurants, and schools, but prioritized movements to grocery stores and gas stations. Additionally, we study the predictability of nodes' in- and out-degrees in the multilayer networks, which encode movements into and out of census tracts. We find that inward movements are harder to predict than outward movements, and even more so during this winter storm. Our findings about the reduction, prioritization, and predictability of sector-specific human movements could inform mobility-related decisions arising from future extreme weather events.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying the Economic Impact of COVID-19 in Mainland China Using Human Mobility Data,"To contain the pandemic of coronavirus (COVID-19) in Mainland China, the authorities have put in place a series of measures, including quarantines, social distancing, and travel restrictions. While these strategies have effectively dealt with the critical situations of outbreaks, the combination of the pandemic and mobility controls has slowed China's economic growth, resulting in the first quarterly decline of Gross Domestic Product (GDP) since GDP began to be calculated, in 1992. To characterize the potential shrinkage of the domestic economy, from the perspective of mobility, we propose two new economic indicators: the New Venues Created (NVC) and the Volumes of Visits to Venue (V^3), as the complementary measures to domestic investments and consumption activities, using the data of Baidu Maps. The historical records of these two indicators demonstrated strong correlations with the past figures of Chinese GDP, while the status quo has dramatically changed this year, due to the pandemic. We hereby presented a quantitative analysis to project the impact of the pandemic on economies, using the recent trends of NVC and V^3. We found that the most affected sectors would be travel-dependent businesses, such as hotels, educational institutes, and public transportation, while the sectors that are mandatory to human life, such as workplaces, residential areas, restaurants, and shopping sites, have been recovering rapidly. Analysis at the provincial level showed that the self-sufficient and self-sustainable economic regions, with internal supplies, production, and consumption, have recovered faster than those regions relying on global supply chains.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Patterns, entropy, and predictability of human mobility and life","Cellular phones are now offering an ubiquitous means for scientists to observe life: how people act, move and respond to external influences. They can be utilized as measurement devices of individual persons and for groups of people of the social context and the related interactions. The picture of human life that emerges shows complexity, which is manifested in such data in properties of the spatiotemporal tracks of individuals. We extract from smartphone-based data for a set of persons important locations such as ""home"", ""work"" and so forth over fixed length time-slots covering the days in the data-set. This set of typical places is heavy-tailed, a power-law distribution with an exponent close to -1.7. To analyze the regularities and stochastic features present, the days are classified for each person into regular, personal patterns. To this are superimposed fluctuations for each day. This randomness is measured by ""life"" entropy, computed both before and after finding the clustering so as to subtract the contribution of a number of patterns. The main issue, that we then address, is how predictable individuals are in their mobility. The patterns and entropy are reflected in the predictability of the mobility of the life both individually and on average. We explore the simple approaches to guess the location from the typical behavior, and of exploiting the transition probabilities with time from location or activity A to B. The patterns allow an enhanced predictability, at least up to a few hours into the future from the current location. Such fixed habits are most clearly visible in the working-day length.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exploring the relationship between the spatial distribution of roads and universal pattern of travel-route efficiency in urban road networks,"Urban road networks are well known to have universal characteristics and scale-invariant patterns, despite the different geographical and historical environments of cities. Previous studies on universal characteristics of the urban road networks mostly have paid attention to their network properties but often ignored the spatial networked structures. To fill the research gap, we explore the underlying spatial patterns of road networks. In doing so, we inspect the travel-route efficiency in a given road network across 70 global cities which provides information on the usage pattern and functionality of the road structure. The efficiency is quantified by the detour patterns of the travel routes, estimated by the detour index (DI). The DI is a long-standing popular measure, but its spatiality has been barely considered so far. In this study, we probe the behavior of DI with respect to spatial variables by scanning the network radially from a city center. Through empirical analysis, we first discover universal properties in DI throughout most cities, which are summarized as a constant behavior of DI regardless of the radial position from a city center and clear collapse into a single curve for DIs for various radii with respect to the angular distance. Especially, the latter enables us to know the scaling factor in the length scale. We also reveal that the core-periphery spatial structure of the roads induces the universal pattern, which is supported by an artificial road network model. Furthermore, we visualize the spatial DI pattern on the city map to figure out the city-specific characteristics. The most and least efficient connections of several representative cities show the potential for practical implications in analyzing individual cities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human behavior-driven epidemic surveillance in urban landscapes,"We introduce a surveillance strategy specifically designed for urban areas to enhance preparedness and response to disease outbreaks by leveraging the unique characteristics of human behavior within urban contexts. By integrating data on individual residences and travel patterns, we construct a Mixing matrix that facilitates the identification of critical pathways that ease pathogen transmission across urban landscapes enabling targeted testing strategies. Our approach not only enhances public health systems' ability to provide early epidemiological alerts but also underscores the variability in strategy effectiveness based on urban layout. We prove the feasibility of our mobility-informed policies by mapping essential mobility flows to major transit stations, showing that few resources focused on specific stations yields a more effective surveillance than non-targeted approaches. This study emphasizes the critical role of integrating human behavioral patterns into epidemic management strategies to improve the preparedness and resilience of major cities against future outbreaks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urn Modeling of Random Graphs Across Granularity Scales: A Framework for Origin-Destination Human Mobility Networks,"We model human mobility as a combinatorial allocation process, treating trips as distinguishable balls assigned to location-bins and generating origin-destination (OD) networks. From this analogy, we construct a unified three-scale framework, enumerative, probabilistic, and continuum graphon ensembles, and prove a renormalization theorem showing that, in the large sparse regime, these representations converge to a universal mixed-Poisson law. The framework yields compact formulas for key mobility observables, including destination occupancy, vacancy of unvisited sites, coverage (a stopping-time extension of the coupon collector problem), and overflow beyond finite capacities. Simulations with gravity-like kernels, calibrated on empirical OD data, closely match the asymptotic predictions. By connecting exact combinatorial models with continuum analysis, the results offer a principled toolkit for synthetic network generation, congestion assessment, and the design of sustainable urban mobility policies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Remote sensing and GPS mobility reveal heat's impact on human activity across diverse climates,"Extreme heat is a growing threat to both individual livelihoods and broader economies, killing a growing number of people each year as temperatures rise in many parts of the world and limiting productivity. Many studies document the link between heat waves and mortality or morbidity, and others explore the economic consequences of them, but few are able to determine how populations respond to the shock of extreme heat in day-to-day activity. Toward this end, we investigate the link between human mobility and ambient temperature. Examining Indonesia, India and Mexico, we show that extreme heat reduces mobility by up to 10% in urban settings, with losses concentrated midday. We examine the shape of the relationship, finding that while heat reduces activity, very hot days and very long heat waves may induce more of it, indicating different adaptation. Effects are stronger in poorer areas. Twinning these models with climate projections, we show that without adaptation mobility may fall 1-2% per year on aggregate, with certain seasons and places seeing activity fall by as much as 10%. According to our estimates, small cities will face the highest relative losses and large cities will experience the greatest absolute impacts.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multilevel Modeling as a Methodology for the Simulation of Human Mobility,"Multilevel modeling is increasingly relevant in the context of modelling and simulation since it leads to several potential benefits, such as software reuse and integration, the split of semantically separated levels into sub-models, the possibility to employ different levels of detail, and the potential for parallel execution. The coupling that inevitably exists between the sub-models, however, implies the need for maintaining consistency between the various components, more so when different simulation paradigms are employed (e.g., sequential vs parallel, discrete vs continuous). In this paper we argue that multilevel modelling is well suited for the simulation of human mobility, since it naturally leads to the decomposition of the model into two layers, the ""micro"" and ""macro"" layer, where individual entities (micro) and long-range interactions (macro) are described. In this paper we investigate the challenges of multilevel modeling, and describe some preliminary results using prototype implementations of multilayer simulators in the context of epidemic diffusion and vehicle pollution.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Microscopic modeling of attention-based movement behaviors,"For transportation hubs, leveraging pedestrian flows for commercial activities presents an effective strategy for funding maintenance and infrastructure improvements. However, this introduces new challenges, as consumer behaviors can disrupt pedestrian flow and efficiency. To optimize both retail potential and pedestrian efficiency, careful strategic planning in store layout and facility dimensions was done by expert judgement due to the complexity in pedestrian dynamics in the retail areas of transportation hubs. This paper introduces an attention-based movement model to simulate these dynamics. By simulating retail potential of an area through the duration of visual attention it receives, and pedestrian efficiency via speed loss in pedestrian walking behaviors, the study further explores how design features can influence the retail potential and pedestrian efficiency in a bi-directional corridor inside a transportation hub. Project webpage: https://danruili.github.io/AttentionMove","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
An alternative approach to the limits of predictability in human mobility,"Next place prediction algorithms are invaluable tools, capable of increasing the efficiency of a wide variety of tasks, ranging from reducing the spreading of diseases to better resource management in areas such as urban planning. In this work we estimate upper and lower limits on the predictability of human mobility to help assess the performance of competing algorithms. We do this using GPS traces from 604 individuals participating in a multi year long experiment, The Copenhagen Networks study. Earlier works, focusing on the prediction of a participant's whereabouts in the next time bin, have found very high upper limits (>90%). We show that these upper limits are highly dependent on the choice of a spatiotemporal scales and mostly reflect stationarity, i.e. the fact that people tend to not move during small changes in time. This leads us to propose an alternative approach, which aims to predict the next location, rather than the location in the next bin. Our approach is independent of the temporal scale and introduces a natural length scale. By removing the effects of stationarity we show that the predictability of the next location is significantly lower (~71%) than the predictability of the location in the next bin.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Analysis of travel activity determinants using robust statistics,"This study investigates travel behavior determinants based on a multiday travel survey conducted in the region of Ghent, Belgium. Due to the limited data reliability of the data sample and the influence of outliers exerted on classical principal component analysis, robust principal component analysis (ROBPCA) is employed in order to reveal the explanatory variables responsible for most of the variability. Interpretation of the results is eased by utilizing ROSPCA. The application of ROSPCA reveals six distinct principal components where each is determined by a few variables. Among others, our results suggest a key role of variable categories such as journey purpose-related impedance and journey inherent constraints. Surprisingly, the variables associated with journey timing turn out to be less important. Finally, our findings reveal the critical role of outliers in travel behavior analysis. This suggests that a systematic understanding of how outliers contribute to observed mobility behavior patterns, as derived from travel surveys, is needed. In this regard, the proposed methods serve for processing raw data typically used in activity-based modelling.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Analysing Human Mobility Patterns of Hiking Activities through Complex Network Theory,"The exploitation of high volume of geolocalized data from social sport tracking applications of outdoor activities can be useful for natural resource planning and to understand the human mobility patterns during leisure activities. This geolocalized data represents the selection of hike activities according to subjective and objective factors such as personal goals, personal abilities, trail conditions or weather conditions. In our approach, human mobility patterns are analysed from trajectories which are generated by hikers. We propose the generation of the trail network identifying special points in the overlap of trajectories. Trail crossings and trailheads define our network and shape topological features. We analyse the trail network of Balearic Islands, as a case of study, using complex weighted network theory. The analysis is divided into the four seasons of the year to observe the impact of weather conditions on the network topology. The number of visited places does not decrease despite the large difference in the number of samples of the two seasons with larger and lower activity. It is in summer season where it is produced the most significant variation in the frequency and localization of activities from inland regions to coastal areas. Finally, we compare our model with other related studies where the network possesses a different purpose. One finding of our approach is the detection of regions with relevant importance where landscape interventions can be applied in function of the communities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying COVID-19 transmission risks based on human mobility data: A personalized PageRank approach for efficient contact-tracing,"Given its wide-ranging and long-lasting impacts, COVID-19, especially its spatial spreading dynamics has received much attention. Knowledge of such dynamics helps public health professionals and city managers devise and deploy efficient contact-tracing and treatment measures. However, most existing studies focus on aggregate mobility flows and have rarely exploited the widely available disaggregate-level human mobility data. In this paper, we propose a Personalized PageRank (PPR) method to estimate COVID-19 transmission risks based on a bipartite network of people and locations. The method incorporates both mobility patterns of individuals and their spatiotemporal interactions. To validate the applicability and relevance of the proposed method, we examine the interplay between the spread of COVID-19 cases and intra-city mobility patterns in a small synthetic network and a real-world mobility network from Hong Kong, China based on transit smart card data. We compare the recall (sensitivity), accuracy, and Spearmans correlation coefficient between the estimated transmission risks and number of actual cases based on various mass tracing and testing strategies, including PPR-based, PageRank (PR)-based, location-based, route-based, and base case (no strategy). The results show that the PPR-based method achieves the highest efficiency, accuracy, and Spearmans correlation coefficient with the actual case number. This demonstrates the value of PPR for transmission risk estimation and the importance of incorporating individual mobility patterns for efficient contact-tracing and testing.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding Mode Choice Behavior of People with Disabilities: A Case Study in Utah,"Despite the growing recognition of the importance of inclusive transportation policies nationwide, there is still a gap, as the existing transportation models often fail to capture the unique travel behavior of people with disabilities. This research study focuses on understanding the mode choice behavior of individuals with travel-limited disabilities and comparing the group with no such disability. The study identified key factors influencing mode preferences for both groups by utilizing Utah's household travel survey, simulation algorithm and Multinomial Logit model. Explanatory variables include household and socio-demographic attributes, personal, trip characteristics, and built environment variables. The analysis revealed intriguing trends, including a shift towards carpooling among disabled individuals. People with disabilities placed less emphasis on travel time saving. A lower value of travel time for people with disabilities is potentially due to factors like part-time work, reduced transit fare, and no or shared cost for carpooling. Despite a 50% fare reduction for the disabled group, transit accessibility remains a significant barrier in their choice of Transit mode. In downtown areas, people with no disability were found to choose transit compared to driving, whereas disabled people preferred carpooling. Travelers with no driving licenses and disabled people who use transit daily showed complex travel patterns among multiple modes. The study emphasizes the need for accessible and inclusive transportation options, such as improved public transit services, shorter first and last miles in transit, and better connectivity for non-motorized modes, to cater to the unique needs of disabled travelers. The findings of this study have significant policy implications such as an inclusive mode choice modeling framework for creating a more sustainable and inclusive transportation system.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Dissecting the Spatial Structure of Cities from Human Mobility Patterns to Define Functional Urban Boundaries,"Since the industrial revolution, accelerated urban growth has overflown administrative divisions, merged cities into large built extensions, and blurred the boundaries between urban and rural land-uses. These traits, present in most of contemporary metropolis, complicate the definition of cities, a crucial issue considering that objective and comparable metrics are the basic inputs needed for the planning and design of sustainable urban environments. In this context, city definitions that respond to administrative or political criteria usually overlook human dynamics, a key factor that could help to make cities comparable across the urban fabric of diverse social, cultural and economic realities. Using a technique based on the spectral analysis of complex networks, we rank places in 11 of the major Chilean urban regions from a high-resolution human mobility dataset: Official origin-destination (OD) surveys. We propose a method for further distinguishing urban and rural land-uses within these regions, by means of a network centrality measure from which we construct a spectre of geographic places. This spectre, constructed from the ranking of locations as measured by their approximate number of embedded human flows, allows us to probe several urban boundaries. From the analysis of the urban scaling exponent of trips in relation to the population across these city delineations, we identify two clearly distinct scaling regimes occurring in urban and rural areas. The comparison of our results with land cover derived from remote sensing suggests that, for the case of trips, the scaling exponent in urban areas is close to linear. We conclude with estimations for well-formed cities in the Chilean urban system, which according to our analysis could emerge from clusters composed by places that capture at least ~138 trips (over the expectation) of the underlying mobility network.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Travel distance, frequency of return and the spread of disease","In 2020 and 2021, the spread of COVID-19 was globally addressed by imposing restrictions on the distance of individual travel. Recent literature has uncovered a clear pattern in human mobility that underlies the complexity of urban mobility: $r \cdot f$, the product of distance traveled $r$ and frequency of return $f$ per user to a given location, is invariant across space. This paper asks whether the invariant $r\cdot f$ also serves as a driver for epidemic spread, so that the risk associated with human movement can be modeled by a unifying variable $r\cdot f$. We use two large-scale datasets of individual human mobility to show that there is in fact a simple relation between $r$ and $f$ and both speed and spatial dispersion of disease spread. This discovery could assist in modeling spread of disease and inform travel policies in future epidemics -- based not only on travel distance $r$ but also on frequency of return $f$.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Extending Dynamic Origin-Destination Estimation to Understand Traffic Patterns During COVID-19,"Estimating dynamic Origin-Destination (OD) traffic flow is crucial for understanding traffic patterns and the traffic network. While dynamic origin-destination estimation (DODE) has been studied for decades as a useful tool for estimating traffic flow, few existing models have considered its potential in evaluating the influence of policy on travel activity. This paper proposes a data-driven approach to estimate OD traffic flow using sensor data on highways and local roads. We extend prior DODE models to improve accuracy and realism in order to estimate how policies affect OD traffic flow in large urban networks. We applied our approach to a case study in Los Angeles County, where we developed a traffic network, estimated OD traffic flow between health districts during COVID-19, and analyzed the relationship between OD traffic flow and demographic characteristics such as income. Our findings demonstrate that the proposed approach provides valuable insights into traffic flow patterns and their underlying demographic factors for a large-scale traffic network. Specifically, our approach allows for evaluating the impact of policy changes on travel activity. The approach has practical applications for transportation planning and traffic management, enabling a better understanding of traffic flow patterns and the impact of policy changes on travel activity.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Risk mapping for COVID-19 outbreaks in Australia using mobility data,"COVID-19 is highly transmissible and containing outbreaks requires a rapid and effective response. Because infection may be spread by people who are pre-symptomatic or asymptomatic, substantial undetected transmission is likely to occur before clinical cases are diagnosed. Thus, when outbreaks occur there is a need to anticipate which populations and locations are at heightened risk of exposure. In this work, we evaluate the utility of aggregate human mobility data for estimating the geographic distribution of transmission risk. We present a simple procedure for producing spatial transmission risk assessments from near-real-time population mobility data. We validate our estimates against three well-documented COVID-19 outbreak scenarios in Australia. Two of these were well-defined transmission clusters and one was a community transmission scenario. Our results indicate that mobility data can be a good predictor of geographic patterns of exposure risk from transmission centres, particularly in scenarios involving workplaces or other environments associated with habitual travel patterns. For community transmission scenarios, our results demonstrate that mobility data adds the most value to risk predictions when case counts are low and spatially clustered. Our method could assist health systems in the allocation of testing resources, and potentially guide the implementation of geographically-targeted restrictions on movement and social interaction.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mobilkit: A Python Toolkit for Urban Resilience and Disaster Risk Management Analytics using High Frequency Human Mobility Data,"Increasingly available high-frequency location datasets derived from smartphones provide unprecedented insight into trajectories of human mobility. These datasets can play a significant and growing role in informing preparedness and response to natural disasters. However, limited tools exist to enable rapid analytics using mobility data, and tend not to be tailored specifically for disaster risk management. We present an open-source, Python-based toolkit designed to conduct replicable and scalable post-disaster analytics using GPS location data. Privacy, system capabilities, and potential expansions of \textit{Mobilkit} are discussed.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Impact of mobility structure on the optimization of small-world networks of mobile agents,"In ad hoc wireless networking, units are connected to each other rather than to a central, fixed, infrastructure. Constructing and maintaining such networks create several trade-off problems between robustness, communication speed, power consumption, etc., that bridges engineering, computer science and the physics of complex systems. In this work, we address the role of mobility patterns of the agents on the optimal tuning of a small-world type network construction method. By this method, the network is updated periodically and held static between the updates. We investigate the optimal updating times for different scenarios of the movement of agents (modeling, for example, the fat-tailed trip distances, and periodicities, of human travel). We find that these mobility patterns affect the power consumption in non-trivial ways and discuss how these effects can best be handled.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Delineating Intra-Urban Spatial Connectivity Patterns by Travel-Activities: A Case Study of Beijing, China","Travel activities have been widely applied to quantify spatial interactions between places, regions and nations. In this paper, we model the spatial connectivities between 652 Traffic Analysis Zones (TAZs) in Beijing by a taxi OD dataset. First, we unveil the gravitational structure of intra-urban spatial connectivities of Beijing. On overall, the inter-TAZ interactions are well governed by the Gravity Model $G_{ij} = p_{i}p_{j}/d_{ij}$, where $p_{i}$, $p_{j}$ are degrees of TAZ $i$, $j$ and $d_{ij}$ the distance between them, with a goodness-of-fit around 0.8. Second, the network based analysis well reveals the polycentric form of Beijing. Last, we detect the semantics of inter-TAZ connectivities based on their spatiotemporal patterns. We further find that inter-TAZ connections deviating from the Gravity Model can be well explained by link semantics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A Machine Learning Approach to Modeling Human Migration,"Human migration is a type of human mobility, where a trip involves a person moving with the intention of changing their home location. Predicting human migration as accurately as possible is important in city planning applications, international trade, spread of infectious diseases, conservation planning, and public policy development. Traditional human mobility models, such as gravity models or the more recent radiation model, predict human mobility flows based on population and distance features only. These models have been validated on commuting flows, a different type of human mobility, and are mainly used in modeling scenarios where large amounts of prior ground truth mobility data are not available. One downside of these models is that they have a fixed form and are therefore not able to capture more complicated migration dynamics. We propose machine learning models that are able to incorporate any number of exogenous features, to predict origin/destination human migration flows. Our machine learning models outperform traditional human mobility models on a variety of evaluation metrics, both in the task of predicting migrations between US counties as well as international migrations. In general, predictive machine learning models of human migration will provide a flexible base with which to model human migration under different what-if conditions, such as potential sea level rise or population growth scenarios.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
User-based representation of time-resolved multimodal public transportation networks,"Multimodal transportation systems can be represented as time-resolved multilayer networks where different transportation modes connecting the same set of nodes are associated to distinct network layers. Their quantitative description became possible recently due to openly accessible datasets describing the geolocalised transportation dynamics of large urban areas. Advancements call for novel analytics, which combines earlier established methods and exploits the inherent complexity of the data. Here, our aim is to provide a novel user-based methodological framework to represent public transportation systems considering the total travel time, its variability across the schedule, and taking into account the number of transfers necessary. Using this framework we analyse public transportation systems in several French municipal areas. We incorporate travel routes and times over multiple transportation modes to identify efficient transportation connections and non-trivial connectivity patterns. The proposed method enables us to quantify the network's overall efficiency as compared to the specific demand and to the car alternative.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Effects of human dynamics on epidemic spreading in Cte d'Ivoire,"Understanding and predicting outbreaks of contagious diseases are crucial to the development of society and public health, especially for underdeveloped countries. However, challenging problems are encountered because of complex epidemic spreading dynamics influenced by spatial structure and human dynamics (including both human mobility and human interaction intensity). We propose a systematical model to depict nationwide epidemic spreading in Cte d'Ivoire, which integrates multiple factors, such as human mobility, human interaction intensity, and demographic features. We provide insights to aid in modeling and predicting the epidemic spreading process by data-driven simulation and theoretical analysis, which is otherwise beyond the scope of local evaluation and geometrical views. We show that the requirement that the average local basic reproductive number to be greater than unity is not necessary for outbreaks of epidemics. The observed spreading phenomenon can be roughly explained as a heterogeneous diffusion-reaction process by redefining mobility distance according to the human mobility volume between nodes, which is beyond the geometrical viewpoint. However, the heterogeneity of human dynamics still poses challenges to precise prediction.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mobility signatures: a tool for characterizing cities using intercity mobility flows,"Understanding the patterns of human mobility between cities has various applications from transport engineering to spatial modeling of the spreading of contagious diseases. We adopt a city-centric, data-driven perspective to quantify such patterns and introduce the mobility signature as a tool for understanding how a city (or a region) is embedded in the wider mobility network. We demonstrate the potential of the mobility signature approach through two applications that build on mobile-phone-based data from Finland. First, we use mobility signatures to show that the well-known radiation model is more accurate for mobility flows associated with larger cities, while the traditional gravity model appears a better fit for less populated areas. Second, we illustrate how the SARS-CoV-2 pandemic disrupted the mobility patterns in Finland in the spring of 2020. These two cases demonstrate the ability of the mobility signatures to quickly capture features of mobility flows that are harder to extract using more traditional methods.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Recurrent host mobility in spatial epidemics: beyond reaction-diffusion,"Human mobility is a key factor in spatial disease dynamics and related phenomena. In computational models host mobility is typically modelled by diffusion in space or on metapolulation networks. Alternatively, an effective force of infection across distance has been introduced to capture spatial dispersal implicitly. Both approaches do not account for important aspects of natural human mobility, diffusion does not capture the high degree of predictability in natural human mobility patters, e.g. the high percentage of return movements to individuals' base location, the effective force of infection approach assumes immediate equilibrium with respect to dispersal. These conditions are typically not met in natural scenarios. We investigate an epidemiological model that explicitly captures natural individual mobility patterns. We systematically investigate generic dynamical features of the model on regular lattices as well as metapopulation networks and show that generally the model exhibits significant dynamical differences in comparison to ordinary diffusion and effective force of infection models. For instance, the natural human mobility model exhibits a saturation of wave front speeds and a novel type of invasion threshold that is a function of the return rate in mobility patterns. In the light of these new findings and with the availability of precise and pervasive data on human mobility our approach provides a framework for a more sophisticated modeling of spatial disease dynamics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Travel Cadence and Epidemic Spread,"In this paper, we study how interactions between populations impact epidemic spread. We extend the classical SEIR model to include both integration-based disease transmission simulation and population flow. Our model differs from existing ones by having a more detailed representation of travel patterns, without losing tractability. This allows us to study the epidemic consequence of inter-regional travel with high fidelity. In particular, we define \emph{travel cadence} as a two-dimensional measure of inter-regional travel, and show that both dimensions modulate epidemic spread. This technical insight leads to policy recommendations, pointing to a family of simple policy trajectories that can effectively curb epidemic spread while maintaining a basic level of mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Active and reactive behaviour in human mobility: the influence of attraction points on pedestrians,"Human mobility is becoming an accessible field of study thanks to the progress and availability of tracking technologies as a common feature of smart phones. We describe an example of a scalable experiment exploiting these circumstances at a public, outdoor fair in Barcelona (Spain). Participants were tracked while wandering through an open space with activity stands attracting their attention. We develop a general modeling framework based on Langevin Dynamics, which allows us to test the influence of two distinct types of ingredients on mobility: reactive or context-dependent factors, modelled by means of a force field generated by attraction points in a given spatial configuration, and active or inherent factors, modelled from intrinsic movement patterns of the subjects. The additive and constructive framework model accounts for the observed features. Starting with the simplest model (purely random walkers) as a reference, we progressively introduce different ingredients such as persistence, memory, and perceptual landscape, aiming to untangle active and reactive contributions and quantify their respective relevance. The proposed approach may help in anticipating the spatial distribution of citizens in alternative scenarios and in improving the design of public events based on a facts-based approach.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility and time spent at destination: Impact on spatial epidemic spreading,"Host mobility plays a fundamental role in the spatial spread of infectious diseases. Previous theoretical works based on the integration of network theory into the metapopulation framework have shown that the heterogeneities that characterize real mobility networks favor the propagation of epidemics. Nevertheless, the studies conducted so far assumed the mobility process to be either Markovian or non-Markovian with a fixed traveling time scale. Available statistics however show that the time spent by travelers at destination is characterized by wide fluctuations, ranging between a single day up to several months. Such varying length of stay crucially affects the chance and duration of mixing events among hosts and may therefore have a strong impact on the spread of an emerging disease. Here, we present an analytical and computational study of epidemic processes on a complex subpopulation network where travelers have memory of their origin and spend a heterogeneously distributed time interval at their destination. Through analytical calculations and numerical simulations we show that the heterogeneity of the length of stay alters the expression of the threshold between local outbreak and global invasion, and, moreover, it changes the epidemic behavior of the system in case of a global outbreak. Additionally, our theoretical framework allows us to study the effect of changes in the traveling behavior in response to the infection, by considering a scenario in which sick individuals do not leave their home location. Finally, we compare the results of our non-Markovian framework with those obtained with a classic Markovian approach and find relevant differences between the two, in the estimate of the epidemic invasion potential, as well as of the timing and the pattern of its spatial spread.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
How liveable are London's 15-minute neighbourhoods? Exploring liveability profiles and active travel patterns in London,"Healthy and liveable neighbourhoods have increasingly been recognised as essential components of sustainable urban development. Yet, ambiguity surrounding their definition and constituent elements presents challenges in understanding and evaluating neighbourhood profiles, highlighting the need for a more detailed and systematic assessment. This research develops a composite Liveability Index for Greater London based on metrics related to the proximity, density, and diversity of POIs, along with population density, and investigates how neighbourhood liveability relates to active travel behaviour. The Index is based on the principles of the 15-minute city paradigm (Moreno et al. 2021) and developed in line with OECD guidelines for composite indicators (European Union and Joint Research Centre 2008). The Index revealed distinct spatial patterns of neighbourhood liveability, with high liveability neighbourhoods predominantly clustered in Inner London. Decomposing the Index provided further insights into the strengths and weaknesses of each neighbourhood. Footfall modelling using ordinary least squares (OLS) and geographically weighted regression (GWR) indicates a generally positive relationship between liveability and footfall, with spatial variation in the strength of this association. This research offers a new perspective on conceptualising and measuring liveability, demonstrating its role as an urban attractor that fosters social interaction and active engagement.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Group Mobility: Detection, Tracking and Characterization","In the era of mobile computing, understanding human mobility patterns is crucial in order to better design protocols and applications. Many studies focus on different aspects of human mobility such as people's points of interests, routes, traffic, individual mobility patterns, among others. In this work, we propose to look at human mobility through a social perspective, i.e., analyze the impact of social groups in mobility patterns. We use the MIT Reality Mining proximity trace to detect, track and investigate group's evolution throughout time. Our results show that group meetings happen in a periodical fashion and present daily and weekly periodicity. We analyze how groups' dynamics change over day hours and find that group meetings lasting longer are those with less changes in members composition and with members having stronger social bonds with each other. Our findings can be used to propose meeting prediction algorithms, opportunistic routing and information diffusion protocols, taking advantage of those revealed properties.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A generalized vector-field framework for mobility,"Trip flow between areas is a fundamental metric for human mobility research. Given its identification with travel demand and its relevance for transportation and urban planning, many models have been developed for its estimation. These models focus on flow intensity, disregarding the information provided by the local mobility orientation. A field-theoretic approach can overcome this issue and handling both intensity and direction at once. Here we propose a general vector-field representation starting from individuals' trajectories valid for any type of mobility. By introducing four models of spatial exploration, we show how individuals' elections determine the mesoscopic properties of the mobility field. Distance optimization in long displacements and random-like local exploration are necessary to reproduce empirical field features observed in Chinese logistic data and in New York City Foursquare check-ins. Our framework is an essential tool to capture hidden symmetries in mesoscopic urban mobility, it establishes a benchmark to test the validity of mobility models and opens the doors to the use of field theory in a wide spectrum of applications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Ride-pooling potential under alternative spatial demand patterns,"Shared rides are often considered to be a promising travel alternative that could efficiently pool people together while offering a door-to-door service. Notwithstanding, even though demand distribution patterns are expected to greatly affect the potential for ride-pooling, their impact remains unknown. In this study we explore the shareability of various demand patterns. We devise a set of experiments tailored to identify the most promising demand patterns for introducing ride-pooling services by varying the number of centers, the dispersion of destinations around each of these centers and the trip length distribution. When matching trips into rides, we do not only ensure their mutual compatibility in time and space but also that shared rides are only composed by travellers who find the ride-pooling offer to be more attractive than the private ride-hailing alternative given the trade-offs between travel time, fare and discomfort. We measure the shareability potential using a series of metrics related to the extent to which passenger demand can be assigned to shared rides. Our findings indicate that introducing a ride-pooling service can reduce vehicle-hours by 18-59% under a fixed demand level and depending on the concentration of travel destinations around the center and the trip length distribution. System efficiency correlates positively with the former and negatively with the latter. A shift from a monocentric to a polycentric demand pattern is found to have a limited impact on the prospects of shared rides.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding the complexity of the Lvy-walk nature of human mobility with a multi-scale cost/benefit model,"Probability distributions of human displacements has been fit with exponentially truncated Lvy flights or fat tailed Pareto inverse power law probability distributions. Thus, people usually stay within a given location (for example, the city of residence), but with a non-vanishing frequency they visit nearby or far locations too. Herein, we show that an important empirical distribution of human displacements (range: from 1 to 1000 km) can be well fit by three consecutive Pareto distributions with simple integer exponents equal to 1, 2 and ($\gtrapprox$) 3. These three exponents correspond to three displacement range zones of about 1 km $\lesssim r \lesssim$ 10 km, 10 km $\lesssim r \lesssim$ 300 km and 300 km $\lesssim r \lesssim $ 1000 km, respectively. These three zones can be geographically and physically well determined as displacements within a city, visits to nearby cities that may occur within just one-day trips, and visit to far locations that may require multi-days trips. The incremental integer values of the three exponents can be easily explained with a three-scale mobility cost/benefit model for human displacements based on simple geometrical constrains. Essentially, people would divide the space into three major regions (close, medium and far distances) and would assume that the travel benefits are randomly/uniformly distributed mostly only within specific urban-like areas.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Integrating Travel Demand and Network Modelling: a Myth or Future of Transport Modelling,"In this paper, a novel transport planning model system (TPMS) is formulated which is built on the concepts of supernetworks, multi-modality, integrity and calibration. In the proposed formulation, activity travel pattern (ATP) choice facets including the choices of activity, activity sequence, mode, departure time, and parking location, are all unified into a time-dependent supernetwork. The proposed model accounts for the dynamicity of the network, including time-of-day and congestion effects. These help capturing the interdependencies among all different attributes of a full transport planning system. Moreover, the proposed TPMS explicitly formulates an operating capacitated public transport system. To allow visiting locations multiple times and to alleviate the complexity of the proposed supernetwork, a novel multi-visit vehicle routing problem is proposed which does not enumerate the node and link visits. In order to calibrate the model based on the major travel attributes of the travel survey data, a set of splitting ratios are introduced to distribute trips on the supernetwork. The model uses the splitting ratios to integrate the supernetwork and the traffic assignment model in a unified TPMS structure. At last, numerical examples are provided to demonstrate the advantages of the proposed approach.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban Exodus? Understanding Human Mobility in Britain During the COVID-19 Pandemic Using Facebook Data,"Existing empirical work has focused on assessing the effectiveness of non-pharmaceutical interventions on human mobility to contain the spread of COVID-19. Less is known about the ways in which the COVID-19 pandemic has reshaped the spatial patterns of population movement within countries. Anecdotal evidence of an urban exodus from large cities to rural areas emerged during early phases of the pandemic across western societies. Yet, these claims have not been empirically assessed. Traditional data sources, such as censuses offer coarse temporal frequency to analyse population movement over short-time intervals. Drawing on a data set of 21 million observations from Facebook users, we aim to analyse the extent and evolution of changes in the spatial patterns of population movement across the rural-urban continuum in Britain over an 18-month period from March, 2020 to August, 2021. Our findings show an overall and sustained decline in population movement during periods of high stringency measures, with the most densely populated areas reporting the largest reductions. During these periods, we also find evidence of higher-than-average mobility from highly dense population areas to low densely populated areas, lending some support to claims of large-scale population movements from large cities. Yet, we show that these trends were temporary. Overall mobility levels trended back to pre-coronavirus levels after the easing of non-pharmaceutical interventions. Following these interventions, we also found a reduction in movement to low density areas and a rise in mobility to high density agglomerations. Overall, these findings reveal that while COVID-19 generated shock waves leading to temporary changes in the patterns of population movement in Britain, the resulting vibrations have not significantly reshaped the prevalent structures in the national pattern of population movement.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban Mobility,"In this chapter, we discuss urban mobility from a complexity science perspective. First, we give an overview of the datasets that enable this approach, such as mobile phone records, location-based social network traces, or GPS trajectories from sensors installed on vehicles. We then review the empirical and theoretical understanding of the properties of human movements, including the distribution of travel distances and times, the entropy of trajectories, and the interplay between exploration and exploitation of locations. Next, we explain generative and predictive models of individual mobility, and their limitations due to intrinsic limits of predictability. Finally, we discuss urban transport from a systemic perspective, including system-wide challenges like ridesharing, multimodality, and sustainable transport.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Intermunicipal Travel Networks of Mexico (2020-2021),"We present a collection of networks that describe the travel patterns between municipalities in Mexico between 2020 and 2021. Using anonymized mobile device geo-location data we constructed directed, weighted networks representing the (normalized) volume of travels between municipalities. We analysed changes in global (graph total weight sum), local (centrality measures), and mesoscale (community structure) network features. We observe that changes in these features are associated with factors such as Covid-19 restrictions and population size. In general, events in early 2020 (when initial Covid-19 restrictions were implemented) induced more intense changes in network features, whereas later events had a less notable impact in network features. We believe these networks will be useful for researchers and decision makers in the areas of transportation, infrastructure planning, epidemic control and network science at large.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Optimal forwarding ratio on dynamical networks with heterogeneous mobility,"As the discovery of non-Poissonian statistics of human mobility trajectories, more attention has been paid to understanding the role of these patterns in different dynamics. In this study, we first introduce the heterogeneous mobility of mobile agents into dynamical networks, and then investigate the forwarding strategy on the heterogeneous dynamical networks. We find that the faster speed and the higher proportion of high-speed agents can enhance the network throughput and reduce the mean traveling time in the case of random forwarding. A hierarchical structure in the dependence of high-speed is observed: the network throughput remains unchanged in small and large high-speed value. It is interesting to find that the slightly preferential forwarding to high-speed agents can maximize the network capacity. Through theoretical analysis and numerical simulations, we show that the optimal forwarding ratio stems from local structural heterogeneity of low-speed agents.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The effect of travel restrictions on the spread of a highly contagious disease in Sweden,"Travel restrictions may reduce the spread of a contagious disease that threatens public health. In this study we investigate what effect different levels of travel restrictions may have on the speed and geographical spread of an outbreak of a disease similar to SARS. We use a stochastic simulation model of the Swedish population, calibrated with survey data of travel patterns between municipalities in Sweden collected over three years. We find that a ban on journeys longer than 50 km drastically reduces the speed and the geographical spread of outbreaks, even with when compliance is less than 100%. The result is found to be robust for different rates of inter-municipality transmission intensities. Travel restrictions may therefore be an effective way to mitigate the effect of a future outbreak.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Inferring High Quality Co-Travel Networks,"Social networks provide a new perspective for enterprises to better understand their customers and have attracted substantial attention in industry. However, inferring high quality customer social networks is a great challenge while there are no explicit customer relations in many traditional OLTP environments. In this paper, we study this issue in the field of passenger transport and introduce a new member to the family of social networks, which is named Co-Travel Networks, consisting of passengers connected by their co-travel behaviors. We propose a novel method to infer high quality co-travel networks of civil aviation passengers from their co-booking behaviors derived from the PNRs (Passenger Naming Records). In our method, to accurately evaluate the strength of ties, we present a measure of Co-Journey Times to count the co-travel times of complete journeys between passengers. We infer a high quality co-travel network based on a large encrypted PNR dataset and conduct a series of network analyses on it. The experimental results show the effectiveness of our inferring method, as well as some special characteristics of co-travel networks, such as the sparsity and high aggregation, compared with other kinds of social networks. It can be expected that such co-travel networks will greatly help the industry to better understand their passengers so as to improve their services. More importantly, we contribute a special kind of social networks with high strength of ties generated from very close and high cost travel behaviors, for further scientific researches on human travel behaviors, group travel patterns, high-end travel market evolution, etc., from the perspective of social networks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban Street Network Design and Transport-Related Greenhouse Gas Emissions around the World,"This study estimates the relationships between street network characteristics and transport-sector CO2 emissions across every urban area in the world and investigates whether they are the same across development levels and urban design paradigms. The prior literature has estimated relationships between street network design and transport emissions -- including greenhouse gases implicated in climate change -- primarily through case studies focusing on certain world regions or relatively small samples of cities, complicating generalizability and applicability for evidence-informed practice. Our worldwide study finds that straighter, more-connected, and less-overbuilt street networks are associated with lower transport emissions, all else equal. Importantly, these relationships vary across development levels and design paradigms -- yet most prior literature reports findings from urban areas that are outliers by global standards. Planners need a better empirical base for evidence-informed practice in under-studied regions, particularly the rapidly urbanizing Global South.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Adaptive Reinforcement Learning Model for Simulation of Urban Mobility during Crises,"The objective of this study is to propose and test an adaptive reinforcement learning model that can learn the patterns of human mobility in a normal context and simulate the mobility during perturbations caused by crises, such as flooding, wildfire, and hurricanes. Understanding and predicting human mobility patterns, such as destination and trajectory selection, can inform emerging congestion and road closures raised by disruptions in emergencies. Data related to human movement trajectories are scarce, especially in the context of emergencies, which places a limitation on applications of existing urban mobility models learned from empirical data. Models with the capability of learning the mobility patterns from data generated in normal situations and which can adapt to emergency situations are needed to inform emergency response and urban resilience assessments. To address this gap, this study creates and tests an adaptive reinforcement learning model that can predict the destinations of movements, estimate the trajectory for each origin and destination pair, and examine the impact of perturbations on humans' decisions related to destinations and movement trajectories. The application of the proposed model is shown in the context of Houston and the flooding scenario caused by Hurricane Harvey in August 2017. The results show that the model can achieve more than 76\% precision and recall. The results also show that the model could predict traffic patterns and congestion resulting from to urban flooding. The outcomes of the analysis demonstrate the capabilities of the model for analyzing urban mobility during crises, which can inform the public and decision-makers about the response strategies and resilience planning to reduce the impacts of crises on urban mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Entropic measures of individual mobility patterns,"Understanding human mobility from a microscopic point of view may represent a fundamental breakthrough for the development of a statistical physics for cognitive systems and it can shed light on the applicability of macroscopic statistical laws for social systems. Even if the complexity of individual behaviors prevents a true microscopic approach, the introduction of mesoscopic models allows the study of the dynamical properties for the non-stationary states of the considered system. We propose to compute various entropy measures of the individual mobility patterns obtained from GPS data that record the movements of private vehicles in the Florence district, in order to point out new features of human mobility related to the use of time and space and to define the dynamical properties of a stochastic model that could generate similar patterns. Moreover, we can relate the predictability properties of human mobility to the distribution of time passed between two successive trips. Our analysis suggests the existence of a hierarchical structure in the mobility patterns which divides the performed activities into three different categories, according to the time cost, with different information contents. We show that a Markov process defined by using the individual mobility network is not able to reproduce this hierarchy, which seems the consequence of different strategies in the activity choice. Our results could contribute to the development of governance policies for a sustainable mobility in modern cities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Supersampling and network reconstruction of urban mobility,"Understanding human mobility is of vital importance for urban planning, epidemiology, and many other fields that aim to draw policies from the activities of humans in space. Despite recent availability of large scale data sets related to human mobility such as GPS traces, mobile phone data, etc., it is still true that such data sets represent a subsample of the population of interest, and then might give an incomplete picture of the entire population in question. Notwithstanding the abundant usage of such inherently limited data sets, the impact of sampling biases on mobility patterns is unclear -- we do not have methods available to reliably infer mobility information from a limited data set. Here, we investigate the effects of sampling using a data set of millions of taxi movements in New York City. On the one hand, we show that mobility patterns are highly stable once an appropriate simple rescaling is applied to the data, implying negligible loss of information due to subsampling over long time scales. On the other hand, contrasting an appropriate null model on the weighted network of vehicle flows reveals distinctive features which need to be accounted for. Accordingly, we formulate a ""supersampling"" methodology which allows us to reliably extrapolate mobility data from a reduced sample and propose a number of network-based metrics to reliably assess its quality (and that of other human mobility models). Our approach provides a well founded way to exploit temporal patterns to save effort in recording mobility data, and opens the possibility to scale up data from limited records when information on the full system is needed.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Microsimulation Analysis for Network Traffic Assignment (MANTA) at Metropolitan-Scale for Agile Transportation Planning,"Abrupt changes in the environment, such as unforeseen events due to climate change, have triggered massive and precipitous changes in human mobility. The ability to quickly predict traffic patterns in different scenarios has become more urgent to support short-term operations and long-term transportation planning. This requires modeling entire metropolitan areas to recognize the upstream and downstream effects on the network. However, there is a well-known trade-off between increasing the level of detail of a model and decreasing computational performance. To achieve the level of detail required for traffic microsimulation, current implementations often compromise by simulating small spatial scales, and those that operate at larger scales often require access to expensive high-performance computing systems or have computation times on the order of days or weeks that discourage productive research and real-time planning. This paper addresses the performance shortcomings by introducing a new platform, MANTA (Microsimulation Analysis for Network Traffic Assignment), for traffic microsimulation at the metropolitan-scale. MANTA employs a highly parallelized GPU implementation that is capable of running metropolitan-scale simulations within a few minutes. The runtime to simulate all morning trips, using half-second timesteps, for the nine-county San Francisco Bay Area is just over four minutes, not including routing and initialization. This computational performance significantly improves the state of the art in large-scale traffic microsimulation. MANTA expands the capacity to analyze detailed travel patterns and travel choices of individuals for infrastructure planning.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mobile Homophily and Social Location Prediction,"The mobility behavior of human beings is predictable to a varying degree e.g. depending on the traits of their personality such as the trait extraversion - introversion: the mobility of introvert users may be more dominated by routines and habitual movement patterns, resulting in a more predictable mobility behavior on the basis of their own location history while, in contrast, extrovert users get about a lot and are explorative by nature, which may hamper the prediction of their mobility. However, socially more active and extrovert users meet more people and share information, experiences, believes, thoughts etc. with others. which in turn leads to a high interdependency between their mobility and social lives. Using a large LBSN dataset, his paper investigates the interdependency between human mobility and social proximity, the influence of social networks on enhancing location prediction of an individual and the transmission of social trends/influences within social networks.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Detecting Robust Patterns in the Spread of Epidemics: A Case Study of Influenza in the United States and France,"In this paper, the authors develop a method of detecting correlations between epidemic patterns in different regions that are due to human movement and introduce a null model in which the travel-induced correlations are cancelled. They apply this method to the well-documented cases of seasonal influenza outbreaks in the United States and France. In the United States (using data for 1972-2002), the authors observed strong short-range correlations between several states and their immediate neighbors, as well as robust long-range spreading patterns resulting from large domestic air-traffic flows. The stability of these results over time allowed the authors to draw conclusions about the possible impact of travel restrictions on epidemic spread. The authors also applied this method to the case of France (1984-2004) and found that on the regional scale, there was no transportation mode that clearly dominated disease spread. The simplicity and robustness of this method suggest that it could be a useful tool for detecting transmission channels in the spread of epidemics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Leveraging Mobility Flows from Location Technology Platforms to Test Crime Pattern Theory in Large Cities,"Crime has been previously explained by social characteristics of the residential population and, as stipulated by crime pattern theory, might also be linked to human movements of non-residential visitors. Yet a full empirical validation of the latter is lacking. The prime reason is that prior studies are limited to aggregated statistics of human visitors rather than mobility flows and, because of that, neglect the temporal dynamics of individual human movements. As a remedy, we provide the first work which studies the ability of granular human mobility in describing and predicting crime concentrations at an hourly scale. For this purpose, we propose the use of data from location technology platforms. This type of data allows us to trace individual transitions and, therefore, we succeed in distinguishing different mobility flows that (i) are incoming or outgoing from a neighborhood, (ii) remain within it, or (iii) refer to transitions where people only pass through the neighborhood. Our evaluation infers mobility flows by leveraging an anonymized dataset from Foursquare that includes almost 14.8 million consecutive check-ins in three major U.S. cities. According to our empirical results, mobility flows are significantly and positively linked to crime. These findings advance our theoretical understanding, as they provide confirmatory evidence for crime pattern theory. Furthermore, our novel use of digital location services data proves to be an effective tool for crime forecasting. It also offers unprecedented granularity when studying the connection between human mobility and crime.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Urban transport systems shape experiences of social segregation,"Mobility is a fundamental feature of human life, and through it our interactions with the world and people around us generate complex and consequential social phenomena. Social segregation, one such process, is increasingly acknowledged as a product of one's entire lived experience rather than mere residential location. Increasingly granular sources of data on human mobility have evidenced how segregation persists outside the home, in workplaces, cafes, and on the street. Yet there remains only a weak evidential link between the production of social segregation and urban policy. This study addresses this gap through an assessment of the role of the urban transportation systems in shaping social segregation. Using city-scale GPS mobility data and a novel probabilistic mobility framework, we establish social interactions at the scale of transportation infrastructure, by rail and bus service segment, individual roads, and city blocks. The outcomes show how social segregation is more than a single process in space, but varying by time of day, urban design and structure, and service design. These findings reconceptualize segregation as a product of likely encounters during one's daily mobility practice. We then extend these findings through exploratory simulations, highlighting how transportation policy to promote sustainable transport may have potentially unforeseen impacts on segregation. The study underscores that to understand social segregation and achieve positive social change urban policymakers must consider the broadest impacts of their interventions and seek to understand their impact on the daily lived experience of their citizens.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
COVID-19 and Social Distancing: Disparities in Mobility Adaptation between Income Groups,"In response to the coronavirus disease 2019 (COVID-19) pandemic, governments have encouraged and ordered citizens to practice social distancing, particularly by working and studying at home. Intuitively, only a subset of people have the ability to practice remote work. However, there has been little research on the disparity of mobility adaptation across different income groups in US cities during the pandemic. The authors worked to fill this gap by quantifying the impacts of the pandemic on human mobility by income in Greater Houston, Texas. In this paper, we determined human mobility using pseudonymized, spatially disaggregated cell phone location data. A longitudinal study across estimated income groups was conducted by measuring the total travel distance, radius of gyration, number of visited locations, and per-trip distance in April 2020 compared to the data in a baseline. An apparent disparity in mobility was found across estimated income groups. In particular, there was a strong negative correlation ($$ = -0.90) between a traveler's estimated income and travel distance in April. Disparities in mobility adaptability were further shown since those in higher income brackets experienced larger percentage drops in the radius of gyration and the number of distinct visited locations than did those in lower income brackets. The findings of this study suggest a need to understand the reasons behind the mobility inflexibility among low-income populations during the pandemic. The study illuminates an equity issue which may be of interest to policy makers and researchers alike in the wake of an epidemic.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Epidemic modeling in metapopulation systems with heterogeneous coupling pattern: theory and simulations,"The spatial structure of populations is a key element in the understanding of the large scale spreading of epidemics. Motivated by the recent empirical evidence on the heterogeneous properties of transportation and commuting patterns among urban areas, we present a thorough analysis of the behavior of infectious diseases in metapopulation models characterized by heterogeneous connectivity and mobility patterns. We derive the basic reaction-diffusion equation describing the metapopulation system at the mechanistic level and derive an early stage dynamics approximation for the subpopulation invasion dynamics. The analytical description uses degree block variables that allows us to take into account arbitrary degree distribution of the metapopulation network. We show that along with the usual single population epidemic threshold the metapopulation network exhibits a global threshold for the subpopulation invasion. We find an explicit analytic expression for the invasion threshold that determines the minimum number of individuals traveling among subpopulations in order to have the infection of a macroscopic number of subpopulations. The invasion threshold is a function of factors such as the basic reproductive number, the infectious period and the mobility process and it is found to decrease for increasing network heterogeneity. We provide extensive mechanistic numerical Monte Carlo simulations that recover the analytical finding in a wide range of metapopulation network connectivity patterns. The results can be useful in the understanding of recent data driven computational approaches to disease spreading in large transportation networks and the effect of containment measures such as travel restrictions.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Has the Relationship between Urban and Suburban Automobile Travel Changed across Generations? Comparing Millennials and Generation Xers in the United States,"Using U.S. nationwide travel surveys for 1995, 2001, 2009 and 2017, this study compares Millennials with their previous generation (Gen Xers) in terms of their automobile travel across different neighborhood patterns. At the age of 16 to 28 years old, Millennials have lower daily personal vehicle miles traveled and car trips than Gen Xers in urban (higher-density) and suburban (lower-density) neighborhoods. Such differences remain unchanged after adjusting for the socio-economic, vehicle ownership, life cycle, year-specific and regional-specific factors. In addition, the associations between residential density and automobile travel for the 16- to 28-year-old Millennials are flatter than that for Gen Xers, controlling for the aforementioned covariates. These generational differences remain for the 24- to 36-year-old Millennials, during the period when the U.S. economy was recovering from the recession. These findings show that, in both urban and suburban neighborhoods, Millennials in the U.S. are less auto-centric than the previous generation during early life stages, regardless of economic conditions. Whether such difference persists over later life stages remains an open question and is worth continuous attention.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Estimating Probability Distributions of Travel Times by Fitting a Markovian Velocity Model,"To improve the routing decisions of individual drivers and the management policies designed by traffic operators, one needs reliable estimates of travel time distributions. Since congestion caused by both recurrent patterns (e.g., rush hours) and non-recurrent events (e.g., traffic incidents) leads to potentially substantial delays in highway travel times, we focus on a framework capable of incorporating both effects. To this end, we propose to work with the Markovian Velocity model (MVM), based on an environmental background process that tracks both random and (semi-)predictable events affecting the vehicle speeds in a highway network. We show how to operationalize this flexible data-driven model in order to obtain the travel time distribution for a vehicle departing at a known day and time to traverse a given path. Specifically, we detail how to structure the background process and set the speed levels corresponding to the different states of this process. First, for the inclusion of non-recurrent events, we study incident data to describe the random durations of the incident and inter-incident times. Both of them depend on the time of day, but we identify periods in which they can be considered time-independent. Second, for an estimation of the speed patterns in both incident and inter-incident regime, loop detector data for each of the identified periods is studied. In numerical examples that use road network detector data of the Dutch highway network, we obtain the travel time distribution estimates that arise under different traffic regimes, and illustrate the advantages compared to traditional travel-time prediction methods.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Correlation between social proximity and mobility similarity,"Human behaviors exhibit ubiquitous correlations in many aspects, such as individual and collective levels, temporal and spatial dimensions, content, social and geographical layers. With rich Internet data of online behaviors becoming available, it attracts academic interests to explore human mobility similarity from the perspective of social network proximity. Existent analysis shows a strong correlation between online social proximity and offline mobility similari- ty, namely, mobile records between friends are significantly more similar than between strangers, and those between friends with common neighbors are even more similar. We argue the importance of the number and diversity of com- mon friends, with a counter intuitive finding that the number of common friends has no positive impact on mobility similarity while the diversity plays a key role, disagreeing with previous studies. Our analysis provides a novel view for better understanding the coupling between human online and offline behaviors, and will help model and predict human behaviors based on social proximity.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Collective Prediction of Individual Mobility Traces with Exponential Weights,"We present and test a sequential learning algorithm for the short-term prediction of human mobility. This novel approach pairs the Exponential Weights forecaster with a very large ensemble of experts. The experts are individual sequence prediction algorithms constructed from the mobility traces of 10 million roaming mobile phone users in a European country. Average prediction accuracy is significantly higher than that of individual sequence prediction algorithms, namely constant order Markov models derived from the user's own data, that have been shown to achieve high accuracy in previous studies of human mobility prediction. The algorithm uses only time stamped location data, and accuracy depends on the completeness of the expert ensemble, which should contain redundant records of typical mobility patterns. The proposed algorithm is applicable to the prediction of any sufficiently large dataset of sequences.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
One rule does not fit all: deviations from universality in human mobility modeling,"The accurate modeling of individual movement in cities has significant implications for policy decisions across various sectors. Existing research emphasizes the universality of human mobility, positing that simple models can capture population-level movements. However, population-level accuracy does not guarantee consistent performance across all individuals. By overlooking individual differences, universality laws may accurately describe certain groups while less precisely representing others, resulting in aggregate accuracy from a balance of discrepancies. Using large-scale mobility data, we assess individual-level accuracy of a universal model, the Exploration and Preferential Return (EPR), by examining deviations from expected behavior in two scaling laws - one related to exploration and the other to return patterns. Our findings reveal that, while the model can describe population-wide movement patterns, it displays widespread deviations linked to individuals' behavioral traits, socioeconomic status, and lifestyles, contradicting model assumptions like non-bursty exploration and preferential return. Specifically, individuals poorly represented by the EPR model tend to visit routine locations in sequences, exploring rarely but in a bursty manner when they do. Among socioeconomic factors, income most strongly correlates with significant deviations. Consequently, spatial inhomogeneity emerges in model accuracy, with lower performance concentrated in urbanized, densely populated areas, underscoring policy implications. Our results show that emphasizing population-wide models can propagate socioeconomic inequalities by poorly representing vulnerable population sectors.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
How Political is the Spread of COVID-19 in the United States? An Analysis using Transportation and Weather Data,"We investigate the difference in the spread of COVID-19 between the states won by Donald Trump (Red) and the states won by Hillary Clinton (Blue) in the 2016 presidential election, by mining transportation patterns of US residents from March 2020 to July 2020. To ensure a fair comparison, we first use a K-means clustering method to group the 50 states into five clusters according to their population, area and population density. We then characterize daily transportation patterns of the residents of different states using the mean percentage of residents traveling and the number of trips per person. For each state, we study the correlations between travel patterns and infection rate for a 2-month period before and after the official states reopening dates. We observe that during the lock-down, Red and Blue states both displayed strong positive correlations between their travel patterns and infection rates. However, after states reopened we find that Red states had higher travel-infection correlations than Blue states in all five state clusters. We find that the residents of both Red and Blue states displayed similar travel patterns during the period post the reopening of states, leading us to conclude that, on average, the residents in Red states might be mobilizing less safely than the residents in Blue states. Furthermore, we use temperature data to attempt to explain the difference in the way residents travel and practice safety measures.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Impact of temporal scales and recurrent mobility patterns on the unfolding of epidemics,"Human mobility plays a key role on the transformation of local disease outbreaks into global pandemics. Thus, the inclusion of human movements into epidemic models has become mandatory for understanding current epidemic episodes and to design efficient prevention policies. Following this challenge, here we develop a Markovian framework which enables to address the impact of recurrent mobility patterns on the epidemic onset at different temporal scales. This formalism is validated by comparing their predictions with results from mechanistic simulations. The fair agreement between both theory and simulations enables to get an analytical expression for the epidemic threshold which captures the critical conditions triggering epidemic outbreaks. Finally, by performing an exhaustive analysis of this epidemic threshold, we reveal that the impact of tuning human mobility on the emergence of diseases is strongly affected by the temporal scales associated to both epidemiological and mobility processes.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The Eigenmode Analysis of Human Motion,"Rapid advances in modern communication technology are enabling the accumulation of large-scale, high-resolution observational data of spatiotemporal movements of humans. Classification and prediction of human mobility based on the analysis of such data carry great potential in applications such as urban planning as well as being of theoretical interest. A robust theoretical framework is therefore required to study and properly understand human motion. Here we perform the eigenmode analysis of human motion data gathered from mobile communication records, which allows us to explore the scaling properties and characteristics of human motion.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Identifying sinks and sources of human flows: A new approach to characterizing urban structures,"Human flow data are rich behavioral data relevant to people's decision-making regarding where to live, work, go shopping, etc., and provide vital information for identifying city centers. However, it is not as easy to understand massive relational data, and datasets have often been reduced merely to the statistics of trip counts at destinations, discarding relational information from origin to destination. In this study, we propose an alternative center identification method based on human mobility data. This method extracts the scalar potential field of human trips based on combinatorial Hodge theory. It detects not only statistically significant attractive locations as the sinks of human trips but also significant origins as the sources of trips. As a case study, we identify the sinks and sources of commuting and shopping trips in the Tokyo metropolitan area. This aim-specific analysis leads to a combinatorial classification of city centers based on the distinct aspects of human mobility. The proposed method can be applied to other mobility datasets with relevant properties and helps us examine the complex spatial structures in contemporary metropolitan areas from the multiple perspectives of human mobility.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Agent-based modeling for realistic reproduction of human mobility and contact behavior to evaluate test and isolation strategies in epidemic infectious disease spread,"Agent-based models have proven to be useful tools in supporting decision-making processes in different application domains. The advent of modern computers and supercomputers has enabled these bottom-up approaches to realistically model human mobility and contact behavior. The COVID-19 pandemic showcased the urgent need for detailed and informative models that can answer research questions on transmission dynamics. We present a sophisticated agent-based model to simulate the spread of respiratory diseases. The model is highly modularized and can be used on various scales, from a small collection of buildings up to cities or countries. Although not being the focus of this paper, the model has undergone performance engineering on a single core and provides an efficient intra- and inter-simulation parallelization for time-critical decision-making processes.   In order to allow answering research questions on individual level resolution, nonpharmaceutical intervention strategies such as face masks or venue closures can be implemented for particular locations or agents. In particular, we allow for sophisticated testing and isolation strategies to study the effects of minimal-invasive infectious disease mitigation. With realistic human mobility patterns for the region of Brunswick, Germany, we study the effects of different interventions between March 1st and May 30, 2021 in the SARS-CoV-2 pandemic. Our analyses suggest that symptom-independent testing has limited impact on the mitigation of disease dynamics if the dark figure in symptomatic cases is high. Furthermore, we found that quarantine length is more important than quarantine efficiency but that, with sufficient symptomatic control, also short quarantines can have a substantial effect.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Morphology of travel routes and the organization of cities,"The city is a complex system that evolves through its inherent social and economic interactions. Mediating the movements of people and resources, urban street networks offer a spatial footprint of these activities; consequently their structural characteristics have been of great interest in the literature. In comparison, relatively limited attention has been devoted to the interplay between street structure and its functional usage, i.e., the movement patterns of people and resources. To address this, we study the shape of 472,040 spatiotemporally optimized travel routes in the 92 most populated cities in the world. The routes are sampled in a geographically unbiased way such that their properties can be mapped on to each city, with their summary statistics capturing mesoscale connectivity patterns representing the complete space of possible movement in cities. The collective morphology of routes exhibits a directional bias that could be described as influenced by the attractive (or repulsive) forces resulting from congestion, accessibility and travel demand that relate to various socioeconomic factors. To capture this feature, we propose a simple metric, inness, that maps this force field. An analysis of the morphological patterns of individual cities reveals structural and socioeconomic commonalities among cities with similar inness patterns, in particular that they cluster into groups that are correlated with their size and putative stage of urban development as measured by a series of socioeconomic and infrastructural indicators. Our results lend weight to the insight that levels of urban socioeconomic development are intrinsically tied to increasing physical connectivity and diversity of road hierarchies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Metapopulation epidemic models with heterogeneous mixing and travel behaviour,"The complex interplay between population movements in space and non-homogeneous mixing patterns have so far hindered the fundamental understanding of the conditions for spatial invasion through a general theoretical framework. To address this issue, we present an analytical modelling approach taking into account such interplay under general conditions of mobility and interactions, in the simplifying assumption of two population classes. We describe a spatially structured population with non-homogeneous mixing and travel behaviour through a multi-host stochastic epidemic metapopulation model. Different population partitions, mixing patterns and mobility structures are considered, along with a specific application for the study of the role of age partition in the early spread of the 2009 H1N1 pandemic influenza.   We provide a complete mathematical formulation of the model and derive a semi-analytical expression of the threshold condition for global invasion of an emerging infectious disease in the metapopulation system. A rich solution space is found that depends on the social partition of the population, the pattern of contacts across groups and their relative social activity, the travel attitude of each class, and the topological and traffic features of the mobility network. Reducing the activity of the less social group and reducing the cross-group mixing are predicted to be the most efficient strategies for controlling the pandemic potential in the case the less active group constitutes the majority of travellers. If instead traveling is dominated by the more social class, our model predicts the existence of an optimal across-groups mixing that maximises the pandemic potential of the disease, whereas the impact of variations in the activity of each group is less important.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Private Sources of Mobility Data Under COVID-19,"The COVID-19 pandemic is changing the world in unprecedented and unpredictable ways. Human mobility is at the epicenter of that change, as the greatest facilitator for the spread of the virus. To study the change in mobility, to evaluate the efficiency of mobility restriction policies, and to facilitate a better response to possible future crisis, we need to properly understand all mobility data sources at our disposal. Our work is dedicated to the study of private mobility sources, gathered and released by large technological companies. This data is of special interest because, unlike most public sources, it is focused on people, not transportation means. i.e., its unit of measurement is the closest thing to a person in a western society: a phone. Furthermore, the sample of society they cover is large and representative. On the other hand, this sort of data is not directly accessible for anonymity reasons. Thus, properly interpreting its patterns demands caution. Aware of that, we set forth to explore the behavior and inter-relations of private sources of mobility data in the context of Spain. This country represents a good experimental setting because of its large and fast pandemic peak, and for its implementation of a sustained, generalized lockdown. We find private mobility sources to be both correlated and complementary. Using them, we evaluate the efficiency of implemented policies, and provide a insights into what new normal means in Spain.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Customer mobility and congestion in supermarkets,"The analysis and characterization of human mobility using population-level mobility models is important for numerous applications, ranging from the estimation of commuter flows in cities to modeling trade flows between countries. However, almost all of these applications have focused on large spatial scales, which typically range between intra-city scales to inter-country scales. In this paper, we investigate population-level human mobility models on a much smaller spatial scale by using them to estimate customer mobility flow between supermarket zones. We use anonymized, ordered customer-basket data to infer empirical mobility flow in supermarkets, and we apply variants of the gravity and intervening-opportunities models to fit this mobility flow and estimate the flow on unseen data. We find that a doubly-constrained gravity model and an extended radiation model (which is a type of intervening-opportunities model) can successfully estimate 65--70\% of the flow inside supermarkets. Using a gravity model as a case study, we then investigate how to reduce congestion in supermarkets using mobility models. We model each supermarket zone as a queue, and we use a gravity model to identify store layouts with low congestion, which we measure either by the maximum number of visits to a zone or by the total mean queue size. We then use a simulated-annealing algorithm to find store layouts with lower congestion than a supermarket's original layout. In these optimized store layouts, we find that popular zones are often in the perimeter of a store. Our research gives insight both into how customers move in supermarkets and into how retailers can arrange stores to reduce congestion. It also provides a case study of human mobility on small spatial scales.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Infectious disease dynamics in metapopulations with heterogeneous transmission and recurrent mobility,"Human mobility, contact patterns, and their interplay are key aspects of our social behavior that shape the spread of infectious diseases across different regions. In the light of new evidence and data sets about these two elements, epidemic models should be refined to incorporate both the heterogeneity of human contacts and the complexity of mobility patterns. Here, we propose a theoretical framework that allows accommodating these two aspects in the form of a set of Markovian equations. We validate these equations with extensive mechanistic simulations and derive analytically the epidemic threshold. The expression of this critical value allows us to evaluate its dependence on the specific demographic distribution, the structure of mobility flows, and the heterogeneity of contact patterns, thus shedding light on the microscopic mechanisms responsible for the epidemic detriment driven by recurrent mobility patterns reported in the literature.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Street-level Travel-time Estimation via Aggregated Uber Data,"Estimating temporal patterns in travel times along road segments in urban settings is of central importance to traffic engineers and city planners. In this work, we propose a methodology to leverage coarse-grained and aggregated travel time data to estimate the street-level travel times of a given metropolitan area. Our main focus is to estimate travel times along the arterial road segments where relevant data are often unavailable. The central idea of our approach is to leverage easy-to-obtain, aggregated data sets with broad spatial coverage, such as the data published by Uber Movement, as the fabric over which other expensive, fine-grained datasets, such as loop counter and probe data, can be overlaid. Our proposed methodology uses a graph representation of the road network and combines several techniques such as graph-based routing, trip sampling, graph sparsification, and least-squares optimization to estimate the street-level travel times. Using sampled trips and weighted shortest-path routing, we iteratively solve constrained least-squares problems to obtain the travel time estimates. We demonstrate our method on the Los Angeles metropolitan-area street network, where aggregated travel time data is available for trips between traffic analysis zones. Additionally, we present techniques to scale our approach via a novel graph pseudo-sparsification technique.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Evacuation patterns and socioeconomic stratification in the context of wildfires in Chile,"Climate change is altering the frequency and intensity of wildfires, leading to increased evacuation events that disrupt human mobility and socioeconomic structures. These disruptions affect access to resources, employment, and housing, amplifying existing vulnerabilities within communities. Understanding the interplay between climate change, wildfires, evacuation patterns, and socioeconomic factors is crucial for developing effective mitigation and adaptation strategies. To contribute to this challenge, we use high-definition mobile phone records to analyse evacuation patterns during the wildfires in Valparaso, Chile, that took place between February 2-3, 2024. This data allows us to track the movements of individuals in the disaster area, providing insight into how people respond to large-scale evacuations in the context of severe wildfires. We apply a causal inference approach that combines regression discontinuity and difference-in-differences methodologies to observe evacuation behaviours during wildfires, with a focus on socioeconomic stratification. This approach allows us to isolate the impact of the wildfires on different socioeconomic groups by comparing the evacuation patterns of affected populations before and after the event, while accounting for underlying trends and discontinuities at the threshold of the disaster. We find that many people spent nights away from home, with those in the lowest socioeconomic segment stayed away the longest. In general, people reduced their travel distance during the evacuation, and the lowest socioeconomic group moved the least. Initially, movements became more random, as people sought refuge in a rush, but eventually gravitated towards areas with similar socioeconomic status. Our results show that socioeconomic differences play a role in evacuation dynamics, providing useful insights for response planning.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Mobility-GCN: a human mobility-based graph convolutional network for tracking and analyzing the spatial dynamics of the synthetic opioid crisis in the USA, 2013-2020","Synthetic opioids are the most common drugs involved in drug-involved overdose mortalities in the U.S. The Center for Disease Control and Prevention reported that in 2018, about 70% of all drug overdose deaths involved opioids and 67% of all opioid-involved deaths were accounted for by synthetic opioids. In this study, we investigated the spread of synthetic opioids between 2013 and 2020 in the U.S. We analyzed the relationship between the spatiotemporal pattern of synthetic opioid-involved deaths and another key opioid, heroin, and compared patterns of deaths involving these two types of drugs during this period. Spatial connections and human mobility between counties were incorporated into a graph convolutional neural network model to represent and analyze the spread of synthetic opioid-involved deaths in the context of previous heroin-involved death patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility networks reveal increased segregation in large cities,"A long-standing expectation is that large, dense, and cosmopolitan areas support socioeconomic mixing and exposure between diverse individuals. It has been difficult to assess this hypothesis because past approaches to measuring socioeconomic mixing have relied on static residential housing data rather than real-life exposures between people at work, in places of leisure, and in home neighborhoods. Here we develop a new measure of exposure segregation (ES) that captures the socioeconomic diversity of everyday encounters. Leveraging cell phone mobility data to represent 1.6 billion exposures among 9.6 million people in the United States, we measure exposure segregation across 382 Metropolitan Statistical Areas (MSAs) and 2829 counties. We discover that exposure segregation is 67% higher in the 10 largest Metropolitan Statistical Areas (MSAs) than in small MSAs with fewer than 100,000 residents. This means that, contrary to expectation, residents of large cosmopolitan areas have significantly less exposure to diverse individuals. Second, we find evidence that large cities offer a greater choice of differentiated spaces targeted to specific socioeconomic groups, a dynamic that accounts for this increase in everyday socioeconomic segregation. Third, we discover that this segregation-increasing effect is countered when a city's hubs (e.g. shopping malls) are positioned to bridge diverse neighborhoods and thus attract people of all socioeconomic statuses. Overall, our findings challenge a long-standing conjecture in human geography and urban design, and highlight how built environment can both prevent and facilitate exposure between diverse individuals.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The dynamic pattern of human attention,"A mass of traces of human activities show diverse dynamic patterns. In this paper, we comprehensively investigate the dynamic pattern of human attention defined by the quantity of interests on subdisciplines in an online academic communication forum. Both the expansion and exploration of human attention have a power-law scaling relation with browsing actions, of which the exponent is close to that in one-dimension random walk. Furthermore, the memory effect of human attention is characterized by the power-law distributions of both the return interval time and return interval steps, which is reinforced by studying the attention shift that monotonically increase with the interval order between pairs of continuously segmental sequences of expansion. At last, the observing dynamic pattern of human attention in the browsing process is analytically described by a dynamic model whose generic mechanism is analogy to that of human spatial mobility. Thus, our work not only enlarges the research scope of human dynamics, but also provides an insight to understand the relationship between the interest transitivity in online activities and human spatial mobility in real world.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Uncovering the role of spatial constraints in the differences and similarities between physical and virtual mobility,"The recent availability of digital traces from Information and Communications Technologies (ICT) has facilitated the study of both individual- and population-level movement with unprecedented spatiotemporal resolution, enabling us to better understand a plethora of socioeconomic processes such as urbanization, transportation, impact on the environment and epidemic spreading to name a few. Using empirical spatiotemporal trends, several mobility models have been proposed to explain the observed regularities in human movement. With the advent of the World Wide Web, a new type of virtual mobility has emerged that has begun to supplant many traditional facets of human activity. Here we conduct a systematic analysis of physical and virtual movement, uncovering both similarities and differences in their statistical patterns. The differences manifest themselves primarily in the temporal regime, as a signature of the spatial and economic constraints inherent in physical movement, features that are predominantly absent in the virtual space. We demonstrate that once one moves to the time-independent space of events, i.e the sequences of visited locations, these differences vanish, and the statistical patterns of physical and virtual mobility are identical. The observed similarity in navigating these markedly different domains point towards a common mechanism governing the movement patterns, a feature we describe through a Metropolis-Hastings type optimization model, where individuals navigate locations through decision-making processes resembling a cost-benefit analysis of the utility of locations. In contrast to existing phenomenological models of mobility, we show that our model can reproduce the commonalities in the empirically observed statistics with minimal input.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Field theory for recurrent mobility,"Understanding human mobility is crucial for applications such as forecasting epidemic spreading, planning transport infrastructure and urbanism in general. While, traditionally, mobility information has been collected via surveys, the pervasive adoption of mobile technologies has brought a wealth of (real time) data. The easy access to this information opens the door to study theoretical questions so far unexplored. In this work, we show for a series of worldwide cities that commuting daily flows can be mapped into a well behaved vector field, fulfilling the divergence theorem and which is, besides, irrotational. This property allows us to define a potential for the field that can become a major instrument to determine separate mobility basins and discern contiguous urban areas. We also show that empirical fluxes and potentials can be well reproduced and analytically characterized using the so-called gravity model, while other models based on intervening opportunities have serious difficulties.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
From human mobility to renewable energies: Big data analysis to approach worldwide multiscale phenomena,"We address and discuss recent trends in the analysis of big data sets, with the emphasis on studying multiscale phenomena. Applications of big data analysis in different scientific fields are described and two particular examples of multiscale phenomena are explored in more detail. The first one deals with wind power production at the scale of single wind turbines, the scale of entire wind farms and also at the scale of a whole country. Using open source data we show that the wind power production has an intermittent character at all those three scales, with implications for defining adequate strategies for stable energy production. The second example concerns the dynamics underlying human mobility, which presents different features at different scales. For that end, we analyze $12$-month data of the Eduroam database within Portuguese universities, and find that, at the smallest scales, typically within a set of a few adjacent buildings, the characteristic exponents of average displacements are different from the ones found at the scale of one country or one continent.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Uncovering socioeconomic gaps in mobility reduction during the COVID-19 pandemic using location data,"Using smartphone location data from Colombia, Mexico, and Indonesia, we investigate how non-pharmaceutical policy interventions intended to mitigate the spread of the COVID-19 pandemic impact human mobility. In all three countries, we find that following the implementation of mobility restriction measures, human movement decreased substantially. Importantly, we also uncover large and persistent differences in mobility reduction between wealth groups: on average, users in the top decile of wealth reduced their mobility up to twice as much as users in the bottom decile. For decision-makers seeking to efficiently allocate resources to response efforts, these findings highlight that smartphone location data can be leveraged to tailor policies to the needs of specific socioeconomic groups, especially the most vulnerable.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Heterogeneous Interventions Reduce the Spread of COVID-19 in Simulations on Real Mobility Data,"Major interventions have been introduced worldwide to slow down the spread of the SARS-CoV-2 virus. Large-scale lockdowns of human movements are effective in reducing the spread, but they come at a cost of significantly limited societal functions. We show that natural human movements are statistically diverse, and the spread of the disease is significantly influenced by a small group of active individuals and gathering venues. We find that interventions focused on these most mobile individuals and popular venues reduce both the peak infection rate and the total infected population while retaining high social activity levels. These trends are seen consistently in simulations with real human mobility data of different scales, resolutions, and modalities from multiple cities across the world. The observation implies that compared to broad sweeping interventions, more heterogeneous strategies that are targeted based on the network effects in human mobility provide a better balance between pandemic control and regular social activities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Interplay between intra-urban population density and mobility in determining the spread of epidemics,"In this work, we address the connection between population density centers in urban areas, and the nature of human flows between such centers, in shaping the vulnerability to the onset of contagious diseases. A study of 163 cities, chosen from four different continents reveals a universal trend, whereby the risk induced by human mobility increases in those cities where mobility flows are predominantly between high population density centers. We apply our formalism to the spread of SARS-COV-2 in the United States, providing a plausible explanation for the observed heterogeneity in the spreading process across cities. Armed with this insight, we propose realistic mitigation strategies (less severe than lockdowns), based on modifying the mobility in cities. Our results suggest that an optimal control strategy involves an asymmetric policy that restricts flows entering the most vulnerable areas but allowing residents to continue their usual mobility patterns.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human movement decisions during Coronavirus Disease 2019,"Modelling host behavioral change in response to epidemics is important to describe disease dynamics and many previous studies proposed mathematical models describing it. Indeed, the epidemic of COVID-19 clearly demonstrated that people changed their activity in response to the epidemic, which subsequently modified the disease dynamics. To predict the behavioral change relevant to the disease dynamics, we need to know the epidemic situation (e.g., the number of reported cases) at the moment of decision to change behavior. However, it is difficult to identify the timing of decision-making. In this study, we analyzed travel accommodation reservation data in four prefectures of Japan to observe decision-making timings and how it responded to the changing epidemic situation during Japan's Coronavirus Disease 2019 (eight waves until February 2023). To this end, we defined 'mobility avoidance index' to indicate people's decision of mobility avoidance and quantified it using the time-series of the accommodation booking/cancellation data. Our analysis revealed semi-quantitative rules for day-to-day decision-making of human mobility under a given epidemic situation. We observed matches of the peak dates of the index and the number of reported cases. Additionally, we found that mobility avoidance index increased/decreased linearly with the logarithmic number of reported cases during the first epidemic wave. This pattern agrees with Weber-Fechner law in psychophysics. We also found that the slope of the mobility avoidance index against the change of the logarithmic number of reported cases were similar among the waves, while the intercept of that was much reduced as the first epidemic wave passed by. It suggests that the people's response became weakened after the first experience, as if the number of reported cases were multiplied by a constant small factor.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Revealing urban area from mobile positioning data,"Researchers face the trade-off between publishing mobility data along with their papers while simultaneously protecting the privacy of the individuals. In addition to the fundamental anonymization process, other techniques, such as spatial discretization and, in certain cases, location concealing or complete removal, are applied to achieve these dual objectives. The primary research question is whether concealing the observation area is an adequate form of protection or whether human mobility patterns in urban areas are inherently revealing of location. The characteristics of the mobility data, such as the number of activity records or the number of unique users in a given spatial unit, reveal the silhouette of the urban landscape, which can be used to infer the identity of the city in question. It was demonstrated that even without disclosing the exact location, the patterns of human mobility can still reveal the urban area from which the data was collected. The presented locating method was tested on other cities using different open data sets and against coarser spatial discretization units. While publishing mobility data is essential for research, it was demonstrated that concealing the observation area is insufficient to prevent the identification of the urban area. Furthermore, using larger discretization units alone is an ineffective solution to the problem of the observation area re-identification. Instead of obscuring the observation area, noise should be added to the trajectories to prevent user identification.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
An In-Depth Analysis of Ride-Hailing Travel Using a Large-scale Trip-Based Dataset,"With the rapid increase in ride-hailing (RH) use, a need to better understand and regulate the industry arises. This paper analyzes a year's worth of RH trip data from the Greater Chicago Area to study RH trip patterns. More than 104 million trips were analyzed. For trip rates, the results show that the total number of trips remained stable over the year, with pooled trips steadily decreasing from 20 to 9 percent. People tend to use RH more on weekends compared to weekdays. Specifically, weekend RH trip counts (per day) are, on average, 20 percent higher than weekday trip counts. The results of this work will help policy makers and transportation administrators better understand the nature of RH trips, which in turn allows for the design of a better regulation and guidance system for the ride-hailing industry.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mapping mobile service usage diversity in cities,"The ubiquitous use of mobile devices and associated Internet services generates vast volumes of geolocated data, offering valuable insights into human behaviors and their interactions with urban environments. Over the past decade, mobile phone data have proven indispensable in various fields such as demography, geography, transport planning, and epidemiology. They enable researchers to examine human mobility patterns on unprecedented scales and analyze the spatial structure and function of cities. The relationship between mobile phone data and land use has also been extensively explored, particularly in inferring land use patterns from spatiotemporal activity. However, many studies rely on Call Detail Records (CDR) or eXtended Detail Records (XDR), which may not capture specific mobile application usage. This study aims to address this gap by mapping mobile service usage diversity in 20 French cities and investigating its correlation with land use distribution. Utilizing a Shannon diversity index, the study evaluates mobile service usage diversity based on hourly traffic volume data from 17 mobile services. Furthermore, the study compares temporal diversity with land distribution both within and among cities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Delay propagation patterns in Japan's domestic air transport network,"We experience air traffic delays every day, but are there any recurrent patterns in these delays? In this study, we investigate the recurrence of delay propagation patterns in Japan's domestic air transport network in 2019 by integrating delay causality networks and temporal network analysis. Additionally, we examine characteristics unique to delay propagation by comparing delay causality networks with corresponding randomized networks generated by a directed configuration model. As a result, we found that the structure of the delay propagation patterns can be classified into several groups. The identified groups exhibit statistically significant differences in total delay time and average out-degree, with different airports playing central roles in spreading delays. The results also suggest that some delay propagation patterns are particularly prominent during specific times of the year, which could be influenced by Japan's seasonal and geographical factors. Moreover, we discovered that specific network motifs appear significantly more (or less) frequently in delay causality networks than their corresponding randomized counterparts. This characteristic is particularly pronounced in groups with more significant delays. These results suggest that delays propagate following specific directional patterns, which could significantly contribute to predicting air traffic delays. We expect the present study to trigger further research on recurrent and non-recurrent natures of air traffic delay propagation.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multi-scale Population and Mobility Estimation with Geo-tagged Tweets,"Recent outbreaks of Ebola and Dengue viruses have again elevated the significance of the capability to quickly predict disease spread in an emergent situation. However, existing approaches usually rely heavily on the time-consuming census processes, or the privacy-sensitive call logs, leading to their unresponsive nature when facing the abruptly changing dynamics in the event of an outbreak. In this paper we study the feasibility of using large-scale Twitter data as a proxy of human mobility to model and predict disease spread. We report that for Australia, Twitter users' distribution correlates well the census-based population distribution, and that the Twitter users' travel patterns appear to loosely follow the gravity law at multiple scales of geographic distances, i.e. national level, state level and metropolitan level. The radiation model is also evaluated on this dataset though it has shown inferior fitness as a result of Australia's sparse population and large landmass. The outcomes of the study form the cornerstones for future work towards a model-based, responsive prediction method from Twitter data for disease spread.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Platial mobility: expanding place and mobility in GIS via platio-temporal representations and the mobilities paradigm,"While platial representations are being developed for sedentary entities, a parallel and useful endeavour would be to consider time in so-called ""platio-temporal"" representations that would also expand notions of mobility in GIScience, that are solely dependent on Euclidean space and time. Besides enhancing such aspects of place and mobility via spatio-temporal, we also include human aspects of these representations via considerations of the sociological notions of mobility via the mobilities paradigm that can systematically introduce representation of both platial information along with mobilities associated with 'moving places.' We condense these aspects into 'platial mobility,' a novel conceptual framework, as an integration in GIScience and the mobilities paradigm in sociology, that denotes movement of places in our platio-temporal and sociology-based representations. As illustrative cases for further study using platial mobility as a framework, we explore its benefits and methodological aspects toward developing better understanding for disaster management, disaster risk reduction and pandemics. We then discuss some of the illustrative use cases to clarify the concept of platial mobility and its application prospects in the areas of disaster management, disaster risk reduction and pandemics. These use cases, which include flood events and the ongoing COVID-19 pandemic, have led to displaced and restricted communities having to change practices and places, which would be particularly amenable to the conceptual framework developed in our work.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multiscale mobility networks and the large scale spreading of infectious diseases,"Among the realistic ingredients to be considered in the computational modeling of infectious diseases, human mobility represents a crucial challenge both on the theoretical side and in view of the limited availability of empirical data. In order to study the interplay between small-scale commuting flows and long-range airline traffic in shaping the spatio-temporal pattern of a global epidemic we i) analyze mobility data from 29 countries around the world and find a gravity model able to provide a global description of commuting patterns up to 300 kms; ii) integrate in a worldwide structured metapopulation epidemic model a time-scale separation technique for evaluating the force of infection due to multiscale mobility processes in the disease dynamics. Commuting flows are found, on average, to be one order of magnitude larger than airline flows. However, their introduction into the worldwide model shows that the large scale pattern of the simulated epidemic exhibits only small variations with respect to the baseline case where only airline traffic is considered. The presence of short range mobility increases however the synchronization of subpopulations in close proximity and affects the epidemic behavior at the periphery of the airline transportation infrastructure. The present approach outlines the possibility for the definition of layered computational approaches where different modeling assumptions and granularities can be used consistently in a unifying multi-scale framework.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Job loss disrupts individuals' mobility and their exploratory patterns,"In recent years, human mobility research has discovered universal patterns capable of describing how people move. These regularities have been shown to partly depend on individual and environmental characteristics (e.g., gender, rural/urban, country). In this work, we show that life-course events, such as job loss, can disrupt individual mobility patterns. Adversely affecting individuals' well-being and potentially increasing the risk of social and economic inequalities, we show that job loss drives a significant change in the exploratory behaviour of individuals with changes that intensify over time since job loss. Our findings shed light on the dynamics of employment-related behavior at scale, providing a deeper understanding of key components in human mobility regularities. These drivers can facilitate targeted social interventions to support the most vulnerable populations.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Modelling exposure between populations using networks of mobility during Covid-19,"The use of mobile phone call detail records and device location data for the calling patterns, movements, and social contacts of individuals, has proven to be valuable for devising models and understanding of their mobility and behaviour patterns. In this study we investigate weighted exposure-networks of human daily activities in the capital region of Finland as a proxy for contacts between postal code areas during the pre-pandemic year 2019 and pandemic years 2020, 2021 and early 2022. We investigate the suitability of gravity and radiation type models for reconstructing the exposure-networks based on geo-spatial and population mobility information. For this we use a mobile phone dataset of aggregated daily visits from a postal code area to cellphone grid locations, and treat it as a bipartite network to create weighted one mode projections using a weighted co-occurrence function. We fit a gravitation model and a radiation model to the averaged weekly and yearly projection networks with geo-spatial and socioeconomic variables of the postal code areas and their populations. We also consider an extended gravity type model comprising of additional postal area information such as distance via public transportation and population density. The results show that the co-occurrence of human activities, or exposure, between postal code areas follows both the gravity and radiation type interactions, once fitted to the empirical network. The effects of the pandemic beginning in 2020 can be observed as a decrease of the overall activity as well as of the exposure of the projected networks. In general, the results show that the postal code level networks changed to be more proximity weighted after the pandemic began, following the government imposed non-pharmaceutical interventions, with differences based on the geo-spatial and socioeconomic structure of the areas.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
From centre to centres: polycentric structures in individual mobility,"The availability of large-scale datasets collected via mobile phones has opened up opportunities to study human mobility at an individual level. The granular nature of these datasets calls for the design of summary statistics that can be used to describe succinctly mobility patterns. In this work, we show that the radius of gyration, a popular summary statistic to quantify the extent of an individual's whereabouts, suffers from a sensitivity to outliers, and is incapable of capturing mobility organised around multiple centres. We propose a natural generalisation of the radius of gyration to a polycentric setting, as well as a novel metric to assess the quality of its description. With these notions, we propose a method to identify the centres in an individual's mobility and apply it to two large mobility datasets with socio-demographic features, showing that a polycentric description can capture features that a monocentric model is incapable of.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"On the Interplay of Regional Mobility, Social Connectedness, and the Spread of COVID-19 in Germany","Since the primary mode of respiratory virus transmission is person-to-person interaction, we are required to reconsider physical interaction patterns to mitigate the number of people infected with COVID-19. While research has shown that non-pharmaceutical interventions (NPI) had an evident impact on national mobility patterns, we investigate the relative regional mobility behaviour to assess the effect of human movement on the spread of COVID-19. In particular, we explore the impact of human mobility and social connectivity derived from Facebook activities on the weekly rate of new infections in Germany between March 3rd and June 22nd, 2020. Our results confirm that reduced social activity lowers the infection rate, accounting for regional and temporal patterns. The extent of social distancing, quantified by the percentage of people staying put within a federal administrative district, has an overall negative effect on the incidence of infections. Additionally, our results show spatial infection patterns based on geographic as well as social distances.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Measurement of human activity using velocity GPS data obtained from mobile phones,"Human movement is used as an indicator of human activity in modern society. The velocity of moving humans is calculated based on position information obtained from mobile phones. The level of human activity, as recorded by velocity, varies throughout the day. Therefore, velocity can be used to identify the intervals of highest and lowest activity. More specifically, we obtained mobile-phone GPS data from the people around Shibuya station in Tokyo, which has the highest population density in Japan. From these data, we observe that velocity tends to consistently increase with the changes in social activities. For example, during the earthquake in Kumamoto Prefecture in April 2016, the activity on that day was much lower than usual. In this research, we focus on natural disasters such as earthquakes owing to their significant effects on human activities in developed countries like Japan. In the event of a natural disaster in another developed country, considering the change in human behavior at the time of the disaster (e.g., the 2016 Kumamoto Great Earthquake) from the viewpoint of velocity allows us to improve our planning for mitigation measures. Thus, we analyze the changes in human activity through velocity calculations in Shibuya, Tokyo, and compare times of disasters with normal times.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Estimating cross-border mobility from the difference in peak-timing: A case study in Poland-Germany border regions,"Human mobility contributes to the fast spatio-temporal propagation of infectious diseases. During an outbreak, monitoring the infection situation on either side of an international border is very crucial as there is always a higher risk of disease importation associated with cross-border migration. Mechanistic models are effective tools to investigate the consequences of cross-border mobility on disease dynamics and help in designing effective control strategies. However, in practice, due to the unavailability of cross-border mobility data, it becomes difficult to propose reliable, model-based strategies. In this study, we propose a method for estimating cross-border mobility flux between any pair of regions that share an international border from the observed difference in the timing of the infection peak in each region. Assuming the underlying disease dynamics is governed by a Susceptible-Infected-Recovered (SIR) model, we employ stochastic simulations to obtain the maximum likelihood cross-border mobility estimate for any pair of regions where the difference in peak time can be measured. We then investigate how the estimate of cross-border mobility flux varies depending on the disease transmission rate, which is a key epidemiological parameter. We further show that the uncertainty in mobility flux estimates decreases for higher disease transmission rates and larger observed differences in peak timing. Finally, as a case study, we apply the method to some selected regions along the Poland-Germany border which are directly connected through multiple modes of transportation and quantify the cross-border fluxes from the COVID-19 cases data during the period $20^{\rm th}$ February $2021$ to $20^{\rm th}$ June $2021$.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Phase transitions in contagion processes mediated by recurrent mobility patterns,"Human mobility and activity patterns mediate contagion on many levels, including the spatial spread of infectious diseases, diffusion of rumors, and emergence of consensus. These patterns however are often dominated by specific locations and recurrent flows and poorly modeled by the random diffusive dynamics generally used to study them. Here we develop a theoretical framework to analyze contagion within a network of locations where individuals recall their geographic origins. We find a phase transition between a regime in which the contagion affects a large fraction of the system and one in which only a small fraction is affected. This transition cannot be uncovered by continuous deterministic models due to the stochastic features of the contagion process and defines an invasion threshold that depends on mobility parameters, providing guidance for controlling contagion spread by constraining mobility processes. We recover the threshold behavior by analyzing diffusion processes mediated by real human commuting data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Active-travel modelling: a methodological approach to networks for walking and cycling commuting analysis,"Walking and cycling, commonly referred to as active travel, have become integral components of modern transport planning. Recently, there has been growing recognition of the substantial role that active travel can play in making cities more liveable, sustainable and healthy, as opposed to traditional vehicle-centred approaches. This shift in perspective has spurred interest in developing new data sets of varying resolution levels to represent, for instance, walking and cycling street networks. This has also led to the development of tailored computational tools and quantitative methods to model and analyse active travel flows.   In response to this surge in active travel-related data and methods, our study develops a methodological framework primarily focused on walking and cycling as modes of commuting. We explore commonly used data sources and tools for constructing and analysing walking and cycling networks, with a particular emphasis on distance as a key factor that influences, describes, and predicts commuting behaviour. Our ultimate aim is to investigate the role of different network distances in predicting active commuting flows.   To achieve this, we analyse the flows in the constructed networks by looking at the detour index of shortest paths. We then use the Greater London Area as a case study, and construct a spatial interaction model to investigate the observed commuting patterns through the different networks. Our results highlight the differences between chosen data sets, the uneven spatial distribution of their performance throughout the city and its consequent effect on the spatial interaction model and prediction of walking and cycling commuting flows.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Smartphone Transportation Mode Recognition Using a Hierarchical Machine Learning Classifier and Pooled Features From Time and Frequency Domains,"This paper develops a novel two-layer hierarchical classifier that increases the accuracy of traditional transportation mode classification algorithms. This paper also enhances classification accuracy by extracting new frequency domain features. Many researchers have obtained these features from global positioning system data; however, this data was excluded in this paper, as the system use might deplete the smartphone's battery and signals may be lost in some areas. Our proposed two-layer framework differs from previous classification attempts in three distinct ways: 1) the outputs of the two layers are combined using Bayes' rule to choose the transportation mode with the largest posterior probability; 2) the proposed framework combines the new extracted features with traditionally used time domain features to create a pool of features; and 3) a different subset of extracted features is used in each layer based on the classified modes. Several machine learning techniques were used, including k-nearest neighbor, classification and regression tree, support vector machine, random forest, and a heterogeneous framework of random forest and support vector machine. Results show that the classification accuracy of the proposed framework outperforms traditional approaches. Transforming the time domain features to the frequency domain also adds new features in a new space and provides more control on the loss of information. Consequently, combining the time domain and the frequency domain features in a large pool and then choosing the best subset results in higher accuracy than using either domain alone. The proposed two-layer classifier obtained a maximum classification accuracy of 97.02%.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Cross-checking different sources of mobility information,"The pervasive use of new mobile devices has allowed a better characterization in space and time of human concentrations and mobility in general. Besides its theoretical interest, describing mobility is of great importance for a number of practical applications ranging from the forecast of disease spreading to the design of new spaces in urban environments. While classical data sources, such as surveys or census, have a limited level of geographical resolution (e.g., districts, municipalities, counties are typically used) or are restricted to generic workdays or weekends, the data coming from mobile devices can be precisely located both in time and space. Most previous works have used a single data source to study human mobility patterns. Here we perform instead a cross-check analysis by comparing results obtained with data collected from three different sources: Twitter, census and cell phones. The analysis is focused on the urban areas of Barcelona and Madrid, for which data of the three types is available. We assess the correlation between the datasets on different aspects: the spatial distribution of people concentration, the temporal evolution of people density and the mobility patterns of individuals. Our results show that the three data sources are providing comparable information. Even though the representativeness of Twitter geolocated data is lower than that of mobile phone and census data, the correlations between the population density profiles and mobility patterns detected by the three datasets are close to one in a grid with cells of 2x2 and 1x1 square kilometers. This level of correlation supports the feasibility of interchanging the three data sources at the spatio-temporal scales considered.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying the influence of inter-county mobility patterns on the COVID-19 outbreak in the United States,"As a highly infectious respiratory disease, COVID-19 has become a pandemic that threatens global health. Without an effective treatment, non-pharmaceutical interventions, such as travel restrictions, have been widely promoted to mitigate the outbreak. Current studies analyze mobility metrics such as travel distance; however, there is a lack of research on interzonal travel flow and its impact on the pandemic. Our study specifically focuses on the inter-county mobility pattern and its influence on the COVID-19 spread in the United States. To retrieve real-world mobility patterns, we utilize an integrated set of mobile device location data including over 100 million anonymous devices. We first investigate the nationwide temporal trend and spatial distribution of inter-county mobility. Then we zoom in on the epicenter of the U.S. outbreak, New York City, and evaluate the impacts of its outflow on other counties. Finally, we develop a ""log-linear double-risk"" model at the county level to quantify the influence of both ""external risk"" imported by inter-county mobility flows and the ""internal risk"" defined as the vulnerability of a county in terms of population with high-risk phenotypes. Our study enhances the situation awareness of inter-county mobility in the U.S. and can help improve non-pharmaceutical interventions for COVID-19.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The spread of COVID-19 increases with individual mobility and depends on political leaning,"The implementation of social distancing policies is key to reducing the impact of the current COVID-19 pandemic. However, their effectiveness ultimately depends on human behavior. In the United States, compliance with social distancing policies has widely varied thus far during the pandemic. But what drives such variability? Through six open datasets, including actual human mobility, we estimated the association between mobility and the growth rate of COVID-19 cases across 3,107 U.S. counties, generalizing previous reports. In addition, data from the 2016 U.S. presidential election was used to measure how the association between mobility and COVID-19 growth rate differed based on voting patterns. A significant association between political leaning and the COVID-19 growth rate was measured. Our results demonstrate that political orientation may inform models predicting the impact of policies in reducing the spread of COVID-19.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Quantifying the overall characteristics of urban mobility considering spatial information,"Quantification of the overall characteristics of urban mobility using coarse-grained methods is crucial for urban management, planning and sustainable development. Although some recent studies have provided quantification methods for coarse-grained numerical information regarding urban mobility, a method that can simultaneously capture numerical and spatial information remains an outstanding problem. Here, we use mathematical vectors to depict human mobility, with mobility magnitude representing numerical information and mobility direction representing spatial information. We then define anisotropy and centripetality metrics by vector computation to measure imbalance in direction distribution and orientation toward the city center of mobility flows, respectively. As a case study, we apply our method to 60 Chinese cities and identify three mobility patterns: strong monocentric, weak monocentric and polycentric. To better understand mobility pattern, we further study the allometric scaling of the average commuting distance and the spatiotemporal variations of the two metrics in different patterns. Finally, we build a microscopic model to explain the key mechanisms driving the diversity in anisotropy and centripetality. Our work offers a comprehensive method that considers both numerical and spatial information to quantify and classify the overall characteristics of urban mobility, enhancing our understanding of the structure and evolution of urban mobility systems.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mobility constraints in segregation models,"Since the development of the original Schelling model of urban segregation, several enhancements have been proposed, but none have considered the impact of mobility constraints on model dynamics. Recent studies have shown that human mobility follows specific patterns, such as a preference for short distances and dense locations. This paper proposes a segregation model incorporating mobility constraints to make agents select their location based on distance and location relevance. Our findings indicate that the mobility-constrained model produces lower segregation levels but takes longer to converge than the original Schelling model. We identified a few persistently unhappy agents from the minority group who cause this prolonged convergence time and lower segregation level as they move around the grid centre. Our study presents a more realistic representation of how agents move in urban areas and provides a novel and insightful approach to analyzing the impact of mobility constraints on segregation models. We highlight the significance of incorporating mobility constraints when policymakers design interventions to address urban segregation.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The Phone Walkers: A study of human dependence on inactive mobile devices,"The development of mobile phones has largely increased human interactions. Whilst the use of these devices for communication has received significant attention, there has been little analysis of more passive interactions. Through census data on casual social groups, this work suggests a clear pattern of mobile phones being carried in people's hands, without the person using it (that is, not looking at it). Moreover, this study suggests that when individuals join members of the opposite sex there is a clear tendency to stop holding mobile phones whilst walking. Although it is not clear why people hold their phones whilst walking in such large proportions (38% of solitary women, and 31% of solitary men), we highlight several possible explanation for holding the device, including the need to advertise status and affluence, to maintain immediate connection with friends and family, and to mitigate feelings related to anxiety and security.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Estimation of Regional Economic Development Indicator from Transportation Network Analytics,"With the booming economy in China, many researches have pointed out that the improvement of regional transportation infrastructure among other factors had an important effect on economic growth. Utilizing a large-scale dataset which includes 3.5 billion entry and exit records of vehicles along highways generated from toll collection systems, we attempt to establish the relevance of mid-distance land transport patterns to regional economic status through transportation network analyses. We apply standard measurements of complex networks to analyze the highway transportation networks. A set of traffic flow features are computed and correlated to the regional economic development indicator. The multi-linear regression models explain about 89% to 96% of the variation of cities' GDP across three provinces in China. We then fit gravity models using annual traffic volumes of cars, buses, and freight trucks between pairs of cities for each province separately as well as for the whole dataset. We find the temporal changes of distance-decay effects on spatial interactions between cities in transportation networks, which link to the economic development patterns of each province. We conclude that transportation big data reveal the status of regional economic development and contain valuable information of human mobility, production linkages, and logistics for regional management and planning. Our research offers insights into the investigation of regional economic development status using highway transportation big data.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exploring the effect of spatial scales in studying urban mobility pattern,"Urban mobility plays a crucial role in the functioning of cities, influencing economic activity, accessibility, and quality of life. However, the effectiveness of analytical models in understanding urban mobility patterns can be significantly affected by the spatial scales employed in the analysis. This paper explores the impact of spatial scales on the performance of the gravity model in explaining urban mobility patterns using public transport flow data in Singapore. The model is evaluated across multiple spatial scales of origin and destination locations, ranging from individual bus stops and train stations to broader regional aggregations. Results indicate the existence of an optimal intermediate spatial scale at which the gravity model performs best. At the finest scale, where individual transport nodes are considered, the model exhibits poor performance due to noisy and highly variable travel patterns. Conversely, at larger scales, model performance also suffers as over-aggregation of transport nodes results in excessive generalisation which obscures the underlying mobility dynamics. Furthermore, distance-based spatial aggregation of transport nodes proves to outperform administrative boundary-based aggregation, suggesting that actual urban organisation and movement patterns may not necessarily align with imposed administrative divisions. These insights highlight the importance of selecting appropriate spatial scales in mobility analysis and urban modelling in general, offering valuable guidance for urban and transport planning efforts aimed at enhancing mobility in complex urban environments.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human mobility networks and persistence of rapidly mutating pathogens,"Rapidly mutating pathogens may be able to persist in the population and reach an endemic equilibrium by escaping hosts' acquired immunity. For such diseases, multiple biological, environmental and population-level mechanisms determine the dynamics of the outbreak, including pathogen's epidemiological traits (e.g. transmissibility, infectious period and duration of immunity), seasonality, interaction with other circulating strains and hosts' mixing and spatial fragmentation. Here, we study a susceptible-infected-recovered-susceptible model on a metapopulation where individuals are distributed in subpopulations connected via a network of mobility flows. Through extensive numerical simulations, we explore the phase space of pathogen's persistence and map the dynamical regimes of the pathogen following emergence. Our results show that spatial fragmentation and mobility play a key role in the persistence of the disease whose maximum is reached at intermediate mobility values. We describe the occurrence of different phenomena including local extinction and emergence of epidemic waves, and assess the conditions for large scale spreading. Findings are highlighted in reference to previous works and to real scenarios. Our work uncovers the crucial role of hosts' mobility on the ecological dynamics of rapidly mutating pathogens, opening the path for further studies on disease ecology in the presence of a complex and heterogeneous environment.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Social Physics: Uncovering Human Behaviour from Communication,"In the post year 2000 era the technologies that facilitate human communication have rapidly multiplied. While the adoption of these technologies has hugely impacted the behaviour and sociality of people, specifically in urban but also in rural environments, their ""digital footprints"" on different data bases have become an active area of research. The existence and accessibility of such large population-level datasets, has allowed scientists to study and model innate human tendencies and social patterns in an unprecedented way that complements traditional research approaches like questionnaire studies. In this review we focus on data analytics and modelling research - we call Social Physics - as it has been carried out using the mobile phone data sets to get insight into the various aspects of human sociality, burstiness in communication, mobility patterns, and daily rhythms.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Growth Patterns of Subway/Metro Systems Tracked by Degree Correlation,"Urban transportation systems grow over time as city populations grow and move and their transportation needs evolve. Typical network growth models, such as preferential attachment, grow the network node by node whereas rail and metro systems grow by adding entire lines with all their nodes. The objective of this paper is to see if any canonical regular network forms such as stars or grids capture the growth patterns of urban metro systems for which we have historical data in terms of old maps. Data from these maps reveal that the systems' Pearson degree correlation grows increasingly from initially negative values toward positive values over time and in some cases becomes decidedly positive. We have derived closed form expressions for degree correlation and clustering coefficient for a variety of canonical forms that might be similar to metro systems. Of all those examined, only a few types patterned after a wide area network (WAN) with a ""core-periphery"" structure show similar positive-trending degree correlation as network size increases. This suggests that large metro systems either are designed or evolve into the equivalent of message carriers that seek to balance travel between arbitrary node-destination pairs with avoidance of congestion in the central regions of the network.   Keywords: metro, subway, urban transport networks, degree correlation","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Epidemic Dynamics in Homes and Destinations under Recurrent Mobility Patterns,"The structure of heterogeneous networks and human mobility patterns profoundly influence the spreading of endemic diseases. In small-scale communities, individuals engage in social interactions within confined environments, such as homes and workplaces, where daily routines facilitate virus transmission through predictable mobility pathways. Here, we introduce a metapopulation model grounded in a Microscopic Markov Chain Approach to simulate susceptible--infected--susceptible dynamics within structured populations. There are two primary types of nodes, homes and destinations, where individuals interact and transmit infections through recurrent mobility patterns. We derive analytical expressions for the epidemic threshold and validate our theoretical findings through comparative simulations on Watts--Strogatz and Barabsi--Albert networks. The experimental results reveal a nonlinear relationship between mobility probability and the epidemic threshold, indicating that further increases can inhibit disease transmission beyond a certain critical mobility level.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Contribution of hidden modes to nonlinear epidemic dynamics in urban human proximity networks,"Recently developed techniques to acquire high-quality human mobility data allow large-scale simulations of the spread of infectious diseases with high spatial and temporal resolution.Analysis of such data has revealed the oversimplification of existing theoretical frameworks to infer the final epidemic size or influential nodes from the network topology. Here we propose a spectral decomposition-based framework for the quantitative analysis of epidemic processes on realistic networks of human proximity derived from urban mobility data. Common wisdom suggests that modes with larger eigenvalues contribute more to the epidemic dynamics. However, we show that hidden dominant structures, namely modes with smaller eigenvalues but a greater contribution to the epidemic dynamics, exist in the proximity network. This framework provides a basic understanding of the relationship between urban human motion and epidemic dynamics, and will contribute to strategic mitigation policy decisions.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Sensing and Modeling Human Behavior Using Social Media and Mobile Data,"In the past years we have witnessed the emergence of the new discipline of computational social science, which promotes a new data-driven and computation-based approach to social sciences. In this article we discuss how the availability of new technologies such as online social media and mobile smartphones has allowed researchers to passively collect human behavioral data at a scale and a level of granularity that were just unthinkable some years ago. We also discuss how these digital traces can then be used to prove (or disprove) existing theories and develop new models of human behavior.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Revealing Critical Characteristics of Mobility Patterns in New York City during the Onset of COVID-19 Pandemic,"New York has become one of the worst-affected COVID-19 hotspots and a pandemic epicenter due to the ongoing crisis. This paper identifies the impact of the pandemic and the effectiveness of government policies on human mobility by analyzing multiple datasets available at both macro and micro levels for the New York City. Using data sources related to population density, aggregated population mobility, public rail transit use, vehicle use, hotspot and non-hotspot movement patterns, and human activity agglomeration, we analyzed the inter-borough and intra-borough moment for New York City by aggregating the data at the borough level. We also assessed the internodal population movement amongst hotspot and non-hotspot points of interest for the month of March and April 2020. Results indicate a drop of about 80% in people's mobility in the city, beginning in mid-March. The movement to and from Manhattan showed the most disruption for both public transit and road traffic. The city saw its first case on March 1, 2020, but disruptions in mobility can be seen only after the second week of March when the shelter in place orders was put in effect. Owing to people working from home and adhering to stay-at-home orders, Manhattan saw the largest disruption to both inter- and intra-borough movement. But the risk of spread of infection in Manhattan turned out to be high because of higher hotspot-linked movements. The stay-at-home restrictions also led to an increased population density in Brooklyn and Queens as people were not commuting to Manhattan. Insights obtained from this study would help policymakers better understand human behavior and their response to the news and governmental policies.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Uncovering individual and collective human dynamics from mobile phone records,"Novel aspects of human dynamics and social interactions are investigated by means of mobile phone data. Using extensive phone records resolved in both time and space, we study the mean collective behavior at large scales and focus on the occurrence of anomalous events. We discuss how these spatiotemporal anomalies can be described using standard percolation theory tools. We also investigate patterns of calling activity at the individual level and show that the interevent time of consecutive calls is heavy-tailed. This finding, which has implications for dynamics of spreading phenomena in social networks, agrees with results previously reported on other human activities.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Human Mobility in Response to COVID-19 in France, Italy and UK","The policies implemented to hinder the COVID-19 outbreak represent one of the largest critical events in history. The understanding of this process is fundamental for crafting and tailoring post-disaster relief. In this work we perform a massive data analysis, through geolocalized data from 13M Facebook users, on how such a stress affected mobility patterns in France, Italy and UK. We find that the general reduction of the overall efficiency in the network of movements is accompanied by geographical fragmentation with a massive reduction of long-range connections. The impact, however, differs among nations according to their initial mobility structure. Indeed, we find that the mobility network after the lockdown is more concentrated in the case of France and UK and more distributed in Italy. Such a process can be approximated through percolation to quantify the substantial impact of the lockdown.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Patterns of Regional Travel Behavior: An Analysis of Japanese Hotel Reservation Data,"This study considers the availability of room opportunities collected from a Japanese hotel booking site. We empirically analyze the daily number of room opportunities for four areas. To determine the migration trends of travelers, we discuss a finite mixture of Poisson distributions and the EM-algorithm as its parameter estimation method. We further propose a method to infer the probability of opportunities existing for each observation. We characterize demand-supply situations by means of relationship between the averaged room prices and the probability of opportunity existing.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Examining mobility data justice during 2017 Hurricane Harvey,"Natural disasters can significantly disrupt human mobility in urban areas. Studies have attempted to understand and quantify such disruptions using crowdsourced mobility data sets. However, limited research has studied the justice issues of mobility data in the context of natural disasters. The lack of research leaves us without an empirical foundation to quantify and control the possible biases in the data. This study, using 2017 Hurricane Harvey as a case study, explores three aspects of mobility data that could potentially cause injustice: representativeness, quality, and precision. We find representativeness being a major factor contributing to mobility data injustice. There is a persistent disparity of representativeness across neighborhoods of different socioeconomic characteristics before, during, and after the hurricane's landfall. Additionally, we observed significant drops of data precision during the hurricane, adding uncertainty to locate people and understand their movements during extreme weather events. The findings highlight the necessity in understanding and controlling the possible bias of mobility data as well as developing practical tools through data justice lenses in collecting and analyzing data during disasters.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Highly coordinated nationwide massive travel restrictions are central to effective mitigation and control of COVID-19 outbreaks in China,"The COVID-19, the disease caused by the novel coronavirus 2019 (SARS-CoV-2), has caused graving woes across the globe since first reported in the epicenter Wuhan, Hubei, China, December 2019. The spread of COVID-19 in China has been successfully curtailed by massive travel restrictions that put more than 900 million people housebound for more than two months since the lockdown of Wuhan on 23 January 2020 when other provinces in China followed suit. Here, we assess the impact of China's massive lockdowns and travel restrictions reflected by the changes in mobility patterns before and during the lockdown period. We quantify the synchrony of mobility patterns across provinces and within provinces. Using these mobility data, we calibrate movement flow between provinces in combination with an epidemiological compartment model to quantify the effectiveness of lockdowns and reductions in disease transmission. Our analysis demonstrates that the onset and phase of local community transmission in other provinces depends on the cumulative population outflow received from the epicenter Hubei. As such, infections can propagate further into other interconnected places both near and far, thereby necessitating synchronous lockdowns. Moreover, our data-driven modeling analysis shows that lockdowns and consequently reduced mobility lag a certain time to elicit an actual impact on slowing down the spreading and ultimately putting the epidemic under check. In spite of the vastly heterogeneous demographics and epidemiological characteristics across China, mobility data shows that massive travel restrictions have been applied consistently via a top-down approach along with high levels of compliance from the bottom up.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding the spreading patterns of mobile phone viruses,"We model the mobility of mobile phone users to study the fundamental spreading patterns characterizing a mobile virus outbreak. We find that while Bluetooth viruses can reach all susceptible handsets with time, they spread slowly due to human mobility, offering ample opportunities to deploy antiviral software. In contrast, viruses utilizing multimedia messaging services could infect all users in hours, but currently a phase transition on the underlying call graph limits them to only a small fraction of the susceptible users. These results explain the lack of a major mobile virus breakout so far and predict that once a mobile operating system's market share reaches the phase transition point, viruses will pose a serious threat to mobile communications.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Exploring the Mobility of Mobile Phone Users,"Mobile phone datasets allow for the analysis of human behavior on an unprecedented scale. The social network, temporal dynamics and mobile behavior of mobile phone users have often been analyzed independently from each other using mobile phone datasets. In this article, we explore the connections between various features of human behavior extracted from a large mobile phone dataset. Our observations are based on the analysis of communication data of 100000 anonymized and randomly chosen individuals in a dataset of communications in Portugal. We show that clustering and principal component analysis allow for a significant dimension reduction with limited loss of information. The most important features are related to geographical location. In particular, we observe that most people spend most of their time at only a few locations. With the help of clustering methods, we then robustly identify home and office locations and compare the results with official census data. Finally, we analyze the geographic spread of users' frequent locations and show that commuting distances can be reasonably well explained by a gravity model.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"Lockdown Strategies, Mobility Patterns and COVID-19","We develop a multiple-events model and exploit within and between country variation in the timing, type and level of intensity of various public policies to study their dynamic effects on the daily incidence of COVID-19 and on population mobility patterns across 135 countries. We remove concurrent policy bias by taking into account the contemporaneous presence of multiple interventions. The main result of the paper is that cancelling public events and imposing restrictions on private gatherings followed by school closures have quantitatively the most pronounced effects on reducing the daily incidence of COVID-19. They are followed by workplace as well as stay-at-home requirements, whose statistical significance and levels of effect are not as pronounced. Instead, we find no effects for international travel controls, public transport closures and restrictions on movements across cities and regions. We establish that these findings are mediated by their effect on population mobility patterns in a manner consistent with time-use and epidemiological factors.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The ubiquitous efficiency of going further: how street networks affect travel speed,"As cities struggle to adapt to more ``people-centered'' urbanism, transportation planning and engineering must innovate to expand the street network strategically in order to ensure efficiency but also to deter sprawl. Here, we conducted a study of over 200 cities around the world to understand the impact that the patterns of deceleration points in streets due to traffic signs has in trajectories done from motorized vehicles. We demonstrate that there is a ubiquitous nonlinear relationship between time and distance in the optimal trajectories within each city. More precisely, given a specific period of time $$, without any traffic, one can move on average up to the distance $\left \langle D \right \rangle \sim^$. We found a super-linear relationship for almost all cities in which $>1.0$. This points to an efficiency of scale when traveling large distances, meaning the average speed will be higher for longer trips when compared to shorter trips. We demonstrate that this efficiency is a consequence of the spatial distribution of large segments of streets without deceleration points, favoring access to routes in which a vehicle can cross large distances without stops. These findings show that cities must consider how their street morphology can affect travel speed.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Decoding street network morphologies and their correlation to travel mode choice,"Urban morphology has long been recognized as a factor shaping human mobility, yet comparative and formal classifications of urban form across metropolitan areas remain limited. Building on theoretical principles of urban structure and advances in unsupervised learning, we systematically classified the built environment of nine U.S. metropolitan areas using structural indicators such as density, connectivity, and spatial configuration. The resulting morphological types were linked to mobility patterns through descriptive statistics, marginal effects estimation, and post hoc statistical testing. Here we show that distinct urban forms are systematically associated with different mobility behaviors, such as reticular morphologies being linked to significantly higher public transport use (marginal effect = 0.49) and reduced car dependence (-0.41), while organic forms are associated with increased car usage (0.44), and substantial declines in public transport (-0.47) and active mobility (-0.30). These effects are statistically robust (p < 1e-19), highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices. Our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research. These results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Ranking dynamics of urban mobility,"Human mobility, a pivotal aspect of urban dynamics, displays a profound and multifaceted relationship with urban sustainability. Despite considerable efforts analyzing mobility patterns over decades, the ranking dynamics of urban mobility has received limited attention. This study aims to contribute to the field by investigating changes in rank and size of hourly inflows to various locations across 60 Chinese cities throughout the day. We find that the rank-size distribution of hourly inflows over the course of the day is stable across cities. To uncover the microdynamics beneath the stable aggregate distribution amidst shifting location inflows, we analyzed consecutive-hour inflow size and ranking variations. Our findings reveal a dichotomy: locations with higher daily average inflow display a clear monotonic trend, with more pronounced increases or decreases in consecutive-hour inflow. In contrast, ranking variations exhibit a non-monotonic pattern, distinguished by the stability of not only the top and bottom rankings but also those in moderately-inflowed locations. Finally, we compare ranking dynamics across cities using a ranking metric, the rank turnover. The results advance our understanding of urban mobility dynamics, providing a basis for applications in urban planning and traffic engineering.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multiplex mobility network and metapopulation epidemic simulations of Italy based on Open Data,"The patterns of human mobility play a key role in the spreading of infectious diseases and thus represent a key ingredient of epidemic modeling and forecasting. Unfortunately, as the Covid-19 pandemic has dramatically highlighted, for the vast majority of countries there is no availability of granular mobility data. This hinders the possibility of developing computational frameworks to monitor the evolution of the disease and to adopt timely and adequate prevention policies. Here we show how this problem can be addressed in the case study of Italy. We build a multiplex mobility network based solely on open data, and implement a SIR metapopulation model that allows scenario analysis through data-driven stochastic simulations. The mobility flows that we estimate are in agreement with real-time proprietary data from smartphones. Our modeling approach can thus be useful in contexts where high-resolution mobility data is not available.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility,"Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Identifying latent activity behaviors and lifestyles using mobility data to describe urban dynamics,"Urbanization and its problems require an in-depth and comprehensive understanding of urban dynamics, especially the complex and diversified lifestyles in modern cities. Digitally acquired data can accurately capture complex human activity, but it lacks the interpretability of demographic data. In this paper, we study a privacy-enhanced dataset of the mobility visitation patterns of 1.2 million people to 1.1 million places in 11 metro areas in the U.S. to detect the latent mobility behaviors and lifestyles in the largest American cities. Despite the considerable complexity of mobility visitations, we found that lifestyles can be automatically decomposed into only 12 latent interpretable activity behaviors on how people combine shopping, eating, working, or using their free time. Rather than describing individuals with a single lifestyle, we find that city dwellers' behavior is a mixture of those behaviors. Those detected latent activity behaviors are equally present across cities and cannot be fully explained by main demographic features. Finally, we find those latent behaviors are associated with dynamics like experienced income segregation, transportation, or healthy behaviors in cities, even after controlling for demographic features. Our results signal the importance of complementing traditional census data with activity behaviors to understand urban dynamics.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
An analytical framework to nowcast well-being using mobile phone data,"An intriguing open question is whether measurements made on Big Data recording human activities can yield us high-fidelity proxies of socio-economic development and well-being. Can we monitor and predict the socio-economic development of a territory just by observing the behavior of its inhabitants through the lens of Big Data? In this paper, we design a data-driven analytical framework that uses mobility measures and social measures extracted from mobile phone data to estimate indicators for socio-economic development and well-being. We discover that the diversity of mobility, defined in terms of entropy of the individual users' trajectories, exhibits (i) significant correlation with two different socio-economic indicators and (ii) the highest importance in predictive models built to predict the socio-economic indicators. Our analytical framework opens an interesting perspective to study human behavior through the lens of Big Data by means of new statistical indicators that quantify and possibly ""nowcast"" the well-being and the socio-economic development of a territory.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Limitations of gravity models in predicting fine-scale spatial-temporal urban mobility networks,"This study identifies the limitations and underlying characteristics of urban mobility networks that influence the performance of the gravity model. The gravity model is a widely-used approach for estimating and predicting population flows in urban mobility networks, assuming the scale-free property. Prior studies have reported good performance results for the gravity model at certain levels of aggregation. However, the characteristics of urban mobility networks might vary depending on the spatial and temporal resolutions of data. Hence, the sensitivity of gravity model performance to variation in the level of aggregation of data and the temporal and spatial scale of urban mobility networks needs to be examined. The basic gravity model is tested on urban mobility networks on an hourly and daily scale using fine-grained location-based human mobility data for multiple US metropolitan counties to address this gap. The findings suggest that: (1) finer-scale urban mobility networks do not demonstrate a scale-free property; (2) the performance of the basic gravity model decays for predicting population flow in the finer-scale urban mobility networks; (3) the variations in population density distribution and mobility network structure and properties across counties do not significantly influence the performance of gravity models. Hence, gravity models may not be suitable for modeling urban mobility networks with daily or hourly aggregation of census tract to census tract movements. The findings highlight the need for new urban mobility network models or machine learning approaches to predict fine-scale and high temporal-resolution urban mobility networks better.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Multi-Dimensional Geometric Complexity in Urban Transportation Systems,"Transportation networks serve as windows into the complex world of urban systems. By properly characterizing a road network, we can therefore better understand its encompassing urban system. This study offers a geometrical approach towards capturing inherent properties of urban road networks. It offers a robust and efficient methodology towards defining and extracting three relevant indicators of road networks: area, line, and point thresholds, through measures of their grid equivalents. By applying the methodology to 50 U.S. urban systems, we successfully observe differences between eastern versus western, coastal versus inland, and old versus young, cities. Moreover, we show that many socio-economic characteristics as well as travel patterns within urban systems are directly correlated with their corresponding area, line, and point thresholds.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Study on Key Technologies of Transit Passengers Travel Pattern Mining and Applications based on Multiple Sources of Data,"In this research, we propose a series of methodologies to mine transit riders travel pattern and behavioral preferences, and then we use these knowledges to adjust and optimize the transit systems. Contributions are: 1) To increase the data validity: a) we propose a novel approach to rectify the time discrepancy of data between the AFC (Automated Fare Collection) systems and AVL (Automated Vehicle Location) system, our approach transforms data events into signals and applies time domain correlation the detect and rectify their relative discrepancies. b) By combining historical data and passengers ticketing time stamps, we induct and compensate missing information in AVL datasets. 2) To infer passengers alighting point, we introduce a maximum probabilistic model incorporating passengers home place to recover their complete transit trajectory from semi-complete boarding records.Then we propose an enhance activity identification algorithm which is capable of specifying passengers short-term activity from ordinary transfers. Finally, we analyze the temporal-spatial characteristic of transit ridership. 3) To discover passengers travel demands. We integrate each passengers trajectory data in multiple days and construct a Hybrid Trip Graph (HTG). We then use a depth search algorithm to derive the spatially closed transit trip chains; Finally, we use closed transit trip chains of passengers to study their travel pattern from various perspectives. Finally, we analyze urban transit corridors by aggregating the passengers critical transit chains.4) We derive eight influential factors, and then construct passengers choice models under various scenarios. Next, we validate our model using ridership re-distribute simulations. Finally, we conduct a comprehensive analysis on passengers temporal choice preference and use this information to optimize urban transit systems.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A dataset to assess mobility changes in Chile following local quarantines,"Fighting the COVID-19 pandemic, most countries have implemented non-pharmaceutical interventions like wearing masks, physical distancing, lockdown, and travel restrictions. Because of their economic and logistical effects, tracking mobility changes during quarantines is crucial in assessing their efficacy and predicting the virus spread. Chile, one of the worst-hit countries in the world, unlike many other countries, implemented quarantines at a more localized level, shutting down small administrative zones, rather than the whole country or large regions. Given the non-obvious effects of these localized quarantines, tracking mobility becomes even more critical in Chile. To assess the impact on human mobility of the localized quarantines in Chile, we analyze a mobile phone dataset made available by Telefnica Chile, which comprises 31 billion eXtended Detail Records and 5.4 million users covering the period February 26th to September 20th, 2020. From these records, we derive three epidemiologically relevant metrics describing the mobility within and between comunas. The datasets made available can be used to fight the COVID-19 epidemics, particularly for localized quarantines' less understood effect.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
The ecological and evolutionary energetics of hunter-gatherer residential mobility,"Residential mobility is deeply entangled with all aspects of hunter-gatherer life ways, and is therefore an issue of central importance in hunter-gatherer studies. Hunter-gatherers vary widely in annual rates of residential mobility, and understanding the sources of this variation has long been of interest to anthropologists and archaeologists. Since mobility is, to a large extent, driven by the need for a continuous supply of food, a natural framework for addressing this question is provided by the metabolic theory of ecology. This provides a powerful framework for formulating formal testable hypotheses concerning evolutionary and ecological constraints on the scale and variation of hunter-gatherer residential mobility. We evaluate these predictions using extant data and show strong support for the hypotheses. We show that the overall scale of hunter-gatherer residential mobility is predicted by average human body size, and the limited capacity of mobile hunter-gatherers to store energy internally. We then show that the majority of variation in residential mobility observed across cultures is predicted by energy availability in local ecosystems. Our results demonstrate that large-scale evolutionary and ecological processes, common to all plants and animals, constrain hunter-gatherers in predictable ways as they move through territories to effectively exploit resources over the course of a year. Moreover, our results extend the scope of the metabolic theory of ecology by showing how it successfully predicts variation in the behavioral ecology of populations within a species.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Understanding everyday public transit travel habits: a measurement framework for the peakedness of departure time distributions,"Persuasive scholarship presents how individual daily travel habits implicate congestion, environmental pollution, and the travel experience. However, the empirical characteristics and dynamics of travel habits remain poorly understood. Quantifying both our individual travel habits and how these habits aggregate to form system-wide dynamics is of critical importance to enable the smart design of public transit systems that are better tailored to our daily mobility needs. We contribute to this need through the development and implementation of a new measurement framework capturing the 'peakedness' of users' departure time distributions. Departure time 'peakedness' reflects a user's tendency to repeatedly choose the same departure time for a given origin-destination trip, offering a clearer and more intuitive representation of regularity and habitual patterns compared to traditional metrics like standard deviation or entropy. Our framework demonstrates that system-wide departure time peakedness can be decomposed into individual users' departure time peakedness and the alignment of their peak times. This allows for a systematic analysis of both individual and collective behaviours. We apply our framework to departure time data from a 12-month period, encompassing 5,947,907 bus journeys made by 29,640 individuals across three urban networks within a large regional metropolis. Our findings reveal that departure time peakedness is more deeply tied to inherent, passenger-specific characteristics, such as passenger type, rather than external factors like weather or holidays. Additionally, individual-level departure time peakedness shows notable dynamics over time, indicating that habitual routines can evolve in the long term, while system-level peakedness exhibits remarkable long-term stability.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Human Mobility Disproportionately Extends PM2.5 Emission Exposure for Low Income Populations,"Ambient exposure to fine particulate matters of diameters smaller than 2.5m (PM2.5) has been identified as one critical cause for respiratory disease. Disparities in exposure to PM2.5 among income groups at individual residences are known to exist and are easy to calculate. Existing approaches for exposure assessment, however, do not capture the exposure implied by the dynamic mobility of city dwellers that accounts for a large proportion of the exposure outside homes. To overcome the challenge of gauging the exposure to PM2.5 for city dwellers, we analyzed billions of anonymized and privacy-enhanced location-based data generated by mobile phone users in Harris County, Texas, to characterize the mobility patterns of the populations and associated exposure. We introduce the metric for exposure extent based on the time people spent at places with the air pollutant and examine the disparities in mobility-based exposure across income groups. Our results show that PM2.5 emissions disproportionately expose low-income populations due to their mobility activities. People with higher-than-average income are exposed to lower levels of PM2.5 emissions. These disparities in mobility-based exposure are the result of frequent visits of low-income people to the industrial sectors of urban areas with high PM2.5 emissions, and the larger mobility scale of these people for life needs. The results inform about environmental justice and public health strategies, not only to reduce the overall PM2.5 exposure but also to mitigate the disproportional impacts on low-income populations. The findings also suggest that an integration of extensive fine-scale population mobility and pollution emissions data can unveil new insights into inequality in air pollution exposures at the urban scale.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Beyond Distance: Mobility Neural Embeddings Reveal Visible and Invisible Barriers in Urban Space,"Human mobility in cities is shaped not only by visible structures such as highways, rivers, and parks but also by invisible barriers rooted in socioeconomic segregation, uneven access to amenities, and administrative divisions. Yet identifying and quantifying these barriers at scale and their relative importance on people's movements remains a major challenge. Neural embedding models, originally developed for language, offer a powerful way to capture the complexity of human mobility from large-scale data. Here, we apply this approach to 25.4 million observed trajectories across 11 major U.S. cities, learning mobility embeddings that reveal how people move through urban space. These mobility embeddings define a functional distance between places, one that reflects behavioral rather than physical proximity, and allow us to detect barriers between neighborhoods that are geographically close but behaviorally disconnected. We find that the strongest predictors of these barriers are differences in access to amenities, administrative borders, and residential segregation by income and race. These invisible borders are concentrated in urban cores and persist across cities, spatial scales, and time periods. Physical infrastructure, such as highways and parks, plays a secondary but still significant role, especially at short distances. We also find that individuals who cross barriers tend to do so outside of traditional commuting hours and are more likely to live in areas with greater racial diversity, and higher transit use or income. Together, these findings reveal how spatial, social, and behavioral forces structure urban accessibility and provide a scalable framework to detect and monitor barriers in cities, with applications in planning, policy evaluation, and equity analysis.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
A model of human population motion,"We introduce a basic model for human mobility that accounts for the different dynamics arising from individuals embarking on short trips (and returning to their home locations) and individuals relocating to a new home. The differences between the two modes of motion comes to light on contrasting two recent studies, one tracking the geographical location of dollar bills \cite{brockmann}, the other that of mobile cell phones \cite{gonzalez}. Trips introduce two characteristic time scales; the time between trips, $$, and the duration of each trip, $$, and relocations introduces a third time scale, $T$, for the time between relocations. In practice, $T\sim{\rm years}$, $\sim{\rm months}$, and $\sim{\rm days}$, so the three time scales are widely separated. Traditionally, studies incorporating human motion assume only a single mode, using a generic rate to account for all types of motion.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Weak nodes detection in urban transport systems: Planning for resilience in Singapore,"The availability of massive data-sets describing human mobility offers the possibility to design simulation tools to monitor and improve the resilience of transport systems in response to traumatic events such as natural and man-made disasters (e.g. floods terroristic attacks, etc...). In this perspective, we propose ACHILLES, an application to model people's movements in a given transport system mode through a multiplex network representation based on mobility data. ACHILLES is a web-based application which provides an easy-to-use interface to explore the mobility fluxes and the connectivity of every urban zone in a city, as well as to visualize changes in the transport system resulting from the addition or removal of transport modes, urban zones, and single stops. Notably, our application allows the user to assess the overall resilience of the transport network by identifying its weakest node, i.e. Urban Achilles Heel, with reference to the ancient Greek mythology. To demonstrate the impact of ACHILLES for humanitarian aid we consider its application to a real-world scenario by exploring human mobility in Singapore in response to flood prevention.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Differences of communication activity and mobility patterns between urban and rural people,"Human mobility and other social activity patterns influence various aspects of society such as urban planning, traffic predictions, crisis resilience, and epidemic prevention. The behaviour of individuals, like their communication frequencies and movements, are shaped by societal and socio-economic factors. In addition, the differences in the geolocation of people as well as their gender and age cast effects on their activity patterns. In this study we focus on investigating these patterns by using mobile phone data, specifically the call detail records (CDRs), to analyze the social communication and mobility patterns of people. This dataset can provide us insight into the individual and population-level behaviours in rural and urban environments on a daily, weekly and seasonal basis. The results of our analyses show that in the urban areas people have high calling activity but low mobility, while in the rural areas they show the opposite behaviour, i.e. low calling activity combined with high mobility. Overall, there is a decreasing trend in people's mobility through the year even though their calling activity remained consistent except for the holidays during which time the communication frequency drops markedly. We have also observed that there are significant differences in the mobility between the work days and free days. Finally, the age and gender of individuals have also been observed to play a role in the seasonal patterns differently in urban and rural areas.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Lvy flights in human behavior and cognition,"Lvy flights represent the best strategy to randomly search for a target in an unknown environment, and have been widely observed in many animal species. Here, we inspect and discuss recent results concerning human behavior and cognition. Different studies have shown that human mobility can be described in terms of Lvy flights, while fresh evidence indicates that the same pattern accounts for human mental searches in online gambling sites. Thus, Lvy flights emerge as a unifying concept with broad cross-disciplinary implications. We argue that the ubiquity of such a pattern, both in behavior and cognition, suggests that the brain regions responsible for this behavior are likely to be evolutionarily old (i.e. no frontal cortex is involved), and that fMRI techniques might help to confirm this hypothesis.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Network percolation reveals adaptive bridges of the mobility network response to COVID-19,"Human mobility is crucial to understand the transmission pattern of COVID-19 on spatially embedded geographic networks. This pattern seems unpredictable, and the propagation appears unstoppable, resulting in over 350,000 death tolls in the U.S. by the end of 2020. Here, we create the spatiotemporal inter-county mobility network using 10 TB (Terabytes) trajectory data of 30 million smart devices in the U.S. in the first six months of 2020. We investigate its bound percolation by removing the weakly connected edges. The mobility network becomes vulnerable and prone to reach its criticality and thus experience surprisingly abrupt phase transitions. Despite the complex behaviors of the mobility network, we devised a novel approach to identify a small, manageable set of recurrent critical bridges, connecting the giant component and the second-largest component. These adaptive links, located across the United States, played a key role as valves connecting components in divisions and regions during the pandemic. Beyond, our numerical results unveil that network characteristics determine the critical thresholds and the bridge locations. The findings provide new insights into managing and controlling the connectivity of mobility networks during unprecedented disruptions. The work can also potentially offer practical future infectious diseases both globally and locally.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
"One City, Two Tales: Using Mobility Networks to Understand Neighborhood Resilience and Fragility during the COVID-19 Pandemic","What predicts a neighborhood's resilience and adaptability to essential public health policies and shelter-in-place regulations that prevent the harmful spread of COVID-19? To answer this question, in this paper we present a novel application of human mobility patterns and human behavior in a network setting. We analyze mobility data in New York City over two years, from January 2019 to December 2020, and create weekly mobility networks between Census Block Groups by aggregating Point of Interest level visit patterns. Our results suggest that both the socioeconomic and geographic attributes of neighborhoods significantly predict neighborhood adaptability to the shelter-in-place policies active at that time. That is, our findings and simulation results reveal that in addition to factors such as race, education, and income, geographical attributes such as access to amenities in a neighborhood that satisfy community needs were equally important factors for predicting neighborhood adaptability and the spread of COVID-19. The results of our study provide insights that can enhance urban planning strategies that contribute to pandemic alleviation efforts, which in turn may help urban areas become more resilient to exogenous shocks such as the COVID-19 pandemic.","cat:physics.soc-ph AND (""human mobility"" OR ""travel patterns"" OR transportation)",0
Character-Centric Storytelling,"Sequential vision-to-language or visual storytelling has recently been one of the areas of focus in computer vision and language modeling domains. Though existing models generate narratives that read subjectively well, there could be cases when these models miss out on generating stories that account and address all prospective human and animal characters in the image sequences. Considering this scenario, we propose a model that implicitly learns relationships between provided characters and thereby generates stories with respective characters in scope. We use the VIST dataset for this purpose and report numerous statistics on the dataset. Eventually, we describe the model, explain the experiment and discuss our current status and future work.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Causal Micro-Narratives,"We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Pipeline for Creative Visual Storytelling,"Computational visual storytelling produces a textual description of events and interpretations depicted in a sequence of images. These texts are made possible by advances and cross-disciplinary approaches in natural language processing, generation, and computer vision. We define a computational creative visual storytelling as one with the ability to alter the telling of a story along three aspects: to speak about different environments, to produce variations based on narrative goals, and to adapt the narrative to the audience. These aspects of creative storytelling and their effect on the narrative have yet to be explored in visual storytelling. This paper presents a pipeline of task-modules, Object Identification, Single-Image Inferencing, and Multi-Image Narration, that serve as a preliminary design for building a creative visual storyteller. We have piloted this design for a sequence of images in an annotation task. We present and analyze the collected corpus and describe plans towards automation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Hierarchical Approach for Visual Storytelling Using Image Description,"One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories. In this paper, we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem. To better help our network maintain this context while also generating long and diverse sentences, we incorporate natural language image descriptions along with the images themselves to generate each story sentence. We evaluate our system on the Visual Storytelling (VIST) dataset and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics. The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
An Analysis of Emotion Communication Channels in Fan Fiction: Towards Emotional Storytelling,"Centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology. The research in automatic storytelling has recently turned towards emotional storytelling, in which characters' emotions play an important role in the plot development. However, these studies mainly use emotion to generate propositional statements in the form ""A feels affection towards B"" or ""A confronts B"". At the same time, emotional behavior does not boil down to such propositional descriptions, as humans display complex and highly variable patterns in communicating their emotions, both verbally and non-verbally. In this paper, we analyze how emotions are expressed non-verbally in a corpus of fan fiction short stories. Our analysis shows that stories written by humans convey character emotions along various non-verbal channels. We find that some non-verbal channels, such as facial expressions and voice characteristics of the characters, are more strongly associated with joy, while gestures and body postures are more likely to occur with trust. Based on our analysis, we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters' emotions.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Using Inter-Sentence Diverse Beam Search to Reduce Redundancy in Visual Storytelling,"Visual storytelling includes two important parts: coherence between the story and images as well as the story structure. For image to text neural network models, similar images in the sequence would provide close information for story generator to obtain almost identical sentence. However, repeatedly narrating same objects or events will undermine a good story structure. In this paper, we proposed an inter-sentence diverse beam search to generate a more expressive story. Comparing to some recent models of visual storytelling task, which generate story without considering the generated sentence of the previous picture, our proposed method can avoid generating identical sentence even given a sequence of similar pictures.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology,"In this paper, we collect an anthology of 100 visual stories from authors who participated in our systematic creative process of improvised story-building based on image sequences. Following close reading and thematic analysis of our anthology, we present five themes that characterize the variations found in this creative visual storytelling process: (1) Narrating What is in Vision vs. Envisioning; (2) Dynamically Characterizing Entities/Objects; (3) Sensing Experiential Information About the Scenery; (4) Modulating the Mood; (5) Encoding Narrative Biases. In understanding the varied ways that people derive stories from images, we offer considerations for collecting story-driven training data to inform automatic story generation. In correspondence with each theme, we envision narrative intelligence criteria for computational visual storytelling as: creative, reliable, expressive, grounded, and responsible. From these criteria, we discuss how to foreground creative expression, account for biases, and operate in the bounds of visual storyworlds.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Complicating the Social Networks for Better Storytelling: An Empirical Study of Chinese Historical Text and Novel,"Digital humanities is an important subject because it enables developments in history, literature, and films. In this paper, we perform an empirical study of a Chinese historical text, Records of the Three Kingdoms (\textit{Records}), and a historical novel of the same story, Romance of the Three Kingdoms (\textit{Romance}). We employ natural language processing techniques to extract characters and their relationships. Then, we characterize the social networks and sentiments of the main characters in the historical text and the historical novel. We find that the social network in \textit{Romance} is more complex and dynamic than that of \textit{Records}, and the influence of the main characters differs. These findings shed light on the different styles of storytelling in the two literary genres and how the historical novel complicates the social networks of characters to enrich the literariness of the story.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Towards annotation of text worlds in a literary work,"Literary texts are usually rich in meanings and their interpretation complicates corpus studies and automatic processing. There have been several attempts to create collections of literary texts with annotation of literary elements like the author's speech, characters, events, scenes etc. However, they resulted in small collections and standalone rules for annotation. The present article describes an experiment on lexical annotation of text worlds in a literary work and quantitative methods of their comparison. The experiment shows that for a well-agreed tag assignment annotation rules should be set much more strictly. However, if borders between text worlds and other elements are the result of a subjective interpretation, they should be modeled as fuzzy entities.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"""Once Upon a Time..."" Literary Narrative Connectedness Progresses with Grade Level: Potential Impact on Reading Fluency and Literacy Skills","Selecting an appropriate book is crucial for fostering reading habits in children. While children exhibit varying levels of complexity when generating oral narratives, the question arises: do children's books also differ in narrative complexity? This study explores the narrative dynamics of literary texts used in schools, focusing on how their complexity evolves across different grade levels. Using Word-Recurrence Graph Analysis, we examined a dataset of 1,627 literary texts spanning 13 years of education. The findings reveal significant exponential growth in connectedness, particularly during the first three years of schooling, mirroring patterns observed in children's oral narratives. These results highlight the potential of literary texts as a tool to support the development of literacy skills.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Evaluating the Creativity of LLMs in Persian Literary Text Generation,"Large language models (LLMs) have demonstrated notable creative abilities in generating literary texts, including poetry and short stories. However, prior research has primarily centered on English, with limited exploration of non-English literary traditions and without standardized methods for assessing creativity. In this paper, we evaluate the capacity of LLMs to generate Persian literary text enriched with culturally relevant expressions. We build a dataset of user-generated Persian literary spanning 20 diverse topics and assess model outputs along four creativity dimensions-originality, fluency, flexibility, and elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce evaluation costs, we adopt an LLM as a judge for automated scoring and validate its reliability against human judgments using intraclass correlation coefficients, observing strong agreement. In addition, we analyze the models' ability to understand and employ four core literary devices: simile, metaphor, hyperbole, and antithesis. Our results highlight both the strengths and limitations of LLMs in Persian literary text generation, underscoring the need for further refinement.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Survey on Sentiment and Emotion Analysis for Computational Literary Studies,"Emotions are a crucial part of compelling narratives: literature tells us about people with goals, desires, passions, and intentions. Emotion analysis is part of the broader and larger field of sentiment analysis, and receives increasing attention in literary studies. In the past, the affective dimension of literature was mainly studied in the context of literary hermeneutics. However, with the emergence of the research field known as Digital Humanities (DH), some studies of emotions in a literary context have taken a computational turn. Given the fact that DH is still being formed as a field, this direction of research can be rendered relatively new. In this survey, we offer an overview of the existing body of research on emotion analysis as applied to literature. The research under review deals with a variety of topics including tracking dramatic changes of a plot development, network analysis of a literary text, and understanding the emotionality of texts, among other topics.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling,"The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct and controlled comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more progressive in terms of gender roles and sexuality than those written by humans. While AI narratives with default settings and no additional prompting can occasionally provide innovative plot twists, they offer less imaginative scenarios and rhetoric than human-authored texts. The proposed framework argues that fiction can be used as a window into human and AI-based collective imaginary and social dimensions.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Comparative Computational Analysis of Global Structure in Canonical, Non-Canonical and Non-Literary Texts","This study investigates global properties of literary and non-literary texts. Within the literary texts, a distinction is made between canonical and non-canonical works. The central hypothesis of the study is that the three text types (non-literary, literary/canonical and literary/non-canonical) exhibit systematic differences with respect to structural design features as correlates of aesthetic responses in readers. To investigate these differences, we compiled a corpus containing texts of the three categories of interest, the Jena Textual Aesthetics Corpus. Two aspects of global structure are investigated, variability and self-similar (fractal) patterns, which reflect long-range correlations along texts. We use four types of basic observations, (i) the frequency of POS-tags per sentence, (ii) sentence length, (iii) lexical diversity in chunks of text, and (iv) the distribution of topic probabilities in chunks of texts. These basic observations are grouped into two more general categories, (a) the low-level properties (i) and (ii), which are observed at the level of the sentence (reflecting linguistic decoding), and (b) the high-level properties (iii) and (iv), which are observed at the textual level (reflecting comprehension). The basic observations are transformed into time series, and these time series are subject to multifractal detrended fluctuation analysis (MFDFA). Our results show that low-level properties of texts are better discriminators than high-level properties, for the three text types under analysis. Canonical literary texts differ from non-canonical ones primarily in terms of variability. Fractality seems to be a universal feature of text, more pronounced in non-literary than in literary texts. Beyond the specific results of the study, we intend to open up new perspectives on the experimental study of textual aesthetics.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Lemmatization of Historical Old Literary Finnish Texts in Modern Orthography,"Texts written in Old Literary Finnish represent the first literary work ever written in Finnish starting from the 16th century. There have been several projects in Finland that have digitized old publications and made them available for research use. However, using modern NLP methods in such data poses great challenges. In this paper we propose an approach for simultaneously normalizing and lemmatizing Old Literary Finnish into modern spelling. Our best model reaches to 96.3\% accuracy in texts written by Agricola and 87.7\% accuracy in other contemporary out-of-domain text. Our method has been made freely available on Zenodo and Github.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Story Grammar Semantic Matching for Literary Study,"In Natural Language Processing (NLP), semantic matching algorithms have traditionally relied on the feature of word co-occurrence to measure semantic similarity. While this feature approach has proven valuable in many contexts, its simplistic nature limits its analytical and explanatory power when used to understand literary texts. To address these limitations, we propose a more transparent approach that makes use of story structure and related elements. Using a BERT language model pipeline, we label prose and epic poetry with story element labels and perform semantic matching by only considering these labels as features. This new method, Story Grammar Semantic Matching, guides literary scholars to allusions and other semantic similarities across texts in a way that allows for characterizing patterns and literary technique.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Annotating Character Relationships in Literary Texts,"We present a dataset of manually annotated relationships between characters in literary texts, in order to support the training and evaluation of automatic methods for relation type prediction in this domain (Makazhanov et al., 2014; Kokkinakis, 2013) and the broader computational analysis of literary character (Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and Gurevych, 2015). In this work, we solicit annotations from workers on Amazon Mechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_ on four dimensions of interest: for a given pair of characters, we collect judgments as to the coarse-grained category (professional, social, familial), fine-grained category (friend, lover, parent, rival, employer), and affinity (positive, negative, neutral) that describes their primary relationship in a text. We do not assume that this relationship is static; we also collect judgments as to whether it changes at any point in the course of the text.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Entropic analysis of the role of words in literary texts,"Beyond the local constraints imposed by grammar, words concatenated in long sequences carrying a complex message show statistical regularities that may reflect their linguistic role in the message. In this paper, we perform a systematic statistical analysis of the use of words in literary English corpora. We show that there is a quantitative relation between the role of content words in literary English and the Shannon information entropy defined over an appropriate probability distribution. Without assuming any previous knowledge about the syntactic structure of language, we are able to cluster certain groups of words according to their specific role in the text.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution in Literary Texts,"We present the Project Dialogism Novel Corpus, or PDNC, an annotated dataset of quotations for English literary texts. PDNC contains annotations for 35,978 quotations across 22 full-length novels, and is by an order of magnitude the largest corpus of its kind. Each quotation is annotated for the speaker, addressees, type of quotation, referring expression, and character mentions within the quotation text. The annotated attributes allow for a comprehensive evaluation of models of quotation attribution and coreference for literary texts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
What Level of Quality can Neural Machine Translation Attain on Literary Text?,"Given the rise of a new approach to MT, Neural MT (NMT), and its promising performance on different text types, we assess the translation quality it can attain on what is perceived to be the greatest challenge for MT: literary text. Specifically, we target novels, arguably the most popular type of literary text. We build a literary-adapted NMT system for the English-to-Catalan translation direction and evaluate it against a system pertaining to the previous dominant paradigm in MT: statistical phrase-based MT (PBSMT). To this end, for the first time we train MT systems, both NMT and PBSMT, on large amounts of literary text (over 100 million words) and evaluate them on a set of twelve widely known novels spanning from the the 1920s to the present day. According to the BLEU automatic evaluation metric, NMT is significantly better than PBSMT (p < 0.01) on all the novels considered. Overall, NMT results in a 11% relative improvement (3 points absolute) over PBSMT. A complementary human evaluation on three of the books shows that between 17% and 34% of the translations, depending on the book, produced by NMT (versus 8% and 20% with PBSMT) are perceived by native speakers of the target language to be of equivalent quality to translations produced by a professional human translator.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models,"Large Language Models (LLMs) excel in understanding and generating text but struggle with providing professional literary criticism for works with profound thoughts and complex narratives. This paper proposes GLASS (Greimas Literary Analysis via Semiotic Square), a structured analytical framework based on Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth literary analysis. GLASS facilitates the rapid dissection of narrative structures and deep meanings in narrative works. We propose the first dataset for GSS-based literary criticism, featuring detailed analyses of 48 works. Then we propose quantitative metrics for GSS-based literary criticism using the LLM-as-a-judge paradigm. Our framework's results, compared with expert criticism across multiple works and LLMs, show high performance. Finally, we applied GLASS to 39 classic works, producing original and high-quality analyses that address existing research gaps. This research provides an AI-based tool for literary research and education, offering insights into the cognitive mechanisms underlying literary engagement.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Guiding Generative Storytelling with Knowledge Graphs,"Large Language Models (LLMs) have shown great potential in automated story generation, but challenges remain in maintaining long-form coherence and providing users with intuitive and effective control. Retrieval-Augmented Generation (RAG) has proven effective in reducing hallucinations in text generation; however, the use of structured data to support generative storytelling remains underexplored. This paper investigates how knowledge graphs (KGs) can enhance LLM-based storytelling by improving narrative quality and enabling user-driven modifications. We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants. Participants created their own story prompts, generated stories, and edited knowledge graphs to shape their narratives. Through quantitative and qualitative analysis, our findings demonstrate that knowledge graphs significantly enhance story quality in action-oriented and structured narratives within our system settings. Additionally, editing the knowledge graph increases users' sense of control, making storytelling more engaging, interactive, and playful.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Temporal Embeddings and Transformer Models for Narrative Text Understanding,"We present two deep learning approaches to narrative text understanding for character relationship modelling. The temporal evolution of these relations is described by dynamic word embeddings, that are designed to learn semantic changes over time. An empirical analysis of the corresponding character trajectories shows that such approaches are effective in depicting dynamic evolution. A supervised learning approach based on the state-of-the-art transformer model BERT is used instead to detect static relations between characters. The empirical validation shows that such events (e.g., two characters belonging to the same family) might be spotted with good accuracy, even when using automatically annotated data. This provides a deeper understanding of narrative plots based on the identification of key facts. Standard clustering techniques are finally used for character de-aliasing, a necessary pre-processing step for both approaches. Overall, deep learning models appear to be suitable for narrative text understanding, while also providing a challenging and unexploited benchmark for general natural language understanding.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Literary Evidence Retrieval via Long-Context Language Models,"How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Computational analyses of the topics, sentiments, literariness, creativity and beauty of texts in a large Corpus of English Literature","The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich source of textual data for research in digital humanities, computational linguistics or neurocognitive poetics. In this study we address differences among the different literature categories in GLEC, as well as differences between authors. We report the results of three studies providing i) topic and sentiment analyses for six text categories of GLEC (i.e., children and youth, essays, novels, plays, poems, stories) and its >100 authors, ii) novel measures of semantic complexity as indices of the literariness, creativity and book beauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two experiments on text classification and authorship recognition using novel features of semantic complexity. The data on two novel measures estimating a text's literariness, intratextual variance and stepwise distance (van Cranenburgh et al., 2019) revealed that plays are the most literary texts in GLEC, followed by poems and novels. Computation of a novel index of text creativity (Gray et al., 2016) revealed poems and plays as the most creative categories with the most creative authors all being poets (Milton, Pope, Keats, Byron, or Wordsworth). We also computed a novel index of perceived beauty of verbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the theoretically most beautiful of Austen's novels. Finally, we demonstrate that these novel measures of semantic complexity are important features for text classification and authorship recognition with overall predictive accuracies in the range of .75 to .97. Our data pave the way for future computational and empirical studies of literature or experiments in reading psychology and offer multiple baselines and benchmarks for analysing and validating other book corpora.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Multifractal analysis of sentence lengths in English literary texts,"This paper presents analysis of 30 literary texts written in English by different authors. For each text, there were created time series representing length of sentences in words and analyzed its fractal properties using two methods of multifractal analysis: MFDFA and WTMM. Both methods showed that there are texts which can be considered multifractal in this representation but a majority of texts are not multifractal or even not fractal at all. Out of 30 books, only a few have so-correlated lengths of consecutive sentences that the analyzed signals can be interpreted as real multifractals. An interesting direction for future investigations would be identifying what are the specific features which cause certain texts to be multifractal and other to be monofractal or even not fractal at all.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A multi-level multi-label text classification dataset of 19th century Ottoman and Russian literary and critical texts,"This paper introduces a multi-level, multi-label text classification dataset comprising over 3000 documents. The dataset features literary and critical texts from 19th-century Ottoman Turkish and Russian. It is the first study to apply large language models (LLMs) to this dataset, sourced from prominent literary periodicals of the era. The texts have been meticulously organized and labeled. This was done according to a taxonomic framework that takes into account both their structural and semantic attributes. Articles are categorized and tagged with bibliometric metadata by human experts. We present baseline classification results using a classical bag-of-words (BoW) naive Bayes model and three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that in certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs), emphasizing the need for additional research, especially in low-resource language settings. This dataset is expected to be a valuable resource for researchers in natural language processing and machine learning, especially for historical and low-resource languages. The dataset is publicly available^1.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Simple Text Analytics Model To Assist Literary Criticism: comparative approach and example on James Joyce against Shakespeare and the Bible,"Literary analysis, criticism or studies is a largely valued field with dedicated journals and researchers which remains mostly within the humanities scope. Text analytics is the computer-aided process of deriving information from texts. In this article we describe a simple and generic model for performing literary analysis using text analytics. The method relies on statistical measures of: 1) token and sentence sizes and 2) Wordnet synset features. These measures are then used in Principal Component Analysis where the texts to be analyzed are observed against Shakespeare and the Bible, regarded as reference literature. The model is validated by analyzing selected works from James Joyce (1882-1941), one of the most important writers of the 20th century. We discuss the consistency of this approach, the reasons why we did not use other techniques (e.g. part-of-speech tagging) and the ways by which the analysis model might be adapted and enhanced.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model,"The exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling. However, automated data analysis systems still face challenges in leveraging large language models (LLMs) for data insight discovery, augmented analysis, and data storytelling. This paper introduces the Multidimensional Data Storytelling Framework (MDSF) based on large language models for automated insight generation and context-aware storytelling. The framework incorporates advanced preprocessing techniques, augmented analysis algorithms, and a unique scoring mechanism to identify and prioritize actionable insights. The use of fine-tuned LLMs enhances contextual understanding and generates narratives with minimal manual intervention. The architecture also includes an agent-based mechanism for real-time storytelling continuation control. Key findings reveal that MDSF outperforms existing methods across various datasets in terms of insight ranking accuracy, descriptive quality, and narrative coherence. The experimental evaluation demonstrates MDSF's ability to automate complex analytical tasks, reduce interpretive biases, and improve user satisfaction. User studies further underscore its practical utility in enhancing content structure, conclusion extraction, and richness of detail.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation","Extracting metaphors and analogies from free text requires high-level reasoning abilities such as abstraction and language understanding. Our study focuses on the extraction of the concepts that form metaphoric analogies in literary texts. To this end, we construct a novel dataset in this domain with the help of domain experts. We compare the out-of-the-box ability of recent large language models (LLMs) to structure metaphoric mappings from fragments of texts containing proportional analogies. The models are further evaluated on the generation of implicit elements of the analogy, which are indirectly suggested in the texts and inferred by human readers. The competitive results obtained by LLMs in our experiments are encouraging and open up new avenues such as automatically extracting analogies and metaphors from text instead of investing resources in domain experts to manually label data.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Perspective on Literary Metaphor in the Context of Generative AI,"At the intersection of creative text generation and literary theory, this study explores the role of literary metaphor and its capacity to generate a range of meanings. In this regard, literary metaphor is vital to the development of any particular language. To investigate whether the inclusion of original figurative language improves textual quality, we trained an LSTM-based language model in Afrikaans. The network produces phrases containing compellingly novel figures of speech. Specifically, the emphasis falls on how AI might be utilised as a defamiliarisation technique, which disrupts expected uses of language to augment poetic expression. Providing a literary perspective on text generation, the paper raises thought-provoking questions on aesthetic value, interpretation and evaluation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Feature-based analysis of oral narratives from Afrikaans and isiXhosa children,"Oral narrative skills are strong predictors of later literacy development. This study examines the features of oral narratives from children who were identified by experts as requiring intervention. Using simple machine learning methods, we analyse recorded stories from four- and five-year-old Afrikaans- and isiXhosa-speaking children. Consistent with prior research, we identify lexical diversity (unique words) and length-based features (mean utterance length) as indicators of typical development, but features like articulation rate prove less informative. Despite cross-linguistic variation in part-of-speech patterns, the use of specific verbs and auxiliaries associated with goal-directed storytelling is correlated with a reduced likelihood of requiring intervention. Our analysis of two linguistically distinct languages reveals both language-specific and shared predictors of narrative proficiency, with implications for early assessment in multilingual contexts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Data-Oriented Model of Literary Language,"We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features, and explains 76.0 % of the variation in literary ratings.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Visual Storytelling,"We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection","Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype","Arabic Rhetoric is the field of Arabic linguistics which governs the art and science of conveying a message with greater beauty, impact and persuasiveness. The field is as ancient as the Arabic language itself and is found extensively in classical and contemporary Arabic poetry, free verse and prose. In practical terms, it is the intelligent use of word order, figurative speech and linguistic embellishments to enhance message delivery. Despite the volumes that have been written about it and the high status accorded to it, there is no way to objectively know whether a speaker or writer has used Arabic rhetoric in a given text, to what extent, and why. There is no objective way to compare the use of Arabic rhetoric across genres, authors or epochs. It is impossible to know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary genres are richer in Arabic rhetoric. The aim of the current study was to devise a way to measure the density of the literary devices which constitute Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself. A comprehensive list of 84 of the commonest literary devices and their definitions was compiled. A system of identifying literary devices in texts was constructed. A method of calculating the density of literary devices based on the morpheme count of the text was utilised. Four electronic tools and an analogue tool were created to support the calculation of an Arabic text's rhetorical literary device density, including a website and online calculator. Additionally, a technique of reporting the distribution of literary devices used across the three sub-domains of Arabic rhetoric was created. The output of this project is a working tool which can accurately report the density of Arabic rhetoric in any Arabic text or speech.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese Poetry,"The birth and rapid development of large language models (LLMs) have caused quite a stir in the field of literature. Once considered unattainable, AI's role in literary creation is increasingly becoming a reality. In genres such as poetry, jokes, and short stories, numerous AI tools have emerged, offering refreshing new perspectives. However, it's difficult to further improve the quality of these works. This is primarily because understanding and appreciating a good literary work involves a considerable threshold, such as knowledge of literary theory, aesthetic sensibility, interdisciplinary knowledge. Therefore, authoritative data in this area is quite lacking. Additionally, evaluating literary works is often complex and hard to fully quantify, which directly hinders the further development of AI creation.   To address this issue, this paper attempts to explore the mysteries of literary texts from the perspective of LLMs, using ancient Chinese poetry as an example for experimentation. First, we collected a variety of ancient poems from different sources and had experts annotate a small portion of them. Then, we designed a range of comprehension metrics based on LLMs to evaluate all these poems. Finally, we analyzed the correlations and differences between various poem collections to identify literary patterns. Through our experiments, we observed a series of enlightening phenomena that provide technical support for the future development of high-level literary creation based on LLMs.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Protagonists' Tagger in Literary Domain -- New Datasets and a Method for Person Entity Linkage,"Semantic annotation of long texts, such as novels, remains an open challenge in Natural Language Processing (NLP). This research investigates the problem of detecting person entities and assigning them unique identities, i.e., recognizing people (especially main characters) in novels. We prepared a method for person entity linkage (named entity recognition and disambiguation) and new testing datasets. The datasets comprise 1,300 sentences from 13 classic novels of different genres that a novel reader had manually annotated. Our process of identifying literary characters in a text, implemented in protagonistTagger, comprises two stages: (1) named entity recognition (NER) of persons, (2) named entity disambiguation (NED) - matching each recognized person with the literary character's full name, based on approximate text matching. The protagonistTagger achieves both precision and recall of above 83% on the prepared testing sets. Finally, we gathered a corpus of 13 full-text novels tagged with protagonistTagger that comprises more than 35,000 mentions of literary characters.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network","""Art is the lie that enables us to realize the truth."" - Pablo Picasso. For centuries, humans have dedicated themselves to producing arts to convey their imagination. The advancement in technology and deep learning in particular, has caught the attention of many researchers trying to investigate whether art generation is possible by computers and algorithms. Using generative adversarial networks (GANs), applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized. This survey takes a comprehensive look at the recent works using GANs for generating visual arts, music, and literary text. A performance comparison and description of the various GAN architecture are also presented. Finally, some of the key challenges in art generation using GANs are highlighted along with recommendations for future work.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Collaborative Storytelling with Large-scale Neural Language Models,"Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present qualitative evaluation of our system's capabilities.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
From Image Captioning to Visual Storytelling,"Visual Storytelling is a challenging multimodal task between Vision & Language, where the purpose is to generate a story for a stream of images. Its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent. The aim of this work is to balance between these aspects, by treating Visual Storytelling as a superset of Image Captioning, an approach quite different compared to most of prior relevant studies. This means that we firstly employ a vision-to-language model for obtaining captions of the input images, and then, these captions are transformed into coherent narratives using language-to-language methods. Our multifarious evaluation shows that integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. In addition, compared to numerous previous studies, this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested. Lastly, we propose a new metric/tool, named ideality, that can be used to simulate how far some results are from an oracle model, and we apply it to emulate human-likeness in visual storytelling.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Variation of word frequencies in Russian literary texts,"We study the variation of word frequencies in Russian literary texts. Our findings indicate that the standard deviation of a word's frequency across texts depends on its average frequency according to a power law with exponent $0.62,$ showing that the rarer words have a relatively larger degree of frequency volatility (i.e., ""burstiness"").   Several latent factors models have been estimated to investigate the structure of the word frequency distribution. The dependence of a word's frequency volatility on its average frequency can be explained by the asymmetry in the distribution of latent factors.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI,"Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
LLM for Comparative Narrative Analysis,"In this paper, we conducted a Multi-Perspective Comparative Narrative Analysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied identical prompts and evaluated their outputs on specific tasks, ensuring an equitable and unbiased comparison between various LLMs. Our study revealed that the three LLMs generated divergent responses to the same prompt, indicating notable discrepancies in their ability to comprehend and analyze the given task. Human evaluation was used as the gold standard, evaluating four perspectives to analyze differences in LLM performance.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Extracting Networks of Characters and Places from Written Works with CHAPLIN,"We are proposing a tool able to gather information on social networks from narrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction Network, implemented in VB.NET. Characters and places of the narrative works are extracted in a list of raw words. Aided by the interface, the user selects names out of them. After this choice, the tool allows the user to enter some parameters, and, according to them, creates a network where the nodes are the characters and places, and the edges their interactions. Edges are labelled by performances. The output is a GV file, written in the DOT graph scripting language, which is rendered by means of the free open source software Graphviz.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Incorporating Textual Evidence in Visual Storytelling,"Previous work on visual storytelling mainly focused on exploring image sequence as evidence for storytelling and neglected textual evidence for guiding story generation. Motivated by human storytelling process which recalls stories for familiar images, we exploit textual evidence from similar images to help generate coherent and meaningful stories. To pick the images which may provide textual experience, we propose a two-step ranking method based on image object recognition techniques. To utilize textual information, we design an extended Seq2Seq model with two-channel encoder and attention. Experiments on the VIST dataset show that our method outperforms state-of-the-art baseline models without heavy engineering.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories,"The improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling. It is therefore important to quantitatively understand the nature of generated stories, and how they differ from human storytelling. We augment the Reddit WritingPrompts dataset with short stories generated by GPT-3.5, given the same prompts. We quantify and compare the emotional and descriptive features of storytelling from both generative processes, human and machine, along a set of six dimensions. We find that generated stories differ significantly from human stories along all six dimensions, and that human and machine generations display similar biases when grouped according to the narrative point-of-view and gender of the main protagonist. We release our dataset and code at https://github.com/KristinHuangg/gpt-writing-prompts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Affect as a proxy for literary mood,"We propose to use affect as a proxy for mood in literary texts. In this study, we explore the differences in computationally detecting tone versus detecting mood. Methodologically we utilize affective word embeddings to look at the affective distribution in different text segments. We also present a simple yet efficient and effective method of enhancing emotion lexicons to take both semantic shift and the domain of the text into account producing real-world congruent results closely matching both contemporary and modern qualitative analyses.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Computational Storytelling and Emotions: A Survey,"Storytelling has always been vital for human nature. From ancient times, humans have used stories for several objectives including entertainment, advertisement, and education. Various analyses have been conducted by researchers and creators to determine the way of producing good stories. The deep relationship between stories and emotions is a prime example. With the advancement in deep learning technology, computers are expected to understand and generate stories. This survey paper is intended to summarize and further contribute to the development of research being conducted on the relationship between stories and emotions. We believe creativity research is not to replace humans with computers, but to find a way of collaboration between humans and computers to enhance the creativity. With the intention of creating a new intersection between computational storytelling research and human creative writing, we introduced creative techniques used by professional storytellers.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models,"We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus, we extract activation patterns from 355 million parameters across 32,768 neurons in late layers. We find 27,122 statistically significant discriminative neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic ablation studies, we discover a paradoxical result: while these neurons correlate with literary text during analysis, removing them often improves rather than degrades generated prose quality. Specifically, ablating 50 high-discriminating neurons yields a 25.7% improvement in literary style metrics. This demonstrates a critical gap between observational correlation and causal necessity in neural networks. Our findings challenge the assumption that neurons which activate on desirable inputs will produce those outputs during generation, with implications for mechanistic interpretability research and AI alignment.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Continuous sentiment scores for literary and multilingual contexts,"Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?,"Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
DREAMT -- Embodied Motivational Conversational Storytelling,"Storytelling is fundamental to language, including culture, conversation and communication in their broadest senses. It thus emerges as an essential component of intelligent systems, including systems where natural language is not a primary focus or where we do not usually think of a story being involved. In this paper we explore the emergence of storytelling as a requirement in embodied conversational agents, including its role in educational and health interventions, as well as in a general-purpose computer interface for people with disabilities or other constraints that prevent the use of traditional keyboard and speech interfaces. We further present a characterization of storytelling as an inventive fleshing out of detail according to a particular personal perspective, and propose the DREAMT model to focus attention on the different layers that need to be present in a character-driven storytelling system. Most if not all aspects of the DREAMT model have arisen from or been explored in some aspect of our implemented research systems, but currently only at a primitive and relatively unintegrated level. However, this experience leads us to formalize and elaborate the DREAMT model mnemonically as follows: - Description/Dialogue/Definition/Denotation - Realization/Representation/Role - Explanation/Education/Entertainment - Actualization/Activation - Motivation/Modelling - Topicalization/Transformation","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Framing Analysis of Health-Related Narratives: Conspiracy versus Mainstream Media,"Understanding how online media frame issues is crucial due to their impact on public opinion. Research on framing using natural language processing techniques mainly focuses on specific content features in messages and neglects their narrative elements. Also, the distinction between framing in different sources remains an understudied problem. We address those issues and investigate how the framing of health-related topics, such as COVID-19 and other diseases, differs between conspiracy and mainstream websites. We incorporate narrative information into the framing analysis by introducing a novel frame extraction approach based on semantic graphs. We find that health-related narratives in conspiracy media are predominantly framed in terms of beliefs, while mainstream media tend to present them in terms of science. We hope our work offers new ways for a more nuanced frame analysis.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts,"Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT,"This study investigates the internal mechanisms of BERT, a transformer-based large language model, with a focus on its ability to cluster narrative content and authorial style across its layers. Using a dataset of narratives developed via GPT-4, featuring diverse semantic content and stylistic variations, we analyze BERT's layerwise activations to uncover patterns of localized neural processing. Through dimensionality reduction techniques such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that BERT exhibits strong clustering based on narrative content in its later layers, with progressively compact and distinct clusters. While strong stylistic clustering might occur when narratives are rephrased into different text types (e.g., fables, sci-fi, kids' stories), minimal clustering is observed for authorial style specific to individual writers. These findings highlight BERT's prioritization of semantic content over stylistic features, offering insights into its representational capabilities and processing hierarchy. This study contributes to understanding how transformer models like BERT encode linguistic information, paving the way for future interdisciplinary research in artificial intelligence and cognitive neuroscience.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
ComicScene154: A Scene Dataset for Comic Analysis,"Comics offer a compelling yet under-explored domain for computational narrative analysis, combining text and imagery in ways distinct from purely textual or audiovisual media. We introduce ComicScene154, a manually annotated dataset of scene-level narrative arcs derived from public-domain comic books spanning diverse genres. By conceptualizing comics as an abstraction for narrative-driven, multimodal data, we highlight their potential to inform broader research on multi-modal storytelling. To demonstrate the utility of ComicScene154, we present a baseline scene segmentation pipeline, providing an initial benchmark that future studies can build upon. Our results indicate that ComicScene154 constitutes a valuable resource for advancing computational methods in multimodal narrative understanding and expanding the scope of comic analysis within the Natural Language Processing community.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Sentiment Progression based Searching and Indexing of Literary Textual Artefacts,"Literary artefacts are generally indexed and searched based on titles, meta data and keywords over the years. This searching and indexing works well when user/reader already knows about that particular creative textual artefact or document. This indexing and search hardly takes into account interest and emotional makeup of readers and its mapping to books. When a person is looking for a literary textual artefact, he/she might be looking for not only information but also to seek the joy of reading. In case of literary artefacts, progression of emotions across the key events could prove to be the key for indexing and searching. In this paper, we establish clusters among literary artefacts based on computational relationships among sentiment progressions using intelligent text analysis. We have created a database of 1076 English titles + 20 Marathi titles and also used database http://www.cs.cmu.edu/~dbamman/booksummaries.html with 16559 titles and their summaries. We have proposed Sentiment Progression based Indexing for searching and recommending books. This can be used to create personalized clusters of book titles of interest to readers. The analysis clearly suggests better searching and indexing when we are targeting book lovers looking for a particular type of book or creative artefact. This indexing and searching can find many real-life applications for recommending books.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Classic4Children: Adapting Chinese Literary Classics for Children with Large Language Model,"Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children's reading preferences (\ie, vivid character portrayals, concise narrative structures, and appropriate readability), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters' personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children's reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves automatic and human evaluation performance.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs","Coreference annotation and resolution is a vital component of computational literary studies. However, it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Towards Discourse Parsing-inspired Semantic Storytelling,"Previous work of ours on Semantic Storytelling uses text analytics procedures including Named Entity Recognition and Event Detection. In this paper, we outline our longer-term vision on Semantic Storytelling and describe the current conceptual and technical approach. In the project that drives our research we develop AI-based technologies that are verified by partners from industry. One long-term goal is the development of an approach for Semantic Storytelling that has broad coverage and that is, furthermore, robust. We provide first results on experiments that involve discourse parsing, applied to a concrete use case, ""Explore the Neighbourhood!"", which is based on a semi-automatically collected data set with documents about noteworthy people in one of Berlin's districts. Though automatically obtaining annotations for coherence relations from plain text is a non-trivial challenge, our preliminary results are promising. We envision our approach to be combined with additional features (NER, coreference resolution, knowledge graphs","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SWAG: Storytelling With Action Guidance,"Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach frames story writing as a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best ""action"" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation. Our SWAG pipeline using only small open-source models surpasses GPT-3.5-Turbo.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Hierarchically-Attentive RNN for Album Summarization and Storytelling,"We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
FairyTailor: A Multimodal Generative Framework for Storytelling,"Storytelling is an open-ended task that entails creative thinking and requires a constant flow of ideas. Natural language generation (NLG) for storytelling is especially challenging because it requires the generated text to follow an overall theme while remaining creative and diverse to engage the reader. In this work, we introduce a system and a web-based demo, FairyTailor, for human-in-the-loop visual story co-creation. Users can create a cohesive children's fairytale by weaving generated texts and retrieved images with their input. FairyTailor adds another modality and modifies the text generation process to produce a coherent and creative sequence of text and images. To our knowledge, this is the first dynamic tool for multimodal story generation that allows interactive co-formation of both texts and images. It allows users to give feedback on co-created stories and share their results.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling,"Visual storytelling is a creative and challenging task, aiming to automatically generate a story-like description for a sequence of images. The descriptions generated by previous visual storytelling approaches lack coherence because they use word-level sequence generation methods and do not adequately consider sentence-level dependencies. To tackle this problem, we propose a novel hierarchical visual storytelling framework which separately models sentence-level and word-level semantics. We use the transformer-based BERT to obtain embeddings for sentences and words. We then employ a hierarchical LSTM network: the bottom LSTM receives as input the sentence vector representation from BERT, to learn the dependencies between the sentences corresponding to images, and the top LSTM is responsible for generating the corresponding word vector representations, taking input from the bottom LSTM. Experimental results demonstrate that our model outperforms most closely related baselines under automatic evaluation metrics BLEU and CIDEr, and also show the effectiveness of our method with human evaluation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models,"Visual storytelling is an intriguing and complex task that only recently entered the research arena. In this work, we survey relevant work to date, and conduct a thorough error analysis of three very recent approaches to visual storytelling. We categorize and provide examples of common types of errors, and identify key shortcomings in current work. Finally, we make recommendations for addressing these limitations in the future.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Text characterization based on recurrence networks,"Several complex systems are characterized by presenting intricate characteristics taking place at several scales of time and space. These multiscale characterizations are used in various applications, including better understanding diseases, characterizing transportation systems, and comparison between cities, among others. In particular, texts are also characterized by a hierarchical structure that can be approached by using multi-scale concepts and methods. The multiscale properties of texts constitute a subject worth further investigation. In addition, more effective approaches to text characterization and analysis can be obtained by emphasizing words with potentially more informational content. The present work aims at developing these possibilities while focusing on mesoscopic representations of networks. More specifically, we adopt an extension to the mesoscopic approach to represent text narratives, in which only the recurrent relationships among tagged parts of speech (subject, verb and direct object) are considered to establish connections among sequential pieces of text (e.g., paragraphs). The characterization of the texts was then achieved by considering scale-dependent complementary methods: accessibility, symmetry and recurrence signatures. In order to evaluate the potential of these concepts and methods, we approached the problem of distinguishing between literary genres (fiction and non-fiction). A set of 300 books organized into the two genres was considered and were compared by using the aforementioned approaches. All the methods were capable of differentiating to some extent between the two genres. The accessibility and symmetry reflected the narrative asymmetries, while the recurrence signature provided a more direct indication about the non-sequential semantic connections taking place along the narrative.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
PoeticTTS -- Controllable Poetry Reading for Literary Studies,"Speech synthesis for poetry is challenging due to specific intonation patterns inherent to poetic speech. In this work, we propose an approach to synthesise poems with almost human like naturalness in order to enable literary scholars to systematically examine hypotheses on the interplay between text, spoken realisation, and the listener's perception of poems. To meet these special requirements for literary studies, we resynthesise poems by cloning prosodic values from a human reference recitation, and afterwards make use of fine-grained prosody control to manipulate the synthetic speech in a human-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We find that finetuning our TTS model on poetry captures poetic intonation patterns to a large extent which is beneficial for prosody cloning and manipulation and verify the success of our approach both in an objective evaluation as well as in human studies.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models,"Narrative data spans all disciplines and provides a coherent model of the world to the reader or viewer. Recent advancement in machine learning and Large Language Models (LLMs) have enable great strides in analyzing natural language. However, Large language models (LLMs) still struggle with complex narrative arcs as well as narratives containing conflicting information. Recent work indicates LLMs augmented with external knowledge bases can improve the accuracy and interpretability of the resulting models. In this work, we analyze the effectiveness of applying knowledge graphs (KGs) in understanding true-crime podcast data from both classical Natural Language Processing (NLP) and LLM approaches. We directly compare KG-augmented LLMs (KGLLMs) with classical methods for KG construction, topic modeling, and sentiment analysis. Additionally, the KGLLM allows us to query the knowledge base in natural language and test its ability to factually answer questions. We examine the robustness of the model to adversarial prompting in order to test the model's ability to deal with conflicting information. Finally, we apply classical methods to understand more subtle aspects of the text such as the use of hearsay and sentiment in narrative construction and propose future directions. Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are more robust to adversarial prompts, and are more capable of summarizing the text into topics.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives,"Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story.   However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and distils facts in this schema from both existing commonsense knowledge graphs and large-scale pretrained language models. Our analysis indicates that PeaCoK contains rich and precise world persona inferences that help downstream systems generate more consistent and engaging narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Combining Qualitative and Computational Approaches for Literary Analysis of Finnish Novels,"What can we learn from the classics of Finnish literature by using computational emotion analysis? This article tries to answer this question by examining how computational methods of sentiment analysis can be used in the study of literary works in conjunction with a qualitative or more 'traditional' approach to literature and affect. We present and develop a simple but robust computational approach of affect analysis that uses a carefully curated emotion lexicon adapted to Finnish turn-of-the-century literary texts combined with word embeddings to map out the semantic emotional spaces of seminal works of Finnish literature. We focus our qualitative analysis on selected case studies: four works by Juhani Aho, Minna Canth, Maria Jotuni, and F. E. Sillanp, but provide emotion arcs for a total of 975 Finnish novels.   We argue that a computational analysis of a text's lexicon can be valuable in evaluating the large distribution of the emotional valence in a text and provide guidelines to help other researchers replicate our findings. We show that computational approaches have a place in traditional studies on affect in literature as a support tool for close-reading-based analyses, but also allowing for large-scale comparison between, for example, genres or national canons.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Applying Large Language Models to Characterize Public Narratives,"Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Dynamical Principles of Storytelling,"When considering the opening part of 1800 short stories, we find that the first dozen paragraphs of the average narrative follow an action principle as defined in arXiv:2309.06600. When the order of the paragraphs is shuffled, the average no longer exhibits this property. The findings show that there is a preferential direction we take in semantic space when starting a story, possibly related to a common Western storytelling tradition as implied by Aristotle in Poetics.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish","This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with ""Translate the following text into [TG] creatively"" at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT).","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Informative Visual Storytelling with Cross-modal Rules,"Existing methods in the Visual Storytelling field often suffer from the problem of generating general descriptions, while the image contains a lot of meaningful contents remaining unnoticed. The failure of informative story generation can be concluded to the model's incompetence of capturing enough meaningful concepts. The categories of these concepts include entities, attributes, actions, and events, which are in some cases crucial to grounded storytelling. To solve this problem, we propose a method to mine the cross-modal rules to help the model infer these informative concepts given certain visual input. We first build the multimodal transactions by concatenating the CNN activations and the word indices. Then we use the association rule mining algorithm to mine the cross-modal rules, which will be used for the concept inference. With the help of the cross-modal rules, the generated stories are more grounded and informative. Besides, our proposed method holds the advantages of interpretation, expandability, and transferability, indicating potential for wider application. Finally, we leverage these concepts in our encoder-decoder framework with the attention mechanism. We conduct several experiments on the VIsual StoryTelling~(VIST) dataset, the results of which demonstrate the effectiveness of our approach in terms of both automatic metrics and human evaluation. Additional experiments are also conducted showing that our mined cross-modal rules as additional knowledge helps the model gain better performance when trained on a small dataset.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Measuring Information Propagation in Literary Social Networks,"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
InvBERT: Reconstructing Text from Contextualized Word Embeddings by inverting the BERT pipeline,"Digital Humanities and Computational Literary Studies apply text mining methods to investigate literature. Such automated approaches enable quantitative studies on large corpora which would not be feasible by manual inspection alone. However, due to copyright restrictions, the availability of relevant digitized literary works is limited. Derived Text Formats (DTFs) have been proposed as a solution. Here, textual materials are transformed in such a way that copyright-critical features are removed, but that the use of certain analytical methods remains possible. Contextualized word embeddings produced by transformer-encoders (like BERT) are promising candidates for DTFs because they allow for state-of-the-art performance on various analytical tasks and, at first sight, do not disclose the original text. However, in this paper we demonstrate that under certain conditions the reconstruction of the original copyrighted text becomes feasible and its publication in the form of contextualized token representations is not safe. Our attempts to invert BERT suggest, that publishing the encoder as a black box together with the contextualized embeddings is critical, since it allows to generate data to train a decoder with a reconstruction accuracy sufficient to violate copyright laws.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs","Climate change has increased demands for transparent and comparable corporate climate disclosures, yet imitation and symbolic reporting often undermine their value. This paper develops a multidimensional framework to assess disclosure maturity among 828 U.S.listed firms using large language models (LLMs) fine-tuned for climate communication. Four classifiers-sentiment, commitment, specificity, and target ambition-extract narrative indicators from sustainability and annual reports, which are linked to firm attributes such as emissions, market capitalization, and sector. Analyses reveal three insights: (1) risk-focused narratives often align with explicit commitments, but quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2) larger and higher-emitting firms disclose more commitments and actions than peers, though inconsistently with quantitative targets; and (3) widespread similarity in disclosure styles suggests mimetic behavior, reducing differentiation and decision usefulness. These results highlight the value of LLMs for ESG narrative analysis and the need for stronger regulation to connect commitments with verifiable transition strategies.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Looking for the Inner Music: Probing LLMs' Understanding of Literary Style,"Recent work has demonstrated that language models can be trained to identify the author of much shorter literary passages than has been thought feasible for traditional stylometry. We replicate these results for authorship and extend them to a new dataset measuring novel genre. We find that LLMs are able to distinguish authorship and genre, but they do so in different ways. Some models seem to rely more on memorization, while others benefit more from training to learn author/genre characteristics. We then use three methods to probe one high-performing LLM for features that define style. These include direct syntactic ablations to input text as well as two methods that look at model internals. We find that authorial style is easier to define than genre-level style and is more impacted by minor syntactic decisions and contextual word usage. However, some traits like pronoun usage and word order prove significant for defining both kinds of literary style.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media,"The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Mapping Out Narrative Structures and Dynamics Using Networks and Textual Information,"Human communication is often executed in the form of a narrative, an account of connected events composed of characters, actions, and settings. A coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication, opening up the possibility of a deeper understanding of a narrative by studying its structural properties. In this paper we present a network-based framework for modeling and analyzing the structure of a narrative, which is further expanded by incorporating methods from computational linguistics to utilize the narrative text. Modeling a narrative as a dynamically unfolding system, we characterize its progression via the growth patterns of the character network, and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords. This is a network framework advanced beyond the simple occurrence-based one most often used until now, allowing one to utilize the unique characteristics of a given narrative to a high degree. Given the ubiquity and importance of narratives, such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions, expression of human sentiments, and communication.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions,"Role-playing games (RPG) are games in which players interact with one another to create narratives. The role of players in the RPG is largely based on the interaction between players and their characters. This emerging form of shared narrative, primarily oral, is receiving increasing attention. In particular, many authors investigated the use of an LLM as an actor in the game. In this paper, we aim to discover to what extent the language of Large Language Models (LLMs) exhibit oral or written features when asked to generate an RPG session without human interference. We will conduct a linguistic analysis of the lexical and syntactic features of the generated texts and compare the results with analyses of conversations, transcripts of human RPG sessions, and books. We found that LLMs exhibit a pattern that is distinct from all other text categories, including oral conversations, human RPG sessions and books. Our analysis has shown how training influences the way LLMs express themselves and provides important indications of the narrative capabilities of these tools.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Collage is the New Writing: Exploring the Fragmentation of Text and User Interfaces in AI Tools,"This essay proposes and explores the concept of Collage for the design of AI writing tools, transferred from avant-garde literature with four facets: 1) fragmenting text in writing interfaces, 2) juxtaposing voices (content vs command), 3) integrating material from multiple sources (e.g. text suggestions), and 4) shifting from manual writing to editorial and compositional decision-making, such as selecting and arranging snippets. The essay then employs Collage as an analytical lens to analyse the user interface design of recent AI writing tools, and as a constructive lens to inspire new design directions. Finally, a critical perspective relates the concerns that writers historically expressed through literary collage to AI writing tools. In a broad view, this essay explores how literary concepts can help advance design theory around AI writing tools. It encourages creators of future writing tools to engage not only with new technological possibilities, but also with past writing innovations.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Dixit: Interactive Visual Storytelling via Term Manipulation,"In this paper, we introduce Dixit, an interactive visual storytelling system that the user interacts with iteratively to compose a short story for a photo sequence. The user initiates the process by uploading a sequence of photos. Dixit first extracts text terms from each photo which describe the objects (e.g., boy, bike) or actions (e.g., sleep) in the photo, and then allows the user to add new terms or remove existing terms. Dixit then generates a short story based on these terms. Behind the scenes, Dixit uses an LSTM-based model trained on image caption data and FrameNet to distill terms from each image and utilizes a transformer decoder to compose a context-coherent story. Users change images or terms iteratively with Dixit to create the most ideal story. Dixit also allows users to manually edit and rate stories. The proposed procedure opens up possibilities for interpretable and controllable visual storytelling, allowing users to understand the story formation rationale and to intervene in the generation process.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Storytelling Agents with Personality and Adaptivity,"We explore the expression of personality and adaptivity through the gestures of virtual agents in a storytelling task. We conduct two experiments using four different dialogic stories. We manipulate agent personality on the extraversion scale, whether the agents adapt to one another in their gestural performance and agent gender. Our results show that subjects are able to perceive the intended variation in extraversion between different virtual agents, independently of the story they are telling and the gender of the agent. A second study shows that subjects also prefer adaptive to nonadaptive virtual agents.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Pattern Recognition in Narrative: Tracking Emotional Expression in Context,"Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Computational thematics: Comparing algorithms for clustering the genres of literary fiction,"What are the best methods of capturing thematic similarity between literary texts? Knowing the answer to this question would be useful for automatic clustering of book genres, or any other thematic grouping. This paper compares a variety of algorithms for unsupervised learning of thematic similarities between texts, which we call ""computational thematics"". These algorithms belong to three steps of analysis: text preprocessing, extraction of text features, and measuring distances between the lists of features. Each of these steps includes a variety of options. We test all the possible combinations of these options: every combination of algorithms is given a task to cluster a corpus of books belonging to four pre-tagged genres of fiction. This clustering is then validated against the ""ground truth"" genre labels. Such comparison of algorithms allows us to learn the best and the worst combinations for computational thematic analysis. To illustrate the sharp difference between the best and the worst methods, we then cluster 5000 random novels from the HathiTrust corpus of fiction.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA,"Question Answering (QA) on narrative text poses a unique challenge to current systems, requiring a deep understanding of long, complex documents. However, the reliability of NarrativeQA, the most widely used benchmark in this domain, is hindered by noisy documents and flawed QA pairs. In this work, we introduce LiteraryQA, a high-quality subset of NarrativeQA focused on literary works. Using a human- and LLM-validated pipeline, we identify and correct low-quality QA samples while removing extraneous text from source documents. We then carry out a meta-evaluation of automatic metrics to clarify how systems should be evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics have a low system-level correlation to human judgment, while LLM-as-a-Judge evaluations, even with small open-weight models, can strongly agree with the ranking identified by humans. Finally, we benchmark a set of long-context LLMs on LiteraryQA. We release our code and data at https://github.com/SapienzaNLP/LiteraryQA.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Narrative Variations in a Virtual Storyteller,"Research on storytelling over the last 100 years has distinguished at least two levels of narrative representation (1) story, or fabula; and (2) discourse, or sujhet. We use this distinction to create Fabula Tales, a computational framework for a virtual storyteller that can tell the same story in different ways through the implementation of general narratological variations, such as varying direct vs. indirect speech, character voice (style), point of view, and focalization. A strength of our computational framework is that it is based on very general methods for re-using existing story content, either from fables or from personal narratives collected from blogs. We first explain how a simple annotation tool allows naive annotators to easily create a deep representation of fabula called a story intention graph, and show how we use this representation to generate story tellings automatically. Then we present results of two studies testing our narratological parameters, and showing that different tellings affect the reader's perception of the story and characters.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
What Makes A Good Story? Designing Composite Rewards for Visual Storytelling,"Previous storytelling approaches mostly focused on optimizing traditional metrics such as BLEU, ROUGE and CIDEr. In this paper, we re-examine this problem from a different angle, by looking deep into what defines a realistically-natural and topically-coherent story. To this end, we propose three assessment criteria: relevance, coherence and expressiveness, which we observe through empirical analysis could constitute a ""high-quality"" story to the human eye. Following this quality guideline, we propose a reinforcement learning framework, ReCo-RL, with reward functions designed to capture the essence of these quality criteria. Experiments on the Visual Storytelling Dataset (VIST) with both automatic and human evaluations demonstrate that our ReCo-RL model achieves better performance than state-of-the-art baselines on both traditional metrics and the proposed new criteria.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Can Sentiment Analysis Reveal Structure in a Plotless Novel?,"Modernist novels are thought to break with traditional plot structure. In this paper, we test this theory by applying Sentiment Analysis to one of the most famous modernist novels, To the Lighthouse by Virginia Woolf. We first assess Sentiment Analysis in light of the critique that it cannot adequately account for literary language: we use a unique statistical comparison to demonstrate that even simple lexical approaches to Sentiment Analysis are surprisingly effective. We then use the Syuzhet.R package to explore similarities and differences across modeling methods. This comparative approach, when paired with literary close reading, can offer interpretive clues. To our knowledge, we are the first to undertake a hybrid model that fully leverages the strengths of both computational analysis and close reading. This hybrid model raises new questions for the literary critic, such as how to interpret relative versus absolute emotional valence and how to take into account subjective identification. Our finding is that while To the Lighthouse does not replicate a plot centered around a traditional hero, it does reveal an underlying emotional structure distributed between characters - what we term a distributed heroine model. This finding is innovative in the field of modernist and narrative studies and demonstrates that a hybrid method can yield significant discoveries.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
States of LLM-generated Texts and Phase Transitions between them,"It is known for some time that autocorrelations of words in human-written texts decay according to a power law. Recent works have also shown that the autocorrelations decay in texts generated by LLMs is qualitatively different from the literary texts. Solid state physics tie the autocorrelations decay laws to the states of matter. In this work, we empirically demonstrate that, depending on the temperature parameter, LLMs can generate text that can be classified as solid, critical state or gas.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts,"Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multiagent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging","In assessing argument strength, the notions of what makes a good argument are manifold. With the broader trend towards treating subjectivity as an asset and not a problem in NLP, new dimensions of argument quality are studied. Although studies on individual subjective features like personal stories exist, there is a lack of large-scale analyses of the relation between these features and argument strength. To address this gap, we conduct regression analysis to quantify the impact of subjective factors $-$ emotions, storytelling, and hedging $-$ on two standard datasets annotated for objective argument quality and subjective persuasion. As such, our contribution is twofold: at the level of contributed resources, as there are no datasets annotated with all studied dimensions, this work compares and evaluates automated annotation methods for each subjective feature. At the level of novel insights, our regression analysis uncovers different patterns of impact of subjective features on the two facets of argument strength encoded in the datasets. Our results show that storytelling and hedging have contrasting effects on objective and subjective argument quality, while the influence of emotions depends on their rhetoric utilization rather than the domain.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
IruMozhi: Automatically classifying diglossia in Tamil,"Tamil, a Dravidian language of South Asia, is a highly diglossic language with two very different registers in everyday use: Literary Tamil (preferred in writing and formal communication) and Spoken Tamil (confined to speech and informal media). Spoken Tamil is under-supported in modern NLP systems. In this paper, we release IruMozhi, a human-annotated dataset of parallel text in Literary and Spoken Tamil. We train classifiers on the task of identifying which variety a text belongs to. We use these models to gauge the availability of pretraining data in Spoken Tamil, to audit the composition of existing labelled datasets for Tamil, and to encourage future work on the variety.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Mapping News Narratives Using LLMs and Narrative-Structured Text Embeddings,"Given the profound impact of narratives across various societal levels, from personal identities to international politics, it is crucial to understand their distribution and development over time. This is particularly important in online spaces. On the Web, narratives can spread rapidly and intensify societal divides and conflicts. While many qualitative approaches exist, quantifying narratives remains a significant challenge. Computational narrative analysis lacks frameworks that are both comprehensive and generalizable. To address this gap, we introduce a numerical narrative representation grounded in structuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a narrative through a constellation of six functional character roles. These so-called actants are genre-agnostic, making the model highly generalizable. We extract the actants using an open-source LLM and integrate them into a Narrative-Structured Text Embedding that captures both the semantics and narrative structure of a text. We demonstrate the analytical insights of the method on the example of 5000 full-text news articles from Al Jazeera and The Washington Post on the Israel-Palestine conflict. Our method successfully distinguishes articles that cover the same topics but differ in narrative structure.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Narrative Media Framing in Political Discourse,"Narrative frames are a powerful way of conceptualizing and communicating complex, controversial ideas, however automated frame analysis to date has mostly overlooked this framing device. In this paper, we connect elements of narrativity with fundamental aspects of framing, and present a framework which formalizes and operationalizes such aspects. We annotate and release a data set of news articles in the climate change domain, analyze the dominance of narrative frame components across political leanings, and test LLMs in their ability to predict narrative frames and their components. Finally, we apply our framework in an unsupervised way to elicit components of narrative framing in a second domain, the COVID-19 crisis, where our predictions are congruent with prior theoretical work showing the generalizability of our approach.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
LLMs Behind the Scenes: Enabling Narrative Scene Illustration,"Generative AI has established the opportunity to readily transform content from one medium to another. This capability is especially powerful for storytelling, where visual illustrations can illuminate a story originally expressed in text. In this paper, we focus on the task of narrative scene illustration, which involves automatically generating an image depicting a scene in a story. Motivated by recent progress on text-to-image models, we consider a pipeline that uses LLMs as an interface for prompting text-to-image models to generate scene illustrations given raw story text. We apply variations of this pipeline to a prominent story corpus in order to synthesize illustrations for scenes in these stories. We conduct a human annotation task to obtain pairwise quality judgments for these illustrations. The outcome of this process is the SceneIllustrations dataset, which we release as a new resource for future work on cross-modal narrative transformation. Through our analysis of this dataset and experiments modeling illustration quality, we demonstrate that LLMs can effectively verbalize scene knowledge implicitly evoked by story text. Moreover, this capability is impactful for generating and evaluating illustrations.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Evaluating Large Language Model Creativity from a Literary Perspective,"This paper assesses the potential for large language models (LLMs) to serve as assistive tools in the creative writing process, by means of a single, in-depth case study. In the course of the study, we develop interactive and multi-voice prompting strategies that interleave background descriptions (scene setting, plot elements), instructions that guide composition, samples of text in the target style, and critical discussion of the given samples. We qualitatively evaluate the results from a literary critical perspective, as well as from the standpoint of computational creativity (a sub-field of artificial intelligence). Our findings lend support to the view that the sophistication of the results that can be achieved with an LLM mirrors the sophistication of the prompting.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study,"Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance the study of LLM capabilities, limitations, and validity but also offer practical insights for narrative generation and natural language processing applications.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation,"Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject verb object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules, the STORYLINE and narrative entity knowledge graph (NEKG),that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Modeling Protagonist Emotions for Emotion-Aware Storytelling,"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"RabindraNet, Creating Literary Works in the Style of Rabindranath Tagore","Bengali literature has a rich history of hundreds of years with luminary figures such as Rabindranath Tagore and Kazi Nazrul Islam. However, analytical works involving the most recent advancements in NLP have barely scratched the surface utilizing the enormous volume of the collected works from the writers of the language. In order to bring attention to the analytical study involving the works of Bengali writers and spearhead the text generation endeavours in the style of existing literature, we are introducing RabindraNet, a character level RNN model with stacked-LSTM layers trained on the works of Rabindranath Tagore to produce literary works in his style for multiple genres. We created an extensive dataset as well by compiling the digitized works of Rabindranath Tagore from authentic online sources and published as open source dataset on data science platform Kaggle.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
An Analysis of Reader Engagement in Literary Fiction through Eye Tracking and Linguistic Features,"Capturing readers' engagement in fiction is a challenging but important aspect of narrative understanding. In this study, we collected 23 readers' reactions to 2 short stories through eye tracking, sentence-level annotations, and an overall engagement scale survey. We analyzed the significance of various qualities of the text in predicting how engaging a reader is likely to find it. As enjoyment of fiction is highly contextual, we also investigated individual differences in our data. Furthering our understanding of what captivates readers in fiction will help better inform models used in creative narrative generation and collaborative writing tools.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Branching Narratives: Character Decision Points Detection,"This paper presents the Character Decision Points Detection (CHADPOD) task, a task of identification of points within narratives where characters make decisions that may significantly influence the story's direction. We propose a novel dataset based on CYOA-like games graphs to be used as a benchmark for such a task. We provide a comparative analysis of different models' performance on this task, including a couple of LLMs and several MLMs as baselines, achieving up to 89% accuracy. This underscores the complexity of narrative analysis, showing the challenges associated with understanding character-driven story dynamics. Additionally, we show how such a model can be applied to the existing text to produce linear segments divided by potential branching points, demonstrating the practical application of our findings in narrative analysis.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Explainable AI Components for Narrative Map Extraction,"As narrative extraction systems grow in complexity, establishing user trust through interpretable and explainable outputs becomes increasingly critical. This paper presents an evaluation of an Explainable Artificial Intelligence (XAI) system for narrative map extraction that provides meaningful explanations across multiple levels of abstraction. Our system integrates explanations based on topical clusters for low-level document relationships, connection explanations for event relationships, and high-level structure explanations for overall narrative patterns. In particular, we evaluate the XAI system through a user study involving 10 participants that examined narratives from the 2021 Cuban protests. The analysis of results demonstrates that participants using the explanations made the users trust in the system's decisions, with connection explanations and important event detection proving particularly effective at building user confidence. Survey responses indicate that the multi-level explanation approach helped users develop appropriate trust in the system's narrative extraction capabilities. This work advances the state-of-the-art in explainable narrative extraction while providing practical insights for developing reliable narrative extraction systems that support effective human-AI collaboration.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication,"Visual storytelling aims to generate a narrative paragraph from a sequence of images automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method's good ability in generating stories with higher quality compared to state-of-the-art methods.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
NAREOR: The Narrative Reordering Problem,"Many implicit inferences exist in text depending on how it is structured that can critically impact the text's interpretation and meaning. One such structural aspect present in text with chronology is the order of its presentation. For narratives or stories, this is known as the narrative order. Reordering a narrative can impact the temporal, causal, event-based, and other inferences readers draw from it, which in turn can have strong effects both on its interpretation and interestingness. In this paper, we propose and investigate the task of Narrative Reordering (NAREOR) which involves rewriting a given story in a different narrative order while preserving its plot. We present a dataset, NAREORC, with human rewritings of stories within ROCStories in non-linear orders, and conduct a detailed analysis of it. Further, we propose novel task-specific training methods with suitable evaluation metrics. We perform experiments on NAREORC using state-of-the-art models such as BART and T5 and conduct extensive automatic and human evaluations. We demonstrate that although our models can perform decently, NAREOR is a challenging task with potential for further exploration. We also investigate two applications of NAREOR: generation of more interesting variations of stories and serving as adversarial sets for temporal/event-related tasks, besides discussing other prospective ones, such as for pedagogical setups related to language skills like essay writing and applications to medicine involving clinical narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Knowledge-Enriched Visual Storytelling,"Stories are diverse and highly personalized, resulting in a large possible output space for story generation. Existing end-to-end approaches produce monotonous stories because they are limited to the vocabulary and knowledge in a single training dataset. This paper introduces KG-Story, a three-stage framework that allows the story generation model to take advantage of external Knowledge Graphs to produce interesting stories. KG-Story distills a set of representative words from the input prompts, enriches the word set by using external knowledge graphs, and finally generates stories based on the enriched word set. This distill-enrich-generate framework allows the use of external resources not only for the enrichment phase, but also for the distillation and generation phases. In this paper, we show the superiority of KG-Story for visual storytelling, where the input prompt is a sequence of five photos and the output is a short story. Per the human ranking evaluation, stories generated by KG-Story are on average ranked better than that of the state-of-the-art systems. Our code and output stories are available at https://github.com/zychen423/KE-VIST.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling,"As a technically challenging topic, visual storytelling aims at generating an imaginary and coherent story with narrative multi-sentences from a group of relevant images. Existing methods often generate direct and rigid descriptions of apparent image-based contents, because they are not capable of exploring implicit information beyond images. Hence, these schemes could not capture consistent dependencies from holistic representation, impairing the generation of reasonable and fluent story. To address these problems, a novel knowledge-enriched attention network with group-wise semantic model is proposed. Three main novel components are designed and supported by substantial experiments to reveal practical advantages. First, a knowledge-enriched attention network is designed to extract implicit concepts from external knowledge system, and these concepts are followed by a cascade cross-modal attention mechanism to characterize imaginative and concrete representations. Second, a group-wise semantic module with second-order pooling is developed to explore the globally consistent guidance. Third, a unified one-stage story generation model with encoder-decoder structure is proposed to simultaneously train and infer the knowledge-enriched attention network, group-wise semantic module and multi-modal story generation decoder in an end-to-end fashion. Substantial experiments on the popular Visual Storytelling dataset with both objective and subjective evaluation metrics demonstrate the superior performance of the proposed scheme as compared with other state-of-the-art methods.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Navigating News Narratives: A Media Bias Analysis Dataset,"The proliferation of biased news narratives across various media platforms has become a prominent challenge, influencing public opinion on critical topics like politics, health, and climate change. This paper introduces the ""Navigating News Narratives: A Media Bias Analysis Dataset"", a comprehensive dataset to address the urgent need for tools to detect and analyze media bias. This dataset encompasses a broad spectrum of biases, making it a unique and valuable asset in the field of media studies and artificial intelligence. The dataset is available at https://huggingface.co/datasets/newsmediabias/news-bias-full-data.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Plot and Rework: Modeling Storylines for Visual Storytelling,"Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST, a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline. PR-VIST then takes this path and learns to generate the final story via an iterative training process. This framework produces stories that are superior in terms of diversity, coherence, and humanness, per both automatic and human evaluations. An ablation study shows that both plotting and reworking contribute to the model's superiority.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
GROOViST: A Metric for Grounding Objects in Visual Storytelling,"A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Characterizing Partisan Political Narrative Frameworks about COVID-19 on Twitter,"The COVID-19 pandemic is a global crisis that has been testing every society and exposing the critical role of local politics in crisis response. In the United States, there has been a strong partisan divide between the Democratic and Republican party's narratives about the pandemic which resulted in polarization of individual behaviors and divergent policy adoption across regions. As shown in this case, as well as in most major social issues, strongly polarized narrative frameworks facilitate such narratives. To understand polarization and other social chasms, it is critical to dissect these diverging narratives. Here, taking the Democratic and Republican political social media posts about the pandemic as a case study, we demonstrate that a combination of computational methods can provide useful insights into the different contexts, framing, and characters and relationships that construct their narrative frameworks which individual posts source from. Leveraging a dataset of tweets from elite politicians in the U.S., we found that the Democrats' narrative tends to be more concerned with the pandemic as well as financial and social support, while the Republicans discuss more about other political entities such as China. We then perform an automatic framing analysis to characterize the ways in which they frame their narratives, where we found that the Democrats emphasize the government's role in responding to the pandemic, and the Republicans emphasize the roles of individuals and support for small businesses. Finally, we present a semantic role analysis that uncovers the important characters and relationships in their narratives as well as how they facilitate a membership categorization process. Our findings concretely expose the gaps in the ""elusive consensus"" between the two parties. Our methodologies may be applied to computationally study narratives in various domains.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors,"Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT4, a state of the art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to these metaphors previously, the AI system consistently produced detailed and incisive interpretations. Human judges, blind to the fact that an AI model was involved, rated metaphor interpretations generated by GPT4 as superior to those provided by a group of college students. In interpreting reversed metaphors, GPT4, as well as humans, exhibited signs of sensitivity to the Gricean cooperative principle. In addition, for several novel English poems GPT4 produced interpretations that were rated as excellent or good by a human literary critic. These results indicate that LLMs such as GPT4 have acquired an emergent ability to interpret complex metaphors, including those embedded in novel poems.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Visual Storytelling with Question-Answer Plans,"Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Improving Narrative Classification and Explanation via Fine Tuned Language Models,"Understanding covert narratives and implicit messaging is essential for analyzing bias and sentiment. Traditional NLP methods struggle with detecting subtle phrasing and hidden agendas. This study tackles two key challenges: (1) multi-label classification of narratives and sub-narratives in news articles, and (2) generating concise, evidence-based explanations for dominant narratives. We fine-tune a BERT model with a recall-oriented approach for comprehensive narrative detection, refining predictions using a GPT-4o pipeline for consistency. For narrative explanation, we propose a ReACT (Reasoning + Acting) framework with semantic retrieval-based few-shot prompting, ensuring grounded and relevant justifications. To enhance factual accuracy and reduce hallucinations, we incorporate a structured taxonomy table as an auxiliary knowledge base. Our results show that integrating auxiliary knowledge in prompts improves classification accuracy and justification reliability, with applications in media analysis, education, and intelligence gathering.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition","Visual storytelling consists in generating a natural language story given a temporally ordered sequence of images. This task is not only challenging for models, but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story 'good'. In this paper, we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work: visual grounding, coherence, and repetitiveness. We then use this method to evaluate the stories generated by several models, showing that the foundation model LLaVA obtains the best result, but only slightly so compared to TAPM, a 50-times smaller visual storytelling model. Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters. Finally, we carry out a human evaluation study, whose results suggest that a 'good' story may require more than a human-like level of visual grounding, coherence, and repetition.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Ordered Attention for Coherent Visual Storytelling,"We address the problem of visual storytelling, i.e., generating a story for a given sequence of images. While each sentence of the story should describe a corresponding image, a coherent story also needs to be consistent and relate to both future and past images. To achieve this we develop ordered image attention (OIA). OIA models interactions between the sentence-corresponding image and important regions in other images of the sequence. To highlight the important objects, a message-passing-like algorithm collects representations of those objects in an order-aware manner. To generate the story's sentences, we then highlight important image attention vectors with an Image-Sentence Attention (ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we introduce an adaptive prior. The obtained results improve the METEOR score on the VIST dataset by 1%. In addition, an extensive human study verifies coherency improvements and shows that OIA and ISA generated stories are more focused, shareable, and image-grounded.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Plan-And-Write: Towards Better Automatic Storytelling,"Automatic storytelling is challenging since it requires generating long, coherent natural language to describes a sensible sequence of events. Despite considerable efforts on automatic story generation in the past, prior work either is restricted in plot planning, or can only generate stories in a narrow domain. In this paper, we explore open-domain story generation that writes stories given a title (topic) as input. We propose a plan-and-write hierarchical generation framework that first plans a storyline, and then generates a story based on the storyline. We compare two planning strategies. The dynamic schema interweaves story planning and its surface realization in text, while the static schema plans out the entire storyline before generating stories. Experiments show that with explicit storyline planning, the generated stories are more diverse, coherent, and on topic than those generated without creating a full plan, according to both automatic and human evaluations.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SentimentArcs: A Novel Method for Self-Supervised Sentiment Analysis of Time Series Shows SOTA Transformers Can Struggle Finding Narrative Arcs,"SOTA Transformer and DNN short text sentiment classifiers report over 97% accuracy on narrow domains like IMDB movie reviews. Real-world performance is significantly lower because traditional models overfit benchmarks and generalize poorly to different or more open domain texts. This paper introduces SentimentArcs, a new self-supervised time series sentiment analysis methodology that addresses the two main limitations of traditional supervised sentiment analysis: limited labeled training datasets and poor generalization. A large ensemble of diverse models provides a synthetic ground truth for self-supervised learning. Novel metrics jointly optimize an exhaustive search across every possible corpus:model combination. The joint optimization over both the corpus and model solves the generalization problem. Simple visualizations exploit the temporal structure in narratives so domain experts can quickly spot trends, identify key features, and note anomalies over hundreds of arcs and millions of data points. To our knowledge, this is the first self-supervised method for time series sentiment analysis and the largest survey directly comparing real-world model performance on long-form narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Modeling Human Mental States with an Entity-based Narrative Graph,"Understanding narrative text requires capturing characters' motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal-states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Contextualize, Show and Tell: A Neural Visual Storyteller","We present a neural model for generating short stories from image sequences, which extends the image description model by Vinyals et al. (Vinyals et al., 2015). This extension relies on an encoder LSTM to compute a context vector of each story from the image sequence. This context vector is used as the first state of multiple independent decoder LSTMs, each of which generates the portion of the story corresponding to each image in the sequence by taking the image embedding as the first input. Our model showed competitive results with the METEOR metric and human ratings in the internal track of the Visual Storytelling Challenge 2018.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
On the origin of long-range correlations in texts,"The complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language. In order to retain and convey such a high dimensional information, the statistical properties of our linguistic output has to be highly correlated in time. An example are the robust observations, still largely not understood, of correlations on arbitrary long scales in literary texts. In this paper we explain how long-range correlations flow from highly structured linguistic levels down to the building blocks of a text (words, letters, etc..). By combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text. The mechanisms we identify are fairly general and can be equally applied to other hierarchical settings.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Collective Memory and Narrative Cohesion: A Computational Study of Palestinian Refugee Oral Histories in Lebanon,"This study uses the Palestinian Oral History Archive (POHA) to investigate how Palestinian refugee groups in Lebanon sustain a cohesive collective memory of the Nakba through shared narratives. Grounded in Halbwachs' theory of group memory, we employ statistical analysis of pairwise similarity of narratives, focusing on the influence of shared gender and location. We use textual representation and semantic embeddings of narratives to represent the interviews themselves. Our analysis demonstrates that shared origin is a powerful determinant of narrative similarity across thematic keywords, landmarks, and significant figures, as well as in semantic embeddings of the narratives. Meanwhile, shared residence fosters cohesion, with its impact significantly amplified when paired with shared origin. Additionally, women's narratives exhibit heightened thematic cohesion, particularly in recounting experiences of the British occupation, underscoring the gendered dimensions of memory formation. This research deepens the understanding of collective memory in diasporic settings, emphasizing the critical role of oral histories in safeguarding Palestinian identity and resisting erasure.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement,"Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy's design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy's usability and suggested design insights for future parent-AI collaboration systems.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
News Article Retrieval in Context for Event-centric Narrative Creation,"Writers such as journalists often use automatic tools to find relevant content to include in their narratives. In this paper, we focus on supporting writers in the news domain to develop event-centric narratives. Given an incomplete narrative that specifies a main event and a context, we aim to retrieve news articles that discuss relevant events that would enable the continuation of the narrative. We formally define this task and propose a retrieval dataset construction procedure that relies on existing news articles to simulate incomplete narratives and relevant articles. Experiments on two datasets derived from this procedure show that state-of-the-art lexical and semantic rankers are not sufficient for this task. We show that combining those with a ranker that ranks articles by reverse chronological order outperforms those rankers alone. We also perform an in-depth quantitative and qualitative analysis of the results that sheds light on the characteristics of this task.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Automated Storytelling via Causal, Commonsense Plot Ordering","Automated story plot generation is the task of generating a coherent sequence of plot events. Causal relations between plot events are believed to increase the perception of story and plot coherence. In this work, we introduce the concept of soft causal relations as causal relations inferred from commonsense reasoning. We demonstrate C2PO, an approach to narrative generation that operationalizes this concept through Causal, Commonsense Plot Ordering. Using human-participant protocols, we evaluate our system against baseline systems with different commonsense reasoning reasoning and inductive biases to determine the role of soft causal relations in perceived story quality. Through these studies we also probe the interplay of how changes in commonsense norms across storytelling genres affect perceptions of story quality.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Machine Learning Framework for Authorship Identification From Texts,"Authorship identification is a process in which the author of a text is identified. Most known literary texts can easily be attributed to a certain author because they are, for example, signed. Yet sometimes we find unfinished pieces of work or a whole bunch of manuscripts with a wide variety of possible authors. In order to assess the importance of such a manuscript, it is vital to know who wrote it. In this work, we aim to develop a machine learning framework to effectively determine authorship. We formulate the task as a single-label multi-class text categorization problem and propose a supervised machine learning framework incorporating stylometric features. This task is highly interdisciplinary in that it takes advantage of machine learning, information retrieval, and natural language processing. We present an approach and a model which learns the differences in writing style between $50$ different authors and is able to predict the author of a new text with high accuracy. The accuracy is seen to increase significantly after introducing certain linguistic stylometric features along with text features.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning,"Visual storytelling systems generate multi-sentence stories from image sequences. In this task, capturing contextual information and bridging visual variation bring additional challenges. We propose a simple yet effective framework that leverages the generalization capabilities of pretrained foundation models, only training a lightweight vision-language mapping network to connect modalities, while incorporating context to enhance coherence. We introduce a multimodal contrastive objective that also improves visual relevance and story informativeness. Extensive experimental results, across both automatic metrics and human evaluations, demonstrate that the stories generated by our framework are diverse, coherent, informative, and interesting.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
GLAC Net: GLocal Attention Cascading Networks for Multi-image Cued Story Generation,"The task of multi-image cued story generation, such as visual storytelling dataset (VIST) challenge, is to compose multiple coherent sentences from a given sequence of images. The main difficulty is how to generate image-specific sentences within the context of overall images. Here we propose a deep learning network model, GLAC Net, that generates visual stories by combining global-local (glocal) attention and context cascading mechanisms. The model incorporates two levels of attention, i.e., overall encoding level and image feature level, to construct image-dependent sentences. While standard attention configuration needs a large number of parameters, the GLAC Net implements them in a very simple way via hard connections from the outputs of encoders or image features onto the sentence generators. The coherency of the generated story is further improved by conveying (cascading) the information of the previous sentence to the next sentence serially. We evaluate the performance of the GLAC Net on the visual storytelling dataset (VIST) and achieve very competitive results compared to the state-of-the-art techniques. Our code and pre-trained models are available here.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The jsRealB Text Realizer: Organization and Use Cases -- Revised version,"This paper describes the design principles behind jsRealB (Version 4.0), a surface realizer written JavaScript for English or French sentences from a specification inspired by the constituent syntax formalism but for which a dependency-based input notation is also available. jsRealB can be used either within a web page or as a node.js module. We show that the seemingly simple process of text realization involves many interesting implementation challenges in order to take into account the specifics of each language. jsRealB has a large coverage of English and French and has been used to develop realistic data-to-text applications and to reproduce existing literary texts and sentences from Universal Dependency annotations. Its source code and that of its applications are available on GitHub. The port of this approach to Python (pyrealb) is also presented.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Listening Between the Lines: Decoding Podcast Narratives with Language Modeling,"Podcasts have become a central arena for shaping public opinion, making them a vital source for understanding contemporary discourse. Their typically unscripted, multi-themed, and conversational style offers a rich but complex form of data. To analyze how podcasts persuade and inform, we must examine their narrative structures -- specifically, the narrative frames they employ.   The fluid and conversational nature of podcasts presents a significant challenge for automated analysis. We show that existing large language models, typically trained on more structured text such as news articles, struggle to capture the subtle cues that human listeners rely on to identify narrative frames. As a result, current approaches fall short of accurately analyzing podcast narratives at scale.   To solve this, we develop and evaluate a fine-tuned BERT model that explicitly links narrative frames to specific entities mentioned in the conversation, effectively grounding the abstract frame in concrete details. Our approach then uses these granular frame labels and correlates them with high-level topics to reveal broader discourse trends. The primary contributions of this paper are: (i) a novel frame-labeling methodology that more closely aligns with human judgment for messy, conversational data, and (ii) a new analysis that uncovers the systematic relationship between what is being discussed (the topic) and how it is being presented (the frame), offering a more robust framework for studying influence in digital media.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Grounding Characters and Places in Narrative Texts,"Tracking characters and locations throughout a story can help improve the understanding of its plot structure. Prior research has analyzed characters and locations from text independently without grounding characters to their locations in narrative time. Here, we address this gap by proposing a new spatial relationship categorization task. The objective of the task is to assign a spatial relationship category for every character and location co-mention within a window of text, taking into consideration linguistic context, narrative tense, and temporal scope. To this end, we annotate spatial relationships in approximately 2500 book excerpts and train a model using contextual embeddings as features to predict these relationships. When applied to a set of books, this model allows us to test several hypotheses on mobility and domestic space, revealing that protagonists are more mobile than non-central characters and that women as characters tend to occupy more interior space than men. Overall, our work is the first step towards joint modeling and analysis of characters and places in narrative text.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts,"The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Unsupervised extraction of local and global keywords from a single text,"We propose an unsupervised, corpus-independent method to extract keywords from a single text. It is based on the spatial distribution of words and the response of this distribution to a random permutation of words. As compared to existing methods (such as e.g. YAKE) our method has three advantages. First, it is significantly more effective at extracting keywords from long texts. Second, it allows inference of two types of keywords: local and global. Third, it uncovers basic themes in texts. Additionally, our method is language-independent and applies to short texts. The results are obtained via human annotators with previous knowledge of texts from our database of classical literary works (the agreement between annotators is from moderate to substantial). Our results are supported via human-independent arguments based on the average length of extracted content words and on the average number of nouns in extracted words. We discuss relations of keywords with higher-order textual features and reveal a connection between keywords and chapter divisions.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's Decameron,"We explore Boccaccio's Decameron to see how digital humanities tools can be used for tasks that have limited data in a language no longer in contemporary use: medieval Italian. We focus our analysis on the question: Do the different storytellers in the text exhibit distinct personalities? To answer this question, we curate and release a dataset based on the authoritative edition of the text. We use supervised classification methods to predict storytellers based on the stories they tell, confirming the difficulty of the task, and demonstrate that topic modeling can extract thematic storyteller ""profiles.""","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI,"Here we introduce Narrative Context Protocol (NCP), an open-source narrative standard designed to enable narrative interoperability, AI-driven authoring tools, real-time emergent narratives, and more. By encoding a story's structure in a ""Storyform,"" which is a structured register of its narrative features, NCP enables narrative portability across systems as well as intent-based constraints for generative storytelling systems. We demonstrate the capabilities of NCP through a year-long experiment, during which an author used NCP and a custom authoring platform to create a playable, text-based experience based on her pre-existing novella. This experience is driven by generative AI, with unconstrained natural language input. NCP functions as a set of ""guardrails"" that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Learning to Generate Text in Arbitrary Writing Styles,"Prior work in style-controlled text generation has focused on tasks such as emulating the style of prolific literary authors, producing formal or informal text, and mitigating toxicity of generated text. Plentiful demonstrations of these styles are available, and as a result modern language models are often able to emulate them, either via prompting or discriminative control. However, in applications such as writing assistants, it is desirable for language models to produce text in an author-specific style on the basis of a potentially small writing sample. For example, someone writing in a particular dialect may prefer writing suggestions that retain the same dialect. We find that instruction-tuned language models can struggle to reproduce author-specific style demonstrated in a prompt. Instead, we propose to guide a language model to generate text in a target style using contrastively-trained representations that capture stylometric features. Our approach (StyleMC) combines an author-adapted language model with sequence-level inference to improve stylistic consistency, and is found to be effective in a variety of conditions, including unconditional generation and style transfer. Additionally, we find that the proposed approach can serve as an effective anonymization method, by editing a document to mask authorship while preserving the original meaning","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Historical German Text Normalization Using Type- and Token-Based Language Modeling,"Historic variations of spelling poses a challenge for full-text search or natural language processing on historical digitized texts. To minimize the gap between the historic orthography and contemporary spelling, usually an automatic orthographic normalization of the historical source material is pursued. This report proposes a normalization system for German literary texts from c. 1700-1900, trained on a parallel corpus. The proposed system makes use of a machine learning approach using Transformer language models, combining an encoder-decoder model to normalize individual word types, and a pre-trained causal language model to adjust these normalizations within their context. An extensive evaluation shows that the proposed system provides state-of-the-art accuracy, comparable with a much larger fully end-to-end sentence-based normalization system, fine-tuning a pre-trained Transformer large language model. However, the normalization of historical text remains a challenge due to difficulties for models to generalize, and the lack of extensive high-quality parallel data.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification,"The proliferation of online news and the increasing spread of misinformation necessitate robust methods for automatic data analysis. Narrative classification is emerging as a important task, since identifying what is being said online is critical for fact-checkers, policy markers and other professionals working on information studies. This paper presents our approach to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a pre-defined two-level taxonomy of main narratives and sub-narratives across multiple languages.   We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual narrative classification. Our methodology follows a three-step Large Language Model (LLM) prompting strategy, where the model first categorises an article into one of two domains (Ukraine-Russia War or Climate Change), then identifies the most relevant main narratives, and finally assigns sub-narratives. Our approach secured the top position on the English test set among 28 competing teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Integrated ensemble of BERT- and features-based models for authorship attribution in Japanese literary works,"Traditionally, authorship attribution (AA) tasks relied on statistical data analysis and classification based on stylistic features extracted from texts. In recent years, pre-trained language models (PLMs) have attracted significant attention in text classification tasks. However, although they demonstrate excellent performance on large-scale short-text datasets, their effectiveness remains under-explored for small samples, particularly in AA tasks. Additionally, a key challenge is how to effectively leverage PLMs in conjunction with traditional feature-based methods to advance AA research. In this study, we aimed to significantly improve performance using an integrated integrative ensemble of traditional feature-based and modern PLM-based methods on an AA task in a small sample. For the experiment, we used two corpora of literary works to classify 10 authors each. The results indicate that BERT is effective, even for small-sample AA tasks. Both BERT-based and classifier ensembles outperformed their respective stand-alone models, and the integrated ensemble approach further improved the scores significantly. For the corpus that was not included in the pre-training data, the integrated ensemble improved the F1 score by approximately 14 points, compared to the best-performing single model. Our methodology provides a viable solution for the efficient use of the ever-expanding array of data processing tools in the foreseeable future.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Analysis of Railway Accidents' Narratives Using Deep Learning,"Automatic understanding of domain specific texts in order to extract useful relationships for later use is a non-trivial task. One such relationship would be between railroad accidents' causes and their correspondent descriptions in reports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B. Railroads involved in accidents are required to submit an accident report to the Federal Railroad Administration (FRA). These reports contain a variety of fixed field entries including primary cause of the accidents (a coded variable with 389 values) as well as a narrative field which is a short text description of the accident. Although these narratives provide more information than a fixed field entry, the terminologies used in these reports are not easy to understand by a non-expert reader. Therefore, providing an assisting method to fill in the primary cause from such domain specific texts(narratives) would help to label the accidents with more accuracy. Another important question for transportation safety is whether the reported accident cause is consistent with narrative description. To address these questions, we applied deep learning methods together with powerful word embeddings such as Word2Vec and GloVe to classify accident cause values for the primary cause field using the text in the narratives. The results show that such approaches can both accurately classify accident causes based on report narratives and find important inconsistencies in accident reporting.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Long Story Generation via Knowledge Graph and Literary Theory,"The generation of a long story consisting of several thousand words is a sub-task in the field of long text generation~(LTG). Previous research has addressed this challenge through outline-based generation, which employs a multi-stage method for generating outlines into stories. However, this approach suffers from two common issues: almost inevitable theme drift caused by the loss of memory of previous outlines, and tedious plots with incoherent logic that are less appealing to human readers.   In this paper, we propose the multi-agent Story Generator structure to improve the multi-stage method, using large language models~(LLMs) as the core components of agents. To avoid theme drift, we introduce a memory storage model comprising two components: a long-term memory storage that identifies the most important memories, thereby preventing theme drift; and a short-term memory storage that retains the latest outlines from each generation round. To incorporate engaging elements into the story, we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline. This framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content. Additionally, we establish a multi-agent interaction stage to simulate writer-reader interaction through dialogue and revise the story text according to feedback, to ensure it remains consistent and logical. Evaluations against previous methods demonstrate that our approach can generate higher-quality long stories.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding,"With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and mainly assess ""visual elements"" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes","Conventional bag-of-words approaches for topic modeling, like latent Dirichlet allocation (LDA), struggle with literary text. Literature challenges lexical methods because narrative language focuses on immersive sensory details instead of abstractive description or exposition: writers are advised to ""show, don't tell."" We propose Retell, a simple, accessible topic modeling approach for literature. Here, we prompt resource-efficient, generative language models (LMs) to tell what passages show, thereby translating narratives' surface forms into higher-level concepts and themes. By running LDA on LMs' retellings of passages, we can obtain more precise and informative topics than by running LDA alone or by directly asking LMs to list topics. To investigate the potential of our method for cultural analytics, we compare our method's outputs to expert-guided annotations in a case study on racial/cultural identity in high school English language arts books.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Analyzing Film Adaptation through Narrative Alignment,"Novels are often adapted into feature films, but the differences between the two media usually require dropping sections of the source text from the movie script. Here we study this screen adaptation process by constructing narrative alignments using the Smith-Waterman local alignment algorithm coupled with SBERT embedding distance to quantify text similarity between scenes and book units. We use these alignments to perform an automated analysis of 40 adaptations, revealing insights into the screenwriting process concerning (i) faithfulness of adaptation, (ii) importance of dialog, (iii) preservation of narrative order, and (iv) gender representation issues reflective of the Bechdel test.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus,"We present a pipeline for a statistical textual exploration, offering a stylometry-based explanation and statistical validation of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and quantifies the importance of features most responsible for the classification, estimates their statistical stability and cluster-wise abundance.   We apply our pipeline to the first two books in the Bible, where one stylistic component stands out in the eyes of biblical scholars, namely, the Priestly component. We identify and explore statistically significant stylistic differences between the Priestly and non-Priestly components.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Extracting narrative signals from public discourse: a network-based approach,"Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, transcripts of parliamentary debates, and political manifestos on party websites. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors and their relationships, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that allude to larger political narratives. By systematically analyzing and re-assembling these signals into networks that guide the researcher to the relevant parts of the text, the underlying narratives can be reconstructed through a combination of distant and close reading. A case study of State of the European Union addresses (2010 -- 2023) demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks,"Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech, as well as, normal, prepared speech, and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
LSTM-Based Text Generation: A Study on Historical Datasets,"This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the work of Shakespeare is 0.9125, indicating a low error rate. The training time of the model is 100, mirroring the efficiency of the Nietzsche dataset. This efficiency demonstrates the effectiveness of the model design and training methodology, especially when handling complex literary texts. This research contributes to the field of natural language processing by showcasing the versatility of LSTM networks in text generation and offering a pathway for future explorations in historical linguistics and beyond.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Heaps' law and Heaps functions in tagged texts: Evidences of their linguistic relevance,"We study the relationship between vocabulary size and text length in a corpus of $75$ literary works in English, authored by six writers, distinguishing between the contributions of three grammatical classes (or ``tags,'' namely, {\it nouns}, {\it verbs}, and {\it others}), and analyze the progressive appearance of new words of each tag along each individual text. While the power-law relation prescribed by Heaps' law is satisfactorily fulfilled by total vocabulary sizes and text lengths, the appearance of new words in each text is on the whole well described by the average of random shufflings of the text, which does not obey a power law. Deviations from this average, however, are statistically significant and show a systematic trend across the corpus. Specifically, they reveal that the appearance of new words along each text is predominantly retarded with respect to the average of random shufflings. Moreover, different tags are shown to add systematically distinct contributions to this tendency, with {\it verbs} and {\it others} being respectively more and less retarded than the mean trend, and {\it nouns} following instead this overall mean. These statistical systematicities are likely to point to the existence of linguistically relevant information stored in the different variants of Heaps' law, a feature that is still in need of extensive assessment.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series,"Serialized TV shows are built on complex storylines that can be hard to track and evolve in ways that defy straightforward analysis. This paper introduces a multi-agent system designed to extract and analyze these narrative arcs. Tested on the first season of Grey's Anatomy (ABC 2005-), the system identifies three types of arcs: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific (strictly related to the series' genre). Episodic progressions of these arcs are stored in both relational and semantic (vectorial) databases, enabling structured analysis and comparison. To bridge the gap between automation and critical interpretation, the system is paired with a graphical interface that allows for human refinement using tools to enhance and visualize the data. The system performed strongly in identifying Anthology Arcs and character entities, but its reliance on textual paratexts (such as episode summaries) revealed limitations in recognizing overlapping arcs and subtler dynamics. This approach highlights the potential of combining computational and human expertise in narrative analysis. Beyond television, it offers promise for serialized written formats, where the narrative resides entirely in the text. Future work will explore the integration of multimodal inputs, such as dialogue and visuals, and expand testing across a wider range of genres to refine the system further.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Once Upon a Time: Interactive Learning for Storytelling with Small Language Models,"Children efficiently acquire language not just by listening, but by interacting with others in their social environment. Conversely, large language models are typically trained with next-word prediction on massive amounts of text. Motivated by this contrast, we investigate whether language models can be trained with less data by learning not only from next-word prediction but also from high-level, cognitively inspired feedback. We train a student model to generate stories, which a teacher model rates on readability, narrative coherence, and creativity. By varying the amount of pretraining before the feedback loop, we assess the impact of this interactive learning on formal and functional linguistic competence. We find that the high-level feedback is highly data efficient: With just 1 M words of input in interactive learning, storytelling skills can improve as much as with 410 M words of next-word prediction.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives,"This paper introduces the concept of an education tool that utilizes Generative Artificial Intelligence (GenAI) to enhance storytelling. We evaluate GenAI-driven narrative co-creation, text-to-speech conversion, text-to-music and text-to-video generation to produce an engaging experience for learners. We describe the co-creation process, the adaptation of narratives into spoken words using text-to-speech models, and the transformation of these narratives into contextually relevant visuals through text-to-video technology. Our evaluation covers the linguistics of the generated stories, the text-to-speech conversion quality, and the accuracy of the generated visuals.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Conflicting narratives and polarization on social media,"Narratives are key interpretative devices by which humans make sense of political reality. In this work, we show how the analysis of conflicting narratives, i.e. conflicting interpretive lenses through which political reality is experienced and told, provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere. Building upon previous work that has identified ideologically polarized issues in the German Twittersphere between 2021 and 2023, we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups. Focusing on a selection of salient issues and events (the war in Ukraine, Covid, climate change), we show evidence for conflicting narratives along two dimensions: (i) different attributions of actantial roles to the same set of actants (e.g. diverging interpretations of the role of NATO in the war in Ukraine), and (ii) emplotment of different actants for the same event (e.g. Bill Gates in the right-leaning Covid narrative). Furthermore, we provide first evidence for patterns of narrative alignment, a discursive strategy that political actors employ to align opinions across issues. These findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling,"Visual Storytelling~(VIST) is a task to tell a narrative story about a certain topic according to the given photo stream. The existing studies focus on designing complex models, which rely on a huge amount of human-annotated data. However, the annotation of VIST is extremely costly and many topics cannot be covered in the training dataset due to the long-tail topic distribution. In this paper, we focus on enhancing the generalization ability of the VIST model by considering the few-shot setting. Inspired by the way humans tell a story, we propose a topic adaptive storyteller to model the ability of inter-topic generalization. In practice, we apply the gradient-based meta-learning algorithm on multi-modal seq2seq models to endow the model the ability to adapt quickly from topic to topic. Besides, We further propose a prototype encoding structure to model the ability of intra-topic derivation. Specifically, we encode and restore the few training story text to serve as a reference to guide the generation at inference time. Experimental results show that topic adaptation and prototype encoding structure mutually bring benefit to the few-shot model on BLEU and METEOR metric. The further case study shows that the stories generated after few-shot adaptation are more relative and expressive.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Non-Standard Words as Features for Text Categorization,"This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. Non-Standard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Results of a Single Blind Literary Taste Test with Short Anonymized Novel Fragments,"It is an open question to what extent perceptions of literary quality are derived from text-intrinsic versus social factors. While supervised models can predict literary quality ratings from textual factors quite successfully, as shown in the Riddle of Literary Quality project (Koolen et al., 2020), this does not prove that social factors are not important, nor can we assume that readers make judgments on literary quality in the same way and based on the same information as machine learning models. We report the results of a pilot study to gauge the effect of textual features on literary ratings of Dutch-language novels by participants in a controlled experiment with 48 participants. In an exploratory analysis, we compare the ratings to those from the large reader survey of the Riddle in which social factors were not excluded, and to machine learning predictions of those literary ratings. We find moderate to strong correlations of questionnaire ratings with the survey ratings, but the predictions are closer to the survey ratings. Code and data: https://github.com/andreasvc/litquest","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
DreamNet: A Multimodal Framework for Semantic and Emotional Analysis of Sleep Narratives,"Dream narratives provide a unique window into human cognition and emotion, yet their systematic analysis using artificial intelligence has been underexplored. We introduce DreamNet, a novel deep learning framework that decodes semantic themes and emotional states from textual dream reports, optionally enhanced with REM-stage EEG data. Leveraging a transformer-based architecture with multimodal attention, DreamNet achieves 92.1% accuracy and 88.4% F1-score in text-only mode (DNet-T) on a curated dataset of 1,500 anonymized dream narratives, improving to 99.0% accuracy and 95.2% F1-score with EEG integration (DNet-M). Strong dream-emotion correlations (e.g., falling-anxiety, r = 0.91, p < 0.01) highlight its potential for mental health diagnostics, cognitive science, and personalized therapy. This work provides a scalable tool, a publicly available enriched dataset, and a rigorous methodology, bridging AI and psychological research.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Agents' Room: Narrative Generation through Multi-step Collaboration,"Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
What distinguishes conspiracy from critical narratives? A computational analysis of oppositional discourse,"The current prevalence of conspiracy theories on the internet is a significant issue, tackled by many computational approaches. However, these approaches fail to recognize the relevance of distinguishing between texts which contain a conspiracy theory and texts which are simply critical and oppose mainstream narratives. Furthermore, little attention is usually paid to the role of inter-group conflict in oppositional narratives. We contribute by proposing a novel topic-agnostic annotation scheme that differentiates between conspiracies and critical texts, and that defines span-level categories of inter-group conflict. We also contribute with the multilingual XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality annotation of Telegram messages related to COVID-19 (5,000 messages per language). We also demonstrate the feasibility of an NLP-based automatization by performing a range of experiments that yield strong baseline solutions. Finally, we perform an analysis which demonstrates that the promotion of intergroup conflict and the presence of violence and anger are key aspects to distinguish between the two types of oppositional narratives, i.e., conspiracy vs. critical.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds,"Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"""My Way of Telling a Story"": Persona based Grounded Story Generation","Visual storytelling is the task of generating stories based on a sequence of images. Inspired by the recent works in neural generation focusing on controlling the form of text, this paper explores the idea of generating these stories in different personas. However, one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas. Having said that, there are independent datasets for both visual storytelling and annotated sentences for various persona. In this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation. We inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction. To this end, we propose five models which are incremental extensions to the baseline model to perform the task at hand. In our experiments we use five different personas to guide the generation process. We find that the models based on our hypotheses perform better at capturing words while generating stories in the target persona.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques,"With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
RealityTalk: Real-Time Speech-Driven Augmented Presentation for AR Live Storytelling,"We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter's perspective to demonstrate the effectiveness of our system.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Commonsense Knowledge Aware Concept Selection For Diverse and Informative Visual Storytelling,"Visual storytelling is a task of generating relevant and interesting stories for given image sequences. In this work we aim at increasing the diversity of the generated stories while preserving the informative content from the images. We propose to foster the diversity and informativeness of a generated story by using a concept selection module that suggests a set of concept candidates. Then, we utilize a large scale pre-trained model to convert concepts and images into full stories. To enrich the candidate concepts, a commonsense knowledge graph is created for each image sequence from which the concept candidates are proposed. To obtain appropriate concepts from the graph, we propose two novel modules that consider the correlation among candidate concepts and the image-concept correlation. Extensive automatic and human evaluation results demonstrate that our model can produce reasonable concepts. This enables our model to outperform the previous models by a large margin on the diversity and informativeness of the story, while retaining the relevance of the story to the image sequence.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Automatic Translation Alignment Pipeline for Multilingual Digital Editions of Literary Works,"This paper investigates the application of translation alignment algorithms in the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's Italian novel ""I promessi sposi"" (""The Betrothed""), with translations in eight languages (English, Spanish, French, German, Dutch, Polish, Russian and Chinese) from the 19th and 20th centuries. We identify key requirements for the MDE to improve both the reader experience and support for translation studies. Our research highlights the limitations of current state-of-the-art algorithms when applied to the translation of literary texts and outlines an automated pipeline for MDE creation. This pipeline transforms raw texts into web-based, side-by-side representations of original and translated texts with different rendering options. In addition, we propose new metrics for evaluating the alignment of literary translations and suggest visualization techniques for future analysis.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,"Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving both training procedures and their overall efficiency. Literary translation is less constrained than more traditional MT settings since translators must balance meaning equivalence, readability, and critical interpretability in the target language. This property, along with the complex discourse-level context present in literary texts, also makes literary MT more challenging to computationally model and evaluate. To explore this task, we collect a dataset (Par3) of non-English language novels in the public domain, each aligned at the paragraph level to both human and automatic English translations. Using Par3, we discover that expert literary translators prefer reference human translations over machine-translated paragraphs at a rate of 84%, while state-of-the-art automatic MT metrics do not correlate with those preferences. The experts note that MT outputs contain not only mistranslations, but also discourse-disrupting errors and stylistic inconsistencies. To address these problems, we train a post-editing model whose output is preferred over normal MT output at a rate of 69% by experts. We publicly release Par3 at https://github.com/katherinethai/par3/ to spur future research into literary MT.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Modeling Event Salience in Narratives via Barthes' Cardinal Functions,"Events in a narrative differ in salience: some are more important to the story than others. Estimating event salience is useful for tasks such as story generation, and as a tool for text analysis in narratology and folkloristics. To compute event salience without any annotations, we adopt Barthes' definition of event salience and propose several unsupervised methods that require only a pre-trained language model. Evaluating the proposed methods on folktales with event salience annotation, we show that the proposed methods outperform baseline methods and find fine-tuning a language model on narrative texts is a key factor in improving the proposed methods.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Automatic Detection of Reuses and Citations in Literary Texts,"For more than forty years now, modern theories of literature (Compagnon, 1979) insist on the role of paraphrases, rewritings, citations, reciprocal borrowings and mutual contributions of any kinds. The notions of intertextuality, transtextuality, hypertextuality/hypotextuality, were introduced in the seventies and eighties to approach these phenomena. The careful analysis of these references is of particular interest in evaluating the distance that the creator voluntarily introduces with his/her masters. Phoebus is collaborative project that makes computer scientists from the University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary teams of Paris-Sorbonne University with the aim to develop efficient tools for literary studies that take advantage of modern computer science techniques. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This paper describes the principles on which is based this program, the significant results that have already been obtained and the perspectives for the near future.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
From narrative descriptions to MedDRA: automagically encoding adverse drug reactions,"The collection of narrative spontaneous reports is an irreplaceable source for the prompt detection of suspected adverse drug reactions (ADRs): qualified domain experts manually revise a huge amount of narrative descriptions and then encode texts according to MedDRA standard terminology. The manual annotation of narrative documents with medical terminology is a subtle and expensive task, since the number of reports is growing up day-by-day. MagiCoder, a Natural Language Processing algorithm, is proposed for the automatic encoding of free-text descriptions into MedDRA terms. MagiCoder procedure is efficient in terms of computational complexity (in particular, it is linear in the size of the narrative input and the terminology). We tested it on a large dataset of about 4500 manually revised reports, by performing an automated comparison between human and MagiCoder revisions. For the current base version of MagiCoder, we measured: on short descriptions, an average recall of $86\%$ and an average precision of $88\%$; on medium-long descriptions (up to 255 characters), an average recall of $64\%$ and an average precision of $63\%$. From a practical point of view, MagiCoder reduces the time required for encoding ADR reports. Pharmacologists have simply to review and validate the MagiCoder terms proposed by the application, instead of choosing the right terms among the 70K low level terms of MedDRA. Such improvement in the efficiency of pharmacologists' work has a relevant impact also on the quality of the subsequent data analysis. We developed MagiCoder for the Italian pharmacovigilance language. However, our proposal is based on a general approach, not depending on the considered language nor the term dictionary.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Textual Analysis for Studying Chinese Historical Documents and Literary Novels,"We analyzed historical and literary documents in Chinese to gain insights into research issues, and overview our studies which utilized four different sources of text materials in this paper. We investigated the history of concepts and transliterated words in China with the Database for the Study of Modern China Thought and Literature, which contains historical documents about China between 1830 and 1930. We also attempted to disambiguate names that were shared by multiple government officers who served between 618 and 1912 and were recorded in Chinese local gazetteers. To showcase the potentials and challenges of computer-assisted analysis of Chinese literatures, we explored some interesting yet non-trivial questions about two of the Four Great Classical Novels of China: (1) Which monsters attempted to consume the Buddhist monk Xuanzang in the Journey to the West (JTTW), which was published in the 16th century, (2) Which was the most powerful monster in JTTW, and (3) Which major role smiled the most in the Dream of the Red Chamber, which was published in the 18th century. Similar approaches can be applied to the analysis and study of modern documents, such as the newspaper articles published about the 228 incident that occurred in 1947 in Taiwan.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Computational Discovery of Chiasmus in Ancient Religious Text,"Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected patterns. Despite its computational efficiency, our method achieves robust results, with high inter-annotator agreement and system precision@k of 0.80 at the verse level and 0.60 at the half-verse level. We further provide a qualitative analysis of the distribution of detected chiasmi, along with selected examples that highlight the effectiveness of our approach.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Generating Different Story Tellings from Semantic Representations of Narrative,"In order to tell stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (NLG). However, there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events. In this paper we present an automatic method for converting from Scheherazade's story intention graph, a semantic representation, to the input required by the Personage NLG engine. Using 36 Aesop Fables distributed in DramaBank, a collection of story encodings, we train translation rules on one story and then test these rules by generating text for the remaining 35. The results are measured in terms of the string similarity metrics Levenshtein Distance and BLEU score. The results show that we can generate the 35 stories with correct content: the test set stories on average are close to the output of the Scheherazade realizer, which was customized to this semantic representation. We provide some examples of story variations generated by personage. In future work, we will experiment with measuring the quality of the same stories generated in different voices, and with techniques for making storytelling interactive.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse,"How can we detect when global events fundamentally reshape public discourse? This study introduces a topological framework for identifying structural change in media narratives using persistent homology. Drawing on international news articles surrounding major events - including the Russian invasion of Ukraine (Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol insurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we construct daily co-occurrence graphs of noun phrases to trace evolving discourse. Each graph is embedded and transformed into a persistence diagram via a Vietoris-Rips filtration. We then compute Wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time. Our results show that major geopolitical and social events align with sharp spikes in both H0 (connected components) and H1 (loops), indicating sudden reorganization in narrative structure and coherence. Cross-correlation analyses reveal a typical lag pattern in which changes to component-level structure (H0) precede higher-order motif shifts (H1), suggesting a bottom-up cascade of semantic change. An exception occurs during the Russian invasion of Ukraine, where H1 entropy leads H0, possibly reflecting top-down narrative framing before local discourse adjusts. Persistence entropy further distinguishes tightly focused from diffuse narrative regimes. These findings demonstrate that persistent homology offers a mathematically principled, unsupervised method for detecting inflection points and directional shifts in public attention - without requiring prior knowledge of specific events. This topological approach advances computational social science by enabling real-time detection of semantic restructuring during crises, protests, and information shocks.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts,"We present and make available MedLatinEpi and MedLatinLit, two datasets of medieval Latin texts to be used in research on computational authorship analysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts, respectively, labelled by author; MedLatinEpi texts are of epistolary nature, while MedLatinLit texts consist of literary comments and treatises about various subjects. As such, these two datasets lend themselves to supporting research in authorship analysis tasks, such as authorship attribution, authorship verification, or same-author verification. Along with the datasets we provide experimental results, obtained on these datasets, for the authorship verification task, i.e., the task of predicting whether a text of unknown authorship was written by a candidate author or not. We also make available the source code of the authorship verification system we have used, thus allowing our experiments to be reproduced, and to be used as baselines, by other researchers. We also describe the application of the above authorship verification system, using these datasets as training data, for investigating the authorship of two medieval epistles whose authorship has been disputed by scholars.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games,"Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem. Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player. When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical. In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves. To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results. We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing,"Post-editing machine translation (MT) for creative texts, such as literature, requires balancing efficiency with the preservation of creativity and style. While neural MT systems struggle with these challenges, large language models (LLMs) offer improved capabilities for context-aware and creative translation. This study evaluates the feasibility of post-editing literary translations generated by LLMs. Using a custom research tool, we collaborated with professional literary translators to analyze editing time, quality, and creativity. Our results indicate that post-editing LLM-generated translations significantly reduces editing time compared to human translation while maintaining a similar level of creativity. The minimal difference in creativity between PE and MT, combined with substantial productivity gains, suggests that LLMs may effectively support literary translators working with high-resource languages.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Quasi Error-free Text Classification and Authorship Recognition in a large Corpus of English Literature based on a Novel Feature Set,"The Gutenberg Literary English Corpus (GLEC) provides a rich source of textual data for research in digital humanities, computational linguistics or neurocognitive poetics. However, so far only a small subcorpus, the Gutenberg English Poetry Corpus, has been submitted to quantitative text analyses providing predictions for scientific studies of literature. Here we show that in the entire GLEC quasi error-free text classification and authorship recognition is possible with a method using the same set of five style and five content features, computed via style and sentiment analysis, in both tasks. Our results identify two standard and two novel features (i.e., type-token ratio, frequency, sonority score, surprise) as most diagnostic in these tasks. By providing a simple tool applicable to both short poems and long novels generating quantitative predictions about features that co-determe the cognitive and affective processing of specific text categories or authors, our data pave the way for many future computational and empirical studies of literature or experiments in reading psychology.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Computational Analysis of Character Development in Holocaust Testimonies,"This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes the inner and outer changes the protagonist undergoes within a narrative, and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice along the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, most present a constant disposition, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,"Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles -- written by students from across the country -- we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classified as high quality. We then demonstrate that the filter's measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"A Lexical, Syntactic, and Semantic Perspective for Understanding Style in Text","With a growing interest in modeling inherent subjectivity in natural language, we present a linguistically-motivated process to understand and analyze the writing style of individuals from three perspectives: lexical, syntactic, and semantic. We discuss the stylistically expressive elements within each of these levels and use existing methods to quantify the linguistic intuitions related to some of these elements. We show that such a multi-level analysis is useful for developing a well-knit understanding of style - which is independent of the natural language task at hand, and also demonstrate its value in solving three downstream tasks: authors' style analysis, authorship attribution, and emotion prediction. We conduct experiments on a variety of datasets, comprising texts from social networking sites, user reviews, legal documents, literary books, and newswire. The results on the aforementioned tasks and datasets illustrate that such a multi-level understanding of style, which has been largely ignored in recent works, models style-related subjectivity in text and can be leveraged to improve performance on multiple downstream tasks both qualitatively and quantitatively.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Toyteller: AI-powered Visual Storytelling Through Toy-Playing with Character Symbols,"We introduce Toyteller, an AI-powered storytelling system where users generate a mix of story text and visuals by directly manipulating character symbols like they are toy-playing. Anthropomorphized symbol motions can convey rich and nuanced social interactions; Toyteller leverages these motions (1) to let users steer story text generation and (2) as a visual output format that accompanies story text. We enabled motion-steered text generation and text-steered motion generation by mapping motions and text onto a shared semantic space so that large language models and motion generation models can use it as a translational layer. Technical evaluations showed that Toyteller outperforms a competitive baseline, GPT-4o. Our user study identified that toy-playing helps express intentions difficult to verbalize. However, only motions could not express all user intentions, suggesting combining it with other modalities like language. We discuss the design space of toy-playing interactions and implications for technical HCI research on human-AI interaction.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection,"Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \method, a simple and reliable way to predict detector performance changes in personalized settings. \method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales,"Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing methods that examine social bias in models and data corpora. Our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories. We propose a computational pipeline that automatically extracts a story's temporal narrative verb-based event chain for each of its characters as well as character attributes such as gender. We also present a verb-based event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes. Through a case study analyzing gender bias in fairy tales, we demonstrate that our framework can reveal bias in not only the unigram verb-based events in which female and male characters participate but also in the temporal narrative order of such event participation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms,"Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling","Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives","Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Breaking Language Barriers with MMTweets: Advancing Cross-Lingual Debunked Narrative Retrieval for Fact-Checking,"Finding previously debunked narratives involves identifying claims that have already undergone fact-checking. The issue intensifies when similar false claims persist in multiple languages, despite the availability of debunks for several months in another language. Hence, automatically finding debunks (or fact-checks) in multiple languages is crucial to make the best use of scarce fact-checkers' resources. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual scenario, i.e. the retrieval of debunks in a language different from the language of the online post being checked. This study introduces cross-lingual debunked narrative retrieval and addresses this research gap by: (i) creating Multilingual Misinformation Tweets (MMTweets): a dataset that stands out, featuring cross-lingual pairs, images, human annotations, and fine-grained labels, making it a comprehensive resource compared to its counterparts; (ii) conducting an extensive experiment to benchmark state-of-the-art cross-lingual retrieval models and introducing multistage retrieval methods tailored for the task; and (iii) comprehensively evaluating retrieval models for their cross-lingual and cross-dataset transfer capabilities within MMTweets, and conducting a retrieval latency analysis. We find that MMTweets presents challenges for cross-lingual debunked narrative retrieval, highlighting areas for improvement in retrieval models. Nonetheless, the study provides valuable insights for creating MMTweets datasets and optimising debunked narrative retrieval models to empower fact-checking endeavours. The dataset and annotation codebook are publicly available at https://doi.org/10.5281/zenodo.10637161.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
DENS: A Dataset for Multi-class Emotion Analysis,"We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives available on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Extraction and Analysis of Fictional Character Networks: A Survey,"A character network is a graph extracted from a narrative, in which vertices represent characters and edges correspond to interactions between them. A number of narrative-related problems can be addressed automatically through the analysis of character networks, such as summarization, classification, or role detection. Character networks are particularly relevant when considering works of fictions (e.g. novels, plays, movies, TV series), as their exploitation allows developing information retrieval and recommendation systems. However, works of fiction possess specific properties making these tasks harder. This survey aims at presenting and organizing the scientific literature related to the extraction of character networks from works of fiction, as well as their analysis. We first describe the extraction process in a generic way, and explain how its constituting steps are implemented in practice, depending on the medium of the narrative, the goal of the network analysis, and other factors. We then review the descriptive tools used to characterize character networks, with a focus on the way they are interpreted in this context. We illustrate the relevance of character networks by also providing a review of applications derived from their analysis. Finally, we identify the limitations of the existing approaches, and the most promising perspectives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Text Classification For Authorship Attribution Analysis,"Authorship attribution mainly deals with undecided authorship of literary texts. Authorship attribution is useful in resolving issues like uncertain authorship, recognize authorship of unknown texts, spot plagiarism so on. Statistical methods can be used to set apart the approach of an author numerically. The basic methodologies that are made use in computational stylometry are word length, sentence length, vocabulary affluence, frequencies etc. Each author has an inborn style of writing, which is particular to himself. Statistical quantitative techniques can be used to differentiate the approach of an author in a numerical way. The problem can be broken down into three sub problems as author identification, author characterization and similarity detection. The steps involved are pre-processing, extracting features, classification and author identification. For this different classifiers can be used. Here fuzzy learning classifier and SVM are used. After author identification the SVM was found to have more accuracy than Fuzzy classifier. Later combined the classifiers to obtain a better accuracy when compared to individual SVM and fuzzy classifier.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models,"The Greek fictional narratives often termed love novels or romances, ranging from the first century CE to the middle of the 15th century, have long been considered as similar in many ways, not least in the use of particular literary motifs. By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common, and in which ways they differ from each other. The results show that while some motifs persist throughout the corpus, others fluctuate in frequency, indicating certain trends or external influences. Conclusively, the method proves to adequately extract literary motifs according to a set definition, providing data for both quantitative and qualitative analyses.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?,"Large Multimodal Models (LMMs) have achieved remarkable success across various visual-language tasks. However, existing benchmarks predominantly focus on single-image understanding, leaving the analysis of image sequences largely unexplored. To address this limitation, we introduce StripCipher, a comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend and reason over sequential images. StripCipher comprises a human-annotated dataset and three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal narrative reordering. Our evaluation of 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a significant performance gap compared to human capabilities, particularly in tasks that require reordering shuffled sequential images. For instance, GPT-4o achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower than human performance. Further quantitative analysis discuss several factors, such as input format of images, affecting the performance of LLMs in sequential understanding, underscoring the fundamental challenges that remain in the development of LMMs.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Identifying Narrative Patterns and Outliers in Holocaust Testimonies Using Topic Modeling,"The vast collection of Holocaust survivor testimonies presents invaluable historical insights but poses challenges for manual analysis. This paper leverages advanced Natural Language Processing (NLP) techniques to explore the USC Shoah Foundation Holocaust testimony corpus. By treating testimonies as structured question-and-answer sections, we apply topic modeling to identify key themes. We experiment with BERTopic, which leverages recent advances in language modeling technology. We align testimony sections into fixed parts, revealing the evolution of topics across the corpus of testimonies. This highlights both a common narrative schema and divergences between subgroups based on age and gender. We introduce a novel method to identify testimonies within groups that exhibit atypical topic distributions resembling those of other groups. This study offers unique insights into the complex narratives of Holocaust survivors, demonstrating the power of NLP to illuminate historical discourse and identify potential deviations in survivor experiences.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA,"The performance of Video Question Answering (VideoQA) models is fundamentally constrained by the nature of their supervision, which typically consists of isolated, factual question-answer pairs. This ""bag-of-facts"" approach fails to capture the underlying narrative and causal structure of events, limiting models to a shallow understanding of video content. To move beyond this paradigm, we introduce a framework to synthesize richer supervisory signals. We propose two complementary strategies: Question-Based Paraphrasing (QBP), which synthesizes the diverse inquiries (what, how, why) from a video's existing set of question-answer pairs into a holistic narrative paragraph that reconstructs the video's event structure; and Question-Based Captioning (QBC), which generates fine-grained visual rationales, grounding the answer to each question in specific, relevant evidence. Leveraging powerful generative models, we use this synthetic data to train VideoQA models under a unified next-token prediction objective. Extensive experiments on STAR and NExT-QA validate our approach, demonstrating significant accuracy gains and establishing new state-of-the-art results, such as improving a 3B model to 72.5\% on STAR (+4.9\%) and a 7B model to 80.8\% on NExT-QA. Beyond accuracy, our analysis reveals that both QBP and QBC substantially enhance cross-dataset generalization, with QBP additionally accelerating model convergence by over 2.5x. These results demonstrate that shifting data synthesis from isolated facts to narrative coherence and grounded rationales yields a more accurate, efficient, and generalizable training paradigm.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing,"ChatGPT is a publicly available chatbot that can quickly generate texts on given topics, but it is unknown whether the chatbot is really superior to human writers in all aspects of writing and whether its writing quality can be prominently improved on the basis of updating commands. Consequently, this study compared the writing performance on a narrative topic by ChatGPT and Chinese intermediate English (CIE) learners so as to reveal the chatbot's advantage and disadvantage in writing. The data were analyzed in terms of five discourse components using Coh-Metrix (a special instrument for analyzing language discourses), and the results revealed that ChatGPT performed better than human writers in narrativity, word concreteness, and referential cohesion, but worse in syntactic simplicity and deep cohesion in its initial version. After more revision commands were updated, while the resulting version was facilitated in syntactic simplicity, yet it is still lagged far behind CIE learners' writing in deep cohesion. In addition, the correlation analysis of the discourse components suggests that narrativity was correlated with referential cohesion in both ChatGPT and human writers, but the correlations varied within each group.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Summary and Distance between Sets of Texts based on Topological Data Analysis,"In this paper, we use topological data analysis (TDA) tools such as persistent homology, persistent entropy and bottleneck distance, to provide a {\it TDA-based summary} of any given set of texts and a general method for computing a distance between any two literary styles, authors or periods. To this aim, deep-learning word-embedding techniques are combined with these tools in order to study the topological properties of texts embedded in a metric space. As a case of study, we use the written texts of three poets of the Spanish Golden Age: Francisco de Quevedo, Luis de Gngora and Lope de Vega. As far as we know, this is the first time that word embedding, bottleneck distance, persistent homology and persistent entropy are used together to characterize texts and to compare different literary styles.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives,"Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the ""gutters"" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called ""closure"". While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Decoding Linguistic Nuances in Mental Health Text Classification Using Expressive Narrative Stories,"Recent advancements in NLP have spurred significant interest in analyzing social media text data for identifying linguistic features indicative of mental health issues. However, the domain of Expressive Narrative Stories (ENS)-deeply personal and emotionally charged narratives that offer rich psychological insights-remains underexplored. This study bridges this gap by utilizing a dataset sourced from Reddit, focusing on ENS from individuals with and without self-declared depression. Our research evaluates the utility of advanced language models, BERT and MentalBERT, against traditional models. We find that traditional models are sensitive to the absence of explicit topic-related words, which could risk their potential to extend applications to ENS that lack clear mental health terminology. Despite MentalBERT is design to better handle psychiatric contexts, it demonstrated a dependency on specific topic words for classification accuracy, raising concerns about its application when explicit mental health terms are sparse (P-value<0.05). In contrast, BERT exhibited minimal sensitivity to the absence of topic words in ENS, suggesting its superior capability to understand deeper linguistic features, making it more effective for real-world applications. Both BERT and MentalBERT excel at recognizing linguistic nuances and maintaining classification accuracy even when narrative order is disrupted. This resilience is statistically significant, with sentence shuffling showing substantial impacts on model performance (P-value<0.05), especially evident in ENS comparisons between individuals with and without mental health declarations. These findings underscore the importance of exploring ENS for deeper insights into mental health-related narratives, advocating for a nuanced approach to mental health text analysis that moves beyond mere keyword detection.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
One Graph to Rule them All: Using NLP and Graph Neural Networks to analyse Tolkien's Legendarium,"Natural Language Processing and Machine Learning have considerably advanced Computational Literary Studies. Similarly, the construction of co-occurrence networks of literary characters, and their analysis using methods from social network analysis and network science, have provided insights into the micro- and macro-level structure of literary texts. Combining these perspectives, in this work we study character networks extracted from a text corpus of J.R.R. Tolkien's Legendarium. We show that this perspective helps us to analyse and visualise the narrative style that characterises Tolkien's works. Addressing character classification, embedding and co-occurrence prediction, we further investigate the advantages of state-of-the-art Graph Neural Networks over a popular word embedding method. Our results highlight the large potential of graph learning in Computational Literary Studies.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models,"This study investigates the relationship between deep learning (DL) model accuracy and expert agreement in classifying crash narratives. We evaluate five DL models -- including BERT variants, USE, and a zero-shot classifier -- against expert labels and narratives, and extend the analysis to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal an inverse relationship: models with higher technical accuracy often show lower agreement with human experts, while LLMs demonstrate stronger expert alignment despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis (PCA) to quantify and visualize model-expert agreement, and employ SHAP analysis to explain misclassifications. Results show that expert-aligned models rely more on contextual and temporal cues than location-specific keywords. These findings suggest that accuracy alone is insufficient for safety-critical NLP tasks. We argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of LLMs as interpretable tools in crash analysis pipelines.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL),"This research investigates the efficacy of machine learning (ML) and deep learning (DL) methods in detecting misclassified intersection-related crashes in police-reported narratives. Using 2019 crash data from the Iowa Department of Transportation, we implemented and compared a comprehensive set of models, including Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT Word Embeddings, and Albert Model. Model performance was systematically validated against expert reviews of potentially misclassified narratives, providing a rigorous assessment of classification accuracy. Results demonstrated that while traditional ML methods exhibited superior overall performance compared to some DL approaches, the Albert Model achieved the highest agreement with expert classifications (73% with Expert 1) and original tabular data (58%). Statistical analysis revealed that the Albert Model maintained performance levels similar to inter-expert consistency rates, significantly outperforming other approaches, particularly on ambiguous narratives. This work addresses a critical gap in transportation safety research through multi-modal integration analysis, which achieved a 54.2% reduction in error rates by combining narrative text with structured crash data. We conclude that hybrid approaches combining automated classification with targeted expert review offer a practical methodology for improving crash data quality, with substantial implications for transportation safety management and policy development.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
KAHANI: Culturally-Nuanced Visual Storytelling Tool for Non-Western Cultures,"Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated the ability to generate compelling text and visual stories. However, their outputs are predominantly aligned with the sensibilities of the Global North, often resulting in an outsider's gaze on other cultures. As a result, non-Western communities have to put extra effort into generating culturally specific stories. To address this challenge, we developed a visual storytelling tool called Kahani that generates culturally grounded visual stories for non-Western cultures. Our tool leverages off-the-shelf models GPT-4 Turbo and Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I prompting techniques, we capture the cultural context from user's prompt and generate vivid descriptions of the characters and scene compositions. To evaluate the effectiveness of Kahani, we conducted a comparative user study with ChatGPT-4 (with DALL-E3) in which participants from different regions of India compared the cultural relevance of stories generated by the two tools. The results of the qualitative and quantitative analysis performed in the user study show that Kahani's visual stories are more culturally nuanced than those generated by ChatGPT-4. In 27 out of 36 comparisons, Kahani outperformed or was on par with ChatGPT-4, effectively capturing cultural nuances and incorporating more Culturally Specific Items (CSI), validating its ability to generate culturally grounded visual stories.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses,"Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting have shown significant multi-step reasoning capabilities in factual content like mathematics, commonsense, and logic. However, their performance in narrative reasoning, which demands greater abstraction capabilities, remains unexplored. This study utilizes tropes in movie synopses to assess the abstract reasoning abilities of state-of-the-art LLMs and uncovers their low performance. We introduce a trope-wise querying approach to address these challenges and boost the F1 score by 11.8 points. Moreover, while prior studies suggest that CoT enhances multi-step reasoning, this study shows CoT can cause hallucinations in narrative content, reducing GPT-4's performance. We also introduce an Adversarial Injection method to embed trope-related text tokens into movie synopses without explicit tropes, revealing CoT's heightened sensitivity to such injections. Our comprehensive analysis provides insights for future research directions.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Mahnma: A Unique Testbed for Literary Entity Discovery and Linking,"High lexical variation, ambiguous references, and long-range dependencies make entity resolution in literary texts particularly challenging. We present Mahnma, the first large-scale dataset for end-to-end Entity Discovery and Linking (EDL) in Sanskrit, a morphologically rich and under-resourced language. Derived from the Mahbhrata, the world's longest epic, the dataset comprises over 109K named entity mentions mapped to 5.5K unique entities, and is aligned with an English knowledge base to support cross-lingual linking. The complex narrative structure of Mahnma, coupled with extensive name variation and ambiguity, poses significant challenges to resolution systems. Our evaluation reveals that current coreference and entity linking models struggle when evaluated on the global context of the test set. These results highlight the limitations of current approaches in resolving entities within such complex discourse. Mahnma thus provides a unique benchmark for advancing entity resolution, especially in literary domains.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning,"Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels","The increasing adoption of text-to-speech technologies has led to a growing demand for natural and emotive voices that adapt to a conversation's context and emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is a unique speech dataset created to enhance conversations' expressiveness and emotive quality in interactive narrative-driven systems. The corpus consists of a 2.3-hour recording featuring a female speaker delivering labelled utterances. It encompasses eight acted emotional states, evenly distributed with a variance of 0.68%, along with expressiveness levels and natural language descriptions with word emphasis labels. The evaluation of audio samples from different datasets revealed that the EMNS corpus achieved the highest average scores in accurately conveying emotions and demonstrating expressiveness. It outperformed other datasets in conveying shared emotions and achieved comparable levels of genuineness. A classification task confirmed the accurate representation of intended emotions in the corpus, with participants recognising the recordings as genuine and expressive. Additionally, the availability of the dataset collection tool under the Apache 2.0 License simplifies remote speech data collection for researchers.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
FRaN-X: FRaming and Narratives-eXplorer,"We present FRaN-X, a Framing and Narratives Explorer that automatically detects entity mentions and classifies their narrative roles directly from raw text. FRaN-X comprises a two-stage system that combines sequence labeling with fine-grained role classification to reveal how entities are portrayed as protagonists, antagonists, or innocents, using a unique taxonomy of 22 fine-grained roles nested under these three main categories. The system supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese) and two domains (the Russia-Ukraine Conflict and Climate Change). It provides an interactive web interface for media analysts to explore and compare framing across different sources, tackling the challenge of automatically detecting and labeling how entities are framed. Our system allows end users to focus on a single article as well as analyze up to four articles simultaneously. We provide aggregate level analysis including an intuitive graph visualization that highlights the narrative a group of articles are pushing. Our system includes a search feature for users to look up entities of interest, along with a timeline view that allows analysts to track an entity's role transitions across different contexts within the article. The FRaN-X system and the trained models are licensed under an MIT License. FRaN-X is publicly accessible at https://fran-x.streamlit.app/ and a video demonstration is available at https://youtu.be/VZVi-1B6yYk.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives,"Standard topic models often struggle to capture culturally specific nuances in text. This study evaluates the effectiveness of contextual embeddings for identifying culturally resonant themes in an underrepresented linguistic context. We compare the performance of KMeans Clustering, Latent Dirichlet Allocation (LDA), and BERTopic on a corpus of nearly 25,000 daily personal narratives written in Belgian-Dutch (Flemish). While LDA achieves strong performance on automated coherence metrics, subsequent human evaluation reveals that BERTopic consistently identifies the most coherent and culturally relevant topics, highlighting the limitations of purely statistical methods on this narrative-rich data. Furthermore, the diminished performance of K-Means compared to prior work on similar Dutch corpora underscores the unique linguistic challenges posed by personal narrative analysis. Our findings demonstrate the critical role of contextual embeddings in robust topic modeling and emphasize the need for human-centered evaluation, particularly when working with low-resource languages and culturally specific domains.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Computer-Aided Modelling of the Bilingual Word Indices to the Ninth-Century Uchitel'noe evangelie,"The development of bilingual dictionaries to medieval translations presents diverse difficulties. These result from two types of philological circumstances: a) the asymmetry between the source language and the target language; and b) the varying available sources of both the original and translated texts. In particular, the full critical edition of Tihova of Constantine of Preslav's Uchitel'noe evangelie ('Didactic Gospel') gives a relatively good idea of the Old Church Slavonic translation but not of its Greek source text. This is due to the fact that Cramer's edition of the catenae - used as the parallel text in it - is based on several codices whose text does not fully coincide with the Slavonic. This leads to the addition of the newly-discovered parallels from Byzantine manuscripts and John Chrysostom's homilies. Our approach to these issues is a step-wise process with two main goals: a) to facilitate the philological annotation of input data and b) to consider the manifestations of the mentioned challenges, first, separately in order to simplify their resolution, and, then, in their combination. We demonstrate how we model various types of asymmetric translation correlates and the variability resulting from the pluralism of sources. We also demonstrate how all these constructions are being modelled and processed into the final indices. Our approach is designed with generalisation in mind and is intended to be applicable also for other translations from Greek into Old Church Slavonic.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Latin writing styles analysis with Machine Learning: New approach to old questions,"In the Middle Ages texts were learned by heart and spread using oral means of communication from generation to generation. Adaptation of the art of prose and poems allowed keeping particular descriptions and compositions characteristic for many literary genres. Taking into account such a specific construction of literature composed in Latin, we can search for and indicate the probability patterns of familiar sources of specific narrative texts. Consideration of Natural Language Processing tools allowed us the transformation of textual objects into numerical ones and then application of machine learning algorithms to extract information from the dataset. We carried out the task consisting of the practical use of those concepts and observation to create a tool for analyzing narrative texts basing on open-source databases. The tool focused on creating specific search tools resources which could enable us detailed searching throughout the text. The main objectives of the study take into account finding similarities between sentences and between documents. Next, we applied machine learning algorithms on chosen texts to calculate specific features of them (for instance authorship or centuries) and to recognize sources of anonymous texts with a certain percentage.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Framing Migration: A Computational Analysis of UK Parliamentary Discourse,"We present a large-scale computational analysis of migration-related discourse in UK parliamentary debates spanning over 75 years and compare it with US congressional discourse. Using open-weight LLMs, we annotate each statement with high-level stances toward migrants and track the net tone toward migrants across time and political parties. For the UK, we extend this with a semi-automated framework for extracting fine-grained narrative frames to capture nuances of migration discourse. Our findings show that, while US discourse has grown increasingly polarised, UK parliamentary attitudes remain relatively aligned across parties, with a persistent ideological gap between Labour and the Conservatives, reaching its most negative level in 2025. The analysis of narrative frames in the UK parliamentary statements reveals a shift toward securitised narratives such as border control and illegal immigration, while longer-term integration-oriented frames such as social integration have declined. Moreover, discussions of national law about immigration have been replaced over time by international law and human rights, revealing nuances in discourse trends. Taken together broadly, our findings demonstrate how LLMs can support scalable, fine-grained discourse analysis in political and historical contexts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Quantifying origin and character of long-range correlations in narrative texts,"In natural language using short sentences is considered efficient for communication. However, a text composed exclusively of such sentences looks technical and reads boring. A text composed of long ones, on the other hand, demands significantly more effort for comprehension. Studying characteristics of the sentence length variability (SLV) in a large corpus of world-famous literary texts shows that an appealing and aesthetic optimum appears somewhere in between and involves selfsimilar, cascade-like alternation of various lengths sentences. A related quantitative observation is that the power spectra S(f) of thus characterized SLV universally develop a convincing `1/f^beta' scaling with the average exponent beta =~ 1/2, close to what has been identified before in musical compositions or in the brain waves. An overwhelming majority of the studied texts simply obeys such fractal attributes but especially spectacular in this respect are hypertext-like, ""stream of consciousness"" novels. In addition, they appear to develop structures characteristic of irreducibly interwoven sets of fractals called multifractals. Scaling of S(f) in the present context implies existence of the long-range correlations in texts and appearance of multifractality indicates that they carry even a nonlinear component. A distinct role of the full stops in inducing the long-range correlations in texts is evidenced by the fact that the above quantitative characteristics on the long-range correlations manifest themselves in variation of the full stops recurrence times along texts, thus in SLV, but to a much lesser degree in the recurrence times of the most frequent words. In this latter case the nonlinear correlations, thus multifractality, disappear even completely for all the texts considered. Treated as one extra word, the full stops at the same time appear to obey the Zipfian rank-frequency distribution, however.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Multilingual Contextual Affective Analysis of LGBT People Portrayals in Wikipedia,"Specific lexical choices in narrative text reflect both the writer's attitudes towards people in the narrative and influence the audience's reactions. Prior work has examined descriptions of people in English using contextual affective analysis, a natural language processing (NLP) technique that seeks to analyze how people are portrayed along dimensions of power, agency, and sentiment. Our work presents an extension of this methodology to multilingual settings, which is enabled by a new corpus that we collect and a new multilingual model. We additionally show how word connotations differ across languages and cultures, highlighting the difficulty of generalizing existing English datasets and methods. We then demonstrate the usefulness of our method by analyzing Wikipedia biography pages of members of the LGBT community across three languages: English, Russian, and Spanish. Our results show systematic differences in how the LGBT community is portrayed across languages, surfacing cultural differences in narratives and signs of social biases. Practically, this model can be used to identify Wikipedia articles for further manual analysis -- articles that might contain content gaps or an imbalanced representation of particular social groups.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Good Books are Complex Matters: Gauging Complexity Profiles Across Diverse Categories of Perceived Literary Quality,"In this study, we employ a classification approach to show that different categories of literary ""quality"" display unique linguistic profiles, leveraging a corpus that encompasses titles from the Norton Anthology, Penguin Classics series, and the Open Syllabus project, contrasted against contemporary bestsellers, Nobel prize winners and recipients of prestigious literary awards. Our analysis reveals that canonical and so called high-brow texts exhibit distinct textual features when compared to other quality categories such as bestsellers and popular titles as well as to control groups, likely responding to distinct (but not mutually exclusive) models of quality. We apply a classic machine learning approach, namely Random Forest, to distinguish quality novels from ""control groups"", achieving up to 77\% F1 scores in differentiating between the categories. We find that quality category tend to be easier to distinguish from control groups than from other quality categories, suggesting than literary quality features might be distinguishable but shared through quality proxies.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"A Preliminary Study for Literary Rhyme Generation based on Neuronal Representation, Semantics and Shallow Parsing","In recent years, researchers in the area of Computational Creativity have studied the human creative process proposing different approaches to reproduce it with a formal procedure. In this paper, we introduce a model for the generation of literary rhymes in Spanish, combining structures of language and neural network models %(\textit{Word2vec}).%, into a structure for semantic assimilation. The results obtained with a manual evaluation of the texts generated by our algorithm are encouraging.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Creativity in translation: machine translation as a constraint for literary texts,"This article presents the results of a study involving the translation of a short story by Kurt Vonnegut from English to Catalan and Dutch using three modalities: machine-translation (MT), post-editing (PE) and translation without aid (HT). Our aim is to explore creativity, understood to involve novelty and acceptability, from a quantitative perspective. The results show that HT has the highest creativity score, followed by PE, and lastly, MT, and this is unanimous from all reviewers. A neural MT system trained on literary data does not currently have the necessary capabilities for a creative translation; it renders literal solutions to translation problems. More importantly, using MT to post-edit raw output constrains the creativity of translators, resulting in a poorer translation often not fit for publication, according to experts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History,"Person names and location names are essential building blocks for identifying events and social networks in historical documents that were written in literary Chinese. We take the lead to explore the research on algorithmically recognizing named entities in literary Chinese for historical studies with language-model based and conditional-random-field based methods, and extend our work to mining the document structures in historical documents. Practical evaluations were conducted with texts that were extracted from more than 220 volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single most important collection that contains information about officers who served in local government in Chinese history. Our methods performed very well on these realistic tests. Thousands of names and addresses were identified from the texts. A good portion of the extracted names match the biographical information currently recorded in the China Biographical Database (CBDB) of Harvard University, and many others can be verified by historians and will become as new additions to CBDB.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Writer Identification Using Microblogging Texts for Social Media Forensics,"Establishing authorship of online texts is fundamental to combat cybercrimes. Unfortunately, text length is limited on some platforms, making the challenge harder. We aim at identifying the authorship of Twitter messages limited to 140 characters. We evaluate popular stylometric features, widely used in literary analysis, and specific Twitter features like URLs, hashtags, replies or quotes. We use two databases with 93 and 3957 authors, respectively. We test varying sized author sets and varying amounts of training/test texts per author. Performance is further improved by feature combination via automatic selection. With a large number of training Tweets (>500), a good accuracy (Rank-5>80%) is achievable with only a few dozens of test Tweets, even with several thousands of authors. With smaller sample sizes (10-20 training Tweets), the search space can be diminished by 9-15% while keeping a high chance that the correct author is retrieved among the candidates. In such cases, automatic attribution can provide significant time savings to experts in suspect search. For completeness, we report verification results. With few training/test Tweets, the EER is above 20-25%, which is reduced to < 15% if hundreds of training Tweets are available. We also quantify the computational complexity and time permanence of the employed features.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure,"Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysis shows that the proposed narrative features remarkably increased feature density and improved performance.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts,"In the context of the rapid development of large language models, we have meticulously trained and introduced the GujiBERT and GujiGPT language models, which are foundational models specifically designed for intelligent information processing of ancient texts. These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters, allowing them to effectively handle various natural language processing tasks related to ancient books, including but not limited to automatic sentence segmentation, punctuation, word segmentation, part-of-speech tagging, entity recognition, and automatic translation. Notably, these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets. Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks. Moreover, it is worth emphasizing that the choice of font, the scale of the corpus, and the initial model selection all exert significant influence over the ultimate experimental outcomes. To cater to the diverse text processing preferences of researchers in digital humanities and linguistics, we have developed three distinct categories comprising a total of nine model variations. We believe that by sharing these foundational language models specialized in the domain of ancient texts, we can facilitate the intelligent processing and scholarly exploration of ancient literary works and, consequently, contribute to the global dissemination of China's rich and esteemed traditional culture in this new era.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model,"We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Analyzing Gender Bias within Narrative Tropes,"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the ""genderedness"" of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work's creator correlates with the types of tropes that they use.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"A Narrative Review of Identity, Data, and Location Privacy Techniques in Edge Computing and Mobile Crowdsourcing","As digital technology advances, the proliferation of connected devices poses significant challenges and opportunities in mobile crowdsourcing and edge computing. This narrative review focuses on the need for privacy protection in these fields, emphasizing the increasing importance of data security in a data-driven world. Through an analysis of contemporary academic literature, this review provides an understanding of the current trends and privacy concerns in mobile crowdsourcing and edge computing. We present insights and highlight advancements in privacy-preserving techniques, addressing identity, data, and location privacy. This review also discusses the potential directions that can be useful resources for researchers, industry professionals, and policymakers.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Narrative-Driven Computational Framework for Clinician Burnout Surveillance,"Clinician burnout poses a substantial threat to patient safety, particularly in high-acuity intensive care units (ICUs). Existing research predominantly relies on retrospective survey tools or broad electronic health record (EHR) metadata, often overlooking the valuable narrative information embedded in clinical notes. In this study, we analyze 10,000 ICU discharge summaries from MIMIC-IV, a publicly available database derived from the electronic health records of Beth Israel Deaconess Medical Center. The dataset encompasses diverse patient data, including vital signs, medical orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. We introduce a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for clinical narratives, a lexical stress lexicon tailored for clinician burnout surveillance, and five-topic latent Dirichlet allocation (LDA) with workload proxies. A provider-level logistic regression classifier achieves a precision of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out set, surpassing metadata-only baselines by greater than or equal to 0.17 F1 score. Specialty-specific analysis indicates elevated burnout risk among providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate that ICU clinical narratives contain actionable signals for proactive well-being monitoring.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Autocorrelations Decay in Texts and Applicability Limits of Language Models,"We show that the laws of autocorrelations decay in texts are closely related to applicability limits of language models. Using distributional semantics we empirically demonstrate that autocorrelations of words in texts decay according to a power law. We show that distributional semantics provides coherent autocorrelations decay exponents for texts translated to multiple languages. The autocorrelations decay in generated texts is quantitatively and often qualitatively different from the literary texts. We conclude that language models exhibiting Markov behavior, including large autoregressive language models, may have limitations when applied to long texts, whether analysis or generation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
The Role of Natural Language Processing Tasks in Automatic Literary Character Network Construction,"The automatic extraction of character networks from literary texts is generally carried out using natural language processing (NLP) cascading pipelines. While this approach is widespread, no study exists on the impact of low-level NLP tasks on their performance. In this article, we conduct such a study on a literary dataset, focusing on the role of named entity recognition (NER) and coreference resolution when extracting co-occurrence networks. To highlight the impact of these tasks' performance, we start with gold-standard annotations, progressively add uniformly distributed errors, and observe their impact in terms of character network quality. We demonstrate that NER performance depends on the tested novel and strongly affects character detection. We also show that NER-detected mentions alone miss a lot of character co-occurrences, and that coreference resolution is needed to prevent this. Finally, we present comparison points with 2 methods based on large language models (LLMs), including a fully end-to-end one, and show that these models are outperformed by traditional NLP pipelines in terms of recall.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Story Ribbons: Reimagining Storyline Visualizations with Large Language Models,"Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Temporal Network Analysis of Literary Texts,"We study temporal networks of characters in literature focusing on ""Alice's Adventures in Wonderland"" (1865) by Lewis Carroll and the anonymous ""La Chanson de Roland"" (around 1100). The former, one of the most influential pieces of nonsense literature ever written, describes the adventures of Alice in a fantasy world with logic plays interspersed along the narrative. The latter, a song of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during Charlemagne's campaign on the Iberian Peninsula. We apply methods recently developed by Taylor and coworkers \cite{Taylor+2015} to find time-averaged eigenvector centralities, Freeman indices and vitalities of characters. We show that temporal networks are more appropriate than static ones for studying stories, as they capture features that the time-independent approaches fail to yield.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Financial Statement Analysis with Large Language Models,"We investigate whether large language models (LLMs) can successfully perform financial statement analysis in a way similar to a professional human analyst. We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of firms' future earnings. Even without narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes directionally. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, we find that the prediction accuracy of the LLM is on par with a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, we find that the LLM generates useful narrative insights about a company's future performance. Lastly, our trading strategies based on GPT's predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Our results suggest that LLMs may take a central role in analysis and decision-making.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding,"The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening/ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Bioinformatics and Classical Literary Study,"This paper describes the Quantitative Criticism Lab, a collaborative initiative between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate authorial style, intertextuality, and related phenomena of literary significance. As a case study in our approach, here we review the use of sequence alignment, a common technique in genomics and computational linguistics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal similarities, which makes it ideal for identifying phonetic echoes in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Using Machine Learning to Fuse Verbal Autopsy Narratives and Binary Features in the Analysis of Deaths from Hyperglycaemia,"Lower-and-middle income countries are faced with challenges arising from a lack of data on cause of death (COD), which can limit decisions on population health and disease management. A verbal autopsy(VA) can provide information about a COD in areas without robust death registration systems. A VA consists of structured data, combining numeric and binary features, and unstructured data as part of an open-ended narrative text. This study assesses the performance of various machine learning approaches when analyzing both the structured and unstructured components of the VA report. The algorithms were trained and tested via cross-validation in the three settings of binary features, text features and a combination of binary and text features derived from VA reports from rural South Africa. The results obtained indicate narrative text features contain valuable information for determining COD and that a combination of binary and text features improves the automated COD classification task.   Keywords: Diabetes Mellitus, Verbal Autopsy, Cause of Death, Machine Learning, Natural Language Processing","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Evaluating LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3,"Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination. We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in English literature. We release publicly our code and data.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Discourse Embellishment Using a Deep Encoder-Decoder Network,"We suggest a new NLG task in the context of the discourse generation pipeline of computational storytelling systems. This task, textual embellishment, is defined by taking a text as input and generating a semantically equivalent output with increased lexical and syntactic complexity. Ideally, this would allow the authors of computational storytellers to implement just lightweight NLG systems and use a domain-independent embellishment module to translate its output into more literary text. We present promising first results on this task using LSTM Encoder-Decoder networks trained on the WikiLarge dataset. Furthermore, we introduce ""Compiled Computer Tales"", a corpus of computationally generated stories, that can be used to test the capabilities of embellishment algorithms.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Formalizing Style in Personal Narratives,"Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives,"Free-text crash narratives recorded in real-world crash databases have been shown to play a significant role in improving traffic safety. However, large-scale analyses remain difficult to implement as there are no documented tools that can batch process the unstructured, non standardized text content written by various authors with diverse experience and attention to detail. In recent years, Transformer-based pre-trained language models (PLMs), such as Bidirectional Encoder Representations from Transformers (BERT) and large language models (LLMs), have demonstrated strong capabilities across various natural language processing tasks. These models can extract explicit facts from crash narratives, but their performance declines on inference-heavy tasks in, for example, Crash Type identification, which can involve nearly 100 categories. Moreover, relying on closed LLMs through external APIs raises privacy concerns for sensitive crash data. Additionally, these black-box tools often underperform due to limited domain knowledge. Motivated by these challenges, we study whether compact open-source PLMs can support reasoning-intensive extraction from crash narratives. We target two challenging objectives: 1) identifying the Manner of Collision for a crash, and 2) Crash Type for each vehicle involved in the crash event from real-world crash narratives. To bridge domain gaps, we apply fine-tuning techniques to inject task-specific knowledge to LLMs with Low-Rank Adaption (LoRA) and BERT. Experiments on the authoritative real-world dataset Crash Investigation Sampling System (CISS) demonstrate that our fine-tuned compact models outperform strong closed LLMs, such as GPT-4o, while requiring only minimal training resources. Further analysis reveals that the fine-tuned PLMs can capture richer narrative details and even correct some mislabeled annotations in the dataset.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Normalization of Relative and Incomplete Temporal Expressions in Clinical Narratives,"We analyze the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotate the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier and a rule-based RI-TIMEX text span parser. We experiment with different feature sets and perform error analysis for each system component. The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under relaxed matching criteria) respectively on the held-out test set of the 2012 i2b2 temporal relation challenge. Experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect. We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only the RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Improving Narrative Relationship Embeddings by Training with Additional Inverse-Relationship Constraints,"We consider the problem of embedding character-entity relationships from the reduced semantic space of narratives, proposing and evaluating the assumption that these relationships hold under a reflection operation. We analyze this assumption and compare the approach to a baseline state-of-the-art model with a unique evaluation that simulates efficacy on a downstream clustering task with human-created labels. Although our model creates clusters that achieve Silhouette scores of -.084, outperforming the baseline -.227, our analysis reveals that the models approach the task much differently and perform well on very different examples. We conclude that our assumption might be useful for specific types of data and should be evaluated on a wider range of tasks.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Towards Coherent and Consistent Use of Entities in Narrative Generation,"Large pre-trained language models (LMs) have demonstrated impressive capabilities in generating long, fluent text; however, there is little to no analysis on their ability to maintain entity coherence and consistency. In this work, we focus on the end task of narrative generation and systematically analyse the long-range entity coherence and consistency in generated stories. First, we propose a set of automatic metrics for measuring model performance in terms of entity usage. Given these metrics, we quantify the limitations of current LMs. Next, we propose augmenting a pre-trained LM with a dynamic entity memory in an end-to-end manner by using an auxiliary entity-related loss for guiding the reads and writes to the memory. We demonstrate that the dynamic entity memory increases entity coherence according to both automatic and human judgment and helps preserving entity-related information especially in settings with a limited context window. Finally, we also validate that our automatic metrics are correlated with human ratings and serve as a good indicator of the quality of generated stories.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
BookWorm: A Dataset for Character Description and Analysis,"Characters are at the heart of every story, driving the plot and engaging readers. In this study, we explore the understanding of characters in full-length books, which contain complex narratives and numerous interacting characters. We define two tasks: character description, which generates a brief factual profile, and character analysis, which offers an in-depth interpretation, including character development, personality, and social context. We introduce the BookWorm dataset, pairing books from the Gutenberg Project with human-written descriptions and analyses. Using this dataset, we evaluate state-of-the-art long-context models in zero-shot and fine-tuning settings, utilizing both retrieval-based and hierarchical processing for book-length inputs. Our findings show that retrieval-based approaches outperform hierarchical ones in both tasks. Additionally, fine-tuned models using coreference-based retrieval produce the most factual descriptions, as measured by fact- and entailment-based metrics. We hope our dataset, experiments, and analysis will inspire further research in character-based narrative understanding.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Automatic transcription of 17th century English text in Contemporary English with NooJ: Method and Evaluation,"Since 2006 we have undertaken to describe the differences between 17th century English and contemporary English thanks to NLP software. Studying a corpus spanning the whole century (tales of English travellers in the Ottoman Empire in the 17th century, Mary Astell's essay A Serious Proposal to the Ladies and other literary texts) has enabled us to highlight various lexical, morphological or grammatical singularities. Thanks to the NooJ linguistic platform, we created dictionaries indexing the lexical variants and their transcription in CE. The latter is often the result of the validation of forms recognized dynamically by morphological graphs. We also built syntactical graphs aimed at transcribing certain archaic forms in contemporary English. Our previous research implied a succession of elementary steps alternating textual analysis and result validation. We managed to provide examples of transcriptions, but we have not created a global tool for automatic transcription. Therefore we need to focus on the results we have obtained so far, study the conditions for creating such a tool, and analyze possible difficulties. In this paper, we will be discussing the technical and linguistic aspects we have not yet covered in our previous work. We are using the results of previous research and proposing a transcription method for words or sequences identified as archaic.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated Narrative Analysis Through Frame Semantics,"We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI's chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold$\unicode{x2014}$from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Analyzing Narrative Processing in Large Language Models (LLMs): Using GPT4 to test BERT,"The ability to transmit and receive complex information via language is unique to humans and is the basis of traditions, culture and versatile social interactions. Through the disruptive introduction of transformer based large language models (LLMs) humans are not the only entity to ""understand"" and produce language any more. In the present study, we have performed the first steps to use LLMs as a model to understand fundamental mechanisms of language processing in neural networks, in order to make predictions and generate hypotheses on how the human brain does language processing. Thus, we have used ChatGPT to generate seven different stylistic variations of ten different narratives (Aesop's fables). We used these stories as input for the open source LLM BERT and have analyzed the activation patterns of the hidden units of BERT using multi-dimensional scaling and cluster analysis. We found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of BERT (1) than narrative content (4-5). Despite the fact that BERT consists of 12 identical building blocks that are stacked and trained on large text corpora, the different layers perform different tasks. This is a very useful model of the human brain, where self-similar structures, i.e. different areas of the cerebral cortex, can have different functions and are therefore well suited to processing language in a very efficient way. The proposed approach has the potential to open the black box of LLMs on the one hand, and might be a further step to unravel the neural processes underlying human language processing and cognition in general.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Using Full-Text Content to Characterize and Identify Best Seller Books,"Artistic pieces can be studied from several perspectives, one example being their reception among readers over time. In the present work, we approach this interesting topic from the standpoint of literary works, particularly assessing the task of predicting whether a book will become a best seller. Dissimilarly from previous approaches, we focused on the full content of books and considered visualization and classification tasks. We employed visualization for the preliminary exploration of the data structure and properties, involving SemAxis and linear discriminant analyses. Then, to obtain quantitative and more objective results, we employed various classifiers. Such approaches were used along with a dataset containing (i) books published from 1895 to 1924 and consecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii) literary works published in the same period but not being mentioned in that list. Our comparison of methods revealed that the best-achieved result - combining a bag-of-words representation with a logistic regression classifier - led to an average accuracy of 0.75 both for the leave-one-out and 10-fold cross-validations. Such an outcome suggests that it is unfeasible to predict the success of books with high accuracy using only the full content of the texts. Nevertheless, our findings provide insights into the factors leading to the relative success of a literary work.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Detecting depression in dyadic conversations with multimodal narratives and visualizations,"Conversations contain a wide spectrum of multimodal information that gives us hints about the emotions and moods of the speaker. In this paper, we developed a system that supports humans to analyze conversations. Our main contribution is the identification of appropriate multimodal features and the integration of such features into verbatim conversation transcripts. We demonstrate the ability of our system to take in a wide range of multimodal information and automatically generated a prediction score for the depression state of the individual. Our experiments showed that this approach yielded better performance than the baseline model. Furthermore, the multimodal narrative approach makes it easy to integrate learnings from other disciplines, such as conversational analysis and psychology. Lastly, this interdisciplinary and automated approach is a step towards emulating how practitioners record the course of treatment as well as emulating how conversational analysts have been analyzing conversations by hand.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Large language models effectively leverage document-level context for literary translation, but critical errors persist","Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact. We publicly release our dataset and error annotations to spur future research on evaluation of document-level literary translation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Multiple References with Meaningful Variations Improve Literary Machine Translation,"While a source sentence can be translated in many ways, most machine translation (MT) models are trained with only a single reference. Previous work has shown that using synthetic paraphrases can improve MT. This paper investigates best practices for employing multiple references by analyzing the semantic similarity among different English translations of world literature in the Par3 dataset. We classify the semantic similarity between paraphrases into three levels: low, medium, and high, and fine-tune three different models (mT5-large, LLaMA-2-7B, and Opus-MT) for literary MT tasks. Across different models, holding the total training instances constant, single-reference but more source texts only marginally outperforms multiple-reference with half of the source texts. Moreover, when fine-tuning an LLM, using paraphrases with medium and high semantic similarity outperforms an unfiltered dataset, with improvements in BLEU (0.3-0.5), COMET (0.1-0.9), and chrF++ (0.17-0.32). Our code is publicly available on GitHub.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Do Massively Pretrained Language Models Make Better Storytellers?,"Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis,"Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Exploring Aviation Incident Narratives Using Topic Modeling and Clustering Techniques,"Aviation safety is a global concern, requiring detailed investigations into incidents to understand contributing factors comprehensively. This study uses the National Transportation Safety Board (NTSB) dataset. It applies advanced natural language processing (NLP) techniques, including Latent Dirichlet Allocation (LDA), Non-Negative Matrix Factorization (NMF), Latent Semantic Analysis (LSA), Probabilistic Latent Semantic Analysis (pLSA), and K-means clustering. The main objectives are identifying latent themes, exploring semantic relationships, assessing probabilistic connections, and cluster incidents based on shared characteristics. This research contributes to aviation safety by providing insights into incident narratives and demonstrating the versatility of NLP and topic modelling techniques in extracting valuable information from complex datasets. The results, including topics identified from various techniques, provide an understanding of recurring themes. Comparative analysis reveals that LDA performed best with a coherence value of 0.597, pLSA of 0.583, LSA of 0.542, and NMF of 0.437. K-means clustering further reveals commonalities and unique insights into incident narratives. In conclusion, this study uncovers latent patterns and thematic structures within incident narratives, offering a comparative analysis of multiple-topic modelling techniques. Future research avenues include exploring temporal patterns, incorporating additional datasets, and developing predictive models for early identification of safety issues. This research lays the groundwork for enhancing the understanding and improvement of aviation safety by utilising the wealth of information embedded in incident narratives.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Indexing and Visualization of Climate Change Narratives Using BERT and Causal Extraction,"In this study, we propose a methodology to extract, index, and visualize ``climate change narratives'' (stories about the connection between causal and consequential events related to climate change). We use two natural language processing methods, BERT (Bidirectional Encoder Representations from Transformers) and causal extraction, to textually analyze newspaper articles on climate change to extract ``climate change narratives.'' The novelty of the methodology could extract and quantify the causal relationships assumed by the newspaper's writers. Looking at the extracted climate change narratives over time, we find that since 2018, an increasing number of narratives suggest the impact of the development of climate change policy discussion and the implementation of climate change-related policies on corporate behaviors, macroeconomics, and price dynamics. We also observed the recent emergence of narratives focusing on the linkages between climate change-related policies and monetary policy. Furthermore, there is a growing awareness of the negative impacts of natural disasters (e.g., abnormal weather and severe floods) related to climate change on economic activities, and this issue might be perceived as a new challenge for companies and governments. The methodology of this study is expected to be applied to a wide range of fields, as it can analyze causal relationships among various economic topics, including analysis of inflation expectation or monetary policy communication strategy.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation,"This paper proposes a novel approach to evaluate Counter Narrative (CN) generation using a Large Language Model (LLM) as an evaluator. We show that traditional automatic metrics correlate poorly with human judgements and fail to capture the nuanced relationship between generated CNs and human perception. To alleviate this, we introduce a model ranking pipeline based on pairwise comparisons of generated CNs from different models, organized in a tournament-style format. The proposed evaluation method achieves a high correlation with human preference, with a $$ score of 0.88. As an additional contribution, we leverage LLMs as zero-shot CN generators and provide a comparative analysis of chat, instruct, and base models, exploring their respective strengths and limitations. Through meticulous evaluation, including fine-tuning experiments, we elucidate the differences in performance and responsiveness to domain-specific data. We conclude that chat-aligned models in zero-shot are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology,"Electronic medical reports (EHR) contain a vast amount of information that can be leveraged for machine learning applications in healthcare. However, existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form. Here, we propose SigBERT, an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient. SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores. The model was trained and evaluated on a real-world oncology dataset from the Lon Brard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Re:Verse -- Can Your VLM Read a Manga?,"Current Vision Language Models (VLMs) demonstrate a critical gap between surface-level recognition and deep narrative reasoning when processing sequential visual storytelling. Through a comprehensive investigation of manga narrative understanding, we reveal that while recent large multimodal models excel at individual panel interpretation, they systematically fail at temporal causality and cross-panel cohesion, core requirements for coherent story comprehension. We introduce a novel evaluation framework that combines fine-grained multimodal annotation, cross-modal embedding analysis, and retrieval-augmented assessment to systematically characterize these limitations.   Our methodology includes (i) a rigorous annotation protocol linking visual elements to narrative structure through aligned light novel text, (ii) comprehensive evaluation across multiple reasoning paradigms, including direct inference and retrieval-augmented generation, and (iii) cross-modal similarity analysis revealing fundamental misalignments in current VLMs' joint representations. Applying this framework to Re:Zero manga across 11 chapters with 308 annotated panels, we conduct the first systematic study of long-form narrative understanding in VLMs through three core evaluation axes: generative storytelling, contextual dialogue grounding, and temporal reasoning. Our findings demonstrate that current models lack genuine story-level intelligence, struggling particularly with non-linear narratives, character consistency, and causal inference across extended sequences. This work establishes both the foundation and practical methodology for evaluating narrative intelligence, while providing actionable insights into the capability of deep sequential understanding of Discrete Visual Narratives beyond basic recognition in Multimodal Models.   Project Page: https://re-verse.vercel.app","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models,"Diplomatic events consistently prompt widespread public discussion and debate. Public sentiment plays a critical role in diplomacy, as a good sentiment provides vital support for policy implementation, helps resolve international issues, and shapes a nation's international image. Traditional methods for gauging public sentiment, such as large-scale surveys or manual content analysis of media, are typically time-consuming, labor-intensive, and lack the capacity for forward-looking analysis. We propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive. First, we train a language model to predict public reaction towards diplomatic events. To this end, we construct a dataset comprising descriptions of diplomatic events and their associated public discussions. Second, guided by communication theories and in collaboration with domain experts, we predetermined several textual features for modification, ensuring that any alterations changed the event's narrative framing while preserving its core facts.We develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text. The results show that this framework successfully shifted public sentiment to a more favorable state with a 70\% success rate. This framework can therefore serve as a practical tool for diplomats, policymakers, and communication specialists, offering data-driven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"""Pale as death"" or ""ple comme la mort"" : Frozen similes used as literary clichs","The present study is focused on the automatic identification and description of frozen similes in British and French novels written between the 19 th century and the beginning of the 20 th century. Two main patterns of frozen similes were considered: adjectival ground + simile marker + nominal vehicle (e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g. sleep like a top). All potential similes and their components were first extracted using a rule-based algorithm. Then, frozen similes were identified based on reference lists of existing similes and semantic distance between the tenor and the vehicle. The results obtained tend to confirm the fact that frozen similes are not used haphazardly in literary texts. In addition, contrary to how they are often presented, frozen similes often go beyond the ground or the eventuality and the vehicle to also include the tenor.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy,"Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts,"Current large language models (LLMs) struggle to answer questions that span tens of thousands of tokens, especially when multi-hop reasoning is involved. While prior benchmarks explore long-context comprehension or multi-hop reasoning in isolation, none jointly vary context length and reasoning depth in natural narrative settings. We introduce NovelHopQA, the first benchmark to evaluate 1-4 hop QA over 64k-128k-token excerpts from 83 full-length public-domain novels. A keyword-guided pipeline builds hop-separated chains grounded in coherent storylines. We evaluate seven state-of-the-art models and apply oracle-context filtering to ensure all questions are genuinely answerable. Human annotators validate both alignment and hop depth. We additionally present retrieval-augmented generation (RAG) evaluations to test model performance when only selected passages are provided instead of the full context. We noticed consistent accuracy drops with increased hops and context length increase, even for frontier models-revealing that sheer scale does not guarantee robust reasoning. Failure-mode analysis highlights common breakdowns such as missed final-hop integration and long-range drift. NovelHopQA offers a controlled diagnostic setting to test multi-hop reasoning at scale. All code and datasets are available at https://novelhopqa.github.io.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Sentiment Analysis : A Literature Survey,"Our day-to-day life has always been influenced by what people think. Ideas and opinions of others have always affected our own opinions. The explosion of Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging, Contributing to RSS, Social Bookmarking, and Social Networking. As a result there has been an eruption of interest in people to mine these vast resources of data for opinions. Sentiment Analysis or Opinion Mining is the computational treatment of opinions, sentiments and subjectivity of text. In this report, we take a look at the various challenges and applications of Sentiment Analysis. We will discuss in details various approaches to perform a computational treatment of sentiments and opinions. Various supervised or data-driven techniques to SA like Nave Byes, Maximum Entropy, SVM, and Voted Perceptrons will be discussed and their strengths and drawbacks will be touched upon. We will also see a new dimension of analyzing sentiments by Cognitive Psychology mainly through the work of Janyce Wiebe, where we will see ways to detect subjectivity, perspective in narrative and understanding the discourse structure. We will also study some specific topics in Sentiment Analysis and the contemporary works in those areas.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Statistical keyword detection in literary corpora,"Understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts. We consider the information retrieval problem of detecting and ranking the relevant words of a text by means of statistical information referring to the ""spatial"" use of the words. Shannon's entropy of information is used as a tool for automatic keyword extraction. By using The Origin of Species by Charles Darwin as a representative text sample, we show the performance of our detector and compare it with another proposals in the literature. The random shuffled text receives special attention as a tool for calibrating the ranking indices.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
WebNovelBench: Placing LLM Novelists on the Web Novel Distribution,"Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames","Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Characterizing the Investigative Methods of Fictional Detectives with Large Language Models,"Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Uncovering Conspiratorial Narratives within Arabic Online Content,"This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups,"Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from sociological foundations of stigmatization theory, our stigmatization analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures,"Evaluating the creative capabilities of large language models (LLMs) in complex tasks often requires human assessments that are difficult to scale. We introduce a novel, scalable methodology for evaluating LLM story generation by analyzing underlying social structures in narratives as signed character networks. To demonstrate its effectiveness, we conduct a large-scale comparative analysis using networks from over 1,200 stories, generated by four leading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a human-written corpus. Our findings, based on network properties like density, clustering, and signed edge weights, show that LLM-generated stories consistently exhibit a strong bias toward tightly-knit, positive relationships, which aligns with findings from prior research using human assessment. Our proposed approach provides a valuable tool for evaluating limitations and tendencies in the creative storytelling of current and future LLMs.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4","In traffic safety research, extracting information from crash narratives using text analysis is a common practice. With recent advancements of large language models (LLM), it would be useful to know how the popular LLM interfaces perform in classifying or extracting information from crash narratives. To explore this, our study has used the three most popular publicly available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas. During the investigation, their capabilities and limitations were assessed and their responses to the queries were compared. Five questions were asked related to the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5) What are the sequence of harmful events in the crash? For questions 1 through 4, the overall similarity among the LLMs were 70%, 35%, 96% and 89%, respectively. The similarities were higher while answering direct questions requiring binary responses and significantly lower for complex questions. To compare the responses to question 5, network diagram and centrality measures were analyzed. The network diagram from the three LLMs were not always similar although they sometimes have the same influencing events with high in-degree, out-degree and betweenness centrality. This study suggests using multiple models to extract viable information from narratives. Also, caution must be practiced while using these interfaces to obtain crucial safety related information.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Towards a Computational Analysis of Suspense: Detecting Dangerous Situations,"Suspense is an important tool in storytelling to keep readers engaged and wanting to read more. However, it has so far not been studied extensively in Computational Literary Studies. In this paper, we focus on one of the elements authors can use to build up suspense: dangerous situations. We introduce a corpus of texts annotated with dangerous situations, distinguishing between 7 types of danger. Additionally, we annotate parts of the text that describe fear experienced by a character, regardless of the actual presence of danger. We present experiments towards the automatic detection of these situations, finding that unsupervised baseline methods can provide valuable signals for the detection, but more complex methods are necessary for further analysis. Not unexpectedly, the description of danger and fear often relies heavily on the context, both local (e.g., situations where danger is only mentioned, but not actually present) and global (e.g., ""storm"" being used in a literal sense in an adventure novel, but metaphorically in a romance novel).","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Character Distributions of Classical Chinese Literary Texts: Zipf's Law, Genres, and Epochs","We collect 14 representative corpora for major periods in Chinese history in this study. These corpora include poetic works produced in several dynasties, novels of the Ming and Qing dynasties, and essays and news reports written in modern Chinese. The time span of these corpora ranges between 1046 BCE and 2007 CE. We analyze their character and word distributions from the viewpoint of the Zipf's law, and look for factors that affect the deviations and similarities between their Zipfian curves. Genres and epochs demonstrated their influences in our analyses. Specifically, the character distributions for poetic works of between 618 CE and 1644 CE exhibit striking similarity. In addition, although texts of the same dynasty may tend to use the same set of characters, their character distributions still deviate from each other.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation,"Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at https://dongqi.me/projects/SciNews.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Natural Language Processing for Cardiology: A Narrative Review,"Cardiovascular diseases are becoming increasingly prevalent in modern society, with a profound impact on global health and well-being. These Cardiovascular disorders are complex and multifactorial, influenced by genetic predispositions, lifestyle choices, and diverse socioeconomic and clinical factors. Information about these interrelated factors is dispersed across multiple types of textual data, including patient narratives, medical records, and scientific literature. Natural language processing (NLP) has emerged as a powerful approach for analysing such unstructured data, enabling healthcare professionals and researchers to gain deeper insights that may transform the diagnosis, treatment, and prevention of cardiac disorders. This review provides a comprehensive overview of NLP research in cardiology from 2014 to 2025. We systematically searched six literature databases for studies describing NLP applications across a range of cardiovascular diseases. After a rigorous screening process, we identified 265 relevant articles. Each study was analysed across multiple dimensions, including NLP paradigms, cardiology-related tasks, disease types, and data sources. Our findings reveal substantial diversity within these dimensions, reflecting the breadth and evolution of NLP research in cardiology. A temporal analysis further highlights methodological trends, showing a progression from rule-based systems to large language models. Finally, we discuss key challenges and future directions, such as developing interpretable LLMs and integrating multimodal data. To the best of our knowledge, this review represents the most comprehensive synthesis of NLP research in cardiology to date.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Using Clinical Narratives and Structured Data to Identify Distant Recurrences in Breast Cancer,"Accurately identifying distant recurrences in breast cancer from the Electronic Health Records (EHR) is important for both clinical care and secondary analysis. Although multiple applications have been developed for computational phenotyping in breast cancer, distant recurrence identification still relies heavily on manual chart review. In this study, we aim to develop a model that identifies distant recurrences in breast cancer using clinical narratives and structured data from EHR. We apply MetaMap to extract features from clinical narratives and also retrieve structured clinical data from EHR. Using these features, we train a support vector machine model to identify distant recurrences in breast cancer patients. We train the model using 1,396 double-annotated subjects and validate the model using 599 double-annotated subjects. In addition, we validate the model on a set of 4,904 single-annotated subjects as a generalization test. We obtained a high area under curve (AUC) score of 0.92 (SD=0.01) in the cross-validation using the training dataset, then obtained AUC scores of 0.95 and 0.93 in the held-out test and generalization test using 599 and 4,904 samples respectively. Our model can accurately and efficiently identify distant recurrences in breast cancer by combining features extracted from unstructured clinical narratives and structured clinical data.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using Language Models,"Over the past decades, there has been an increase in the prevalence of abusive and violent content in Hollywood movies. In this study, we use language models to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. We provide an analysis of subtitles for over a thousand movies, which are categorised into four genres. We employ fine-tuned language models to examine the trends and shifts in emotional and abusive content over the past seven decades. Findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis,"Adult content detection still poses a great challenge for automation. Existing classifiers primarily focus on distinguishing between erotic and non-erotic texts. However, they often need more nuance in assessing the potential harm. Unfortunately, the content of this nature falls beyond the reach of generative models due to its potentially harmful nature. Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models. In such instances where data is scarce and challenging, a thorough analysis of the structure of such texts rather than a large model may offer a viable solution. Especially given that harmful erotic narratives, despite appearing similar to harmless ones, usually reveal their harmful nature first through contextual information hidden in the non-sexual parts of the narrative.   This paper introduces a hybrid neural and rule-based context-aware system that leverages coreference resolution to identify harmful contextual cues in erotic content. Collaborating with professional moderators, we compiled a dataset and developed a classifier capable of distinguishing harmful from non-harmful erotic content. Our hybrid model, tested on Polish text, demonstrates a promising accuracy of 84% and a recall of 80%. Models based on RoBERTa and Longformer without explicit usage of coreference chains achieved significantly weaker results, underscoring the importance of coreference resolution in detecting such nuanced content as harmful erotics. This approach also offers the potential for enhanced visual explainability, supporting moderators in evaluating predictions and taking necessary actions to address harmful content.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis","The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
ExU: AI Models for Examining Multilingual Disinformation Narratives and Understanding their Spread,"Addressing online disinformation requires analysing narratives across languages to help fact-checkers and journalists sift through large amounts of data. The ExU project focuses on developing AI-based models for multilingual disinformation analysis, addressing the tasks of rumour stance classification and claim retrieval. We describe the ExU project proposal and summarise the results of a user requirements survey regarding the design of tools to support fact-checking.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Zipf's law for word frequencies: word forms versus lemmas in long texts,"Zipf's law is a fundamental paradigm in the statistics of written and spoken natural language as well as in other communication systems. We raise the question of the elementary units for which Zipf's law should hold in the most natural way, studying its validity for plain word forms and for the corresponding lemma forms. In order to have as homogeneous sources as possible, we analyze some of the longest literary texts ever written, comprising four different languages, with different levels of morphological complexity. In all cases Zipf's law is fulfilled, in the sense that a power-law distribution of word or lemma frequencies is valid for several orders of magnitude. We investigate the extent to which the word-lemma transformation preserves two parameters of Zipf's law: the exponent and the low-frequency cut-off. We are not able to demonstrate a strict invariance of the tail, as for a few texts both exponents deviate significantly, but we conclude that the exponents are very similar, despite the remarkable transformation that going from words to lemmas represents, considerably affecting all ranges of frequencies. In contrast, the low-frequency cut-offs are less stable.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Beyond Retrieval: Generating Narratives in Conversational Recommender Systems,"The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.   First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
"Gender stereotypes in the mediated personalization of politics: Empirical evidence from a lexical, syntactic and sentiment analysis","The media attention to the personal sphere of famous and important individuals has become a key element of the gender narrative. Here we combine lexical, syntactic and sentiment analysis to investigate the role of gender in the personalization of a wide range of political office holders in Italy during the period 2017-2020. On the basis of a score for words that is introduced to account for gender unbalance in both representative and news coverage, we show that the political personalization in Italy is more detrimental for women than men, with the persistence of entrenched stereotypes including a masculine connotation of leadership, the resulting women's unsuitability to hold political functions, and a greater deal of focus on their attractiveness and body parts. In addition, women politicians are covered with a more negative tone than their men counterpart when personal details are reported. Further, the major contribution to the observed gender differences comes from online news rather than print news, suggesting that the expression of certain stereotypes may be better conveyed when click baiting and personal targeting have a major impact.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Coordinating Narratives and the Capitol Riots on Parler,"Coordinated disinformation campaigns are used to influence social media users, potentially leading to offline violence. In this study, we introduce a general methodology to uncover coordinated messaging through analysis of user parleys on Parler. The proposed method constructs a user-to-user coordination network graph induced by a user-to-text graph and a text-to-text similarity graph. The text-to-text graph is constructed based on the textual similarity of Parler posts. We study three influential groups of users in the 6 January 2020 Capitol riots and detect networks of coordinated user clusters that are all posting similar textual content in support of different disinformation narratives related to the U.S. 2020 elections.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review,"Peer review is a key component of the publishing process in most fields of science. The increasing submission rates put a strain on reviewing quality and efficiency, motivating the development of applications to support the reviewing and editorial work. While existing NLP studies focus on the analysis of individual texts, editorial assistance often requires modeling interactions between pairs of texts -- yet general frameworks and datasets to support this scenario are missing. Relationships between texts are the core object of the intertextuality theory -- a family of approaches in literary studies not yet operationalized in NLP. Inspired by prior theoretical work, we propose the first intertextual model of text-based collaboration, which encompasses three major phenomena that make up a full iteration of the review-revise-and-resubmit cycle: pragmatic tagging, linking and long-document version alignment. While peer review is used across the fields of science and publication formats, existing datasets solely focus on conference-style review in computer science. Addressing this, we instantiate our proposed model in the first annotated multi-domain corpus in journal-style post-publication open peer review, and provide detailed insights into the practical aspects of intertextual annotation. Our resource is a major step towards multi-domain, fine-grained applications of NLP in editorial support for peer review, and our intertextual framework paves the path for general-purpose modeling of text-based collaboration. Our corpus and accompanying code are publicly available.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model,"The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees,"Prominent applications of sentiment analysis are countless, covering areas such as marketing, customer service and communication. The conventional bag-of-words approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetorical structure, we propose a tensor-based, tree-structured deep neural network (named Discourse-LSTM) in order to process the complete discourse tree. The underlying tensors infer the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Moreover, our tensor structure reveals the salient text passages and thereby provides explanatory insights.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Authorship attribution for Differences between Literary Texts by Bilingual Russian-French and Non-Bilingual French Authors,"Do bilingual Russian-French authors of the end of the twentieth century such as Andre Makine, Valry Afanassiev, Vladimir Fdorovski, Iegor Gran, Luba Jurgenson have common stylistic traits in the novels they wrote in French? Can we distinguish between them and non-bilingual French writers' texts? Is the phenomenon of interference observable in French texts of Russian authors? This paper applies authorship attribution methods including Support Vector Machine (SVM), $K$-Nearest Neighbors (KNN), Ridge classification, and Neural Network to answer these questions.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Electoral Programs of German Parties 2021: A Computational Analysis Of Their Comprehensibility and Likeability Based On SentiArt,"The electoral programs of six German parties issued before the parliamentary elections of 2021 are analyzed using state-of-the-art computational tools for quantitative narrative, topic and sentiment analysis. We compare different methods for computing the textual similarity of the programs, Jaccard Bag similarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational and computational complexity increasing from the 1st to the 4th method. A new similarity measure for entire documents derived from the Fowlkes Mallows Score is applied to kmeans clustering of sBERT transformed sentences. Using novel indices of the readability and emotion potential of texts computed via SentiArt (Jacobs, 2019), our data shed light on the similarities and differences of the programs regarding their length, main ideas, comprehensibility, likeability, and semantic complexity. Among others, they reveal that the programs of the SPD and CDU have the best chances to be comprehensible and likeable -all other things being equal-, and they raise the important issue of which similarity measure is optimal for comparing texts such as electoral programs which necessarily share a lot of words. While such analyses can not replace qualitative analyses or a deep reading of the texts, they offer predictions that can be verified in empirical studies and may serve as a motivation for changing aspects of future electoral programs potentially making them more comprehensible and/or likeable.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Latent Structures of Intertextuality in French Fiction,"Intertextuality is a key concept in literary theory that challenges traditional notions of text, signification or authorship. It views texts as part of a vast intertextual network that is constantly evolving and being reconfigured. This paper argues that the field of computational literary studies is the ideal place to conduct a study of intertextuality since we have now the ability to systematically compare texts with each others. Specifically, we present a work on a corpus of more than 12.000 French fictions from the 18th, 19th and early 20th century. We focus on evaluating the underlying roles of two literary notions, sub-genres and the literary canon in the framing of textuality. The article attempts to operationalize intertextuality using state-of-the-art contextual language models to encode novels and capture features that go beyond simple lexical or thematic approaches. Previous research (Hughes, 2012) supports the existence of a literary ""style of a time"", and our findings further reinforce this concept. Our findings also suggest that both subgenres and canonicity play a significant role in shaping textual similarities within French fiction. These discoveries point to the importance of considering genre and canon as dynamic forces that influence the evolution and intertextual connections of literary works within specific historical contexts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Large language models for newspaper sentiment analysis during COVID-19: The Guardian,"During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking,"As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.   We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.   Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Quantitative Discourse Cohesion Analysis of Scientific Scholarly Texts using Multilayer Networks,"Discourse cohesion facilitates text comprehension and helps the reader form a coherent narrative. In this study, we aim to computationally analyze the discourse cohesion in scientific scholarly texts using multilayer network representation and quantify the writing quality of the document. Exploiting the hierarchical structure of scientific scholarly texts, we design section-level and document-level metrics to assess the extent of lexical cohesion in text. We use a publicly available dataset along with a curated set of contrasting examples to validate the proposed metrics by comparing them against select indices computed using existing cohesion analysis tools. We observe that the proposed metrics correlate as expected with the existing cohesion indices.   We also present an analytical framework, CHIAA (CHeck It Again, Author), to provide pointers to the author for potential improvements in the manuscript with the help of the section-level and document-level metrics. The proposed CHIAA framework furnishes a clear and precise prescription to the author for improving writing by localizing regions in text with cohesion gaps. We demonstrate the efficacy of CHIAA framework using succinct examples from cohesion-deficient text excerpts in the experimental dataset.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction,"Automated story writing has been a subject of study for over 60 years. Large language models can generate narratively consistent and linguistically coherent short fiction texts. Despite these advancements, rigorous assessment of such outputs for literary merit - especially concerning aesthetic qualities - has received scant attention. In this paper, we address the challenge of evaluating AI-generated microfictions and argue that this task requires consideration of literary criteria across various aspects of the text, such as thematic coherence, textual clarity, interpretive depth, and aesthetic quality. To facilitate this, we present GrAImes: an evaluation protocol grounded in literary theory, specifically drawing from a literary perspective, to offer an objective framework for assessing AI-generated microfiction. Furthermore, we report the results of our validation of the evaluation protocol, as answered by both literature experts and literary enthusiasts. This protocol will serve as a foundation for evaluating automatically generated microfictions and assessing their literary value.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning,"Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Interconnected Kingdoms: Comparing 'A Song of Ice and Fire' Adaptations Across Media Using Complex Networks,"In this article, we propose and apply a method to compare adaptations of the same story across different media. We tackle this task by modelling such adaptations through character networks. We compare them by leveraging two concepts at the core of storytelling: the characters involved, and the dynamics of the story. We propose several methods to match characters between media and compare their position in the networks; and perform narrative matching, i.e. match the sequences of narrative units that constitute the plots. We apply these methods to the novel series \textit{A Song of Ice and Fire}, by G.R.R. Martin, and its comics and TV show adaptations. Our results show that interactions between characters are not sufficient to properly match individual characters between adaptations, but that using some additional information such as character affiliation or gender significantly improves the performance. On the contrary, character interactions convey enough information to perform narrative matching, and allow us to detect the divergence between the original novels and its TV show adaptation.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Determining sentiment in citation text and analyzing its impact on the proposed ranking index,"Whenever human beings interact with each other, they exchange or express opinions, emotions, and sentiments. These opinions can be expressed in text, speech or images. Analysis of these sentiments is one of the popular research areas of present day researchers. Sentiment analysis, also known as opinion mining tries to identify or classify these sentiments or opinions into two broad categories - positive and negative. In recent years, the scientific community has taken a lot of interest in analyzing sentiment in textual data available in various social media platforms. Much work has been done on social media conversations, blog posts, newspaper articles and various narrative texts. However, when it comes to identifying emotions from scientific papers, researchers have faced some difficulties due to the implicit and hidden nature of opinion. By default, citation instances are considered inherently positive in emotion. Popular ranking and indexing paradigms often neglect the opinion present while citing. In this paper, we have tried to achieve three objectives. First, we try to identify the major sentiment in the citation text and assign a score to the instance. We have used a statistical classifier for this purpose. Secondly, we have proposed a new index (we shall refer to it hereafter as M-index) which takes into account both the quantitative and qualitative factors while scoring a paper. Thirdly, we developed a ranking of research papers based on the M-index. We also try to explain how the M-index impacts the ranking of scientific papers.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A Corpus for Named Entity Recognition in Chinese Novels with Multi-genres,"Entities like person, location, organization are important for literary text analysis. The lack of annotated data hinders the progress of named entity recognition (NER) in literary domain. To promote the research of literary NER, we build the largest multi-genre literary NER corpus containing 263,135 entities in 105,851 sentences from 260 online Chinese novels spanning 13 different genres. Based on the corpus, we investigate characteristics of entities from different genres. We propose several baseline NER models and conduct cross-genre and cross-domain experiments. Experimental results show that genre difference significantly impact NER performance though not as much as domain difference like literary domain and news domain. Compared with NER in news domain, literary NER still needs much improvement and the Out-of-Vocabulary (OOV) problem is more challenging due to the high variety of entities in literary works. Our data and models are open-sourced at https://github.com/hjzhao73/MultiGenre-ChineseNovel","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
A decomposition of book structure through ousiometric fluctuations in cumulative word-time,"While quantitative methods have been used to examine changes in word usage in books, studies have focused on overall trends, such as the shapes of narratives, which are independent of book length. We instead look at how words change over the course of a book as a function of the number of words, rather than the fraction of the book, completed at any given point; we define this measure as ""cumulative word-time"". Using ousiometrics, a reinterpretation of the valence-arousal-dominance framework of meaning obtained from semantic differentials, we convert text into time series of power and danger scores in cumulative word-time. Each time series is then decomposed using empirical mode decomposition into a sum of constituent oscillatory modes and a non-oscillatory trend. By comparing the decomposition of the original power and danger time series with those derived from shuffled text, we find that shorter books exhibit only a general trend, while longer books have fluctuations in addition to the general trend. These fluctuations typically have a period of a few thousand words regardless of the book length or library classification code, but vary depending on the content and structure of the book. Our findings suggest that, in the ousiometric sense, longer books are not expanded versions of shorter books, but are more similar in structure to a concatenation of shorter texts. Further, they are consistent with editorial practices that require longer texts to be broken down into sections, such as chapters. Our method also provides a data-driven denoising approach that works for texts of various lengths, in contrast to the more traditional approach of using large window sizes that may inadvertently smooth out relevant information, especially for shorter texts. These results open up avenues for future work in computational literary analysis, particularly the measurement of a basic unit of narrative.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications,"Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.","cat:cs.CL AND (""narrative analysis"" OR ""literary text"" OR storytelling)",0
Exploring the Interconnectedness of Cryptocurrencies using Correlation Networks,"Correlation networks were used to detect characteristics which, although fixed over time, have an important influence on the evolution of prices over time. Potentially important features were identified using the websites and whitepapers of cryptocurrencies with the largest userbases. These were assessed using two datasets to enhance robustness: one with fourteen cryptocurrencies beginning from 9 November 2017, and a subset with nine cryptocurrencies starting 9 September 2016, both ending 6 March 2018. Separately analysing the subset of cryptocurrencies raised the number of data points from 115 to 537, and improved robustness to changes in relationships over time. Excluding USD Tether, the results showed a positive association between different cryptocurrencies that was statistically significant. Robust, strong positive associations were observed for six cryptocurrencies where one was a fork of the other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the existence of a group of cryptocurrencies particularly associated with Cardano, and a separate group correlated with Ethereum. The data was not consistent with a token's functionality or creation mechanism being the dominant determinants of the evolution of prices over time but did suggest that factors other than speculation contributed to the price.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Bayesian framework for characterizing cryptocurrency market dynamics, structural dependency, and volatility using potential field","Identifying the structural dependence between the cryptocurrencies and predicting market trend are fundamental for effective portfolio management in cryptocurrency trading. In this paper, we present a unified Bayesian framework based on potential field theory and Gaussian Process to characterize the structural dependency of various cryptocurrencies, using historic price information. The following are our significant contributions: (i) Proposed a novel model for cryptocurrency price movements as a trajectory of a dynamical system governed by a time-varying non-linear potential field. (ii) Validated the existence of the non-linear potential function in cryptocurrency market through Lyapunov stability analysis. (iii) Developed a Bayesian framework for inferring the non-linear potential function from observed cryptocurrency prices. (iv) Proposed that attractors and repellers inferred from the potential field are reliable cryptocurrency market indicators, surpassing existing attributes, such as, mean, open price or close price of an observation window, in the literature. (v) Analysis of cryptocurrency market during various Bitcoin crash durations from April 2017 to November 2021, shows that attractors captured the market trend, volatility, and correlation. In addition, attractors aids explainability and visualization. (vi) The structural dependence inferred by the proposed approach was found to be consistent with results obtained using the popular wavelet coherence approach. (vii) The proposed market indicators (attractors and repellers) can be used to improve the prediction performance of state-of-art deep learning price prediction models. As, an example, we show improvement in Litecoin price prediction up to a horizon of 12 days.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deciphering Bitcoin Blockchain Data by Cohort Analysis,"Bitcoin is a peer-to-peer electronic payment system that has rapidly grown in popularity in recent years. Usually, the complete history of Bitcoin blockchain data must be queried to acquire variables with economic meaning. This task has recently become increasingly difficult, as there are over 1.6 billion historical transactions on the Bitcoin blockchain. It is thus important to query Bitcoin transaction data in a way that is more efficient and provides economic insights. We apply cohort analysis that interprets Bitcoin blockchain data using methods developed for population data in the social sciences. Specifically, we query and process the Bitcoin transaction input and output data within each daily cohort. This enables us to create datasets and visualizations for some key Bitcoin transaction indicators, including the daily lifespan distributions of spent transaction output (STXO) and the daily age distributions of the cumulative unspent transaction output (UTXO). We provide a computationally feasible approach for characterizing Bitcoin transactions that paves the way for future economic studies of Bitcoin.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Risk of Bitcoin Market: Volatility, Jumps, and Forecasts","Cryptocurrency, the most controversial and simultaneously the most interesting asset, has attracted many investors and speculators in recent years. The visibly significant market capitalization of cryptos also motivates modern financial instruments such as futures and options. Those will depend on the dynamics, volatility, or even the jumps of cryptos. We provide a comprehensive investigation of the risk dynamics of the Bitcoin Market from a realized volatility perspective. The Bitcoin market is extremely risky in the sense of volatility, entangled jumps, and extensive consecutive jumps, which reflect the major incidents worldwide. Empirical study shows that the lagged realized variance increases the future realized variance, while the jumps, especially positive ones, significantly reduce future realized variance. The out-of-sample forecasting model reveals that, in terms of forecasting accuracy and utility gain, investors interested in the long-term realized variance benefit from explicitly modelling the jumps and signed estimators, which is unnecessary for the short-term realized variance forecast.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Evaluation of Dynamic Cointegration-Based Pairs Trading Strategy in the Cryptocurrency Market,"This research aims to demonstrate a dynamic cointegration-based pairs trading strategy, including an optimal look-back window framework in the cryptocurrency market, and evaluate its return and risk by applying three different scenarios. We employ the Engle-Granger methodology, the Kapetanios-Snell-Shin (KSS) test, and the Johansen test as cointegration tests in different scenarios. We calibrate the mean-reversion speed of the Ornstein-Uhlenbeck process to obtain the half-life used for the asset selection phase and look-back window estimation. By considering the main limitations in the market microstructure, our strategy exceeds the naive buy-and-hold approach in the Bitmex exchange. Another significant finding is that we implement a numerous collection of cryptocurrency coins to formulate the model's spread, which improves the risk-adjusted profitability of the pairs trading strategy. Besides, the strategy's maximum drawdown level is reasonably low, which makes it useful to be deployed. The results also indicate that a class of coins has better potential arbitrage opportunities than others. This research has some noticeable advantages, making it stand out from similar studies in the cryptocurrency market. First is the accuracy of data in which minute-binned data create the signals in the formation period. Besides, to backtest the strategy during the trading period, we simulate the trading signals using best bid/ask quotes and market trades. We exclusively take the order execution into account when the asset size is already available at its quoted price (with one or more period gaps after signal generation). This action makes the backtesting much more realistic.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The economic dependency of the Bitcoin security,"We study to what extent the Bitcoin blockchain security permanently depends on the underlying distribution of cryptocurrency market outcomes. We use daily blockchain and Bitcoin data for 2014-2019 and employ the ARDL approach. We test three equilibrium hypotheses: (i) sensitivity of the Bitcoin blockchain to mining reward; (ii) security outcomes of the Bitcoin blockchain and the proof-of-work cost; and (iii) the speed of adjustment of the Bitcoin blockchain security to deviations from the equilibrium path. Our results suggest that the Bitcoin price and mining rewards are intrinsically linked to Bitcoin security outcomes. The Bitcoin blockchain security's dependency on mining costs is geographically differenced - it is more significant for the global mining leader China than for other world regions. After input or output price shocks, the Bitcoin blockchain security reverts to its equilibrium security level.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multi-Agent Analysis of Off-Exchange Public Information for Cryptocurrency Market Trend Prediction,"Cryptocurrency markets present unique prediction challenges due to their extreme volatility, 24/7 operation, and hypersensitivity to news events, with existing approaches suffering from key information extraction and poor sideways market detection critical for risk management. We introduce a theoretically-grounded multi-agent cryptocurrency trend prediction framework that advances the state-of-the-art through three key innovations: (1) an information-preserving news analysis system with formal theoretical guarantees that systematically quantifies market impact, regulatory implications, volume dynamics, risk assessment, technical correlation, and temporal effects using large language models; (2) an adaptive volatility-conditional fusion mechanism with proven optimal properties that dynamically combines news sentiment and technical indicators based on market regime detection; (3) a distributed multi-agent coordination architecture with low communication complexity enabling real-time processing of heterogeneous data streams. Comprehensive experimental evaluation on Bitcoin across three prediction horizons demonstrates statistically significant improvements over state-of-the-art natural language processing baseline, establishing a new paradigm for financial machine learning with broad implications for quantitative trading and risk management systems.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"The Rise and Fall of Cryptocurrencies: Defining the Economic and Social Values of Blockchain Technologies, assessing the Opportunities, and defining the Financial and Cybersecurity Risks of the Metaverse","This paper contextualises the common queries of ""why is crypto crashing?"" and ""why is crypto down?"", the research transcends beyond the frequent market fluctuations to unravel how cryptocurrencies fundamentally work and the step-by-step process on how to create a cryptocurrency.   The study examines blockchain technologies and their pivotal role in the evolving Metaverse, shedding light on topics such as how to invest in cryptocurrency, the mechanics behind crypto mining, and strategies to effectively buy and trade cryptocurrencies. Through an interdisciplinary approach, the research transitions from the fundamental principles of fintech investment strategies to the overarching implications of blockchain within the Metaverse. Alongside exploring machine learning potentials in financial sectors and risk assessment methodologies, the study critically assesses whether developed or developing nations are poised to reap greater benefits from these technologies. Moreover, it probes into both enduring and dubious crypto projects, drawing a distinct line between genuine blockchain applications and Ponzi-like schemes. The conclusion resolutely affirms the continuing dominance of blockchain technologies, underlined by a profound exploration of their intrinsic value and a reflective commentary by the author on the potential risks confronting individual investors.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Detecting Financial Market Manipulation with Statistical Physics Tools,"We take inspiration from statistical physics to develop a novel conceptual framework for the analysis of financial markets. We model the order book dynamics as a motion of particles and define the momentum measure of the system as a way to summarise and assess the state of the market. Our approach proves useful in capturing salient financial market phenomena: in particular, it helps detect the market manipulation activities called spoofing and layering. We apply our method to identify pathological order book behaviours during the flash crash of the LUNA cryptocurrency, uncovering widespread instances of spoofing and layering in the market. Furthermore, we establish that our technique outperforms the conventional Z-score-based anomaly detection method in identifying market manipulations across both LUNA and Bitcoin cryptocurrency markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Neural Network-Based Algorithmic Trading Systems: Multi-Timeframe Analysis and High-Frequency Execution in Cryptocurrency Markets,"This paper explores neural network-based approaches for algorithmic trading in cryptocurrency markets. Our approach combines multi-timeframe trend analysis with high-frequency direction prediction networks, achieving positive risk-adjusted returns through statistical modeling and systematic market exploitation. The system integrates diverse data sources including market data, on-chain metrics, and orderbook dynamics, translating these into unified buy/sell pressure signals. We demonstrate how machine learning models can effectively capture cross-timeframe relationships, enabling sub-second trading decisions with statistical confidence.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries Cryptocurrency Trend Forecasting,"In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities. Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences. This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data. Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets. By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\% in prediction accuracy. This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies. Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Bitcoin Gold, Litecoin Silver:An Introduction to Cryptocurrency's Valuation and Trading Strategy","Historically, gold and silver have played distinct roles in traditional monetary systems. While gold has primarily been revered as a superior store of value, prompting individuals to hoard it, silver has commonly been used as a medium of exchange. As the financial world evolves, the emergence of cryptocurrencies has introduced a new paradigm of value and exchange. However, the store-of-value characteristic of these digital assets remains largely uncharted. Charlie Lee, the founder of Litecoin, once likened Bitcoin to gold and Litecoin to silver. To validate this analogy, our study employs several metrics, including unspent transaction outputs (UTXO), spent transaction outputs (STXO), Weighted Average Lifespan (WAL), CoinDaysDestroyed (CDD), and public on-chain transaction data. Furthermore, we've devised trading strategies centered around the Price-to-Utility (PU) ratio, offering a fresh perspective on crypto-asset valuation beyond traditional utilities. Our back-testing results not only display trading indicators for both Bitcoin and Litecoin but also substantiate Lee's metaphor, underscoring Bitcoin's superior store-of-value proposition relative to Litecoin. We anticipate that our findings will drive further exploration into the valuation of crypto assets. For enhanced transparency and to promote future research, we've made our datasets available on Harvard Dataverse and shared our Python code on GitHub as open source.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Editorial: Understanding Cryptocurrencies,"Cryptocurrency refers to a type of digital asset that uses distributed ledger, or blockchain, technology to enable a secure transaction. Although the technology is widely misunderstood, many central banks are considering launching their own national cryptocurrency. In contrast to most data in financial economics, detailed data on the history of every transaction in the cryptocurrency complex are freely available. Furthermore, empirically-oriented research is only now beginning, presenting an extraordinary research opportunity for academia. We provide some insights into the mechanics of cryptocurrencies, describing summary statistics and focusing on potential future research avenues in financial economics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The inefficiency of Bitcoin revisited: a dynamic approach,"This letter revisits the informational efficiency of the Bitcoin market. In particular we analyze the time-varying behavior of long memory of returns on Bitcoin and volatility 2011 until 2017, using the Hurst exponent. Our results are twofold. First, R/S method is prone to detect long memory, whereas DFA method can discriminate more precisely variations in informational efficiency across time. Second, daily returns exhibit persistent behavior in the first half of the period under study, whereas its behavior is more informational efficient since 2014. Finally, price volatility, measured as the logarithmic difference between intraday high and low prices exhibits long memory during all the period. This reflects a different underlying dynamic process generating the prices and volatility.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Market Dynamics vs. Statistics: Limit Order Book Example,"Commonly used limit order book attributes are empirically considered based on NASDAQ ITCH data. It is shown that some of them have the properties drastically different from the ones assumed in many market dynamics study. Because of this difference we propose to make a transition from ""Statistical"" type of order book study (typical for academics) to ""Dynamical"" type of study (typical for market practitioners). Based on market data analysis we conclude, that most of market dynamics information is contained in attributes with spikes (e.g. executed trades flow $I=dv/dt$), there is no any ""stationary case"" on the market and typical market dynamics is a ""fast excitation and then slow relaxation"" type of behavior with a wide distribution of excitation frequencies and relaxation times. A computer code, providing full depth order book information and recently executed trades is available from authors [1].","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Pricing Bitcoin Derivatives under Jump-Diffusion Models,In recent years cryptocurrency trading has captured the attention of practitioners and academics. The volume of the exchange with standard currencies has known a dramatic increasing of late. This paper addresses to the need of models describing a bitcoin-US dollar exchange dynamic and their use to evaluate European option having bitcoin as underlying asset.,"cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Market Dynamics. On A Muse Of Cash Flow And Liquidity Deficit,"A first attempt at obtaining market--directional information from a non--stationary solution of the dynamic equation ""future price tends to the value that maximizes the number of shares traded per unit time"" [1] is presented. We demonstrate that the concept of price impact is poorly applicable to market dynamics. Instead, we consider the execution flow $I=dV/dt$ operator with the ""impact from the future"" term providing information about not--yet--executed trades. The ""impact from the future"" on $I$ can be directly estimated from the already--executed trades, the directional information on price is then obtained from the experimentally observed fact that the $I$ and $p$ operators have the same eigenfunctions (the exact result in the dynamic impact approximation $p=p(I)$). The condition for ""no information about the future"" is found and directional prediction quality is discussed. This work makes a substantial contribution toward solving the ultimate market dynamics problem: find evidence of existence (or proof of non--existence) of an automated trading machine which consistently makes positive P\&L on a free market as an autonomous agent (aka the existence of the market dynamics equation). The software with a reference implementation of the theory is provided.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Quantifying Cryptocurrency Unpredictability: A Comprehensive Study of Complexity and Forecasting,"This paper offers a thorough examination of the univariate predictability in cryptocurrency time-series. By exploiting a combination of complexity measure and model predictions we explore the cryptocurrencies time-series forecasting task focusing on the exchange rate in USD of Litecoin, Binance Coin, Bitcoin, Ethereum, and XRP. On one hand, to assess the complexity and the randomness of these time-series, a comparative analysis has been performed using Brownian and colored noises as a benchmark. The results obtained from the Complexity-Entropy causality plane and power density spectrum analysis reveal that cryptocurrency time-series exhibit characteristics closely resembling those of Brownian noise when analyzed in a univariate context. On the other hand, the application of a wide range of statistical, machine and deep learning models for time-series forecasting demonstrates the low predictability of cryptocurrencies. Notably, our analysis reveals that simpler models such as Naive models consistently outperform the more complex machine and deep learning ones in terms of forecasting accuracy across different forecast horizons and time windows. The combined study of complexity and forecasting accuracies highlights the difficulty of predicting the cryptocurrency market. These findings provide valuable insights into the inherent characteristics of the cryptocurrency data and highlight the need to reassess the challenges associated with predicting cryptocurrency's price movements.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Using an Artificial Financial Market for studying a Cryptocurrency Market,"This paper presents an agent-based artificial cryptocurrency market in which heterogeneous agents buy or sell cryptocurrencies, in particular Bitcoins. In this market, there are two typologies of agents, Random Traders and Chartists, which interact with each other by trading Bitcoins. Each agent is initially endowed with a finite amount of crypto and/or fiat cash and issues buy and sell orders, according to her strategy and resources. The number of Bitcoins increases over time with a rate proportional to the real one, even if the mining process is not explicitly modelled.   The model proposed is able to reproduce some of the real statistical properties of the price absolute returns observed in the Bitcoin real market. In particular, it is able to reproduce the autocorrelation of the absolute returns, and their cumulative distribution function. The simulator has been implemented using object-oriented technology, and could be considered a valid starting point to study and analyse the cryptocurrency market and its future evolutions.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Cryptocurrency Valuation: An Explainable AI Approach,"Currently, there are no convincing proxies for the fundamentals of cryptocurrency assets. We propose a new market-to-fundamental ratio, the price-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We then proxy various existing fundamental-to-market ratios by Bitcoin historical data and find they have little predictive power for short-term bitcoin returns. However, PU ratio effectively predicts long-term bitcoin returns than alternative methods. Furthermore, we verify the explainability of PU ratio using machine learning. Finally, we present an automated trading strategy advised by the PU ratio that outperforms the conventional buy-and-hold and market-timing strategies. Our research contributes to explainable AI in finance from three facets: First, our market-to-fundamental ratio is based on classic monetary theory and the unique UTXO model of Bitcoin accounting rather than ad hoc; Second, the empirical evidence testifies the buy-low and sell-high implications of the ratio; Finally, we distribute the trading algorithms as open-source software via Python Package Index for future research, which is exceptional in finance research.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process,"Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock's LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Comparison of Cryptocurrency Volatility-benchmarking New and Mature Asset Classes,"The paper analyzes the cryptocurrency ecosystem at both the aggregate and individual levels to understand the factors that impact future volatility. The study uses high-frequency panel data from 2020 to 2022 to examine the relationship between several market volatility drivers, such as daily leverage, signed volatility and jumps. Several known autoregressive model specifications are estimated over different market regimes, and results are compared to equity data as a reference benchmark of a more mature asset class. The panel estimations show that the positive market returns at the high-frequency level increase price volatility, contrary to what is expected from the classical financial literature. We attributed this effect to the price dynamics over the last year of the dataset (2022) by repeating the estimation on different time spans. Moreover, the positive signed volatility and negative daily leverage positively impact the cryptocurrencies' future volatility, unlike what emerges from the same study on a cross-section of stocks. This result signals a structural difference in a nascent cryptocurrency market that has to mature yet. Further individual-level analysis confirms the findings of the panel analysis and highlights that these effects are statistically significant and commonly shared among many components in the selected universe.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Technical Analysis Meets Machine Learning: Bitcoin Evidence,"In this note, we compare Bitcoin trading performance using two machine learning models-Light Gradient Boosting Machine (LightGBM) and Long Short-Term Memory (LSTM)-and two technical analysis-based strategies: Exponential Moving Average (EMA) crossover and a combination of Moving Average Convergence/Divergence with the Average Directional Index (MACD+ADX). The objective is to evaluate how trading signals can be used to maximize profits in the Bitcoin market. This comparison was motivated by the U.S. Securities and Exchange Commission's (SEC) approval of the first spot Bitcoin exchange-traded funds (ETFs) on 2024-01-10. Our results show that the LSTM model achieved a cumulative return of approximately 65.23% in under a year, significantly outperforming LightGBM, the EMA and MACD+ADX strategies, as well as the baseline buy-and-hold. This study highlights the potential for deeper integration of machine learning and technical analysis in the rapidly evolving cryptocurrency landscape.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Changes to the extreme and erratic behaviour of cryptocurrencies during COVID-19,"This paper introduces new methods for analysing the extreme and erratic behaviour of time series to evaluate the impact of COVID-19 on cryptocurrency market dynamics. Across 51 cryptocurrencies, we examine extreme behaviour through a study of distribution extremities, and erratic behaviour through structural breaks. First, we analyse the structure of the market as a whole and observe a reduction in self-similarity as a result of COVID-19, particularly with respect to structural breaks in variance. Second, we compare and contrast these two behaviours, and identify individual anomalous cryptocurrencies. Tether (USDT) and TrueUSD (TUSD) are consistent outliers with respect to their returns, while Holo (HOT), NEXO (NEXO), Maker (MKR) and NEM (XEM) are frequently observed as anomalous with respect to both behaviours and time. Even among a market known as consistently volatile, this identifies individual cryptocurrencies that behave most irregularly in their extreme and erratic behaviour and shows these were more affected during the COVID-19 market crisis.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators,"This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward directions. This is particularly advantageous for time series data where past and future data points can influence the current state. The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature. The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Market Dynamics: On Directional Information Derived From (Time, Execution Price, Shares Traded) Transaction Sequences","A new approach to obtaining market--directional information, based on a non-stationary solution to the dynamic equation ""future price tends to the value that maximizes the number of shares traded per unit time"" [1] is presented. In our previous work[2], we established that it is the share execution flow ($I=dV/dt$) and not the share trading volume ($V$) that is the driving force of the market, and that asset prices are much more sensitive to the execution flow $I$ (the dynamic impact) than to the traded volume $V$ (the regular impact). In this paper, an important advancement is achieved: we define the ""scalp-price"" ${\cal P}$ as the sum of only those price moves that are relevant to market dynamics; the criterion of relevance is a high $I$. Thus, only ""follow the market"" (and not ""little bounce"") events are included in ${\cal P}$. Changes in the scalp-price defined this way indicate a market trend change - not a bear market rally or a bull market sell-off; the approach can be further extended to non-local price change. The software calculating the scalp--price given market observations triples (time, execution price, shares traded) is available from the authors.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Cryptocurrency Trading: A Comprehensive Survey,"In recent years, the tendency of the number of financial institutions including cryptocurrencies in their portfolios has accelerated. Cryptocurrencies are the first pure digital assets to be included by asset managers. Although they have some commonalities with more traditional assets, they have their own separate nature and their behaviour as an asset is still in the process of being understood. It is therefore important to summarise existing research papers and results on cryptocurrency trading, including available trading platforms, trading signals, trading strategy research and risk management. This paper provides a comprehensive survey of cryptocurrency trading research, by covering 146 research papers on various aspects of cryptocurrency trading (e.g., cryptocurrency trading systems, bubble and extreme conditions, prediction of volatility and return, crypto-assets portfolio construction and crypto-assets, technical trading and others). This paper also analyses datasets, research trends and distribution among research objects(contents/properties) and technologies, concluding with some promising opportunities that remain open in cryptocurrency trading.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Reconstructing cryptocurrency processes via Markov chains,"The growing attention on cryptocurrencies has led to increasing research on digital stock markets. Approaches and tools usually applied to characterize standard stocks have been applied to the digital ones. Among these tools is the identification of processes of market fluctuations. Being interesting stochastic processes, the usual statistical methods are appropriate tools for their reconstruction. There, besides chance, the description of a behavioural component shall be present whenever a deterministic pattern is ever found. Markov approaches are at the leading edge of this endeavour. In this paper, Markov chains of orders one to eight are considered as a way to forecast the dynamics of three major cryptocurrencies. It is accomplished using an empirical basis of intra-day returns. Besides forecasting, we investigate the existence of eventual long-memory components in each of those stochastic processes. Results show that predictions obtained from using the empirical probabilities are better than random choices.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Bitcoin price formation: Beyond the fundamental sources,"Much significant research has been done to investigate various facets of the link between Bitcoin price and its fundamental sources. This study goes beyond by looking into least to most influential factors-across the fundamental, macroeconomic, financial, speculative and technical determinants as well as the 2016 events-which drove the value of Bitcoin in times of economic and geopolitical chaos. We use a Bayesian quantile regression to inspect how the structure of dependence of Bitcoin price and its determinants varies across the entire conditional distribution of Bitcoin price movements. In doing so, three groups of determinants were derived. The use of Bitcoin in trade and the uncertainty surrounding China's deepening slowdown, Brexit and India's demonetization were found to be the most potential contributors of Bitcoin price when the market is improving. The intense anxiety over Donald Trump being the president of United States was shown to be a positive determinant pushing up the price of Bitcoin when the market is functioning around the normal mode. The velocity of bitcoins in circulation, the gold price, the Venezuelan currency demonetization and the hash rate were found to be the fundamentals influencing the Bitcoin price when the market is heading into decline.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Investor base and idiosyncratic volatility of cryptocurrencies,"This paper investigates how changes in investor base is related to idiosyncratic volatility in cryptocurrency markets. For each cryptocurrency, we set change in its subreddit followers as a proxy for the change in its investor base, and find out that the latter can significantly increase cryptocurrencies idiosyncratic volatility. This finding is not subsumed by effects of size, momentum, liquidity and volume and is robust to various measures of idiosyncratic volatility.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Wavelet Analysis of Cryptocurrencies -- Non-Linear Dynamics in High Frequency Domains,"In this study, we perform some analysis for the probability distributions in the space of frequency and time variables. However, in the domain of high frequencies, it behaves in such a way as the highly non-linear dynamics. The wavelet analysis is a powerful tool to perform such analysis in order to search for the characteristics of frequency variations over time for the prices of major cryptocurrencies. In fact, the wavelet analysis is found to be quite useful as it examine the validity of the efficient market hypothesis in the weak form, especially for the presence of the cyclical persistence at different frequencies. If we could find some cyclical persistence at different frequencies, that means that there exist some intrinsic causal relationship for some given investment horizons defined by some chosen sampling scales. This is one of the characteristic results of the wavelet analysis in the time-frequency domains.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Trade Execution Flow as the Underlying Source of Market Dynamics,"In this work, we demonstrate experimentally that the execution flow, $I = dV/dt$, is the fundamental driving force of market dynamics. We develop a numerical framework to calculate execution flow from sampled moments using the Radon-Nikodym derivative. A notable feature of this approach is its ability to automatically determine thresholds that can serve as actionable triggers. The technique also determines the characteristic time scale directly from the corresponding eigenproblem. The methodology has been validated on actual market data to support these findings. Additionally, we introduce a framework based on the Christoffel function spectrum, which is invariant under arbitrary non-degenerate linear transformations of input attributes and offers an alternative to traditional principal component analysis (PCA), which is limited to unitary invariance.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Forecasting Bitcoin volatility spikes from whale transactions and CryptoQuant data using Synthesizer Transformer models,"The cryptocurrency market is highly volatile compared to traditional financial markets. Hence, forecasting its volatility is crucial for risk management. In this paper, we investigate CryptoQuant data (e.g. on-chain analytics, exchange and miner data) and whale-alert tweets, and explore their relationship to Bitcoin's next-day volatility, with a focus on extreme volatility spikes. We propose a deep learning Synthesizer Transformer model for forecasting volatility. Our results show that the model outperforms existing state-of-the-art models when forecasting extreme volatility spikes for Bitcoin using CryptoQuant data as well as whale-alert tweets. We analysed our model with the Captum XAI library to investigate which features are most important. We also backtested our prediction results with different baseline trading strategies and the results show that we are able to minimize drawdown while keeping steady profits. Our findings underscore that the proposed method is a useful tool for forecasting extreme volatility movements in the Bitcoin market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Signature of maturity in cryptocurrency volatility,"We study the fluctuations, particularly the inequality of fluctuations, in cryptocurrency prices over the last ten years. We calculate the inequality in the price fluctuations through different measures, such as the Gini and Kolkata indices, and also the $Q$ factor (given by the ratio between the highest value and the average value) of these fluctuations. We compare the results with the equivalent quantities in some of the more prominent national currencies and see that while the fluctuations (or inequalities in such fluctuations) for cryptocurrencies were initially significantly higher than national currencies, over time the fluctuation levels of cryptocurrencies tend towards the levels characteristic of national currencies. We also compare similar quantities for a few prominent stock prices.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Predicting digital asset market based on blockchain activity data,"Blockchain technology shows significant results and huge potential for serving as an interweaving fabric that goes through every industry and market, allowing decentralized and secure value exchange, thus connecting our civilization like never before. The standard approach for asset value predictions is based on market analysis with an LSTM neural network. Blockchain technologies, however, give us access to vast amounts of public data, such as the executed transactions and the account balance distribution. We explore whether analyzing this data with modern Deep Leaning techniques results in higher accuracies than the standard approach. During a series of experiments on the Ethereum blockchain, we achieved $4$ times error reduction with blockchain data than an LSTM approach with trade volume data. By utilizing blockchain account distribution histograms, spatial dataset modeling, and a Convolutional architecture, the error was reduced further by 26\%. The proposed methodologies are implemented in an open source cryptocurrency prediction framework, allowing them to be used in other analysis contexts.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamics on/in financial markets: dynamical decoupling and stylized facts,"Stylized facts can be regarded as constraints for any modeling attempt of price dynamics on a financial market, in that an empirically reasonable model has to reproduce these stylized facts at least qualitatively. The dynamics of market prices is modeled on a macro-level as the result of the dynamic coupling of two dynamical components. The degree of their dynamical decoupling is shown to have a significant impact on the stochastic properties of return trials such as the return distribution, volatility clustering, and the multifractal behavior of time scales of asset returns. Particularly we observe a cross over in the return distribution from a Gaussian-like to a Levy-like shape when the degree of decoupling increases. In parallel, the larger the degree of decoupling is the more pronounced is volatility clustering. These findings suggest that the considerations of time in an economic system, in general, and the coupling of constituting processes is essential for understanding the behavior of a financial market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Order book dynamics in liquid markets: limit theorems and diffusion approximations,"We propose a model for the dynamics of a limit order book in a liquid market where buy and sell orders are submitted at high frequency. We derive a functional central limit theorem for the joint dynamics of the bid and ask queues and show that, when the frequency of order arrivals is large, the intraday dynamics of the limit order book may be approximated by a Markovian jump-diffusion process in the positive orthant, whose characteristics are explicitly described in terms of the statistical properties of the underlying order flow. This result allows to obtain tractable analytical approximations for various quantities of interest, such as the probability of a price increase or the distribution of the duration until the next price move, conditional on the state of the order book. Our results allow for a wide range of distributional assumptions and temporal dependence in the order flow and apply to a wide class of stochastic models proposed for order book dynamics, including models based on Poisson point processes, self-exciting point processes and models of the ACD-GARCH family.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
An empirical study of market risk factors for Bitcoin,"The study examines whether fama-french equity factors can effectively explain the idiosyncratic risk and return characteristics of Bitcoin. By incorporating Fama-french factors, the explanatory power of these factors on Bitcoin's excess returns over various moving average periods is tested through applications of several statistical methods. The analysis aims to determine if equity market factors are significant in explaining and modeling systemic risk in Bitcoin.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
An Adaptive Multi Agent Bitcoin Trading System,"This paper presents a Multi Agent Bitcoin Trading system that utilizes Large Language Models (LLMs) for alpha generation and portfolio management in the cryptocurrencies market. Unlike equities, cryptocurrencies exhibit extreme volatility and are heavily influenced by rapidly shifting market sentiments and regulatory announcements, making them difficult to model using static regression models or neural networks trained solely on historical data. The proposed framework overcomes this by structuring LLMs into specialised agents for technical analysis, sentiment evaluation, decision-making, and performance reflection. The agents improve over time via a novel verbal feedback mechanism where a Reflect agent provides daily and weekly natural-language critiques of trading decisions. These textual evaluations are then injected into future prompts of the agents, allowing them to adjust allocation logic without weight updates or finetuning. Back-testing on Bitcoin price data from July 2024 to April 2025 shows consistent outperformance across market regimes: the Quantitative agent delivered over 30\% higher returns in bullish phases and 15\% overall gains versus buy-and-hold, while the sentiment-driven agent turned sideways markets from a small loss into a gain of over 100\%. Adding weekly feedback further improved total performance by 31\% and reduced bearish losses by 10\%. The results demonstrate that verbal feedback represents a new, scalable, and low-cost approach of tuning LLMs for financial goals.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Predicting the Price Movement of Cryptocurrencies Using Linear Law-based Transformation,"The aim of this paper is to investigate the effect of a novel method called linear law-based feature space transformation (LLT) on the accuracy of intraday price movement prediction of cryptocurrencies. To do this, the 1-minute interval price data of Bitcoin, Ethereum, Binance Coin, and Ripple between 1 January 2019 and 22 October 2022 were collected from the Binance cryptocurrency exchange. Then, 14-hour nonoverlapping time windows were applied to sample the price data. The classification was based on the first 12 hours, and the two classes were determined based on whether the closing price rose or fell after the next 2 hours. These price data were first transformed with the LLT, then they were classified by traditional machine learning algorithms with 10-fold cross-validation. Based on the results, LLT greatly increased the accuracy for all cryptocurrencies, which emphasizes the potential of the LLT algorithm in predicting price movements.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Denoising Complex Covariance Matrices with Hybrid ResNet and Random Matrix Theory: Cryptocurrency Portfolio Applications,"Covariance matrices estimated from short, noisy, and non-Gaussian financial time series-particularly cryptocurrencies-are notoriously unstable. Empirical evidence indicates that these covariance structures often exhibit power-law scaling, reflecting complex and hierarchical interactions among assets. Building on this insight, we propose a power-law covariance model to characterize the collective dynamics of cryptocurrencies and develop a hybrid estimator that integrates Random Matrix Theory (RMT) with Residual Neural Networks (ResNets). The RMT component regularizes the eigenvalue spectrum under high-dimensional noise, while the ResNet learns data-driven corrections to recover latent structural dependencies. Monte Carlo simulations show that ResNet-based estimators consistently minimize both Frobenius and minimum-variance (MV) losses across diverse covariance models. Empirical experiments on 89 cryptocurrencies (2020-2025), using a training period ending at the local BTC maximum in November 2021 and testing through the subsequent bear market, demonstrate that a two-step estimator combining hierarchical filtering with ResNet corrections yields the most profitable and balanced portfolios, remaining robust under market regime shifts. These findings highlight the potential of combining RMT, deep learning, and power-law modeling to capture the intrinsic complexity of financial systems and enhance portfolio optimization under realistic conditions.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Forecasting the movements of Bitcoin prices: an application of machine learning algorithms,"Cryptocurrencies, such as Bitcoin, are one of the most controversial and complex technological innovations in today's financial system. This study aims to forecast the movements of Bitcoin prices at a high degree of accuracy. To this aim, four different Machine Learning (ML) algorithms are applied, namely, the Support Vector Machines (SVM), the Artificial Neural Network (ANN), the Naive Bayes (NB) and the Random Forest (RF) besides the logistic regression (LR) as a benchmark model. In order to test these algorithms, besides existing continuous dataset, discrete dataset was also created and used. For the evaluations of algorithm performances, the F statistic, accuracy statistic, the Mean Absolute Error (MAE), the Root Mean Square Error (RMSE) and the Root Absolute Error (RAE) metrics were used. The t test was used to compare the performances of the SVM, ANN, NB and RF with the performance of the LR. Empirical findings reveal that, while the RF has the highest forecasting performance in the continuous dataset, the NB has the lowest. On the other hand, while the ANN has the highest and the NB the lowest performance in the discrete dataset. Furthermore, the discrete dataset improves the overall forecasting performance in all algorithms (models) estimated.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Intraday Trading Algorithm for Predicting Cryptocurrency Price Movements Using Twitter Big Data Analysis,"Cryptocurrencies have emerged as a novel financial asset garnering significant attention in recent years. A defining characteristic of these digital currencies is their pronounced short-term market volatility, primarily influenced by widespread sentiment polarization, particularly on social media platforms such as Twitter. Recent research has underscored the correlation between sentiment expressed in various networks and the price dynamics of cryptocurrencies. This study delves into the 15-minute impact of informative tweets disseminated through foundation channels on trader behavior, with a focus on potential outcomes related to sentiment polarization. The primary objective is to identify factors that can predict positive price movements and potentially be leveraged through a trading algorithm. To accomplish this objective, we conduct a conditional examination of return and excess return rates within the 15 minutes following tweet publication. The empirical findings reveal statistically significant increases in return rates, particularly within the initial three minutes following tweet publication. Notably, adverse effects resulting from the messages were not observed. Surprisingly, sentiments were found to have no discerni-ble impact on cryptocurrency price movements. Our analysis further identifies that inves-tors are primarily influenced by the quality of tweet content, as reflected in the choice of words and tweet volume. While the basic trading algorithm presented in this study does yield some benefits within the 15-minute timeframe, these benefits are not statistically significant. Nevertheless, it serves as a foundational framework for potential enhance-ments and further investigations.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Emoji Driven Crypto Assets Market Reactions,"In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. Our architecture's analysis of emoji sentiment demonstrated a distinct advantage over FinBERT's pure text sentiment analysis in such predicting power. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuanced perspective on the interplay between digital communication and market dynamics in an academic context.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Financial Connectome: A Brain-Inspired Framework for Modeling Latent Market Dynamics,"We propose the Financial Connectome, a new scientific discipline that models financial markets through the lens of brain functional architecture. Inspired by the foundational work of group independent component analysis (groupICA) in neuroscience, we reimagine markets not as collections of assets, but as high-dimensional dynamic systems composed of latent market modules. Treating stocks as functional nodes and their co-fluctuations as expressions of collective cognition, we introduce dynamic Market Network Connectivity (dMNC), the financial analogue of dynamic functional connectivity (dFNC). This biologically inspired framework reveals structurally persistent market subnetworks, captures regime shifts, and uncovers systemic early warning signals all without reliance on predictive labels. Our results suggest that markets, like brains, exhibit modular, self-organizing, and temporally evolving architectures. This work inaugurates the field of financial connectomics, a principled synthesis of systems neuroscience and quantitative finance aimed at uncovering the hidden logic of complex economies.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Simulating and analyzing a sparse order book: an application to intraday electricity markets,"This paper presents a novel model for simulating and analyzing sparse limit order books (LOBs), with a specific application to the European intraday electricity market. In illiquid markets, characterized by significant gaps between order levels due to sparse trading volumes, traditional LOB models often fall short. Our approach utilizes an inhomogeneous Poisson process to accurately capture the sporadic nature of order arrivals and cancellations on both the bid and ask sides of the book. By applying this model to the intraday electricity market, we gain insights into the unique microstructural behaviors and challenges of this dynamic trading environment. The results offer valuable implications for market participants, enhancing their understanding of LOB dynamics in illiquid markets. This work contributes to the broader field of market microstructure by providing a robust framework adaptable to various illiquid market settings beyond electricity trading.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Marketron games: Self-propelling stocks vs dumb money and metastable dynamics of the Good, Bad and Ugly markets","We present a model of price formation in an inelastic market whose dynamics are partially driven by both money flows and their impact on asset prices. The money flow to the market is viewed as an investment policy of outside investors. For the price impact effect, we use an impact function that incorporates the phenomena of market inelasticity and saturation from new money (the $dumb \; money$ effect). Due to the dependence of market investors' flows on market performance, the model implies a feedback mechanism that gives rise to nonlinear dynamics. Consequently, the market price dynamics are seen as a nonlinear diffusion of a particle (the $marketron$) in a two-dimensional space formed by the log-price $x$ and a memory variable $y$. The latter stores information about past money flows, so that the dynamics are non-Markovian in the log price $x$ alone, but Markovian in the pair $(x,y)$, bearing a strong resemblance to spiking neuron models in neuroscience. In addition to market flows, the model dynamics are partially driven by return predictors, modeled as unobservable Ornstein-Uhlenbeck processes. By using a new interpretation of predictive signals as $self$-$propulsion$ components of the price dynamics, we treat the marketron as an active particle, amenable to methods developed in the physics of active matter. We show that, depending on the choice of parameters, our model can produce a rich variety of interesting dynamic scenarios. In particular, it predicts three distinct regimes of the market, which we call the $Good$, the $Bad$, and the $Ugly$ markets. The latter regime describes a scenario of a total market collapse or, alternatively, a corporate default event, depending on whether our model is applied to the whole market or an individual stock.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Adaptive Complementary Ensemble EMD and Energy-Frequency Spectra of Cryptocurrency Prices,"We study the price dynamics of cryptocurrencies using adaptive complementary ensemble empirical mode decomposition (ACE-EMD) and Hilbert spectral analysis. This is a multiscale noise-assisted approach that decomposes any time series into a number of intrinsic mode functions, along with the corresponding instantaneous amplitudes and instantaneous frequencies. The decomposition is adaptive to the time-varying volatility of each cryptocurrency price evolution. Different combinations of modes allow us to reconstruct the time series using components of different timescales. We then apply Hilbert spectral analysis to define and compute the instantaneous energy-frequency spectrum of each cryptocurrency to illustrate the properties of various timescales embedded in the original time series.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Modeling the price of Bitcoin with geometric fractional Brownian motion: a Monte Carlo approach,"The long-term dependence of Bitcoin (BTC), manifesting itself through a Hurst exponent $H>0.5$, is exploited in order to predict future BTC/USD price. A Monte Carlo simulation with $10^4$ geometric fractional Brownian motion realisations is performed as extensions of historical data. The accuracy of statistical inferences is 10\%. The most probable Bitcoin price at the beginning of 2018 is 6358 USD.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Geometric Arbitrage Theory and Market Dynamics Reloaded,We have embedded the classical theory of stochastic finance into a differential geometric framework called Geometric Arbitrage Theory and show that it is possible to:   --Write arbitrage as curvature of a principal fibre bundle.   --Parameterize arbitrage strategies by its holonomy.   --Give the Fundamental Theorem of Asset Pricing a differential homotopic characterization.   --Characterize Geometric Arbitrage Theory by five principles and show they they are consistent with the classical theory of stochastic finance.   --Derive for a closed market the equilibrium solution for market portfolio and dynamics in the cases where:   -->Arbitrage is allowed but minimized.   -->Arbitrage is not allowed.   --Prove that the no-free-lunch-with-vanishing-risk condition implies the zero curvature condition.,"cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Bitcoin Coin Selection with Leverage,"We present a new Bitcoin coin selection algorithm, ""coin selection with leverage"", which aims to improve upon cost savings than that of standard knapsack like approaches. Parameters to the new algorithm are available to be tuned at the users discretion to address other goals of coin selection. Our approach naturally fits as a replacement for the standard knapsack ingredient of full coin selection procedures.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Causality between Sentiment and Cryptocurrency Prices,"This study investigates the relationship between narratives conveyed through microblogging platforms, namely Twitter, and the value of crypto assets. Our study provides a unique technique to build narratives about cryptocurrency by combining topic modelling of short texts with sentiment analysis. First, we used an unsupervised machine learning algorithm to discover the latent topics within the massive and noisy textual data from Twitter, and then we revealed 4-5 cryptocurrency-related narratives, including financial investment, technological advancement related to crypto, financial and political regulations, crypto assets, and media coverage. In a number of situations, we noticed a strong link between our narratives and crypto prices. Our work connects the most recent innovation in economics, Narrative Economics, to a new area of study that combines topic modelling and sentiment analysis to relate consumer behaviour to narratives.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security,"Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on blockchain and social media platforms. Our study contributes to the literature at the interface of blockchain security, MEV solutions, and AI ethics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On the Importance of Opponent Modeling in Auction Markets,"The dynamics of financial markets are driven by the interactions between participants, as well as the trading mechanisms and regulatory frameworks that govern these interactions. Decision-makers would rather not ignore the impact of other participants on these dynamics and should employ tools and models that take this into account. To this end, we demonstrate the efficacy of applying opponent-modeling in a number of simulated market settings. While our simulations are simplified representations of actual market dynamics, they provide an idealized ""playground"" in which our techniques can be demonstrated and tested. We present this work with the aim that our techniques could be refined and, with some effort, scaled up to the full complexity of real-world market scenarios. We hope that the results presented encourage practitioners to adopt opponent-modeling methods and apply them online systems, in order to enable not only reactive but also proactive decisions to be made.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Towards A Post-Quantum Cryptography in Blockchain I: Basic Review on Theoretical Cryptography and Quantum Information Theory,"Recently, the invention of quantum computers was so revolutionary that they bring transformative challenges in a variety of fields, especially for the traditional cryptographic blockchain, and it may become a real thread for most of the cryptocurrencies in the market. That is, it becomes inevitable to consider to implement a post-quantum cryptography, which is also referred to as quantum-resistant cryptography, for attaining quantum resistance in blockchains.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Community Detection in Cryptocurrencies with Potential Applications to Portfolio Diversification,"In this paper, the cross-correlations of cryptocurrency returns are analysed. The paper examines one years worth of data for 146 cryptocurrencies from the period January 1 2019 to December 31 2019. The cross-correlations of these returns are firstly analysed by comparing eigenvalues and eigenvector components of the cross-correlation matrix C with Random Matrix Theory (RMT) assumptions. Results show that C deviates from these assumptions indicating that C contains genuine information about the correlations between the different cryptocurrencies. From here, Louvain community detection method is applied as a clustering mechanism and 15 community groupings are detected. Finally, PCA is completed on the standardised returns of each of these clusters to create a portfolio of cryptocurrencies for investment. This method selects a portfolio which contains a number of high value coins when compared back against their market ranking in the same year. In the interest of assessing continuity of the initial results, the method is also applied to a smaller dataset of the top 50 cryptocurrencies across three time periods of T = 125 days, which produces similar results. The results obtained in this paper show that these methods could be useful for constructing a portfolio of optimally performing cryptocurrencies.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
PRIME: A Price-Reverting Impact Model of a cryptocurrency Exchange,"In a financial exchange, market impact is a measure of the price change of an asset following a transaction. This is an important element of market microstructure, which determines the behaviour of the market following a trade. In this paper, we first provide a discussion on the market impact observed in the BTC/USD Futures market, then we present a novel multi-agent market simulation that can follow an underlying price series, whilst maintaining the ability to reproduce the market impact observed in the market in an explainable manner. This simulation of the financial exchange allows the model to interact realistically with market participants, helping its users better estimate market slippage as well as the knock-on consequences of their market actions. In turn, it allows various stakeholders such as industrial practitioners, governments and regulators to test their market hypotheses, without deploying capital or destabilising the system.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
An Impulse Control Approach to Market Making in a Hawkes LOB Market,"We study the optimal Market Making problem in a Limit Order Book (LOB) market simulated using a high-fidelity, mutually exciting Hawkes process. Departing from traditional Brownian-driven mid-price models, our setup captures key microstructural properties such as queue dynamics, inter-arrival clustering, and endogenous price impact. Recognizing the realistic constraint that market makers cannot update strategies at every LOB event, we formulate the control problem within an impulse control framework, where interventions occur discretely via limit, cancel, or market orders. This leads to a high-dimensional, non-local Hamilton-Jacobi-Bellman Quasi-Variational Inequality (HJB-QVI), whose solution is analytically intractable and computationally expensive due to the curse of dimensionality. To address this, we propose a novel Reinforcement Learning (RL) approximation inspired by auxiliary control formulations. Using a two-network PPO-based architecture with self-imitation learning, we demonstrate strong empirical performance with limited training, achieving Sharpe ratios above 30 in a realistic simulated LOB. In addition to that, we solve the HJB-QVI using a deep learning method inspired by Sirignano and Spiliopoulos 2018 and compare the performance with the RL agent. Our findings highlight the promise of combining impulse control theory with modern deep RL to tackle optimal execution problems in jump-driven microstructural markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Arbitrage-Free Interpolation in Models of Market Observable Interest Rates,"Models which postulate lognormal dynamics for interest rates which are compounded according to market conventions, such as forward LIBOR or forward swap rates, can be constructed initially in a discrete tenor framework. Interpolating interest rates between maturities in the discrete tenor structure is equivalent to extending the model to continuous tenor. The present paper sets forth an alternative way of performing this extension; one which preserves the Markovian properties of the discrete tenor models and guarantees the positivity of all interpolated rates.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Multi Agent Model for the Limit Order Book Dynamics,"In the present work we introduce a novel multi-agent model with the aim to reproduce the dynamics of a double auction market at microscopic time scale through a faithful simulation of the matching mechanics in the limit order book. The agents follow a noise decision making process where their actions are related to a stochastic variable, ""the market sentiment"", which we define as a mixture of public and private information. The model, despite making just few basic assumptions over the trading strategies of the agents, is able to reproduce several empirical features of the high-frequency dynamics of the market microstructure not only related to the price movements but also to the deposition of the orders in the book.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
High-frequency financial market simulation and flash crash scenarios analysis: an agent-based modelling approach,"This paper describes simulations and analysis of flash crash scenarios in an agent-based modelling framework. We design, implement, and assess a novel high-frequency agent-based financial market simulator that generates realistic millisecond-level financial price time series for the E-Mini S&P 500 futures market. Specifically, a microstructure model of a single security traded on a central limit order book is provided, where different types of traders follow different behavioural rules. The model is calibrated using the machine learning surrogate modelling approach. Statistical test and moment coverage ratio results show that the model has excellent capability of reproducing realistic stylised facts in financial markets. By introducing an institutional trader that mimics the real-world Sell Algorithm on May 6th, 2010, the proposed high-frequency agent-based financial market simulator is used to simulate the Flash Crash that took place that day. We scrutinise the market dynamics during the simulated flash crash and show that the simulated dynamics are consistent with what happened in historical flash crash scenarios. With the help of Monte Carlo simulations, we discover functional relationships between the amplitude of the simulated 2010 Flash Crash and three conditions: the percentage of volume of the Sell Algorithm, the market maker inventory limit, and the trading frequency of fundamental traders. Similar analyses are carried out for mini flash crash events. An innovative ""Spiking Trader"" is introduced to the model, aiming at precipitating mini flash crash events. We analyse the market dynamics during the course of a typical simulated mini flash crash event and study the conditions affecting its characteristics. The proposed model can be used for testing resiliency and robustness of trading algorithms and providing advice for policymakers.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A model for a large investor trading at market indifference prices. II: Continuous-time case,"We develop from basic economic principles a continuous-time model for a large investor who trades with a finite number of market makers at their utility indifference prices. In this model, the market makers compete with their quotes for the investor's orders and trade among themselves to attain Pareto optimal allocations. We first consider the case of simple strategies and then, in analogy to the construction of stochastic integrals, investigate the transition to general continuous dynamics. As a result, we show that the model's evolution can be described by a nonlinear stochastic differential equation for the market makers' expected utilities.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines,"We postulates, and then show experimentally, that liquidity deficit is the driving force of the markets. In the first part of the paper a kinematic of liquidity deficit is developed. The calculus-like approach, which is based on Radon--Nikodym derivatives and their generalization, allows us to calculate important characteristics of observable market dynamics. In the second part of the paper this calculus is used in an attempt to build a dynamic equation in the form: future price tend to the value maximizing the number of shares traded per unit time. To build a practical automated trading machine P&L dynamics instead of price dynamics is considered. This allows a trading automate resilient to catastrophic P&L drains to be built. The results are very promising, yet when all the fees and trading commissions are taken into account, are close to breakeven. In the end of the paper important criteria for automated trading systems are presented. We list the system types that can and cannot make money on the market. These criteria can be successfully applied not only by automated trading machines, but also by a human trader.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
What are the main drivers of the Bitcoin price? Evidence from wavelet coherence analysis,"Bitcoin has emerged as a fascinating phenomenon of the financial markets. Without any central authority issuing the currency, it has been associated with controversy ever since its popularity and public interest reached high levels. Here, we contribute to the discussion by examining potential drivers of Bitcoin prices ranging from fundamental to speculative and technical sources as well as a potential influence of the Chinese market. The evolution of the relationships is examined in both time and frequency domains utilizing the continuous wavelets framework so that we comment on development of the interconnections in time but we can also distinguish between short-term and long-term connections.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study,"Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance. Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability. We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods. This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as ""over-the-counter"" (OTC) trading, and commonly occurring between ""market makers"". The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance. The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model. We explore the implications of market rigidity on market structure and consider the element of stability, in market design. This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin,"Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. This volatility, together with its decentralised nature, make Bitcoin highly subjective to speculative trading as compared to more traditional assets. In this paper, we propose a multimodal model for predicting extreme price fluctuations. This model takes as input a variety of correlated assets, technical indicators, as well as Twitter content. In an in-depth study, we explore whether social media discussions from the general public on Bitcoin have predictive power for extreme price movements. A dataset of 5,000 tweets per day containing the keyword `Bitcoin' was collected from 2015 to 2021. This dataset, called PreBit, is made available online. In our hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. By combining these embeddings with a Convolutional Neural Network, we built a predictive model for significant market movements. The final multimodal ensemble model includes this NLP model together with a model based on candlestick data, technical indicators and correlated asset prices. In an ablation study, we explore the contribution of the individual modalities. Finally, we propose and backtest a trading strategy based on the predictions of our models with varying prediction threshold and show that it can used to build a profitable trading strategy with a reduced risk over a `hold' or moving average strategy.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dependency structures in cryptocurrency market from high to low frequency,"We investigate logarithmic price returns cross-correlations at different time horizons for a set of 25 liquid cryptocurrencies traded on the FTX digital currency exchange. We study how the structure of the Minimum Spanning Tree (MST) and the Triangulated Maximally Filtered Graph (TMFG) evolve from high (15 s) to low (1 day) frequency time resolutions. For each horizon, we test the stability, statistical significance and economic meaningfulness of the networks. Results give a deep insight into the evolutionary process of the time dependent hierarchical organization of the system under analysis. A decrease in correlation between pairs of cryptocurrencies is observed for finer time sampling resolutions. A growing structure emerges for coarser ones, highlighting multiple changes in the hierarchical reference role played by mainstream cryptocurrencies. This effect is studied both in its pairwise realizations and intra-sector ones.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets -- A New Microfoundations of GARCH model,"The AI traders in financial markets have sparked significant interest in their effects on price formation mechanisms and market volatility, raising important questions for market stability and regulation. Despite this interest, a comprehensive model to quantitatively assess the specific impacts of AI traders remains undeveloped. This study aims to address this gap by modeling the influence of AI traders on market price formation and volatility within a multi-agent framework, leveraging the concept of microfoundations. Microfoundations involve understanding macroeconomic phenomena, such as market price formation, through the decision-making and interactions of individual economic agents. While widely acknowledged in macroeconomics, microfoundational approaches remain unexplored in empirical finance, particularly for models like the GARCH model, which captures key financial statistical properties such as volatility clustering and fat tails. This study proposes a multi-agent market model to derive the microfoundations of the GARCH model, incorporating three types of agents: noise traders, fundamental traders, and AI traders. By mathematically aggregating the micro-structure of these agents, we establish the microfoundations of the GARCH model. We validate this model through multi-agent simulations, confirming its ability to reproduce the stylized facts of financial markets. Finally, we analyze the impact of AI traders using parameters derived from these microfoundations, contributing to a deeper understanding of their role in market dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Automated Market Making: the case of Pegged Assets,"In this paper, we introduce a novel framework to model the exchange rate dynamics between two intrinsically linked cryptoassets, such as stablecoins pegged to the same fiat currency or a liquid staking token and its associated native token. Our approach employs multi-level nested Ornstein-Uhlenbeck (OU) processes, for which we derive key properties and develop calibration and filtering techniques. Then, we design an automated market maker (AMM) model specifically tailored for the swapping of closely related cryptoassets. Distinct from existing models, our AMM leverages the unique exchange rate dynamics provided by the multi-level nested OU processes, enabling more precise risk management and enhanced liquidity provision. We validate the model through numerical simulations using real-world data for the USDC/USDT and wstETH/WETH pairs, demonstrating that it consistently yields efficient quotes. This approach offers significant potential to improve liquidity in markets for pegged assets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations,"This paper presents a realistic simulated stock market where large language models (LLMs) act as heterogeneous competing trading agents. The open-source framework incorporates a persistent order book with market and limit orders, partial fills, dividends, and equilibrium clearing alongside agents with varied strategies, information sets, and endowments. Agents submit standardized decisions using structured outputs and function calls while expressing their reasoning in natural language. Three findings emerge: First, LLMs demonstrate consistent strategy adherence and can function as value investors, momentum traders, or market makers per their instructions. Second, market dynamics exhibit features of real financial markets, including price discovery, bubbles, underreaction, and strategic liquidity provision. Third, the framework enables analysis of LLMs' responses to varying market conditions, similar to partial dependence plots in machine-learning interpretability. The framework allows simulating financial theories without closed-form solutions, creating experimental designs that would be costly with human participants, and establishing how prompts can generate correlated behaviors affecting market stability.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Explaining herding and volatility in the cyclical price dynamics of urban housing markets using a large scale agent-based model,"Urban housing markets, along with markets of other assets, universally exhibit periods of strong price increases followed by sharp corrections. The mechanisms generating such non-linearities are not yet well understood. We develop an agent-based model populated by a large number of heterogeneous households. The agents' behavior is compatible with economic rationality, with the trend-following behavior found to be essential in replicating market dynamics. The model is calibrated using several large and distributed datasets of the Greater Sydney region (demographic, economic and financial) across three specific and diverse periods since 2006. The model is not only capable of explaining price dynamics during these periods, but also reproduces the novel behavior actually observed immediately prior to the market peak in 2017, namely a sharp increase in the variability of prices. This novel behavior is related to a combination of trend-following aptitude of the household agents (rational herding) and their propensity to borrow.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
From Portfolio Optimization to Quantum Blockchain and Security: A Systematic Review of Quantum Computing in Finance,"In this paper, we provide an overview of the recent work in the quantum finance realm from various perspectives. The applications in consideration are Portfolio Optimization, Fraud Detection, and Monte Carlo methods for derivative pricing and risk calculation. Furthermore, we give a comprehensive overview of the applications of quantum computing in the field of blockchain technology which is a main concept in fintech. In that sense, we first introduce the general overview of blockchain with its main cryptographic primitives such as digital signature algorithms, hash functions, and random number generators as well as the security vulnerabilities of blockchain technologies after the merge of quantum computers considering Shor's quantum factoring and Grover's quantum search algorithms. We then discuss the privacy preserving quantum-resistant blockchain systems via threshold signatures, ring signatures, and zero-knowledge proof systems i.e. ZK-SNARKs in quantum resistant blockchains. After emphasizing the difference between the quantum-resistant blockchain and quantum-safe blockchain we mention the security countermeasures to take against the possible quantumized attacks aiming these systems. We finalize our discussion with quantum blockchain, efficient quantum mining and necessary infrastructures for constructing such systems based on quantum computing. This review has the intention to be a bridge to fill the gap between quantum computing and one of its most prominent application realms: Finance. We provide the state-of-the-art results in the intersection of finance and quantum technology for both industrial practitioners and academicians.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
An Empirical Analysis on Financial Markets: Insights from the Application of Statistical Physics,"In this study, we introduce a physical model inspired by statistical physics for predicting price volatility and expected returns by leveraging Level 3 order book data. By drawing parallels between orders in the limit order book and particles in a physical system, we establish unique measures for the system's kinetic energy and momentum as a way to comprehend and evaluate the state of limit order book. Our model goes beyond examining merely the top layers of the order book by introducing the concept of 'active depth', a computationally-efficient approach for identifying order book levels that have impact on price dynamics. We empirically demonstrate that our model outperforms the benchmarks of traditional approaches and machine learning algorithm. Our model provides a nuanced comprehension of market microstructure and produces more accurate forecasts on volatility and expected returns. By incorporating principles of statistical physics, this research offers valuable insights on understanding the behaviours of market participants and order book dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamic Graph Representation with Contrastive Learning for Financial Market Prediction: Integrating Temporal Evolution and Static Relations,"Temporal Graph Learning (TGL) is crucial for capturing the evolving nature of stock markets. Traditional methods often ignore the interplay between dynamic temporal changes and static relational structures between stocks. To address this issue, we propose the Dynamic Graph Representation with Contrastive Learning (DGRCL) framework, which integrates dynamic and static graph relations to improve the accuracy of stock trend prediction. Our framework introduces two key components: the Embedding Enhancement (EE) module and the Contrastive Constrained Training (CCT) module. The EE module focuses on dynamically capturing the temporal evolution of stock data, while the CCT module enforces static constraints based on stock relations, refined within contrastive learning. This dual-relation approach allows for a more comprehensive understanding of stock market dynamics. Our experiments on two major U.S. stock market datasets, NASDAQ and NYSE, demonstrate that DGRCL significantly outperforms state-of-the-art TGL baselines. Ablation studies indicate the importance of both modules. Overall, DGRCL not only enhances prediction ability but also provides a robust framework for integrating temporal and relational data in dynamic graphs. Code and data are available for public access.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Democratization of Wealth Management: Hedged Mutual Fund Blockchain Protocol,"We develop several innovations to bring the best practices of traditional investment funds to the blockchain landscape. Specifically, we illustrate how: 1) fund prices can be updated regularly like mutual funds; 2) performance fees can be charged like hedge funds; 3) mutually hedged blockchain investment funds can operate with investor protection schemes, such as high water marks; and 4) measures to offset trading related slippage costs when redemptions happen. Using our concepts - and blockchain technology - traditional funds can calculate performance fees in a simplified manner and alleviate several operational issues. Blockchain can solve many problems for traditional finance, while tried and tested wealth management techniques can benefit decentralization, speeding its adoption. We provide detailed steps - including mathematical formulations and instructive pointers - to implement these ideas and discuss how our designs overcome several blockchain bottlenecks, making smart contracts smarter. We provide numerical illustrations of several scenarios related to our mechanisms.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamic portfolio optimization with liquidity cost and market impact: a simulation-and-regression approach,"We present a simulation-and-regression method for solving dynamic portfolio allocation problems in the presence of general transaction costs, liquidity costs and market impacts. This method extends the classical least squares Monte Carlo algorithm to incorporate switching costs, corresponding to transaction costs and transient liquidity costs, as well as multiple endogenous state variables, namely the portfolio value and the asset prices subject to permanent market impacts. To do so, we improve the accuracy of the control randomization approach in the case of discrete controls, and propose a global iteration procedure to further improve the allocation estimates. We validate our numerical method by solving a realistic cash-and-stock portfolio with a power-law liquidity model. We quantify the certainty equivalent losses associated with ignoring liquidity effects, and illustrate how our dynamic allocation protects the investor's capital under illiquid market conditions. Lastly, we analyze, under different liquidity conditions, the sensitivities of certainty equivalent returns and optimal allocations with respect to trading volume, stock price volatility, initial investment amount, risk-aversion level and investment horizon.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Marketron Through the Looking Glass: From Equity Dynamics to Option Pricing in Incomplete Markets,"The Marketron model, introduced by [Halperin, Itkin, 2025], describes price formation in inelastic markets as the nonlinear diffusion of a quasiparticle (the marketron) in a multidimensional space comprising the log-price $x$, a memory variable $y$ encoding past money flows, and unobservable return predictors $z$. While the original work calibrated the model to S\&P 500 time series data, this paper extends the framework to option markets - a fundamentally distinct challenge due to market incompleteness stemming from non-tradable state variables. We develop a utility-based pricing approach that constructs a risk-adjusted measure via the dual solution of an optimal investment problem. The resulting Hamilton-Jacobi-Bellman (HJB) equation, though computationally formidable, is solved using a novel methodology enabling efficient calibration even on standard laptop hardware. Having done that, we look at the additional question to answer: whether the Marketron model, calibrated to market option prices, can simultaneously reproduce the statistical properties of the underlying asset's log-returns. We discuss our results in view of the long-standing challenge in quantitative finance of developing an unified framework capable of jointly capturing equity returns, option smile dynamics, and potentially volatility index behavior.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A stochastic partial differential equation model for limit order book dynamics,"We propose an analytically tractable class of models for the dynamics of a limit order book, described through a stochastic partial differential equation (SPDE) with multiplicative noise for the order book centered at the mid-price, along with stochastic dynamics for the mid-price which is consistent with the order flow dynamics. We provide conditions under which the model admits a finite dimensional realization driven by a (low-dimensional) Markov process, leading to efficient estimation and computation methods. We study two examples of parsimonious models in this class: a two-factor model and a model with mean-reverting order book depth. For each model we analyze in detail the role of different parameters, the dynamics of the price, order book depth, volume and order imbalance, provide an intuitive financial interpretation of the variables involved and show how the model reproduces statistical properties of price changes, market depth and order flow in limit order markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A dynamic optimal execution strategy under stochastic price recovery,"In the present paper, we study the optimal execution problem under stochastic price recovery based on limit order book dynamics. We model price recovery after execution of a large order by accelerating the arrival of the refilling order, which is defined as a Cox process whose intensity increases by the degree of the market impact. We include not only the market order but also the limit order in our strategy in a restricted fashion. We formulate the problem as a combined stochastic control problem over a finite time horizon. The corresponding Hamilton-Jacobi-Bellman quasi-variational inequality is solved numerically. The optimal strategy obtained consists of three components: (i) the initial large trade; (ii) the unscheduled small trades during the period; (iii) the terminal large trade. The size and timing of the trade is governed by the tolerance for market impact depending on the state at each time step, and hence the strategy behaves dynamically. We also provide competitive results due to inclusion of the limit order, even though a limit order is allowed under conservative evaluation of the execution price.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Copula-Based Trading of Cointegrated Cryptocurrency Pairs,"This research introduces a novel pairs trading strategy based on copulas for cointegrated pairs of cryptocurrencies. To identify the most suitable pairs, the study employs linear and non-linear cointegration tests along with a correlation coefficient measure and fits different copula families to generate trading signals formulated from a reference asset for analyzing the mispricing index. The strategy's performance is then evaluated by conducting back-testing for various triggers of opening positions, assessing its returns and risks. The findings indicate that the proposed method outperforms buy-and-hold trading strategies in terms of both profitability and risk-adjusted returns.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
High-frequency lead-lag relationships in the Chinese stock index futures market: tick-by-tick dynamics of calendar spreads,"Lead-lag relationships, integral to market dynamics, offer valuable insights into the trading behavior of high-frequency traders (HFTs) and the flow of information at a granular level. This paper investigates the lead-lag relationships between stock index futures contracts of different maturities in the Chinese financial futures market (CFFEX). Using high-frequency (tick-by-tick) data, we analyze how price movements in near-month futures contracts influence those in longer-dated contracts, such as next-month, quarterly, and semi-annual contracts. Our findings reveal a consistent pattern of price discovery, with the near-month contract leading the others by one tick, driven primarily by liquidity. Additionally, we identify a negative feedback effect of the ""lead-lag spread"" on the leading asset, which can predict returns of leading asset. Backtesting results demonstrate the profitability of trading based on the lead-lag spread signal, even after accounting for transaction costs. Altogether, our analysis offers valuable insights to understand and capitalize on the evolving dynamics of futures markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Sentiment-Based Prediction of Alternative Cryptocurrency Price Fluctuations Using Gradient Boosting Tree Model,"In this paper, we analyze Twitter signals as a medium for user sentiment to predict the price fluctuations of a small-cap alternative cryptocurrency called \emph{ZClassic}. We extracted tweets on an hourly basis for a period of 3.5 weeks, classifying each tweet as positive, neutral, or negative. We then compiled these tweets into an hourly sentiment index, creating an unweighted and weighted index, with the latter giving larger weight to retweets. These two indices, alongside the raw summations of positive, negative, and neutral sentiment were juxtaposed to $\sim 400$ data points of hourly pricing data to train an Extreme Gradient Boosting Regression Tree Model. Price predictions produced from this model were compared to historical price data, with the resulting predictions having a 0.81 correlation with the testing data. Our model's predictive data yielded statistical significance at the $p < 0.0001$ level. Our model is the first academic proof of concept that social media platforms such as Twitter can serve as powerful social signals for predicting price movements in the highly speculative alternative cryptocurrency, or ""alt-coin"", market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Phases of MANES: Multi-Asset Non-Equilibrium Skew Model of a Strongly Non-Linear Market with Phase Transitions,"This paper presents an analytically tractable and practically-oriented model of non-linear dynamics of a multi-asset market in the limit of a large number of assets. The asset price dynamics are driven by money flows into the market from external investors, and their price impact. This leads to a model of a market as an ensemble of interacting non-linear oscillators with the Langevin dynamics. In a homogeneous portfolio approximation, the mean field treatment of the resulting Langevin dynamics produces the McKean-Vlasov equation as a dynamic equation for market returns. Due to the strong non-linearity of the McKean-Vlasov equation, the resulting dynamics give rise to ergodicity breaking and first- or second-order phase transitions under variations of model parameters. Using a tractable potential of the Non-Equilibrium Skew (NES) model previously suggested by the author for a single-stock case, the new Multi-Asset NES (MANES) model enables an analytically tractable framework for a multi-asset market. The equilibrium expected market log-return is obtained as a self-consistent mean field of the McKean-Vlasov equation, and derived in closed form in terms of parameters that are inferred from market prices of S&P 500 index options. The model is able to accurately fit the market data for either a benign or distressed market environments, while using only a single volatility parameter.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Quantifying the Blockchain Trilemma: A Comparative Analysis of Algorand, Ethereum 2.0, and Beyond","Blockchain technology is essential for the digital economy and metaverse, supporting applications from decentralized finance to virtual assets. However, its potential is constrained by the ""Blockchain Trilemma,"" which necessitates balancing decentralization, security, and scalability. This study evaluates and compares two leading proof-of-stake (PoS) systems, Algorand and Ethereum 2.0, against these critical metrics. Our research interprets existing indices to measure decentralization, evaluates scalability through transactional data, and assesses security by identifying potential vulnerabilities. Utilizing real-world data, we analyze each platform's strategies in a structured manner to understand their effectiveness in addressing trilemma challenges. The findings highlight each platform's strengths and propose general methodologies for evaluating key blockchain characteristics applicable to other systems. This research advances the understanding of blockchain technologies and their implications for the future digital economy. Data and code are available on GitHub as open source.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy","We present a simple model of a non-equilibrium self-organizing market where asset prices are partially driven by investment decisions of a bounded-rational agent. The agent acts in a stochastic market environment driven by various exogenous ""alpha"" signals, agent's own actions (via market impact), and noise. Unlike traditional agent-based models, our agent aggregates all traders in the market, rather than being a representative agent. Therefore, it can be identified with a bounded-rational component of the market itself, providing a particular implementation of an Invisible Hand market mechanism. In such setting, market dynamics are modeled as a fictitious self-play of such bounded-rational market-agent in its adversarial stochastic environment. As rewards obtained by such self-playing market agent are not observed from market data, we formulate and solve a simple model of such market dynamics based on a neuroscience-inspired Bounded Rational Information Theoretic Inverse Reinforcement Learning (BRIT-IRL). This results in effective asset price dynamics with a non-linear mean reversion - which in our model is generated dynamically, rather than being postulated. We argue that our model can be used in a similar way to the Black-Litterman model. In particular, it represents, in a simple modeling framework, market views of common predictive signals, market impacts and implied optimal dynamic portfolio allocations, and can be used to assess values of private signals. Moreover, it allows one to quantify a ""market-implied"" optimal investment strategy, along with a measure of market rationality. Our approach is numerically light, and can be implemented using standard off-the-shelf software such as TensorFlow.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Market Dynamics. On Supply and Demand Concepts,"The disbalance of Supply and Demand is typically considered as the driving force of the markets. However, the measurement or estimation of Supply and Demand at price different from the execution price is not possible even after the transaction. An approach in which Supply and Demand are always matched, but the rate $I=dv/dt$ (number of units traded per unit time) of their matching varies, is proposed. The state of the system is determined not by a price $p$, but by a probability distribution defined as the square of a wavefunction $(p)$. The equilibrium state $^{[H]}$ is postulated to be the one giving maximal $I$ and obtained from maximizing the matching rate functional $<I^2(p)>/<^2(p)>$, i.e. solving the dynamic equation of the form ""future price tend to the value maximizing the number of shares traded per unit time"". An application of the theory in a quasi--stationary case is demonstrated. This transition from Supply and Demand concept to Liquidity Deficit concept, described by the matching rate $I$, allows to operate only with observable variables, and have a theory applicable to practical problems.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Financial Markets Prediction with Deep Learning,"Financial markets are difficult to predict due to its complex systems dynamics. Although there have been some recent studies that use machine learning techniques for financial markets prediction, they do not offer satisfactory performance on financial returns. We propose a novel one-dimensional convolutional neural networks (CNN) model to predict financial market movement. The customized one-dimensional convolutional layers scan financial trading data through time, while different types of data, such as prices and volume, share parameters (kernels) with each other. Our model automatically extracts features instead of using traditional technical indicators and thus can avoid biases caused by selection of technical indicators and pre-defined coefficients in technical indicators. We evaluate the performance of our prediction model with strictly backtesting on historical trading data of six futures from January 2010 to October 2017. The experiment results show that our CNN model can effectively extract more generalized and informative features than traditional technical indicators, and achieves more robust and profitable financial performance than previous machine learning approaches.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multi-agent Economics and the Emergence of Critical Markets,"The dual crises of the sub-prime mortgage crisis and the global financial crisis has prompted a call for explanations of non-equilibrium market dynamics. Recently a promising approach has been the use of agent based models (ABMs) to simulate aggregate market dynamics. A key aspect of these models is the endogenous emergence of critical transitions between equilibria, i.e. market collapses, caused by multiple equilibria and changing market parameters. Several research themes have developed microeconomic based models that include multiple equilibria: social decision theory (Brock and Durlauf), quantal response models (McKelvey and Palfrey), and strategic complementarities (Goldstein). A gap that needs to be filled in the literature is a unified analysis of the relationship between these models and how aggregate criticality emerges from the individual agent level. This article reviews the agent-based foundations of markets starting with the individual agent perspective of McFadden and the aggregate perspective of catastrophe theory emphasising connections between the different approaches. It is shown that changes in the uncertainty agents have in the value of their interactions with one another, even if these changes are one-sided, plays a central role in systemic market risks such as market instability and the twin crises effect. These interactions can endogenously cause crises that are an emergent phenomena of markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Financial Wind Tunnel: A Retrieval-Augmented Market Simulator,"Market simulator tries to create high-quality synthetic financial data that mimics real-world market dynamics, which is crucial for model development and robust assessment. Despite continuous advancements in simulation methodologies, market fluctuations vary in terms of scale and sources, but existing frameworks often excel in only specific tasks. To address this challenge, we propose Financial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to generate controllable, reasonable, and adaptable market dynamics for model testing. FWT offers a more comprehensive and systematic generative capability across different data frequencies. By leveraging a retrieval method to discover cross-sectional information as the augmented condition, our diffusion-based simulator seamlessly integrates both macro- and micro-level market patterns. Furthermore, our framework allows the simulation to be controlled with wide applicability, including causal generation through ""what-if"" prompts or unprecedented cross-market trend synthesis. Additionally, we develop an automated optimizer for downstream quantitative models, using stress testing of simulated scenarios via FWT to enhance returns while controlling risks. Experimental results demonstrate that our approach enables the generalizable and reliable market simulation, significantly improve the performance and adaptability of downstream models, particularly in highly complex and volatile market conditions. Our code and data sample is available at https://anonymous.4open.science/r/fwt_-E852","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets,"Deep generative models are becoming increasingly used as tools for financial analysis. However, it is unclear how these models will influence financial markets, especially when they infer financial value in a semi-autonomous way. In this work, we explore the interplay between deep generative models and market dynamics. We develop a form of virtual traders that use deep generative models to make buy/sell decisions, which we term neuro-symbolic traders, and expose them to a virtual market. Under our framework, neuro-symbolic traders are agents that use vision-language models to discover a model of the fundamental value of an asset. Agents develop this model as a stochastic differential equation, calibrated to market data using gradient descent. We test our neuro-symbolic traders on both synthetic data and real financial time series, including an equity stock, commodity, and a foreign exchange pair. We then expose several groups of neuro-symbolic traders to a virtual market environment. This market environment allows for feedback between the traders belief of the underlying value to the observed price dynamics. We find that this leads to price suppression compared to the historical data, highlighting a future risk to market stability. Our work is a first step towards quantifying the effect of deep generative agents on markets dynamics and sets out some of the potential risks and benefits of this approach in the future.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Educational Game on Cryptocurrency Investment: Using Microeconomic Decision Making to Understand Macroeconomics Principles,"Gamification is an effective strategy for motivating and engaging users, which is grounded in business, marketing, and management by designing games in nongame contexts. Gamifying education, which consists of the design and study of educational games, is an emerging trend. However, the existing classroom games for understanding macroeconomics have weak connections to the microfoundations of individual decision-making. We design an educational game on cryptocurrency investment for understanding macroeconomic concepts in microeconomic decisions. We contribute to the literature by designing game-based learning that engages students in understanding macroeconomics in incentivized individual investment decisions. Our game can be widely implemented in online, in-person, and hybrid classrooms. We also reflect on strategies for improving the user experience for future educational game implementations.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A particle model for the herding phenomena induced by dynamic market signals,"In this paper, we study the herding phenomena in financial markets arising from the combined effect of (1) non-coordinated collective interactions between the market players and (2) concurrent reactions of market players to dynamic market signals. By interpreting the expected rate of return of an asset and the favorability on that asset as position and velocity in phase space, we construct an agent-based particle model for herding behavior in finance. We then define two types of herding functionals using this model, and show that they satisfy a Gronwall type estimate and a LaSalle type invariance property respectively, leading to the herding behavior of the market players. Various numerical tests are presented to numerically verify these results.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Learning for Dynamic NFT Valuation,"I study the price dynamics of non-fungible tokens (NFTs) and propose a deep learning framework for dynamic valuation of NFTs. I use data from the Ethereum blockchain and OpenSea to train a deep learning model on historical trades, market trends, and traits/rarity features of Bored Ape Yacht Club NFTs. After hyperparameter tuning, the model is able to predict the price of NFTs with high accuracy. I propose an application framework for this model using zero-knowledge machine learning (zkML) and discuss its potential use cases in the context of decentralized finance (DeFi) applications.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Robust Trading in a Generalized Lattice Market,"This paper introduces a novel robust trading paradigm, called \textit{multi-double linear policies}, situated within a \textit{generalized} lattice market. Distinctively, our framework departs from most existing robust trading strategies, which are predominantly limited to single or paired assets and typically embed asset correlation within the trading strategy itself, rather than as an inherent characteristic of the market. Our generalized lattice market model incorporates both serially correlated returns and asset correlation through a conditional probabilistic model. In the nominal case, where the parameters of the model are known, we demonstrate that the proposed policies ensure survivability and probabilistic positivity. We then derive an analytic expression for the worst-case expected gain-loss and prove sufficient conditions that the proposed policies can maintain a \textit{positive expected profits}, even within a seemingly nonprofitable symmetric lattice market. When the parameters are unknown and require estimation, we show that the parameter space of the lattice model forms a convex polyhedron, and we present an efficient estimation method using a constrained least-squares method. These theoretical findings are strengthened by extensive empirical studies using data from the top 30 companies within the S\&P 500 index, substantiating the efficacy of the generalized model and the robustness of the proposed policies in sustaining the positive expected profit and providing downside risk protection.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Stock market's physical properties description based on Stokes law,"We propose in this paper to consider the stock market as a physical system assimilate to a fluid evolving in a macroscopic space subject to a Force that influences its movement over time where this last is arising from the collision between the supply and the demand of Financial agents. In fluid mechanics, this Force also results from the collisions of fluid molecules led by its physical property such as density, viscosity, and surface tension. The purpose of this article is to show that the dynamism of the stock market behavior can be explained qualitatively and quantitatively by considering the supply & demand collision as the result of Financial agents physical properties defined by Stokes Law. The first objective of this article is to show theoretically that fluid mechanics equations can be used to describe stock market physical properties. The second objective based on the knowledge of stock market physical properties is to propose an Econophysics analog of the stock market viscosity and Reynolds number to measure stock market conditions, whether laminar, transitory, or turbulent. The Reynolds Number defined in this way can be applied in research into the study and classification of stock market dynamics phases through for instance the creation of Econophysics analog of Moddy diagram, this last could be seen as a physical way to quantify asset and stock index idiosyncratic risk. The last objective is to present evidence from a computer simulation that the stock market behavior can be a priori, and posteriori explained by physical properties (viscosity & density) quantifiable by fluid mechanics law (Stokes law) and measurable with the stock market Reynolds Number.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A No-Arbitrage Model of Liquidity in Financial Markets involving Brownian Sheets,"We consider a dynamic market model where buyers and sellers submit limit orders. If at a given moment in time, the buyer is unable to complete his entire order due to the shortage of sell orders at the required limit price, the unmatched part of the order is recorded in the order book. Subsequently these buy unmatched orders may be matched with new incoming sell orders. The resulting demand curve constitutes the sole input to our model. The clearing price is then mechanically calculated using the market clearing condition. We use a Brownian sheet to model the demand curve, and provide some theoretical assumptions under which such a model is justified.   Our main result is the proof that if there exists a unique equivalent martingale measure for the clearing price, then under some mild assumptions there is no arbitrage. We use the Ito- Wentzell formula to obtain that result, and also to characterize the dynamics of the demand curve and of the clearing price in the equivalent measure. We find that the volatility of the clearing price is (up to a stochastic factor) inversely proportional to the sum of buy and sell order flow density (evaluated at the clearing price), which confirms the intuition that volatility is inversely proportional to volume. We also demonstrate that our approach is implementable. We use real order book data and simulate option prices under a particularly simple parameterization of our model.   The no-arbitrage conditions we obtain are applicable to a wide class of models, in the same way that the Heath-Jarrow-Morton conditions apply to a wide class of interest rate models.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Smile Modelling in Commodity Markets,We present a stochastic-local volatility model for derivative contracts on commodity futures able to describe forward-curve and smile dynamics with a fast calibration to liquid market quotes. A parsimonious parametrization is introduced to deal with the limited number of options quoted in the market. Cleared commodity markets for futures and options are analyzed to include in the pricing framework specific trading clauses and margining procedures. Numerical examples for calibration and pricing are provided for different commodity products.,"cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Bounded strategic reasoning explains crisis emergence in multi-agent market games,"The efficient market hypothesis (EMH), based on rational expectations and market equilibrium, is the dominant perspective for modelling economic markets. However, the most notable critique of the EMH is the inability to model periods of out-of-equilibrium behaviour in the absence of any significant external news. When such dynamics emerge endogenously, the traditional economic frameworks provide no explanation for such behaviour and the deviation from equilibrium. This work offers an alternate perspective explaining the endogenous emergence of punctuated out-of-equilibrium dynamics based on bounded rational agents. In a concise market entrance game, we show how boundedly rational strategic reasoning can lead to endogenously emerging crises, exhibiting fat tails in ""returns"". We also show how other common stylised facts of economic markets, such as clustered volatility, can be explained due to agent diversity (or lack thereof) and the varying learning updates across the agents. This work explains various stylised facts and crisis emergence in economic markets, in the absence of any external news, based purely on agent interactions and bounded rational reasoning.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamic Mode Decomposition for Financial Trading Strategies,"We demonstrate the application of an algorithmic trading strategy based upon the recently developed dynamic mode decomposition (DMD) on portfolios of financial data. The method is capable of characterizing complex dynamical systems, in this case financial market dynamics, in an equation-free manner by decomposing the state of the system into low-rank terms whose temporal coefficients in time are known. By extracting key temporal coherent structures (portfolios) in its sampling window, it provides a regression to a best fit linear dynamical system, allowing for a predictive assessment of the market dynamics and informing an investment strategy. The data-driven analytics capitalizes on stock market patterns, either real or perceived, to inform buy/sell/hold investment decisions. Critical to the method is an associated learning algorithm that optimizes the sampling and prediction windows of the algorithm by discovering trading hot-spots. The underlying mathematical structure of the algorithms is rooted in methods from nonlinear dynamical systems and shows that the decomposition is an effective mathematical tool for data-driven discovery of market patterns.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Analysis of Spin Financial Market by GARCH Model,"A spin model is used for simulations of financial markets. To determine return volatility in the spin financial market we use the GARCH model often used for volatility estimation in empirical finance. We apply the Bayesian inference performed by the Markov Chain Monte Carlo method to the parameter estimation of the GARCH model. It is found that volatility determined by the GARCH model exhibits ""volatility clustering"" also observed in the real financial markets. Using volatility determined by the GARCH model we examine the mixture-of-distribution hypothesis (MDH) suggested for the asset return dynamics. We find that the returns standardized by volatility are approximately standard normal random variables. Moreover we find that the absolute standardized returns show no significant autocorrelation. These findings are consistent with the view of the MDH for the return dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multiresolution Signal Processing of Financial Market Objects,"Multiresolution analysis has applications across many disciplines in the study of complex systems and their dynamics. Financial markets are among the most complex entities in our environment, yet mainstream quantitative models operate at predetermined scale, rely on linear correlation measures, and struggle to recognize non-linear or causal structures. In this paper, we combine neural networks known to capture non-linear associations with a multiscale decomposition to facilitate a better understanding of financial market data substructures. Quantization keeps our decompositions calibrated to market at every scale. We illustrate our approach in the context of seven use cases.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Stochastic Price Dynamics in Response to Order Flow Imbalance: Evidence from CSI 300 Index Futures,"We conduct modeling of the price dynamics following order flow imbalance in market microstructure and apply the model to the analysis of Chinese CSI 300 Index Futures. There are three findings. The first is that the order flow imbalance is analogous to a shock to the market. Unlike the common practice of using Hawkes processes, we model the impact of order flow imbalance as an Ornstein-Uhlenbeck process with memory and mean-reverting characteristics driven by a jump-type Lvy process. Motivated by the empirically stable correlation between order flow imbalance and contemporaneous price changes, we propose a modified asset price model where the drift term of canonical geometric Brownian motion is replaced by an Ornstein-Uhlenbeck process. We establish stochastic differential equations and derive the logarithmic return process along with its mean and variance processes under initial boundary conditions, and evolution of cost-effectiveness ratio with order flow imbalance as the trading trigger point, termed as the quasi-Sharpe ratio or response ratio. Secondly, our results demonstrate horizon-dependent heterogeneity in how conventional metrics interact with order flow imbalance. This underscores the critical role of forecast horizon selection for strategies. Thirdly, we identify regime-dependent dynamics in the memory and forecasting power of order flow imbalance. This taxonomy provides both a screening protocol for existing indicators and an ex-ante evaluation paradigm for novel metrics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Realized Volatility Analysis in A Spin Model of Financial Markets,We calculate the realized volatility in the spin model of financial markets and examine the returns standardized by the realized volatility. We find that moments of the standardized returns agree with the theoretical values of standard normal variables. This is the first evidence that the return dynamics of the spin financial market is consistent with the view of the mixture-of-distribution hypothesis that also holds in the real financial markets.,"cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Capturing Financial markets to apply Deep Reinforcement Learning,"In this paper we explore the usage of deep reinforcement learning algorithms to automatically generate consistently profitable, robust, uncorrelated trading signals in any general financial market. In order to do this, we present a novel Markov decision process (MDP) model to capture the financial trading markets. We review and propose various modifications to existing approaches and explore different techniques like the usage of technical indicators, to succinctly capture the market dynamics to model the markets. We then go on to use deep reinforcement learning to enable the agent (the algorithm) to learn how to take profitable trades in any market on its own, while suggesting various methodology changes and leveraging the unique representation of the FMDP (financial MDP) to tackle the primary challenges faced in similar works. Through our experimentation results, we go on to show that our model could be easily extended to two very different financial markets and generates a positively robust performance in all conducted experiments.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Blockchain Risk Parity Line: Moving From The Efficient Frontier To The Final Frontier Of Investments,"We engineer blockchain based risk managed portfolios by creating three funds with distinct risk and return profiles: 1) Alpha - high risk portfolio; 2) Beta - mimics the wider market; and 3) Gamma - represents the risk free rate adjusted to beat inflation. Each of the sub-funds (Alpha, Beta and Gamma) provides risk parity because the weight of each asset in the corresponding portfolio is set to be inversely proportional to the risk derived from investing in that asset. This can be equivalently stated as equal risk contributions from each asset towards the overall portfolio risk.   We provide detailed mechanics of combining assets - including mathematical formulations - to obtain better risk managed portfolios. The descriptions are intended to show how a risk parity based efficient frontier portfolio management engine - that caters to different risk appetites of investors by letting each individual investor select their preferred risk-return combination - can be created seamlessly on blockchain.   Any Investor - using decentralized ledger technology - can select their desired level of risk, or return, and allocate their wealth accordingly among the sub funds, which balance one another under different market conditions. This evolution of the risk parity principle - resulting in a mechanism that is geared to do well under all market cycles - brings more robust performance and can be termed as conceptual parity.   We have given several numerical examples that illustrate the various scenarios that arise when combining Alpha, Beta and Gamma to obtain Parity.   The final investment frontier is now possible - a modification to the efficient frontier, thus becoming more than a mere theoretical construct - on blockchain since anyone from anywhere can participate at anytime to obtain wealth appreciation based on their financial goals.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Lead-lag Relationships in Foreign Exchange Markets,"Lead-lag relationships among assets represent a useful tool for analyzing high frequency financial data. However, research on these relationships predominantly focuses on correlation analyses for the dynamics of stock prices, spots and futures on market indexes, whereas foreign exchange data have been less explored. To provide a valuable insight on the nature of the lead-lag relationships in foreign exchange markets here we perform a detailed study for the one-minute log returns on exchange rates through three different approaches: i) lagged correlations, ii) lagged partial correlations and iii) Granger causality. In all studies, we find that even though for most pairs of exchange rates lagged effects are absent, there are many pairs which pass statistical significance tests. Out of the statistically significant relationships, we construct directed networks and investigate the influence of individual exchange rates through the PageRank algorithm. The algorithm, in general, ranks stock market indexes quoted in their respective currencies, as most influential. In contrast to the claims of the efficient market hypothesis, these findings suggest that all market information does not spread instantaneously.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimal Fees for Liquidity Provision in Automated Market Makers,"Passive liquidity providers (LPs) in automated market makers (AMMs) face losses due to adverse selection (LVR), which static trading fees often fail to offset in practice. We study the key determinants of LP profitability in a dynamic reduced-form model where an AMM operates in parallel with a centralized exchange (CEX), traders route their orders optimally to the venue offering the better price, and arbitrageurs exploit price discrepancies. Using large-scale simulations and real market data, we analyze how LP profits vary with market conditions such as volatility and trading volume, and characterize the optimal AMM fee as a function of these conditions. We highlight the mechanisms driving these relationships through extensive comparative statics, and confirm the model's relevance through market data calibration. A key trade-off emerges: fees must be low enough to attract volume, yet high enough to earn sufficient revenues and mitigate arbitrage losses. We find that under normal market conditions, the optimal AMM fee is competitive with the trading cost on the CEX and remarkably stable, whereas in periods of very high volatility, a high fee protects passive LPs from severe losses. These findings suggest that a threshold-type dynamic fee schedule is both robust enough to market conditions and improves LP outcomes.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Quantitative statistical analysis of order-splitting behaviour of individual trading accounts in the Japanese stock market over nine years,"In this research, we focus on the order-splitting behavior. The order splitting is a trading strategy to execute their large potential metaorder into small pieces to reduce transaction cost. This strategic behavior is believed to be important because it is a promising candidate for the microscopic origin of the long-range correlation (LRC) in the persistent order flow. Indeed, in 2005, Lillo, Mike, and Farmer (LMF) introduced a microscopic model of the order-splitting traders to predict the asymptotic behavior of the LRC from the microscopic dynamics, even quantitatively. The plausibility of this scenario has been qualitatively investigated by Toth et al. 2015. However, no solid support has been presented yet on the quantitative prediction by the LMF model in the lack of large microscopic datasets. In this report, we have provided the first quantitative statistical analysis of the order-splitting behavior at the level of each trading account. We analyse a large dataset of the Tokyo stock exchange (TSE) market over nine years, including the account data of traders (called virtual servers). The virtual server is a unit of trading accounts in the TSE market, and we can effectively define the trader IDs by an appropriate preprocessing. We apply a strategy clustering to individual traders to identify the order-splitting traders and the random traders. For most of the stocks, we find that the metaorder length distribution obeys power laws with exponent $$, such that $P(L)\propto L^{--1}$ with the metaorder length $L$. By analysing the sign correlation $C()\propto ^{-}$, we directly confirmed the LMF prediction $\approx -1$. Furthermore, we discuss how to estimate the total number of the splitting traders only from public data via the ACF prefactor formula in the LMF model. Our work provides the first quantitative evidence of the LMF model.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Joint Bidding on Intraday and Frequency Containment Reserve Markets,"As renewable energy integration increases supply variability, battery energy storage systems (BESS) present a viable solution for balancing supply and demand. This paper proposes a novel approach for optimizing battery BESS participation in multiple electricity markets. We develop a joint bidding strategy that combines participation in the primary frequency reserve market with continuous trading in the intraday market, addressing a gap in the extant literature which typically considers these markets in isolation or simplifies the continuous nature of intraday trading. Our approach utilizes a mixed integer linear programming implementation of the rolling intrinsic algorithm for intraday decisions and state of charge recovery, alongside a learned classifier strategy (LCS) that determines optimal capacity allocation between markets. A comprehensive out-of-sample backtest over more than one year of historical German market data validates our approach: The LCS increases overall profits by over 4% compared to the best-performing static strategy and by more than 3% over a naive dynamic benchmark. Crucially, our method closes the gap to a theoretical perfect foresight strategy to just 4%, demonstrating the effectiveness of dynamic, learning-based allocation in a complex, multi-market environment.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
FinML-Chain: A Blockchain-Integrated Dataset for Enhanced Financial Machine Learning,"Machine learning is critical for innovation and efficiency in financial markets, offering predictive models and data-driven decision-making. However, challenges such as missing data, lack of transparency, untimely updates, insecurity, and incompatible data sources limit its effectiveness. Blockchain technology, with its transparency, immutability, and real-time updates, addresses these challenges. We present a framework for integrating high-frequency on-chain data with low-frequency off-chain data, providing a benchmark for addressing novel research questions in economic mechanism design. This framework generates modular, extensible datasets for analyzing economic mechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights and fairness-driven evaluations. Using four machine learning techniques, including linear regression, deep neural networks, XGBoost, and LSTM models, we demonstrate the framework's ability to produce datasets that advance financial research and improve understanding of blockchain-driven systems. Our contributions include: (1) proposing a research scenario for the Transaction Fee Mechanism and demonstrating how the framework addresses previously unexplored questions in economic mechanism design; (2) providing a benchmark for financial machine learning by open-sourcing a sample dataset generated by the framework and the code for the pipeline, enabling continuous dataset expansion; and (3) promoting reproducibility, transparency, and collaboration by fully open-sourcing the framework and its outputs. This initiative supports researchers in extending our work and developing innovative financial machine-learning models, fostering advancements at the intersection of machine learning, blockchain, and economics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Long Run Feedback in the Broker Call Money Market,"I unravel the basic long run dynamics of the broker call money market, which is the pile of cash that funds margin loans to retail clients (read: continuous time Kelly gamblers). Call money is assumed to supply itself perfectly inelastically, and to continuously reinvest all principal and interest. I show that the relative size of the money market (that is, relative to the Kelly bankroll) is a martingale that nonetheless converges in probability to zero. The margin loan interest rate is a submartingale that converges in mean square to the choke price $r_\infty:=-^2/2$, where $$ is the asymptotic compound growth rate of the stock market and $$ is its annual volatility. In this environment, the gambler no longer beats the market asymptotically a.s. by an exponential factor (as he would under perfectly elastic supply). Rather, he beats the market asymptotically with very high probability (think 98%) by a factor (say 1.87, or 87% more final wealth) whose mean cannot exceed what the leverage ratio was at the start of the model (say, $2:1$). Although the ratio of the gambler's wealth to that of an equivalent buy-and-hold investor is a submartingale (always expected to increase), his realized compound growth rate converges in mean square to $$. This happens because the equilibrium leverage ratio converges to $1:1$ in lockstep with the gradual rise of margin loan interest rates.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimal High Frequency Trading with limit and market orders,"We propose a framework for studying optimal market making policies in a limit order book (LOB). The bid-ask spread of the LOB is modelled by a Markov chain with finite values, multiple of the tick size, and subordinated by the Poisson process of the tick-time clock. We consider a small agent who continuously submits limit buy/sell orders and submits market orders at discrete dates. The objective of the market maker is to maximize her expected utility from revenue over a short term horizon by a tradeoff between limit and market orders, while controlling her inventory position. This is formulated as a mixed regime switching regular/ impulse control problem that we characterize in terms of quasi-variational system by dynamic programming methods. In the case of a mean-variance criterion with martingale reference price or when the asset price follows a Levy process and with exponential utility criterion, the dynamic programming system can be reduced to a system of simple equations involving only the inventory and spread variables. Calibration procedures are derived for estimating the transition matrix and intensity parameters for the spread and for Cox processes modelling the execution of limit orders. Several computational tests are performed both on simulated and real data, and illustrate the impact and profit when considering execution priority in limit orders and market orders","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Estimation of market efficiency process within time-varying autoregressive models by extended Kalman filtering approach,"This paper explores a time-varying version of weak-form market efficiency that is a key component of the so-called Adaptive Market Hypothesis (AMH). One of the most common methodologies used for modeling and estimating a degree of market efficiency lies in an analysis of the serial autocorrelation in observed return series. Under the AMH, a time-varying market efficiency level is modeled by time-varying autoregressive (AR) process and traditionally estimated by the Kalman filter (KF). Being a linear estimator, the KF is hardly capable to track the hidden nonlinear dynamics that is an essential feature of the models under investigation. The contribution of this paper is threefold. We first provide a brief overview of time-varying AR models and estimation methods utilized for testing a weak-form market efficiency in econometrics literature. Secondly, we propose novel accurate estimation approach for recovering the hidden process of evolving market efficiency level by the extended Kalman filter (EKF). Thirdly, our empirical study concerns an examination of the Standard and Poor's 500 Composite stock index and the Dow Jones Industrial Average index. Monthly data covers the period from November 1927 to June 2020, which includes the U.S. Great Depression, the 2008-2009 global financial crisis and the first wave of recent COVID-19 recession. The results reveal that the U.S. market was affected during all these periods, but generally remained weak-form efficient since the mid of 1946 as detected by the estimator.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial Market Dynamics,"We propose a highly efficient and accurate methodology for generating synthetic financial market data using a diffusion model approach. The synthetic data produced by our methodology align closely with observed market data in several key aspects: (i) they pass the two-sample Cramer - von Mises test for portfolios of assets, and (ii) Q - Q plots demonstrate consistency across quantiles, including in the tails, between observed and generated market data. Moreover, the covariance matrices derived from a large set of synthetic market data exhibit significantly lower condition numbers compared to the estimated covariance matrices of the observed data. This property makes them suitable for use as regularized versions of the latter. For model training, we develop an efficient and fast algorithm based on numerical integration rather than Monte Carlo simulations. The methodology is tested on a large set of equity data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multi-Asset Spot and Option Market Simulation,"We construct realistic spot and equity option market simulators for a single underlying on the basis of normalizing flows. We address the high-dimensionality of market observed call prices through an arbitrage-free autoencoder that approximates efficient low-dimensional representations of the prices while maintaining no static arbitrage in the reconstructed surface. Given a multi-asset universe, we leverage the conditional invertibility property of normalizing flows and introduce a scalable method to calibrate the joint distribution of a set of independent simulators while preserving the dynamics of each simulator. Empirical results highlight the goodness of the calibrated simulators and their fidelity.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Gold, currencies and market efficiency","Gold and currency markets form a unique pair with specific interactions and dynamics. We focus on the efficiency ranking of gold markets with respect to the currency of purchase. By utilizing the Efficiency Index (EI) based on fractal dimension, approximate entropy and long-term memory on a wide portfolio of 142 gold price series for different currencies, we construct the efficiency ranking based on the extended EI methodology we provide. Rather unexpected results are uncovered as the gold prices in major currencies lay among the least efficient ones whereas very minor currencies are among the most efficient ones. We argue that such counterintuitive results can be partly attributed to a unique period of examination (2011-2014) characteristic by quantitative easing and rather unorthodox monetary policies together with the investigated illegal collusion of major foreign exchange market participants, as well as some other factors discussed in some detail.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Non-Equilibrium Skewness, Market Crises, and Option Pricing: Non-Linear Langevin Model of Markets with Supersymmetry","This paper presents a tractable model of non-linear dynamics of market returns using a Langevin approach. Due to non-linearity of an interaction potential, the model admits regimes of both small and large return fluctuations. Langevin dynamics are mapped onto an equivalent quantum mechanical (QM) system. Borrowing ideas from supersymmetric quantum mechanics (SUSY QM), a parameterized ground state wave function (WF) of this QM system is used as a direct input to the model, which also fixes a non-linear Langevin potential. Using a two-component Gaussian mixture as a ground state WF with an asymmetric double well potential produces a tractable low-parametric model with interpretable parameters, referred to as the NES (Non-Equilibrium Skew) model. Supersymmetry (SUSY) is then used to find time-dependent solutions of the model in an analytically tractable way. Additional approximations give rise to a final practical version of the NES model, where real-measure and risk-neutral return distributions are given by three component Gaussian mixtures. This produces a closed-form approximation for option pricing in the NES model by a mixture of three Black-Scholes prices, providing accurate calibration to option prices for either benign or distressed market environments, while using only a single volatility parameter. These results stand in stark contrast to the most of other option pricing models such as local, stochastic, or rough volatility models that need more complex specifications of noise to fit the market data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Co-Terminal Swap Market Model with Bergomi Stochastic Volatility,"In this article, we apply the forward variance modeling approach by L.Bergomi to the co-terminal swap market model. We build an interest rate model for which all the market price changes of hedging instruments, interest rate swaps and European swaptions, are interpreted as the state variable variations, and no diffusion parameter calibration procedure is required. The model provides quite simple profit and loss (PnL) formula, with which we can easily understand where a material PnL trend comes from when it appears, and consider how we should modify the model parameters. The model has high flexibility to control the model dynamics because parameter calibration is unnecessary and the model parameters can be used solely for the purpose of the model dynamics control. With the model, the position management of the exotic interest rate products, e.g. Bermudan swaptions, can be carried out in a more sophisticated and systematic manner. A numerical experiment is performed to show the effectiveness of the approach for a Canary swaption, which is a special form of a Bermudan swaption.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Data-driven Market Simulator for Small Data Environments,"Neural network based data-driven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete modelling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a generative model that works reliably in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our Market Generator to deep hedging.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Modelling Crypto Asset Price Dynamics, Optimal Crypto Portfolio, and Crypto Option Valuation","Despite being described as a medium of exchange, cryptocurrencies do not have the typical attributes of a medium of exchange. Consequently, cryptocurrencies are more appropriately described as crypto assets. A common investment attribute shared by the more than 2,500 crypto assets is that they are highly volatile. An investor interested in reducing price volatility of a portfolio of crypto assets can do so by constructing an optimal portfolio through standard optimization techniques that minimize tail risk. Because crypto assets are not backed by any real assets, forming a hedge to reduce the risk contribution of a single crypto asset can only be done with another set of similar assets (i.e., a set of other crypto assets). A major finding of this paper is that crypto portfolios constructed via optimizations that minimize variance and Conditional Value at Risk outperform a major stock market index (the S$\&$P 500). As of this writing, options in which the underlying is a crypto asset index are not traded, one of the reasons being that the academic literature has not formulated an acceptable fair pricing model. We offer a fair valuation model for crypto asset options based on a dynamic pricing model for the underlying crypto assets. The model was carefully backtested and therefore offers a reliable model for the underlying crypto assets in the natural world. We then obtain the valuation of crypto options by passing the natural world to the equivalent martingale measure via the Esscher transform. Because of the absence of traded crypto options we could not compare the prices obtained from our valuation model to market prices. Yet, we can claim that if such options on crypto assets are introduced, they should follow closely our theoretical prices after adjusting for market frictions and design feature nuances.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A nonlinear impact: evidences of causal effects of social media on market prices,"Online social networks offer a new way to investigate financial markets' dynamics by enabling the large-scale analysis of investors' collective behavior. We provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship. We take advantage of an extensive data set composed of social media messages related to DJIA index components. By using information-theoretic measures to cope for possible nonlinear causal coupling between social media and stock markets systems, we point out stunning differences in the results with respect to linear coupling. Two main conclusions are drawn: First, social media significant causality on stocks' returns are purely nonlinear in most cases; Second, social media dominates the directional coupling with stock market, an effect not observable within linear modeling. Results also serve as empirical guidance on model adequacy in the investigation of sociotechnical and financial systems.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Describing the effect of influential spreaders on the different sectors of Indian market: a complex networks perspective,"Market competition has a role which is directly or indirectly associated with influential effects of individual sectors on other sectors of the economy. The present work studies the relative position of a product in the market through the identification of influential spreaders and its corresponding effect on the other sectors of the market using complex network analysis during the pre-, in-, and post-crisis induced lockdown periods using daily data of NSE from December, 2019 to June, 2021. The existing approaches using different centrality measures failed to distinguish between the positive and negative influences of the different sectors in the market which act as spreaders. To obviate this problem, this paper presents an effective measure called LIEST (Local Influential Effects for Specific Target) that can examine the positive and negative influences separately with respect to any crisis period. LIEST considers the combined impact of all possible nodes which are at most three steps away from the specific targets for the networks. The essence of non-linearity in the network dynamics without considering single node effect becomes visible particularly in the proposed network.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Generative Ornstein-Uhlenbeck Markets via Geometric Deep Learning,"We consider the problem of simultaneously approximating the conditional distribution of market prices and their log returns with a single machine learning model. We show that an instance of the GDN model of Kratsios and Papon (2022) solves this problem without having prior assumptions on the market's ""clipped"" log returns, other than that they follow a generalized Ornstein-Uhlenbeck process with a priori unknown dynamics. We provide universal approximation guarantees for these conditional distributions and contingent claims with a Lipschitz payoff function.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
FlowHFT: Imitation Learning via Flow Matching Policy for Optimal High-Frequency Trading under Diverse Market Conditions,"High-frequency trading (HFT) is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds. Traditional HFT approaches fit models with historical data and assume that future market states follow similar patterns. This limits the effectiveness of any single model to the specific conditions it was trained for. Additionally, these models achieve optimal solutions only under specific market conditions, such as assumptions about stock price's stochastic process, stable order flow, and the absence of sudden volatility. Real-world markets, however, are dynamic, diverse, and frequently volatile. To address these challenges, we propose the FlowHFT, a novel imitation learning framework based on flow matching policy. FlowHFT simultaneously learns strategies from numerous expert models, each proficient in particular market scenarios. As a result, our framework can adaptively adjust investment decisions according to the prevailing market state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism. This allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal. We test FlowHFT in multiple market environments. We first show that flow matching policy is applicable in stochastic market environments, thus enabling FlowHFT to learn trading strategies under different market conditions. Notably, our single framework consistently achieves performance superior to the best expert for each market condition.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Market Impact Paradoxes,"The market impact (MI) of Volume Weighted Average Price (VWAP) orders is a convex function of a trading rate, but most empirical estimates of transaction cost are concave functions. How is this possible? We show that isochronic (constant trading time) MI is slightly convex, and isochoric (constant trading volume) MI is concave. We suggest a model that fits all trading regimes and guarantees no-dynamic-arbitrage.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Sea of Coins: The Proliferation of Cryptocurrencies in UniswapV2,"Blockchain technology has revolutionized financial markets by enabling decentralized exchanges (DEXs) that operate without intermediaries. Uniswap V2, a leading DEX, facilitates the rapid creation and trading of new tokens, offering high return potential but exposing investors to significant risks. In this work, we analyze the financial impact of newly created tokens, assessing their market dynamics, profitability and liquidity manipulations. Our findings reveal that a significant portion of market liquidity is trapped in honeypots, reducing market efficiency and misleading investors. Applying a simple buy-and-hold strategy, we are able to uncover some major risks associated with investing in newly created tokens, including the widespread presence of rug pulls and sandwich attacks. We extract the optimal sandwich amount, revealing that their proliferation in new tokens stems from higher profitability in low-liquidity pools. Furthermore, we analyze the fundamental differences between token price evolution in swap time and physical time. Using clustering techniques, we highlight these differences and identify typical patterns of honeypot and sellable tokens. Our study provides insights into the risks and financial dynamics of decentralized markets and their challenges for investors.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Binary Tree Option Pricing Under Market Microstructure Effects: A Random Forest Approach,"We propose a machine learning-based extension of the classical binomial option pricing model that incorporates key market microstructure effects. Traditional models assume frictionless markets, overlooking empirical features such as bid-ask spreads, discrete price movements, and serial return correlations. Our framework augments the binomial tree with path-dependent transition probabilities estimated via Random Forest classifiers trained on high-frequency market data. This approach preserves no-arbitrage conditions while embedding real-world trading dynamics into the pricing model.   Using 46,655 minute-level observations of SPY from January to June 2025, we achieve an AUC of 88.25% in forecasting one-step price movements. Order flow imbalance is identified as the most influential predictor, contributing 43.2% to feature importance. After resolving time-scaling inconsistencies in tree construction, our model yields option prices that deviate by 13.79% from Black-Scholes benchmarks, highlighting the impact of microstructure on fair value estimation. While computational limitations restrict the model to short-term derivatives, our results offer a robust, data-driven alternative to classical pricing methods grounded in empirical market behavior.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multi-Agent Dynamic Pricing in a Blockchain Protocol Using Gaussian Bandits,"The Graph Protocol indexes historical blockchain transaction data and makes it available for querying. As the protocol is decentralized, there are many independent Indexers that index and compete with each other for serving queries to the Consumers. One dimension along which Indexers compete is pricing. In this paper, we propose a bandit-based algorithm for maximization of Indexers' revenue via Consumer budget discovery. We present the design and the considerations we had to make for a dynamic pricing algorithm being used by multiple agents simultaneously. We discuss the results achieved by our dynamic pricing bandits both in simulation and deployed into production on one of the Indexers operating on Ethereum. We have open-sourced both the simulation framework and tools we created, which other Indexers have since started to adapt into their own workflows.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Replicating financial market dynamics with a simple self-organized critical lattice model,"We explore a simple lattice field model intended to describe statistical properties of high frequency financial markets. The model is relevant in the cross-disciplinary area of econophysics. Its signature feature is the emergence of a self-organized critical state. This implies scale invariance of the model, without tuning parameters. Prominent results of our simulation are time series of gains, prices, volatility, and gains frequency distributions, which all compare favorably to features of historical market data. Applying a standard GARCH(1,1) fit to the lattice model gives results that are almost indistinguishable from historical NASDAQ data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Market efficiency, informational asymmetry and pseudo-collusion of adaptively learning agents","We examine the dynamics of informational efficiency in a market with asymmetrically informed, boundedly rational traders who adaptively learn optimal strategies using simple multiarmed bandit (MAB) algorithms. The strategies available to the traders have two dimensions: on the one hand, the traders must endogenously choose whether to acquire a costly information signal, on the other, they must determine how aggressively they trade by choosing the share of their wealth to be invested in the risky asset. Our study contributes to two strands of literature: the literature comparing the effects of competitive and strategic behavior on asset price efficiency under costly information as well as the actively growing literature on algorithmic tacit collusion and pseudo-collusion in financial markets. We find that for certain market environments (with low information costs) our model reproduces the results of Kyle [1989] in that the ability of traders to trade strategically leads to worse price efficiency compared to the purely competitive case. For other environments (with high information costs), on the other hand, our results show that a market with strategically acting traders can be more efficient than a purely competitive one. Furthermore, we obtain novel results on the ability of independently learning traders to coordinate on a pseudo-collusive behavior, leading to non-competitive pricing. Contrary to some recent contributions (see e.g. [Cartea et al. 2022]), we find that the pseudo-collusive behavior in our model is robust to a large number of agents, demonstrating that even in the setting of financial markets with a large number of independently learning traders non-competitive pricing and pseudo-collusive behavior can frequently arise.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Financial Market Modeling with Quantum Neural Networks,"Econophysics has developed as a research field that applies the formalism of Statistical Mechanics and Quantum Mechanics to address Economics and Finance problems. The branch of Econophysics that applies of Quantum Theory to Economics and Finance is called Quantum Econophysics. In Finance, Quantum Econophysics' contributions have ranged from option pricing to market dynamics modeling, behavioral finance and applications of Game Theory, integrating the empirical finding, from human decision analysis, that shows that nonlinear update rules in probabilities, leading to non-additive decision weights, can be computationally approached from quantum computation, with resulting quantum interference terms explaining the non-additive probabilities. The current work draws on these results to introduce new tools from Quantum Artificial Intelligence, namely Quantum Artificial Neural Networks as a way to build and simulate financial market models with adaptive selection of trading rules, leading to turbulence and excess kurtosis in the returns distributions for a wide range of parameters.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
BondBERT: What we learn when assigning sentiment in the bond market,"Bond markets respond differently to macroeconomic news compared to equity markets, yet most sentiment models, including FinBERT, are trained primarily on general financial or equity news data. This mismatch is important because bond prices often move in the opposite direction to economic optimism, making general or equity-based sentiment tools potentially misleading. In this paper, we introduce BondBERT, a transformer-based language model fine-tuned on bond-specific news. BondBERT can act as the perception and reasoning component of a financial decision-support agent, providing sentiment signals that integrate with forecasting models. It is a generalisable framework for adapting transformers to low-volatility, domain-inverse sentiment tasks by compiling and cleaning 30,000 UK bond market articles (2018--2025) for training, validation, and testing. We compare BondBERT's sentiment predictions against FinBERT, FinGPT, and Instruct-FinGPT using event-based correlation, up/down accuracy analyses, and LSTM forecasting across ten UK sovereign bonds. We find that BondBERT consistently produces positive correlations with bond returns, achieves higher alignment and forecasting accuracy than the three baseline models, with lower normalised RMSE and higher information coefficient. These results demonstrate that domain-specific sentiment adaptation better captures fixed income dynamics, bridging a gap between NLP advances and bond market analytics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
CLVSA: A Convolutional LSTM Based Variational Sequence-to-Sequence Model with Attention for Predicting Trends of Financial Markets,"Financial markets are a complex dynamical system. The complexity comes from the interaction between a market and its participants, in other words, the integrated outcome of activities of the entire participants determines the markets trend, while the markets trend affects activities of participants. These interwoven interactions make financial markets keep evolving. Inspired by stochastic recurrent models that successfully capture variability observed in natural sequential data such as speech and video, we propose CLVSA, a hybrid model that consists of stochastic recurrent networks, the sequence-to-sequence architecture, the self- and inter-attention mechanism, and convolutional LSTM units to capture variationally underlying features in raw financial trading data. Our model outperforms basic models, such as convolutional neural network, vanilla LSTM network, and sequence-to-sequence model with attention, based on backtesting results of six futures from January 2010 to December 2017. Our experimental results show that, by introducing an approximate posterior, CLVSA takes advantage of an extra regularizer based on the Kullback-Leibler divergence to prevent itself from overfitting traps.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Causal and Predictive Modeling of Short-Horizon Market Risk and Systematic Alpha Generation Using Hybrid Machine Learning Ensembles,"We present a systematic trading framework that forecasts short-horizon market risk, identifies its underlying drivers, and generates alpha using a hybrid machine learning ensemble built to trade on the resulting signal. The framework integrates neural networks with tree-based voting models to predict five-day drawdowns in the S&P 500 ETF, leveraging a cross-asset feature set spanning equities, fixed income, foreign exchange, commodities, and volatility markets. Interpretable feature attribution methods reveal the key macroeconomic and microstructural factors that differentiate high-risk (crash) from benign (non-crash) weekly regimes. Empirical results show a Sharpe ratio of 2.51 and an annualized CAPM alpha of +0.28, with a market beta of 0.51, indicating that the model delivers substantial systematic alpha with limited directional exposure during the 2005--2025 backtest period. Overall, the findings underscore the effectiveness of hybrid ensemble architectures in capturing nonlinear risk dynamics and identifying interpretable, potentially causal drivers, providing a robust blueprint for machine learning-driven alpha generation in systematic trading.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Impact of Multiple Curve Dynamics in Credit Valuation Adjustments under Collateralization,"We present a detailed analysis of interest rate derivatives valuation under credit risk and collateral modeling. We show how the credit and collateral extended valuation framework in Pallavicini et al (2011), and the related collateralized valuation measure, can be helpful in defining the key market rates underlying the multiple interest rate curves that characterize current interest rate markets. A key point is that spot Libor rates are to be treated as market primitives rather than being defined by no-arbitrage relationships. We formulate a consistent realistic dynamics for the different rates emerging from our analysis and compare the resulting model performances to simpler models used in the industry. We include the often neglected margin period of risk, showing how this feature may increase the impact of different rates dynamics on valuation. We point out limitations of multiple curve models with deterministic basis considering valuation of particularly sensitive products such as basis swaps. We stress that a proper wrong way risk analysis for such products requires a model with a stochastic basis and we show numerical results confirming this fact.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Estimating risks of option books using neural-SDE market models,"In this paper, we examine the capacity of an arbitrage-free neural-SDE market model to produce realistic scenarios for the joint dynamics of multiple European options on a single underlying. We subsequently demonstrate its use as a risk simulation engine for option portfolios. Through backtesting analysis, we show that our models are more computationally efficient and accurate for evaluating the Value-at-Risk (VaR) of option portfolios, with better coverage performance and less procyclicality than standard filtered historical simulation approaches.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,"Energy markets exhibit complex causal relationships between weather patterns, generation technologies, and price formation, with regime changes occurring continuously rather than at discrete break points. Current approaches model electricity prices without explicit causal interpretation or counterfactual reasoning capabilities. We introduce Augmented Time Series Causal Models (ATSCM) for energy markets, extending counterfactual reasoning frameworks to multivariate temporal data with learned causal structure. Our approach models energy systems through interpretable factors (weather, generation mix, demand patterns), rich grid dynamics, and observable market variables. We integrate neural causal discovery to learn time-varying causal graphs without requiring ground truth DAGs. Applied to real-world electricity price data, ATSCM enables novel counterfactual queries such as ""What would prices be under different renewable generation scenarios?"".","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts,"Optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the trade-off between market impact costs and timing or volatility risks. Traditional optimal execution strategies, such as static Almgren-Chriss models, often prove suboptimal in dynamic financial markets. This paper propose flowOE, a novel imitation learning framework based on flow matching models, to address these limitations. FlowOE learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions. A key innovation is the incorporation of a refining loss function during the imitation process, enabling flowOE not only to mimic but also to improve upon the learned expert actions. To the best of our knowledge, this work is the first to apply flow matching models in a stochastic optimal execution problem. Empirical evaluations across various market conditions demonstrate that flowOE significantly outperforms both the specifically calibrated expert models and other traditional benchmarks, achieving higher profits with reduced risk. These results underscore the practical applicability and potential of flowOE to enhance adaptive optimal execution.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Reinforcement Learning for Trade Execution with Market Impact,"In this paper, we introduce a novel reinforcement learning framework for optimal trade execution in a limit order book. We formulate the trade execution problem as a dynamic allocation task whose objective is the optimal placement of market and limit orders to maximize expected revenue. By employing multivariate logistic-normal distributions to model random allocations, the framework enables efficient training of the reinforcement learning algorithm. Numerical experiments show that the proposed method outperforms traditional benchmark strategies in simulated limit order book environments featuring noise traders submitting random orders, tactical traders responding to order book imbalances, and a strategic trader seeking to acquire or liquidate an asset position.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Exploring Sectoral Profitability in the Indian Stock Market Using Deep Learning,"This paper explores using a deep learning Long Short-Term Memory (LSTM) model for accurate stock price prediction and its implications for portfolio design. Despite the efficient market hypothesis suggesting that predicting stock prices is impossible, recent research has shown the potential of advanced algorithms and predictive models. The study builds upon existing literature on stock price prediction methods, emphasizing the shift toward machine learning and deep learning approaches. Using historical stock prices of 180 stocks across 18 sectors listed on the NSE, India, the LSTM model predicts future prices. These predictions guide buy/sell decisions for each stock and analyze sector profitability. The study's main contributions are threefold: introducing an optimized LSTM model for robust portfolio design, utilizing LSTM predictions for buy/sell transactions, and insights into sector profitability and volatility. Results demonstrate the efficacy of the LSTM model in accurately predicting stock prices and informing investment decisions. By comparing sector profitability and prediction accuracy, the work provides valuable insights into the dynamics of the current financial markets in India.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Market Directional Information Derived From (Time, Execution Price, Shares Traded) Sequence of Transactions. On The Impact From The Future","An attempt to obtain market directional information from non-stationary solution of the dynamic equation: ""future price tends to the value maximizing the number of shares traded per unit time"" is presented. A remarkable feature of the approach is an automatic time scale selection. It is determined from the state of maximal execution flow calculated on past transactions. Both lagging and advancing prices are calculated.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Hedging: Learning Risk-Neutral Implied Volatility Dynamics,"We present a numerically efficient approach for learning a risk-neutral measure for paths of simulated spot and option prices up to a finite horizon under convex transaction costs and convex trading constraints. This approach can then be used to implement a stochastic implied volatility model in the following two steps: 1. Train a market simulator for option prices, as discussed for example in our recent; 2. Find a risk-neutral density, specifically the minimal entropy martingale measure. The resulting model can be used for risk-neutral pricing, or for Deep Hedging in the case of transaction costs or trading constraints. To motivate the proposed approach, we also show that market dynamics are free from ""statistical arbitrage"" in the absence of transaction costs if and only if they follow a risk-neutral measure. We additionally provide a more general characterization in the presence of convex transaction costs and trading constraints. These results can be seen as an analogue of the fundamental theorem of asset pricing for statistical arbitrage under trading frictions and are of independent interest.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Efficient Multi-Change Point Analysis to decode Economic Crisis Information from the S&P500 Mean Market Correlation,"Identifying macroeconomic events that are responsible for dramatic changes of economy is of particular relevance to understand the overall economic dynamics. We introduce an open-source available efficient Python implementation of a Bayesian multi-trend change point analysis which solves significant memory and computing time limitations to extract crisis information from a correlation metric. Therefore, we focus on the recently investigated S&P500 mean market correlation in a period of roughly 20 years that includes the dot-com bubble, the global financial crisis and the Euro crisis. The analysis is performed two-fold: first, in retrospect on the whole dataset and second, in an on-line adaptive manner in pre-crisis segments. The on-line sensitivity horizon is roughly determined to be 80 up to 100 trading days after a crisis onset. A detailed comparison to global economic events supports the interpretation of the mean market correlation as an informative macroeconomic measure by a rather good agreement of change point distributions and major crisis events. Furthermore, the results hint to the importance of the U.S. housing bubble as trigger of the global financial crisis, provide new evidence for the general reasoning of locally (meta)stable economic states and could work as a comparative impact rating of specific economic events.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Forecasting Financial Market Structure from Network Features using Machine Learning,"We propose a model that forecasts market correlation structure from link- and node-based financial network features using machine learning. For such, market structure is modeled as a dynamic asset network by quantifying time-dependent co-movement of asset price returns across company constituents of major global market indices. We provide empirical evidence using three different network filtering methods to estimate market structure, namely Dynamic Asset Graph (DAG), Dynamic Minimal Spanning Tree (DMST) and Dynamic Threshold Networks (DTN). Experimental results show that the proposed model can forecast market structure with high predictive performance with up to $40\%$ improvement over a time-invariant correlation-based benchmark. Non-pair-wise correlation features showed to be important compared to traditionally used pair-wise correlation measures for all markets studied, particularly in the long-term forecasting of stock market structure. Evidence is provided for stock constituents of the DAX30, EUROSTOXX50, FTSE100, HANGSENG50, NASDAQ100 and NIFTY50 market indices. Findings can be useful to improve portfolio selection and risk management methods, which commonly rely on a backward-looking covariance matrix to estimate portfolio risk.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs,"We introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Financial Market of Environmental Indices,"This paper introduces the concept of a global financial market for environmental indices, addressing sustainability concerns and aiming to attract institutional investors. Risk mitigation measures are implemented to manage inherent risks associated with investments in this new financial market. We monetize the environmental indices using quantitative measures and construct country-specific environmental indices, enabling them to be viewed as dollar-denominated assets. Our primary goal is to encourage the active engagement of institutional investors in portfolio analysis and trading within this emerging financial market. To evaluate and manage investment risks, our approach incorporates financial econometric theory and dynamic asset pricing tools. We provide an econometric analysis that reveals the relationships between environmental and economic indicators in this market. Additionally, we derive financial put options as insurance instruments that can be employed to manage investment risks. Our factor analysis identifies key drivers in the global financial market for environmental indices. To further evaluate the market's performance, we employ pricing options, efficient frontier analysis, and regression analysis. These tools help us assess the efficiency and effectiveness of the market. Overall, our research contributes to the understanding and development of the global financial market for environmental indices.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks,"The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space,"Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Unveiling Nonlinear Dynamics in Catastrophe Bond Pricing: A Machine Learning Perspective,"This paper explores the implications of using machine learning models in the pricing of catastrophe (CAT) bonds. By integrating advanced machine learning techniques, our approach uncovers nonlinear relationships and complex interactions between key risk factors and CAT bond spreads -- dynamics that are often overlooked by traditional linear regression models. Using primary market CAT bond transaction records between January 1999 and March 2021, our findings demonstrate that machine learning models not only enhance the accuracy of CAT bond pricing but also provide a deeper understanding of how various risk factors interact and influence bond prices in a nonlinear way. These findings suggest that investors and issuers can benefit from incorporating machine learning to better capture the intricate interplay between risk factors when pricing CAT bonds. The results also highlight the potential for machine learning models to refine our understanding of asset pricing in markets characterized by complex risk structures.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multimodal Deep Learning for Finance: Integrating and Forecasting International Stock Markets,"In today's increasingly international economy, return and volatility spillover effects across international equity markets are major macroeconomic drivers of stock dynamics. Thus, information regarding foreign markets is one of the most important factors in forecasting domestic stock prices. However, the cross-correlation between domestic and foreign markets is highly complex. Hence, it is extremely difficult to explicitly express this cross-correlation with a dynamical equation. In this study, we develop stock return prediction models that can jointly consider international markets, using multimodal deep learning. Our contributions are three-fold: (1) we visualize the transfer information between South Korea and US stock markets by using scatter plots; (2) we incorporate the information into the stock prediction models with the help of multimodal deep learning; (3) we conclusively demonstrate that the early and intermediate fusion models achieve a significant performance boost in comparison with the late fusion and single modality models. Our study indicates that jointly considering international stock markets can improve the prediction accuracy and deep neural networks are highly effective for such tasks.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Hedging with Market Impact,"Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset's drift (i.e. the magnitude of its expected return).","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Data Science Pipeline for Algorithmic Trading: A Comparative Study of Applications for Finance and Cryptoeconomics,"Recent advances in Artificial Intelligence (AI) have made algorithmic trading play a central role in finance. However, current research and applications are disconnected information islands. We propose a generally applicable pipeline for designing, programming, and evaluating the algorithmic trading of stock and crypto assets. Moreover, we demonstrate how our data science pipeline works with respect to four conventional algorithms: the moving average crossover, volume-weighted average price, sentiment analysis, and statistical arbitrage algorithms. Our study offers a systematic way to program, evaluate, and compare different trading strategies. Furthermore, we implement our algorithms through object-oriented programming in Python3, which serves as open-source software for future academic research and applications.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Blending Ensemble for Classification with Genetic-algorithm generated Alpha factors and Sentiments (GAS),"With the increasing maturity and expansion of the cryptocurrency market, understanding and predicting its price fluctuations has become an important issue in the field of financial engineering. This article introduces an innovative Genetic Algorithm-generated Alpha Sentiment (GAS) blending ensemble model specifically designed to predict Bitcoin market trends. The model integrates advanced ensemble learning methods, feature selection algorithms, and in-depth sentiment analysis to effectively capture the complexity and variability of daily Bitcoin trading data. The GAS framework combines 34 Alpha factors with 8 news economic sentiment factors to provide deep insights into Bitcoin price fluctuations by accurately analyzing market sentiment and technical indicators. The core of this study is using a stacked model (including LightGBM, XGBoost, and Random Forest Classifier) for trend prediction which demonstrates excellent performance in traditional buy-and-hold strategies. In addition, this article also explores the effectiveness of using genetic algorithms to automate alpha factor construction as well as enhancing predictive models through sentiment analysis. Experimental results show that the GAS model performs competitively in daily Bitcoin trend prediction especially when analyzing highly volatile financial assets with rich data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Bribes to Miners: Evidence from Ethereum,"In blockchain, bribery is an inevitable problem since users with various goals can bribe miners by transferring cryptoassets. To alleviate the negative effects of such collusion, Ethereum blockchain implemented new transaction fee mechanism in the London Fork, which was deployed on August 5th, 2021. In this paper, we first filter potential bribery by scanning Ethereum transactions, and the potential bribers and bribees are centralized in a small group. Then we construct bribing proxies to measure the active level of bribery and then investigate the effects of bribery. Consequently, bribery can influence both Ethereum and other mainstream blockchains, in aspects of underlying cryptocurrency, transaction statistics, and network adoption. Moreover, the London Fork shows complicated effects on relationship between bribery and blockchain factors. Besides, bribery in Ethereum relates to stock markets, e.g., S&P 500 and Nasdaq, implying implicit interlinks between blockchain and traditional finance.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Spiking Neural Network for Cross-Market Portfolio Optimization in Financial Markets: A Neuromorphic Computing Approach,"Cross-market portfolio optimization has become increasingly complex with the globalization of financial markets and the growth of high-frequency, multi-dimensional datasets. Traditional artificial neural networks, while effective in certain portfolio management tasks, often incur substantial computational overhead and lack the temporal processing capabilities required for large-scale, multi-market data. This study investigates the application of Spiking Neural Networks (SNNs) for cross-market portfolio optimization, leveraging neuromorphic computing principles to process equity data from both the Indian (Nifty 500) and US (S&P 500) markets. A five-year dataset comprising approximately 1,250 trading days of daily stock prices was systematically collected via the Yahoo Finance API. The proposed framework integrates Leaky Integrate-andFire neuron dynamics with adaptive thresholding, spike-timingdependent plasticity, and lateral inhibition to enable event-driven processing of financial time series. Dimensionality reduction is achieved through hierarchical clustering, while populationbased spike encoding and multiple decoding strategies support robust portfolio construction under realistic trading constraints, including cardinality limits, transaction costs, and adaptive risk aversion. Experimental evaluation demonstrates that the SNN-based framework delivers superior risk-adjusted returns and reduced volatility compared to ANN benchmarks, while substantially improving computational efficiency. These findings highlight the promise of neuromorphic computation for scalable, efficient, and robust portfolio optimization across global financial markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Proactive Market Making and Liquidity Analysis for Everlasting Options in DeFi Ecosystems,"Everlasting options, a relatively new class of perpetual financial derivatives, have emerged to tackle the challenges of rolling contracts and liquidity fragmentation in decentralized finance markets. This paper offers an in-depth analysis of markets for everlasting options, modeled using a dynamic proactive market maker. We examine the behavior of funding fees and transaction costs across varying liquidity conditions. Using simulations and modeling, we demonstrate that liquidity providers can aim to achieve a net positive PnL by employing effective hedging strategies, even in challenging environments characterized by low liquidity and high transaction costs. Additionally, we provide insights into the incentives that drive liquidity providers to support the growth of everlasting option markets and highlight the significant benefits these instruments offer to traders as a reliable and efficient financial tool.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions,"Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The return of (I)DeFiX,"Decentralized Finance (DeFi) is a nascent set of financial services, using tokens, smart contracts, and blockchain technology as financial instruments. We investigate four possible drivers of DeFi returns: exposure to cryptocurrency market, the network effect, the investor's attention, and the valuation ratio. As DeFi tokens are distinct from classical cryptocurrencies, we design a new dedicated market index, denoted DeFiX. First, we show that DeFi tokens returns are driven by the investor's attention on technical terms such as ""decentralized finance"" or ""DeFi"", and are exposed to their own network variables and cryptocurrency market. We construct a valuation ratio for the DeFi market by dividing the Total Value Locked (TVL) by the Market Capitalization (MC). Our findings do not support the TVL/MC predictive power assumption. Overall, our empirical study shows that the impact of the cryptocurrency market on DeFi returns is stronger than any other considered driver and provides superior explanatory power.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Interplay between endogenous and exogenous fluctuations in financial markets,"We address microscopic, agent based, and macroscopic, stochastic, modeling of the financial markets combining it with the exogenous noise. The interplay between the endogenous dynamics of agents and the exogenous noise is the primary mechanism responsible for the observed long-range dependence and statistical properties of high volatility return intervals. By exogenous noise we mean information flow or/and order flow fluctuations. Numerical results based on the proposed model reveal that the exogenous fluctuations have to be considered as indispensable part of comprehensive modeling of the financial markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Price systems for markets with transaction costs and control problems for some finance problems,"In a market with transaction costs, the price of a derivative can be expressed in terms of (preconsistent) price systems (after Kusuoka (1995)). In this paper, we consider a market with binomial model for stock price and discuss how to generate the price systems. From this, the price formula of a derivative can be reformulated as a stochastic control problem. Then the dynamic programming approach can be used to calculate the price. We also discuss optimization of expected utility using price systems.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Long-Range Dependence in Financial Markets: Empirical Evidence and Generative Modeling Challenges,"This study presents a comprehensive empirical investigation of the presence of long-range dependence (LRD) in the dynamics of major U.S. stock market indexes--S\&P 500, Dow Jones, and Nasdaq--at daily, weekly, and monthly frequencies. We employ three distinct methods: the classical rescaled range (R/S) analysis, the more robust detrended fluctuation analysis (DFA), and a sophisticated ARFIMA--FIGARCH model with Student's $t$-distributed innovations. Our results confirm the presence of LRD, primarily driven by long memory in volatility rather than in the mean returns. Building on these findings, we explore the capability of a modern deep learning approach, Quant generative adversarial networks (GANs), to learn and replicate the LRD observed in the empirical data. While Quant GANs effectively capture heavy-tailed distributions and some aspects of volatility clustering, they suffer from significant limitations in reproducing the LRD, particularly at higher frequencies. This work highlights the challenges and opportunities in using data-driven models for generating realistic financial time series that preserve complex temporal dependencies.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Hedging of Green PPAs in Electricity Markets,"In power markets, Green Power Purchase Agreements have become an important contractual tool of the energy transition from fossil fuels to renewable sources such as wind or solar radiation. Trading Green PPAs exposes agents to price risks and weather risks. Also, developed electricity markets feature the so-called cannibalisation effect : large infeeds induce low prices and vice versa. As weather is a non-tradable entity the question arises how to hedge and risk-manage in this highly incom-plete setting. We propose a ''deep hedging'' framework utilising machine learning methods to construct hedging strategies. The resulting strategies outperform static and dynamic benchmark strategies with respect to different risk measures.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
MANA-Net: Mitigating Aggregated Sentiment Homogenization with News Weighting for Enhanced Market Prediction,"It is widely acknowledged that extracting market sentiments from news data benefits market predictions. However, existing methods of using financial sentiments remain simplistic, relying on equal-weight and static aggregation to manage sentiments from multiple news items. This leads to a critical issue termed ``Aggregated Sentiment Homogenization'', which has been explored through our analysis of a large financial news dataset from industry practice. This phenomenon occurs when aggregating numerous sentiments, causing representations to converge towards the mean values of sentiment distributions and thereby smoothing out unique and important information. Consequently, the aggregated sentiment representations lose much predictive value of news data. To address this problem, we introduce the Market Attention-weighted News Aggregation Network (MANA-Net), a novel method that leverages a dynamic market-news attention mechanism to aggregate news sentiments for market prediction. MANA-Net learns the relevance of news sentiments to price changes and assigns varying weights to individual news items. By integrating the news aggregation step into the networks for market prediction, MANA-Net allows for trainable sentiment representations that are optimized directly for prediction. We evaluate MANA-Net using the S&P 500 and NASDAQ 100 indices, along with financial news spanning from 2003 to 2018. Experimental results demonstrate that MANA-Net outperforms various recent market prediction methods, enhancing Profit & Loss by 1.1% and the daily Sharpe ratio by 0.252.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
An Analysis of the Interdependence Between Peanut and Other Agricultural Commodities in China's Futures Market,"This study analyzes historical data from five agricultural commodities in the Chinese futures market to explore the correlation, cointegration, and Granger causality between Peanut futures and related futures. Multivariate linear regression models are constructed for prices and logarithmic returns, while dynamic relationships are examined using VAR and DCC-EGARCH models. The results reveal a significant dynamic linkage between Peanut and Soybean Oil futures through DCC-EGARCH, whereas the VAR model suggests limited influence from other futures. Additionally, the application of MLP, CNN, and LSTM neural networks for price prediction highlights the critical role of time step configurations in forecasting accuracy. These findings provide valuable insights into the interconnectedness of agricultural futures markets and the efficacy of advanced modeling techniques in financial analysis.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Event-Based Limit Order Book Simulation under a Neural Hawkes Process: Application in Market-Making,"In this paper, we propose an event-driven Limit Order Book (LOB) model that captures twelve of the most observed LOB events in exchange-based financial markets. To model these events, we propose using the state-of-the-art Neural Hawkes process, a more robust alternative to traditional Hawkes process models. More specifically, this model captures the dynamic relationships between different event types, particularly their long- and short-term interactions, using a Long Short-Term Memory neural network. Using this framework, we construct a midprice process that captures the event-driven behavior of the LOB by simulating high-frequency dynamics like how they appear in real financial markets. The empirical results show that our model captures many of the broader characteristics of the price fluctuations, particularly in terms of their overall volatility. We apply this LOB simulation model within a Deep Reinforcement Learning Market-Making framework, where the trading agent can now complete trade order fills in a manner that closely resembles real-market trade execution. Here, we also compare the results of the simulated model with those from real data, highlighting how the overall performance and the distribution of trade order fills closely align with the same analysis on real data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimal consumption and investment for markets with random coefficients,"We consider an optimal investment and consumption problem for a Black-Scholes financial market with stochastic coefficients driven by a diffusion process. We assume that an agent makes consumption and investment decisions based on CRRA utility functions. The dynamical programming approach leads to an investigation of the Hamilton Jacobi Bellman (HJB) equation which is a highly non linear partial differential equation (PDE) of the second oder. By using the Feynman - Kac representation we prove uniqueness and smoothness of the solution. Moreover, we study the optimal convergence rate of the iterative numerical schemes for both the value function and the optimal portfolio. We show, that in this case, the optimal convergence rate is super geometrical, i.e. is more rapid than any geometrical one. We apply our results to a stochastic volatility financial market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem,"Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Microstructure Analysis of Coupling in CFMMs,"The programmable and composable nature of smart contract protocols has enabled the emergence of novel market structures and asset classes that are architecturally frictional to implement in traditional financial paradigms. This fluidity has produced an understudied class of market dynamics, particularly in coupled markets where one market serves as an oracle for the other. In such market structures, purchases or liquidations through the intermediate asset create coupled price action between the intermediate and final assets; leading to basket inflation or deflation when denominated in the riskless asset. This paper examines the microstructure of this inflationary dynamic given two constant function market makers (CFMMs) as the intermediate market structures; attempting to quantify their contributions to the former relative to familiar pool metrics such as price drift, trade size, and market depth. Further, a concrete case study is developed, where both markets are constant product markets. The intention is to shed light on the market design process within such coupled environments.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Utility maximization in incomplete markets with default,"We adress the maximization problem of expected utility from terminal wealth. The special feature of this paper is that we consider a financial market where the price process of risky assets can have a default time. Using dynamic programming, we characterize the value function with a backward stochastic differential equation and the optimal portfolio policies. We separately treat the cases of exponential, power and logarithmic utility.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Variance Gamma++ Process and Applications to Energy Markets,"The purpose of this article is to introduce a new Lvy process, termed Variance Gamma++ process, to model the dynamic of assets in illiquid markets. Such a process has the mathematical tractability of the Variance Gamma process and is obtained applying the self-decomposability of the gamma law. Compared to the Variance Gamma model, it has an additional parameter representing the measure of the trading activity. We give a full characterization of the Variance Gamma++ process in terms of its characteristic triplet, characteristic function and transition density. In addition, we provide efficient path simulation algorithms, both forward and backward in time. We also obtain an efficient ""integral-free"" explicit pricing formula for European options. These results are instrumental to apply Fourier-based option pricing and maximum likelihood techniques for the parameter estimation. Finally, we apply our model to illiquid markets, namely to the calibration of European power future market data. We accordingly evaluate exotic derivatives using the Monte Carlo method and compare these values to those obtained using the Variance Gamma process and give an economic interpretation of the obtained results. Finally, we illustrate an extension to the multivariate framework.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamic Factor Allocation Leveraging Regime-Switching Signals,"This article explores dynamic factor allocation by analyzing the cyclical performance of factors through regime analysis. The authors focus on a U.S. equity investment universe comprising seven long-only indices representing the market and six style factors: value, size, momentum, quality, low volatility, and growth. Their approach integrates factor-specific regime inferences of each factor index's active performance relative to the market into the Black-Litterman model to construct a fully-invested, long-only multi-factor portfolio. First, the authors apply the sparse jump model (SJM) to identify bull and bear market regimes for individual factors, using a feature set based on risk and return measures from historical factor active returns, as well as variables reflecting the broader market environment. The regimes identified by the SJM exhibit enhanced stability and interpretability compared to traditional methods. A hypothetical single-factor long-short strategy is then used to assess these regime inferences and fine-tune hyperparameters, resulting in a positive Sharpe ratio of this strategy across all factors with low correlation among them. These regime inferences are then incorporated into the Black-Litterman framework to dynamically adjust allocations among the seven indices, with an equally weighted (EW) portfolio serving as the benchmark. Empirical results show that the constructed multi-factor portfolio significantly improves the information ratio (IR) relative to the market, raising it from just 0.05 for the EW benchmark to approximately 0.4. When measured relative to the EW benchmark itself, the dynamic allocation achieves an IR of around 0.4 to 0.5. The strategy also enhances absolute portfolio performance across key metrics such as the Sharpe ratio and maximum drawdown.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Arbitrage-free Self-organizing Markets with GARCH Properties: Generating them in the Lab with a Lattice Model,"We extend our studies of a quantum field model defined on a lattice having the dilation group as a local gauge symmetry. The model is relevant in the cross-disciplinary area of econophysics. A corresponding proposal by Ilinski aimed at gauge modeling in non-equilibrium pricing is realized as a numerical simulation of the one-asset version. The gauge field background enforces minimal arbitrage, yet allows for statistical fluctuations. The new feature added to the model is an updating prescription for the simulation that drives the model market into a self-organized critical state. Taking advantage of some flexibility of the updating prescription, stylized features and dynamical behaviors of real-world markets are reproduced in some detail.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Reinforcement Learning in Non-Markov Market-Making,"We develop a deep reinforcement learning (RL) framework for an optimal market-making (MM) trading problem, specifically focusing on price processes with semi-Markov and Hawkes Jump-Diffusion dynamics. We begin by discussing the basics of RL and the deep RL framework used, where we deployed the state-of-the-art Soft Actor-Critic (SAC) algorithm for the deep learning part. The SAC algorithm is an off-policy entropy maximization algorithm more suitable for tackling complex, high-dimensional problems with continuous state and action spaces like in optimal market-making (MM). We introduce the optimal MM problem considered, where we detail all the deterministic and stochastic processes that go into setting up an environment for simulating this strategy. Here we also give an in-depth overview of the jump-diffusion pricing dynamics used, our method for dealing with adverse selection within the limit order book, and we highlight the working parts of our optimization problem. Next, we discuss training and testing results, where we give visuals of how important deterministic and stochastic processes such as the bid/ask, trade executions, inventory, and the reward function evolved. We include a discussion on the limitations of these results, which are important points to note for most diffusion models in this setting.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Regime-based Implied Stochastic Volatility Model for Crypto Option Pricing,"The increasing adoption of Digital Assets (DAs), such as Bitcoin (BTC), rises the need for accurate option pricing models. Yet, existing methodologies fail to cope with the volatile nature of the emerging DAs. Many models have been proposed to address the unorthodox market dynamics and frequent disruptions in the microstructure caused by the non-stationarity, and peculiar statistics, in DA markets. However, they are either prone to the curse of dimensionality, as additional complexity is required to employ traditional theories, or they overfit historical patterns that may never repeat. Instead, we leverage recent advances in market regime (MR) clustering with the Implied Stochastic Volatility Model (ISVM). Time-regime clustering is a temporal clustering method, that clusters the historic evolution of a market into different volatility periods accounting for non-stationarity. ISVM can incorporate investor expectations in each of the sentiment-driven periods by using implied volatility (IV) data. In this paper, we applied this integrated time-regime clustering and ISVM method (termed MR-ISVM) to high-frequency data on BTC options at the popular trading platform Deribit. We demonstrate that MR-ISVM contributes to overcome the burden of complex adaption to jumps in higher order characteristics of option pricing models. This allows us to price the market based on the expectations of its participants in an adaptive fashion.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamical analysis of financial stocks network: improving forecasting using network properties,"Applying a network analysis to stock return correlations, we study the dynamical properties of the network and how they correlate with the market return, finding meaningful variables that partially capture the complex dynamical processes of stock interactions and the market structure. We then use the individual properties of stocks within the network along with the global ones, to find correlations with the future returns of individual S&P 500 stocks. Applying these properties as input variables for forecasting, we find a 50% improvement on the R2score in the prediction of stock returns on long time scales (per year), and 3% on short time scales (2 days), relative to baseline models without network variables.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Global Public Sentiment on Decentralized Finance: A Spatiotemporal Analysis of Geo-tagged Tweets from 150 Countries,"Blockchain technology and decentralized finance (DeFi) are reshaping global financial systems. Despite their impact, the spatial distribution of public sentiment and its economic and geopolitical determinants are often overlooked. This study analyzes over 150 million geo-tagged, DeFi-related tweets from 2012 to 2022, sourced from a larger dataset of 7.4 billion tweets. Using sentiment scores from a BERT-based multilingual classification model, we integrated these tweets with economic and geopolitical data to create a multimodal dataset. Employing techniques like sentiment analysis, spatial econometrics, clustering, and topic modeling, we uncovered significant global variations in DeFi engagement and sentiment. Our findings indicate that economic development significantly influences DeFi engagement, particularly after 2015. Geographically weighted regression analysis revealed GDP per capita as a key predictor of DeFi tweet proportions, with its impact growing following major increases in cryptocurrency values such as bitcoin. While wealthier nations are more actively engaged in DeFi discourse, the lowest-income countries often discuss DeFi in terms of financial security and sudden wealth. Conversely, middle-income countries relate DeFi to social and religious themes, whereas high-income countries view it mainly as a speculative instrument or entertainment. This research advances interdisciplinary studies in computational social science and finance and supports open science by making our dataset and code available on GitHub, and providing a non-code workflow on the KNIME platform. These contributions enable a broad range of scholars to explore DeFi adoption and sentiment, aiding policymakers, regulators, and developers in promoting financial inclusion and responsible DeFi engagement globally.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
From Data Acquisition to Lag Modeling: Quantitative Exploration of A-Share Market with Low-Coupling System Design,"We propose a novel two-stage framework to detect lead-lag relationships in the Chinese A-share market. First, long-term coupling between stocks is measured via daily data using correlation, dynamic time warping, and rank-based metrics. Then, high-frequency data (1-, 5-, and 15-minute) is used to detect statistically significant lead-lag patterns via cross-correlation, Granger causality, and regression models. Our low-coupling modular system supports scalable data processing and improves reproducibility. Results show that strongly coupled stock pairs often exhibit lead-lag effects, especially at finer time scales. These findings provide insights into market microstructure and quantitative trading opportunities.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Inverse Reinforcement Learning for Marketing,"Learning customer preferences from an observed behaviour is an important topic in the marketing literature. Structural models typically model forward-looking customers or firms as utility-maximizing agents whose utility is estimated using methods of Stochastic Optimal Control. We suggest an alternative approach to study dynamic consumer demand, based on Inverse Reinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL that leads to a highly tractable model formulation that amounts to low-dimensional convex optimization in the search for optimal model parameters. Using simulations of consumer demand, we show that observational noise for identical customers can be easily confused with an apparent consumer heterogeneity.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Modeling Stock Price Dynamics with Fuzzy Opinion Networks,"We propose a mathematical model for the word-of-mouth communications among stock investors through social networks and explore how the changes of the investors' social networks influence the stock price dynamics and vice versa. An investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the center and standard deviation as inputs and the fuzzy set itself as output. Investors are connected in the following fashion: the center input of an investor is taken as the average of the neighbors' outputs, where two investors are neighbors if their fuzzy opinions are close enough to each other, and the standard deviation (uncertainty) input is taken with local, global or external reference schemes to model different scenarios of how investors define uncertainties. The centers and standard deviations of the fuzzy opinions are the expected prices and their uncertainties, respectively, that are used as inputs to the price dynamic equation. We prove that with the local reference scheme the investors converge to different groups in finite time, while with the global or external reference schemes all investors converge to a consensus within finite time and the consensus may change with time in the external reference case. We show how to model trend followers, contrarians and manipulators within this mathematical framework and prove that the biggest enemy of a manipulator is the other manipulators. We perform Monte Carlo simulations to show how the model parameters influence the price dynamics, and we apply a modified version of the model to the daily closing prices of fifteen top banking and real estate stocks in Hong Kong for the recent two years from Dec. 5, 2013 to Dec. 4, 2015 and discover that a sharp increase of the combined uncertainty is a reliable signal to predict the reversal of the current price trend.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Dynamic Bayesian Model for Interpretable Decompositions of Market Behaviour,"We propose a heterogeneous simultaneous graphical dynamic linear model (H-SGDLM), which extends the standard SGDLM framework to incorporate a heterogeneous autoregressive realised volatility (HAR-RV) model. This novel approach creates a GPU-scalable multivariate volatility estimator, which decomposes multiple time series into economically-meaningful variables to explain the endogenous and exogenous factors driving the underlying variability. This unique decomposition goes beyond the classic one step ahead prediction; indeed, we investigate inferences up to one month into the future using stocks, FX futures and ETF futures, demonstrating its superior performance according to accuracy of large moves, longer-term prediction and consistency over time.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A general Multidimensional Monte Carlo Approach for Dynamic Hedging under stochastic volatility,"In this work, we introduce a Monte Carlo method for the dynamic hedging of general European-type contingent claims in a multidimensional Brownian arbitrage-free market. Based on bounded variation martingale approximations for Galtchouk-Kunita-Watanabe decompositions, we propose a feasible and constructive methodology which allows us to compute pure hedging strategies w.r.t arbitrary square-integrable claims in incomplete markets. In particular, the methodology can be applied to quadratic hedging-type strategies for fully path-dependent options with stochastic volatility and discontinuous payoffs. We illustrate the method with numerical examples based on generalized Follmer-Schweizer decompositions, locally-risk minimizing and mean-variance hedging strategies for vanilla and path-dependent options written on local volatility and stochastic volatility models.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Learning the dynamics of technical trading strategies,"We use an adversarial expert based online learning algorithm to learn the optimal parameters required to maximise wealth trading zero-cost portfolio strategies. The learning algorithm is used to determine the relative population dynamics of technical trading strategies that can survive historical back-testing as well as form an overall aggregated portfolio trading strategy from the set of underlying trading strategies implemented on daily and intraday Johannesburg Stock Exchange data. The resulting population time-series are investigated using unsupervised learning for dimensionality reduction and visualisation. A key contribution is that the overall aggregated trading strategies are tested for statistical arbitrage using a novel hypothesis test proposed by Jarrow et al. (2012) on both daily sampled and intraday time-scales. The (low frequency) daily sampled strategies fail the arbitrage tests after costs, while the (high frequency) intraday sampled strategies are not falsified as statistical arbitrages after costs. The estimates of trading strategy success, cost of trading and slippage are considered along with an online benchmark portfolio algorithm for performance comparison. In addition, the algorithms generalisation error is analysed by recovering a probability of back-test overfitting estimate using a nonparametric procedure introduced by Bailey et al. (2016). The work aims to explore and better understand the interplay between different technical trading strategies from a data-informed perspective.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Neural Stochastic Agent-Based Limit Order Book Simulation: A Hybrid Methodology,"Modern financial exchanges use an electronic limit order book (LOB) to store bid and ask orders for a specific financial asset. As the most fine-grained information depicting the demand and supply of an asset, LOB data is essential in understanding market dynamics. Therefore, realistic LOB simulations offer a valuable methodology for explaining empirical properties of markets. Mainstream simulation models include agent-based models (ABMs) and stochastic models (SMs). However, ABMs tend not to be grounded on real historical data, while SMs tend not to enable dynamic agent-interaction. To overcome these limitations, we propose a novel hybrid LOB simulation paradigm characterised by: (1) representing the aggregation of market events' logic by a neural stochastic background trader that is pre-trained on historical LOB data through a neural point process model; and (2) embedding the background trader in a multi-agent simulation with other trading agents. We instantiate this hybrid NS-ABM model using the ABIDES platform. We first run the background trader in isolation and show that the simulated LOB can recreate a comprehensive list of stylised facts that demonstrate realistic market behaviour. We then introduce a population of `trend' and `value' trading agents, which interact with the background trader. We show that the stylised facts remain and we demonstrate order flow impact and financial herding behaviours that are in accordance with empirical observations of real markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Dynamic ETF Portfolio Optimization Using enhanced Transformer-Based Models for Covariance and Semi-Covariance Prediction(Work in Progress),"This study explores the use of Transformer-based models to predict both covariance and semi-covariance matrices for ETF portfolio optimization. Traditional portfolio optimization techniques often rely on static covariance estimates or impose strict model assumptions, which may fail to capture the dynamic and non-linear nature of market fluctuations. Our approach leverages the power of Transformer models to generate adaptive, real-time predictions of asset covariances, with a focus on the semi-covariance matrix to account for downside risk. The semi-covariance matrix emphasizes negative correlations between assets, offering a more nuanced approach to risk management compared to traditional methods that treat all volatility equally.   Through a series of experiments, we demonstrate that Transformer-based predictions of both covariance and semi-covariance significantly enhance portfolio performance. Our results show that portfolios optimized using the semi-covariance matrix outperform those optimized with the standard covariance matrix, particularly in volatile market conditions. Moreover, the use of the Sortino ratio, a risk-adjusted performance metric that focuses on downside risk, further validates the effectiveness of our approach in managing risk while maximizing returns.   These findings have important implications for asset managers and investors, offering a dynamic, data-driven framework for portfolio construction that adapts more effectively to shifting market conditions. By integrating Transformer-based models with the semi-covariance matrix for improved risk management, this research contributes to the growing field of machine learning in finance and provides valuable insights for optimizing ETF portfolios.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Characteristics of price related fluctuations in Non-Fungible Token (NFT) market,"A non-fungible token (NFT) market is a new trading invention based on the blockchain technology which parallels the cryptocurrency market. In the present work we study capitalization, floor price, the number of transactions, the inter-transaction times, and the transaction volume value of a few selected popular token collections. The results show that the fluctuations of all these quantities are characterized by heavy-tailed probability distribution functions, in most cases well described by the stretched exponentials, with a trace of power-law scaling at times, long-range memory, and in several cases even the fractal organization of fluctuations, mostly restricted to the larger fluctuations, however. We conclude that the NFT market - even though young and governed by a somewhat different mechanisms of trading - shares several statistical properties with the regular financial markets. However, some differences are visible in the specific quantitative indicators.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Calibrating an adaptive Farmer-Joshi agent-based model for financial markets,"We replicate the contested calibration of the Farmer and Joshi agent based model of financial markets using a genetic algorithm and a Nelder-Mead with threshold accepting algorithm following Fabretti. The novelty of the Farmer-Joshi model is that the dynamics are driven by trade entry and exit thresholds alone. We recover the known claim that some important stylized facts observed in financial markets cannot be easily found under calibration -- in particular those relating to the auto-correlations in the absolute values of the price fluctuations, and sufficient kurtosis. However, rather than concerns relating to the calibration method, what is novel here is that we extended the Farmer-Joshi model to include agent adaptation using an Brock and Hommes approach to strategy fitness based on trading strategy profitability. We call this an adaptive Farmer-Joshi model: the model allows trading agents to switch between strategies by favouring strategies that have been more profitable over some period of time determined by a free-parameter fixing the profit monitoring time-horizon. In the adaptive model we are able to calibrate and recover additional stylized facts, despite apparent degeneracy's. This is achieved by combining the interactions of trade entry levels with trade strategy switching. We use this to argue that for low-frequency trading across days, as calibrated to daily sampled data, feed-backs can be accounted for by strategy die-out based on intermediate term profitability; we find that the average trade monitoring horizon is approximately two to three months (or 40 to 60 days) of trading.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
BBE: Simulating the Microstructural Dynamics of an In-Play Betting Exchange via Agent-Based Modelling,"I describe the rationale for, and design of, an agent-based simulation model of a contemporary online sports-betting exchange: such exchanges, closely related to the exchange mechanisms at the heart of major financial markets, have revolutionized the gambling industry in the past 20 years, but gathering sufficiently large quantities of rich and temporally high-resolution data from real exchanges - i.e., the sort of data that is needed in large quantities for Deep Learning - is often very expensive, and sometimes simply impossible; this creates a need for a plausibly realistic synthetic data generator, which is what this simulation now provides. The simulator, named the ""Bristol Betting Exchange"" (BBE), is intended as a common platform, a data-source and experimental test-bed, for researchers studying the application of AI and machine learning (ML) techniques to issues arising in betting exchanges; and, as far as I have been able to determine, BBE is the first of its kind: a free open-source agent-based simulation model consisting not only of a sports-betting exchange, but also a minimal simulation model of racetrack sporting events (e.g., horse-races or car-races) about which bets may be made, and a population of simulated bettors who each form their own private evaluation of odds and place bets on the exchange before and - crucially - during the race itself (i.e., so-called ""in-play"" betting) and whose betting opinions change second-by-second as each race event unfolds. BBE is offered as a proof-of-concept system that enables the generation of large high-resolution data-sets for automated discovery or improvement of profitable strategies for betting on sporting events via the application of AI/ML and advanced data analytics techniques. This paper offers an extensive survey of relevant literature and explains the motivation and design of BBE, and presents brief illustrative results.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On the new central bank strategy toward monetary and financial instabilities management in finances: Econophysical analysis of nonlinear dynamical financial systems,"We describe the innovations in finances, introduced over the recent decades, and analyze most of the business and regulatory challenges, faced by the financial industry, because of the present disruptive changes in the global capital markets. We use the integrative thinking approach to formulate the new central bank strategy and propose that the new strategy has to be focused on the constant management of the monetary and financial instabilities, using the knowledge base in the field of econophysics. We propose the new theoretical model of economics, which is called the Nonlinear Dynamic Stochastic General Equilibrium (NDSGE), which takes to the account the nonlinearities, appearing during the interaction between the business cycles. We show that the central banks, which will apply the knowledge gained from the econophysical analysis to understand the complex processes in the national financial systems in the time of high volatility in global capital markets, will be able to govern the national financial systems successfully.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
QuantNet: Transferring Learning Across Systematic Trading Strategies,"Systematic financial trading strategies account for over 80% of trade volume in equities and a large chunk of the foreign exchange market. In spite of the availability of data from multiple markets, current approaches in trading rely mainly on learning trading strategies per individual market. In this paper, we take a step towards developing fully end-to-end global trading strategies that leverage systematic trends to produce superior market-specific trading strategies. We introduce QuantNet: an architecture that learns market-agnostic trends and use these to learn superior market-specific trading strategies. Each market-specific model is composed of an encoder-decoder pair. The encoder transforms market-specific data into an abstract latent representation that is processed by a global model shared by all markets, while the decoder learns a market-specific trading strategy based on both local and global information from the market-specific encoder and the global model. QuantNet uses recent advances in transfer and meta-learning, where market-specific parameters are free to specialize on the problem at hand, whilst market-agnostic parameters are driven to capture signals from all markets. By integrating over idiosyncratic market data we can learn general transferable dynamics, avoiding the problem of overfitting to produce strategies with superior returns. We evaluate QuantNet on historical data across 3103 assets in 58 global equity markets. Against the top performing baseline, QuantNet yielded 51% higher Sharpe and 69% Calmar ratios. In addition we show the benefits of our approach over the non-transfer learning variant, with improvements of 15% and 41% in Sharpe and Calmar ratios. Code available in appendix.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Value-at-Risk-Based Portfolio Insurance: Performance Evaluation and Benchmarking Against CPPI in a Markov-Modulated Regime-Switching Market,"Designing dynamic portfolio insurance strategies under market conditions switching between two or more regimes is a challenging task in financial economics. Recently, a promising approach employing the value-at-risk (VaR) measure to assign weights to risky and riskless assets has been proposed in [Jiang C., Ma Y. and An Y. ""The effectiveness of the VaR-based portfolio insurance strategy: An empirical analysis"" , International Review of Financial Analysis 18(4) (2009): 185-197]. In their study, the risky asset follows a geometric Brownian motion with constant drift and diffusion coefficients. In this paper, we first extend their idea to a regime-switching framework in which the expected return of the risky asset and its volatility depend on an unobservable Markovian term which describes the cyclical nature of asset returns in modern financial markets. We then analyze and compare the resulting VaR-based portfolio insurance (VBPI) strategy with the well-known constant proportion portfolio insurance (CPPI) strategy. In this respect, we employ a variety of performance evaluation criteria such as Sharpe, Omega and Kappa ratios to compare the two methods. Our results indicate that the CPPI strategy has a better risk-return tradeoff in most of the scenarios analyzed and maintains a relatively stable return profile for the resulting portfolio at the maturity.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fractional calculus and continuous-time finance II: the waiting-time distribution,"We complement the theory of tick-by-tick dynamics of financial markets based on a Continuous-Time Random Walk (CTRW) model recently proposed by Scalas et al., and we point out its consistency with the behaviour observed in the waiting-time distribution for BUND future prices traded at LIFFE, London.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Pricing Energy Derivatives in Markets Driven by Tempered Stable and CGMY Processes of Ornstein-Uhlenbeck Type,"In this study we consider the pricing of energy derivatives when the evolution of spot prices follows a tempered stable or a CGMY driven Ornstein- Uhlenbeck process. To this end, we first calculate the characteristic function of the transition law of such processes in closed form. This result is instrumental for the derivation of non-arbitrage conditions such that the spot dynamics is consistent with the forward curve. Moreover, based on the results of Cufaro Petroni and Sabino (2020), we also conceive efficient algorithms for the exact simulation of the skeleton of such processes and propose a novel procedure when they coincide with compound Poisson processes of Ornstein-Uhlenbeck type. We illustrate the applicability of the theoretical findings and the simulation algorithms in the context of the pricing different contracts namely, strips of daily call options, Asian options with European style and swing options. Finally, we present an extension to future markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On Deep Learning for computing the Dynamic Initial Margin and Margin Value Adjustment,"The present work addresses the challenge of training neural networks for Dynamic Initial Margin (DIM) computation in counterparty credit risk, a task traditionally burdened by the high costs associated with generating training datasets through nested Monte Carlo (MC) simulations. By condensing the initial market state variables into an input vector, determined through an interest rate model and a parsimonious parameterization of the current interest rate term structure, we construct a training dataset where labels are noisy but unbiased DIM samples derived from single MC paths. A multi-output neural network structure is employed to handle DIM as a time-dependent function, facilitating training across a mesh of monitoring times. The methodology offers significant advantages: it reduces the dataset generation cost to a single MC execution and parameterizes the neural network by initial market state variables, obviating the need for repeated training. Experimental results demonstrate the approach's convergence properties and robustness across different interest rate models (Vasicek and Hull-White) and portfolio complexities, validating its general applicability and efficiency in more realistic scenarios.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies,"This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Trustless Price Feeds of Cryptocurrencies: Pathfinder,"Price feeds of securities is a critical component for many financial services, allowing for collateral liquidation, margin trading, derivative pricing and more. With the advent of blockchain technology, value in reporting accurate prices without a third party has become apparent. There have been many attempts at trying to calculate prices without a third party, in which each of these attempts have resulted in being exploited by an exploiter artificially inflating the price. The industry has then shifted to a more centralized design, fetching price data from multiple centralized sources and then applying statistical methods to reach a consensus price. Even though this strategy is secure compared to reading from a single source, enough number of sources need to report to be able to apply statistical methods. As more sources participate in reporting the price, the feed gets more secure with the slowest feed becoming the bottleneck for query response time, introducing a tradeoff between security and speed. This paper provides the design and implementation details of a novel method to algorithmically compute security prices in a way that artificially inflating targeted pools has no effect on the reported price of the queried asset. We hypothesize that the proposed algorithm can report accurate prices given a set of possibly dishonest sources.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load,"This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Quantifying Qualitative Insights: Leveraging LLMs to Market Predict,"Recent advancements in Large Language Models (LLMs) have the potential to transform financial analytics by integrating numerical and textual data. However, challenges such as insufficient context when fusing multimodal information and the difficulty in measuring the utility of qualitative outputs, which LLMs generate as text, have limited their effectiveness in tasks such as financial forecasting. This study addresses these challenges by leveraging daily reports from securities firms to create high-quality contextual information. The reports are segmented into text-based key factors and combined with numerical data, such as price information, to form context sets. By dynamically updating few-shot examples based on the query time, the sets incorporate the latest information, forming a highly relevant set closely aligned with the query point. Additionally, a crafted prompt is designed to assign scores to the key factors, converting qualitative insights into quantitative results. The derived scores undergo a scaling process, transforming them into real-world values that are used for prediction. Our experiments demonstrate that LLMs outperform time-series models in market forecasting, though challenges such as imperfect reproducibility and limited explainability remain.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Forecasting Liquidity Withdraw with Machine Learning Models,"Liquidity withdrawal is a critical indicator of market fragility. In this project, I test a framework for forecasting liquidity withdrawal at the individual-stock level, ranging from less liquid stocks to highly liquid large-cap tickers, and evaluate the relative performance of competing model classes in predicting short-horizon order book stress. We introduce the Liquidity Withdrawal Index (LWI) -- defined as the ratio of order cancellations to the sum of standing depth and new additions at the best quotes -- as a bounded, interpretable measure of transient liquidity removal.   Using Nasdaq market-by-order (MBO) data, we compare a spectrum of approaches: linear benchmarks (AR, HAR), and non-linear tree ensembles (XGBoost), across horizons ranging from 250\,ms to 5\,s. Beyond predictive accuracy, our results provide insights into order placement and cancellation dynamics, identify regimes where linear versus non-linear signals dominate, and highlight how early-warning indicators of liquidity withdrawal can inform both market surveillance and execution.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fast Agent-Based Simulation Framework with Applications to Reinforcement Learning and the Study of Trading Latency Effects,"We introduce a new software toolbox for agent-based simulation. Facilitating rapid prototyping by offering a user-friendly Python API, its core rests on an efficient C++ implementation to support simulation of large-scale multi-agent systems. Our software environment benefits from a versatile message-driven architecture. Originally developed to support research on financial markets, it offers the flexibility to simulate a wide-range of different (easily customisable) market rules and to study the effect of auxiliary factors, such as delays, on the market dynamics. As a simple illustration, we employ our toolbox to investigate the role of the order processing delay in normal trading and for the scenario of a significant price change. Owing to its general architecture, our toolbox can also be employed as a generic multi-agent system simulator. We provide an example of such a non-financial application by simulating a mechanism for the coordination of no-regret learning agents in a multi-agent network routing scenario previously proposed in the literature.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The general mixture-diffusion SDE and its relationship with an uncertain-volatility option model with volatility-asset decorrelation,"In the present paper, given an evolving mixture of probability densities, we define a candidate diffusion process whose marginal law follows the same evolution. We derive as a particular case a stochastic differential equation (SDE) admitting a unique strong solution and whose density evolves as a mixture of Gaussian densities. We present an interesting result on the comparison between the instantaneous and the terminal correlation between the obtained process and its squared diffusion coefficient. As an application to mathematical finance, we construct diffusion processes whose marginal densities are mixtures of lognormal densities. We explain how such processes can be used to model the market smile phenomenon. We show that the lognormal mixture dynamics is the one-dimensional diffusion version of a suitable uncertain volatility model, and suitably reinterpret the earlier correlation result. We explore numerically the relationship between the future smile structures of both the diffusion and the uncertain volatility versions.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On Robustness of Double Linear Trading with Transaction Costs,"A trading system is said to be {robust} if it generates a robust return regardless of market direction. To this end, a consistently positive expected trading gain is often used as a robustness metric for a trading system. In this paper, we propose a new class of trading policies called the {double linear policy} in an asset trading scenario when the transaction costs are involved. Unlike many existing papers, we first show that the desired robust positive expected gain may disappear when transaction costs are involved. Then we quantify under what conditions the desired positivity can still be preserved. In addition, we conduct heavy Monte-Carlo simulations for an underlying asset whose prices are governed by a geometric Brownian motion with jumps to validate our theory. A more realistic backtesting example involving historical data for cryptocurrency Bitcoin-USD is also studied.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Reinforcement Learning Pair Trading: A Dynamic Scaling approach,"Cryptocurrency is a cryptography-based digital asset with extremely volatile prices. Around USD 70 billion worth of cryptocurrency is traded daily on exchanges. Trading cryptocurrency is difficult due to the inherent volatility of the crypto market. This study investigates whether Reinforcement Learning (RL) can enhance decision-making in cryptocurrency algorithmic trading compared to traditional methods. In order to address this question, we combined reinforcement learning with a statistical arbitrage trading technique, pair trading, which exploits the price difference between statistically correlated assets. We constructed RL environments and trained RL agents to determine when and how to trade pairs of cryptocurrencies. We developed new reward shaping and observation/action spaces for reinforcement learning. We performed experiments with the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data separated by 1 min intervals (n=263,520). The traditional non-RL pair trading technique achieved an annualized profit of 8.33%, while the proposed RL-based pair trading technique achieved annualized profits from 9.94% to 31.53%, depending upon the RL learner. Our results show that RL can significantly outperform manual and traditional pair trading techniques when applied to volatile markets such as~cryptocurrencies.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Learning for Options Trading: An End-To-End Approach,"We introduce a novel approach to options trading strategies using a highly scalable and data-driven machine learning algorithm. In contrast to traditional approaches that often require specifications of underlying market dynamics or assumptions on an option pricing model, our models depart fundamentally from the need for these prerequisites, directly learning non-trivial mappings from market data to optimal trading signals. Backtesting on more than a decade of option contracts for equities listed on the S&P 100, we demonstrate that deep learning models trained according to our end-to-end approach exhibit significant improvements in risk-adjusted performance over existing rules-based trading strategies. We find that incorporating turnover regularization into the models leads to further performance enhancements at prohibitively high levels of transaction costs.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets,"Dynamic knowledge graphs (DKGs) are popular structures to express different types of connections between objects over time. They can also serve as an efficient mathematical tool to represent information extracted from complex unstructured data sources, such as text or images. Within financial applications, DKGs could be used to detect trends for strategic thematic investing, based on information obtained from financial news articles. In this work, we explore the properties of large language models (LLMs) as dynamic knowledge graph generators, proposing a novel open-source fine-tuned LLM for this purpose, called the Integrated Contextual Knowledge Graph Generator (ICKG). We use ICKG to produce a novel open-source DKG from a corpus of financial news articles, called FinDKG, and we propose an attention-based GNN architecture for analysing it, called KGTransformer. We test the performance of the proposed model on benchmark datasets and FinDKG, demonstrating superior performance on link prediction tasks. Additionally, we evaluate the performance of the KGTransformer on FinDKG for thematic investing, showing it can outperform existing thematic ETFs.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Equilibrium Pricing in an Order Book Environment: Case Study for a Spin Model,"When modelling stock market dynamics, the price formation is often based on an equilbrium mechanism. In real stock exchanges, however, the price formation is goverend by the order book. It is thus interesting to check if the resulting stylized facts of a model with equilibrium pricing change, remain the same or, more generally, are compatible with the order book environment. We tackle this issue in the framework of a case study by embedding the Bornholdt-Kaizoji-Fujiwara spin model into the order book dynamics. To this end, we use a recently developed agent based model that realistically incorporates the order book. We find realistic stylized facts. We conclude for the studied case that equilibrium pricing is not needed and that the corresponding assumption of a ""fundamental"" price may be abandoned.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Anomalous diffusion and price impact in the fluid-limit of an order book,"We extend a Discrete Time Random Walk (DTRW) numerical scheme to simulate the anomalous diffusion of financial market orders in a simulated order book. Here using random walks with Sibuya waiting times to include a time-dependent stochastic forcing function with non-uniformly sampled times between order book events in the setting of fractional diffusion. This models the fluid limit of an order book by modelling the continuous arrival, cancellation and diffusion of orders in the presence of information shocks. We study the impulse response and stylised facts of orders undergoing anomalous diffusion for different forcing functions and model parameters. Concretely, we demonstrate the price impact for flash limit-orders and market orders and show how the numerical method generate kinks in the price impact. We use cubic spline interpolation to generate smoothed price impact curves. The work promotes the use of non-uniform sampling in the presence of diffusive dynamics as the preferred simulation method.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Pricing options on illiquid assets with liquid proxies using utility indifference and dynamic-static hedging,"This work addresses the problem of optimal pricing and hedging of a European option on an illiquid asset Z using two proxies: a liquid asset S and a liquid European option on another liquid asset Y. We assume that the S-hedge is dynamic while the Y-hedge is static. Using the indifference pricing approach we derive a HJB equation for the value function, and solve it analytically (in quadratures) using an asymptotic expansion around the limit of the perfect correlation between assets Y and Z. While in this paper we apply our framework to an incomplete market version of the credit-equity Merton's model, the same approach can be used for other asset classes (equity, commodity, FX, etc.), e.g. for pricing and hedging options with illiquid strikes or illiquid exotic options.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Liquidity Risks in Lending Protocols: Evidence from Aave Protocol,"Lending Protocols (LPs), as blockchain-based lending systems, allow any agents to borrow and lend cryptocurrencies. However, liquidity risks could occur, especially when salient loans are initiated by a particular group of borrowers. This paper proposes measurements of liquidity risks, focusing on both available liquidity and market concentration in LPs. By using Aave as a case study, we find that liquidity risks are highly volatile and show complex effects on Aave, and liquidity in Aave may affect across on-chain lending market. Compared to new users, regular users that repeatedly borrow cryptocurrencies may negatively affect Aave protocol, implying that user loyalty is a double-edged sword for LPs.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
LEMs: A Primer On Large Execution Models,"This paper introduces Large Execution Models (LEMs), a novel deep learning framework that extends transformer-based architectures to address complex execution problems with flexible time boundaries and multiple execution constraints. Building upon recent advances in neural VWAP execution strategies, LEMs generalize the approach from fixed-duration orders to scenarios where execution duration is bounded between minimum and maximum time horizons, similar to share buyback contract structures. The proposed architecture decouples market information processing from execution allocation decisions: a common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks (TKANs), Variable Selection Networks (VSNs), and multi-head attention mechanisms processes market data to create informational context, while independent allocation networks handle the specific execution logic for different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders). This architectural separation enables a unified model to handle diverse execution objectives while leveraging shared market understanding across scenarios. Through comprehensive empirical evaluation on intraday cryptocurrency markets and multi-day equity trading using DOW Jones constituents, we demonstrate that LEMs achieve superior execution performance compared to traditional benchmarks by dynamically optimizing execution paths within flexible time constraints. The unified model architecture enables deployment across different execution scenarios (buy/sell orders, varying duration boundaries, volume/notional targets) through a single framework, providing significant operational advantages over asset-specific approaches.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fill Probabilities in a Limit Order Book with State-Dependent Stochastic Order Flows,"This paper focuses on computing the fill probabilities for limit orders positioned at various price levels within the limit order book, which play a crucial role in optimizing executions. We adopt a generic stochastic model to capture the dynamics of the order book as a series of queueing systems. This generic model is state-dependent and also incorporates stylized factors. We subsequently derive semi-analytical expressions to compute the relevant probabilities within the context of state-dependent stochastic order flows. These probabilities cover various scenarios, including the probability of a change in the mid-price, the fill probabilities of orders posted at the best quotes, and those posted at a price level deeper than the best quotes in the book, before the opposite best quote moves. These expressions can be further generalized to accommodate orders posted even deeper in the order book, although the associated probabilities are typically very small in such cases. Lastly, we conduct extensive numerical experiments using real order book data from the foreign exchange spot market. Our findings suggest that the model is tractable and possesses the capability to effectively capture the dynamics of the limit order book. Moreover, the derived formulas and numerical methods demonstrate reasonably good accuracy in estimating the fill probabilities.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Effect of Data Types' on the Performance of Machine Learning Algorithms for Financial Prediction,"Forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits. A small improvement in forecasting performance can lead to increased profitability; therefore, obtaining a realistic forecast is very important for investors. Successful forecasting provides traders with effective buy-or-hold strategies, allowing them to make more profits. The most important thing in this process is to produce accurate forecasts suitable for real-life applications. Bitcoin, frequently mentioned recently due to its volatility and chaotic behavior, has begun to pay great attention and has become an investment tool, especially during and after the COVID-19 pandemic. This study provided a comprehensive methodology, including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning (ML) algorithms to forecast Bitcoin price movement. A binarization procedure was applied using continuous data to construct the trend data representing each input feature trend. Following the related literature, the input features are determined as technical indicators, google trends, and the number of tweets. Random forest (RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB), Support vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks (ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected features for prediction purposes. This work investigates two main research questions: i. How does the sample size affect the prediction performance of ML algorithms? ii. How does the data type affect the prediction performance of ML algorithms? Accuracy and area under the ROC curve (AUC) values were used to compare the model performance. A t-test was performed to test the statistical significance of the prediction results.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors,"The complexity of financial data, characterized by its variability and low signal-to-noise ratio, necessitates advanced methods in quantitative investment that prioritize both performance and interpretability.Transitioning from early manual extraction to genetic programming, the most advanced approach in the alpha factor mining domain currently employs reinforcement learning to mine a set of combination factors with fixed weights. However, the performance of resultant alpha factors exhibits inconsistency, and the inflexibility of fixed factor weights proves insufficient in adapting to the dynamic nature of financial markets. To address this issue, this paper proposes a two-stage formulaic alpha generating framework AlphaForge, for alpha factor mining and factor combination. This framework employs a generative-predictive neural network to generate factors, leveraging the robust spatial exploration capabilities inherent in deep learning while concurrently preserving diversity. The combination model within the framework incorporates the temporal performance of factors for selection and dynamically adjusts the weights assigned to each component alpha factor. Experiments conducted on real-world datasets demonstrate that our proposed model outperforms contemporary benchmarks in formulaic alpha factor mining. Furthermore, our model exhibits a notable enhancement in portfolio returns within the realm of quantitative investment and real money investment.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Deep Learning Approach for Dynamic Balance Sheet Stress Testing,"In the aftermath of the financial crisis, supervisory authorities have considerably altered the mode of operation of financial stress testing. Despite these efforts, significant concerns and extensive criticism have been raised by market participants regarding the considered unrealistic methodological assumptions and simplifications. Current stress testing methodologies attempt to simulate the risks underlying a financial institution's balance sheet by using several satellite models. This renders their integration a really challenging task, leading to significant estimation errors. Moreover, advanced statistical techniques that could potentially capture the non-linear nature of adverse shocks are still ignored. This work aims to address these criticisms and shortcomings by proposing a novel approach based on recent advances in Deep Learning towards a principled method for Dynamic Balance Sheet Stress Testing. Experimental results on a newly collected financial/supervisory dataset, provide strong empirical evidence that our paradigm significantly outperforms traditional approaches; thus, it is capable of more accurately and efficiently simulating real world scenarios.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
CoinTossX: An open-source low-latency high-throughput matching engine,"We deploy and demonstrate the CoinTossX low-latency, high-throughput, open-source matching engine with orders sent using the Julia and Python languages. We show how this can be deployed for small-scale local desk-top testing and discuss a larger scale, but local hosting, with multiple traded instruments managed concurrently and managed by multiple clients. We then demonstrate a cloud based deployment using Microsoft Azure, with large-scale industrial and simulation research use cases in mind. The system is exposed and interacted with via sockets using UDP SBE message protocols and can be monitored using a simple web browser interface using HTTP. We give examples showing how orders can be be sent to the system and market data feeds monitored using the Julia and Python languages. The system is developed in Java with orders submitted as binary encodings (SBE) via UDP protocols using the Aeron Media Driver as the low-latency, high throughput message transport. The system separates the order-generation and simulation environments e.g. agent-based model simulation, from the matching of orders, data-feeds and various modularised components of the order-book system. This ensures a more natural and realistic asynchronicity between events generating orders, and the events associated with order-book dynamics and market data-feeds. We promote the use of Julia as the preferred order submission and simulation environment.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Arbitrage-free neural-SDE market models,"Modelling joint dynamics of liquid vanilla options is crucial for arbitrage-free pricing of illiquid derivatives and managing risks of option trade books. This paper develops a nonparametric model for the European options book respecting underlying financial constraints and while being practically implementable. We derive a state space for prices which are free from static (or model-independent) arbitrage and study the inference problem where a model is learnt from discrete time series data of stock and option prices. We use neural networks as function approximators for the drift and diffusion of the modelled SDE system, and impose constraints on the neural nets such that no-arbitrage conditions are preserved. In particular, we give methods to calibrate \textit{neural SDE} models which are guaranteed to satisfy a set of linear inequalities. We validate our approach with numerical experiments using data generated from a Heston stochastic local volatility model.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Hybrid Vector Auto Regression and Neural Network Model for Order Flow Imbalance Prediction in High Frequency Trading,"In high frequency trading, accurate prediction of Order Flow Imbalance (OFI) is crucial for understanding market dynamics and maintaining liquidity. This paper introduces a hybrid predictive model that combines Vector Auto Regression (VAR) with a simple feedforward neural network (FNN) to forecast OFI and assess trading intensity. The VAR component captures linear dependencies, while residuals are fed into the FNN to model non-linear patterns, enabling a comprehensive approach to OFI prediction. Additionally, the model calculates the intensity on the Buy or Sell side, providing insights into which side holds greater trading pressure. These insights facilitate the development of trading strategies by identifying periods of high buy or sell intensity. Using both synthetic and real trading data from Binance, we demonstrate that the hybrid model offers significant improvements in predictive accuracy and enhances strategic decision-making based on OFI dynamics. Furthermore, we compare the hybrid models performance with standalone FNN and VAR models, showing that the hybrid approach achieves superior forecasting accuracy across both synthetic and real datasets, making it the most effective model for OFI prediction in high frequency trading.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Integrating the implied regularity into implied volatility models: A study on free arbitrage model,"Implied volatility IV is a key metric in financial markets, reflecting market expectations of future price fluctuations. Research has explored IV's relationship with moneyness, focusing on its connection to the implied Hurst exponent H. Our study reveals that H approaches 1/2 when moneyness equals 1, marking a critical point in market efficiency expectations. We developed an IV model that integrates H to capture these dynamics more effectively. This model considers the interaction between H and the underlying-to-strike price ratio S/K, crucial for capturing IV variations based on moneyness. Using Optuna optimization across multiple indexes, the model outperformed SABR and fSABR in accuracy. This approach provides a more detailed representation of market expectations and IV-H dynamics, improving options pricing and volatility forecasting while enhancing theoretical and pratcical financial analysis.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Pricing Illiquid Options with $N+1$ Liquid Proxies Using Mixed Dynamic-Static Hedging,"We study the problem of optimal pricing and hedging of a European option written on an illiquid asset $Z$ using a set of proxies: a liquid asset $S$, and $N$ liquid European options $P_i$, each written on a liquid asset $Y_i, i=1,N$. We assume that the $S$-hedge is dynamic while the multi-name $Y$-hedge is static. Using the indifference pricing approach with an exponential utility, we derive a HJB equation for the value function, and build an efficient numerical algorithm. The latter is based on several changes of variables, a splitting scheme, and a set of Fast Gauss Transforms (FGT), which turns out to be more efficient in terms of complexity and lower local space error than a finite-difference method. While in this paper we apply our framework to an incomplete market version of the credit-equity Merton's model, the same approach can be used for other asset classes (equity, commodity, FX, etc.), e.g. for pricing and hedging options with illiquid strikes or illiquid exotic options.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Stochastic relaxational dynamics applied to finance: towards non-equilibrium option pricing theory,"Non-equilibrium phenomena occur not only in physical world, but also in finance. In this work, stochastic relaxational dynamics (together with path integrals) is applied to option pricing theory. A recently proposed model (by Ilinski et al.) considers fluctuations around this equilibrium state by introducing a relaxational dynamics with random noise for intermediate deviations called ``virtual'' arbitrage returns. In this work, the model is incorporated within a martingale pricing method for derivatives on securities (e.g. stocks) in incomplete markets using a mapping to option pricing theory with stochastic interest rates. Using a famous result by Merton and with some help from the path integral method, exact pricing formulas for European call and put options under the influence of virtual arbitrage returns (or intermediate deviations from economic equilibrium) are derived where only the final integration over initial arbitrage returns needs to be performed numerically. This result is complemented by a discussion of the hedging strategy associated to a derivative, which replicates the final payoff but turns out to be not self-financing in the real world, but self-financing {\it when summed over the derivative's remaining life time}. Numerical examples are given which underline the fact that an additional positive risk premium (with respect to the Black-Scholes values) is found reflecting extra hedging costs due to intermediate deviations from economic equilibrium.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Quantifying horizon dependence of asset prices: a cluster entropy approach,"Market dynamic is quantified in terms of the entropy $S(,n)$ of the clusters formed by the intersections between the series of the prices $p_t$ and the moving average $\widetilde{p}_{t,n}$. The entropy $S(,n)$ is defined according to Shannon as $\sum P(,n)\log P(,n),$ with $P(,n)$ the probability for the cluster to occur with duration $$. \par The investigation is performed on high-frequency data of the Nasdaq Composite, Dow Jones Industrial Avg and Standard \& Poor 500 indexes downloaded from the Bloomberg terminal. The cluster entropy $S(,n)$ is analysed in raw and sampled data over a broad range of temporal horizons $M$ varying from one to twelve months over the year 2018. The cluster entropy $S(,n)$ is integrated over the cluster duration $$ to yield the Market Dynamic Index $I(M,n)$, a synthetic figure of price dynamics. A systematic dependence of the cluster entropy $S(,n)$ and the Market Dynamic Index $I(M,n)$ on the temporal horizon $M$ is evidenced. \par Finally, the Market Horizon Dependence}, defined as $H(M,n)=I(M,n)-I(1,n)$, is compared with the horizon dependence of the pricing kernel with different representative agents obtained via a Kullback-Leibler entropy approach. The Market Horizon Dependence $H(M,n)$ of the three assets is compared against the values obtained by implementing the cluster entropy $S(,n)$ approach on artificially generated series (Fractional Brownian Motion).","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Defaultable bonds with an infinite number of Levy factors,A market with defaultable bonds where the bond dynamics is in a Heath-Jarrow-Morton setting and the forward rates are driven by an infinite number of Levy factors is considered. The setting includes rating migrations driven by a Markov chain. All basic types of recovery are investigated. We formulate necessary and sufficient conditions (generalized HJM conditions) under which the market is arbitrage free. Connections with consistency conditions are discussed.,"cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep reinforcement learning for optimal trading with partial information,"Reinforcement Learning (RL) applied to financial problems has been the subject of a lively area of research. The use of RL for optimal trading strategies that exploit latent information in the market is, to the best of our knowledge, not widely tackled. In this paper we study an optimal trading problem, where a trading signal follows an Ornstein-Uhlenbeck process with regime-switching dynamics. We employ a blend of RL and Recurrent Neural Networks (RNN) in order to make the most at extracting underlying information from the trading signal with latent parameters.   The latent parameters driving mean reversion, speed, and volatility are filtered from observations of the signal, and trading strategies are derived via RL. To address this problem, we propose three Deep Deterministic Policy Gradient (DDPG)-based algorithms that integrate Gated Recurrent Unit (GRU) networks to capture temporal dependencies in the signal. The first, a one -step approach (hid-DDPG), directly encodes hidden states from the GRU into the RL trader. The second and third are two-step methods: one (prob-DDPG) makes use of posterior regime probability estimates, while the other (reg-DDPG) relies on forecasts of the next signal value. Through extensive simulations with increasingly complex Markovian regime dynamics for the trading signal's parameters, as well as an empirical application to equity pair trading, we find that prob-DDPG achieves superior cumulative rewards and exhibits more interpretable strategies. By contrast, reg-DDPG provides limited benefits, while hid-DDPG offers intermediate performance with less interpretable strategies. Our results show that the quality and structure of the information supplied to the agent are crucial: embedding probabilistic insights into latent regimes substantially improves both profitability and robustness of reinforcement learning-based trading strategies.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices,"This paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils. We introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies (AIS) built based on the prices of these assets. We employ four types of diverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH - Autoregressive Integrated Moving Average - Generalized Autoregressive Conditional Heteroskedasticity, momentum, and contrarian) to generate price forecasts, which are then used to produce investment signals in single and complex AIS. In such a way, we are able to verify the diversification potential of different types of investment strategies consisting of various assets (energy commodities, precious metals, cryptocurrencies, or soft commodities) in hedging ensemble AIS built for equity indices (S&P 500 index). Empirical data used in this study cover the period between 2004 and 2022. Our main conclusion is that LSTM-based strategies outperform the other models and that the best diversifier for the AIS built for the S&P 500 index is the AIS built for Bitcoin. Finally, we test the LSTM model for a higher frequency of data (1 hour). We conclude that it outperforms the results obtained using daily data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimum Liquidation Problem Associated with the Poisson Cluster Process,"In this research, we develop a trading strategy for the discrete-time optimal liquidation problem of large order trading with different market microstructures in an illiquid market. In this framework, the flow of orders can be viewed as a point process with stochastic intensity. We model the price impact as a linear function of a self-exciting dynamic process. We formulate the liquidation problem as a discrete-time Markov Decision Processes, where the state process is a Piecewise Deterministic Markov Process (PDMP). The numerical results indicate that an optimal trading strategy is dependent on characteristics of the market microstructure. When no orders above certain value come the optimal solution takes offers in the lower levels of the limit order book in order to prevent not filling of orders and facing final inventory costs.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models,"In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and Improved GRU,"As financial markets grow increasingly complex in the big data era, accurate stock prediction has become more critical. Traditional time series models, such as GRUs, have been widely used but often struggle to capture the intricate nonlinear dynamics of markets, particularly in the flexible selection and effective utilization of key historical information. Recently, methods like Graph Neural Networks and Reinforcement Learning have shown promise in stock prediction but require high data quality and quantity, and they tend to exhibit instability when dealing with data sparsity and noise. Moreover, the training and inference processes for these models are typically complex and computationally expensive, limiting their broad deployment in practical applications. Existing approaches also generally struggle to capture unobservable latent market states effectively, such as market sentiment and expectations, microstructural factors, and participant behavior patterns, leading to an inadequate understanding of market dynamics and subsequently impact prediction accuracy. To address these challenges, this paper proposes a stock prediction model, MCI-GRU, based on a multi-head cross-attention mechanism and an improved GRU. First, we enhance the GRU model by replacing the reset gate with an attention mechanism, thereby increasing the model's flexibility in selecting and utilizing historical information. Second, we design a multi-head cross-attention mechanism for learning unobservable latent market state representations, which are further enriched through interactions with both temporal features and cross-sectional features. Finally, extensive experiments on four main stock markets show that the proposed method outperforms SOTA techniques across multiple metrics. Additionally, its successful application in real-world fund management operations confirms its effectiveness and practicality.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
American option pricing using generalised stochastic hybrid systems,"This paper presents a novel approach to pricing American options using piecewise diffusion Markov processes (PDifMPs), a type of generalised stochastic hybrid system that integrates continuous dynamics with discrete jump processes. Standard models often rely on constant drift and volatility assumptions, which limits their ability to accurately capture the complex and erratic nature of financial markets. By incorporating PDifMPs, our method accounts for sudden market fluctuations, providing a more realistic model of asset price dynamics. We benchmark our approach with the Longstaff-Schwartz algorithm, both in its original form and modified to include PDifMP asset price trajectories. Numerical simulations demonstrate that our PDifMP-based method not only provides a more accurate reflection of market behaviour but also offers practical advantages in terms of computational efficiency. The results suggest that PDifMPs can significantly improve the predictive accuracy of American options pricing by more closely aligning with the stochastic volatility and jumps observed in real financial markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Community-level Contagion among Diverse Financial Assets,"As global financial markets become increasingly interconnected, financial contagion has developed into a major influencer of asset price dynamics. Motivated by this context, our study explores financial contagion both within and between asset communities. We contribute to the literature by examining the contagion phenomenon at the community level rather than among individual assets. Our experiments rely on high-frequency data comprising cryptocurrencies, stocks and US ETFs over the 4-year period from April 2019 to May 2023. Using the Louvain community detection algorithm, Vector Autoregression contagion detection model and Tracy-Widom random matrix theory for noise removal from financial assets, we present three main findings. Firstly, while the magnitude of contagion remains relatively stable over time, contagion density (the percentage of asset pairs exhibiting contagion within a financial system) increases. This suggests that market uncertainty is better characterized by the transmission of shocks more broadly than by the strength of any single spillover. Secondly, there is no significant difference between intra- and inter-community contagion, indicating that contagion is a system-wide phenomenon rather than being confined to specific asset groups. Lastly, certain communities themselves, especially those dominated by Information Technology assets, consistently appear to act as major contagion transmitters in the financial network over the examined period, spreading shocks with high densities to many other communities. Our findings suggest that traditional risk management strategies such as portfolio diversification through investing in low-correlated assets or different types of investment vehicle might be insufficient due to widespread contagion.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A perspective on correlation-based financial networks and entropy measures,"In this brief review, we critically examine the recent work done on correlation-based networks in financial systems. The structure of empirical correlation matrices constructed from the financial market data changes as the individual stock prices fluctuate with time, showing interesting evolutionary patterns, especially during critical events such as market crashes, bubbles, etc. We show that the study of correlation-based networks and their evolution with time is useful for extracting important information of the underlying market dynamics. We, also, present our perspective on the use of recently developed entropy measures such as structural entropy and eigen-entropy for continuous monitoring of correlation-based networks.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Regime recovery using implied volatility in Markov modulated market model,"In the regime switching extension of Black-Scholes-Merton model of asset price dynamics, one assumes that the volatility coefficient evolves as a hidden pure jump process. Under the assumption of Markov regime switching, we have considered the locally risk minimizing price of European vanilla options. By pretending these prices or their noisy versions as traded prices, we have first computed the implied volatility (IV) of the underlying asset. Then by performing several numerical experiments we have investigated the dependence of IV on the time to maturity (TTM) and strike price of the vanilla options. We have observed a clear dependence that is at par with the empirically observed stylized facts. Furthermore, we have experimentally validated that IV time series, obtained from contracts with moneyness and TTM varying in particular narrow ranges, can recover the transition instances of the hidden Markov chain. Such regime recovery has also been proved in a theoretical setting. Moreover, the novel scheme for computing option price is shown to be stable.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Estimating Tipping Points in Feedback-Driven Financial Networks,"Much research has been conducted arguing that tipping points at which complex systems experience phase transitions are difficult to identify. To test the existence of tipping points in financial markets, based on the alternating offer strategic model we propose a network of bargaining agents who mutually either cooperate or where the feedback mechanism between trading and price dynamics is driven by an external ""hidden"" variable R that quantifies the degree of market overpricing. Due to the feedback mechanism, R fluctuates and oscillates over time, and thus periods when the market is underpriced and overpriced occur repeatedly. As the market becomes overpriced, bubbles are created that ultimately burst in a market crash. The probability that the index will drop in the next year exhibits a strong hysteresis behavior from which we calculate the tipping point. The probability distribution function of R has a bimodal shape characteristic of small systems near the tipping point. By examining the S&P500 index we illustrate the applicability of the model and demonstate that the financial data exhibits a hysteresis and a tipping point that agree with the model predictions. We report a cointegration between the returns of the S&P 500 index and its intrinsic value.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Problem of Calibrating an Agent-Based Model of High-Frequency Trading,"Agent-based models, particularly those applied to financial markets, demonstrate the ability to produce realistic, simulated system dynamics, comparable to those observed in empirical investigations. Despite this, they remain fairly difficult to calibrate due to their tendency to be computationally expensive, even with recent advances in technology. For this reason, financial agent-based models are frequently validated by demonstrating an ability to reproduce well-known log return time series and central limit order book stylized facts, as opposed to being rigorously calibrated to transaction data. We thus apply an established financial agent-based model calibration framework to a simple model of high- and low-frequency trader interaction and demonstrate possible inadequacies of a stylized fact-centric approach to model validation. We further argue for the centrality of calibration to the validation of financial agent-based models and possible pitfalls of current approaches to financial agent-based modeling.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Basket Options with Volatility Skew: Calibrating a Local Volatility Model by Sample Rearrangement,"The pricing of derivatives tied to baskets of assets demands a sophisticated framework that aligns with the available market information to capture the intricate non-linear dependency structure among the assets. We describe the dynamics of the multivariate process of constituents with a copula model and propose an efficient method to extract the dependency structure from the market. The proposed method generates coherent sets of samples of the constituents process through systematic sampling rearrangement. These samples are then utilized to calibrate a local volatility model (LVM) of the basket process, which is used to price basket derivatives. We show that the method is capable of efficiently pricing basket options based on a large number of basket constituents, accomplishing the calibration process within a matter of seconds, and achieving near-perfect calibration to the index options of the market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Hedging,"We present a framework for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs, market impact, liquidity constraints or risk limits using modern deep reinforcement machine learning methods.   We discuss how standard reinforcement learning methods can be applied to non-linear reward structures, i.e. in our case convex risk measures. As a general contribution to the use of deep learning for stochastic processes, we also show that the set of constrained trading strategies used by our algorithm is large enough to $$-approximate any optimal solution.   Our algorithm can be implemented efficiently even in high-dimensional situations using modern machine learning tools. Its structure does not depend on specific market dynamics, and generalizes across hedging instruments including the use of liquid derivatives. Its computational performance is largely invariant in the size of the portfolio as it depends mainly on the number of hedging instruments available.   We illustrate our approach by showing the effect on hedging under transaction costs in a synthetic market driven by the Heston model, where we outperform the standard ""complete market"" solution.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Reinforcement Learning Framework for Quantitative Trading,"The inherent volatility and dynamic fluctuations within the financial stock market underscore the necessity for investors to employ a comprehensive and reliable approach that integrates risk management strategies, market trends, and the movement trends of individual securities. By evaluating specific data, investors can make more informed decisions. However, the current body of literature lacks substantial evidence supporting the practical efficacy of reinforcement learning (RL) agents, as many models have only demonstrated success in back testing using historical data. This highlights the urgent need for a more advanced methodology capable of addressing these challenges. There is a significant disconnect in the effective utilization of financial indicators to better understand the potential market trends of individual securities. The disclosure of successful trading strategies is often restricted within financial markets, resulting in a scarcity of widely documented and published strategies leveraging RL. Furthermore, current research frequently overlooks the identification of financial indicators correlated with various market trends and their potential advantages.   This research endeavors to address these complexities by enhancing the ability of RL agents to effectively differentiate between positive and negative buy/sell actions using financial indicators. While we do not address all concerns, this paper provides deeper insights and commentary on the utilization of technical indicators and their benefits within reinforcement learning. This work establishes a foundational framework for further exploration and investigation of more complex scenarios.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Reinforcement Trading with Predictable Returns,"Classical portfolio optimization often requires forecasting asset returns and their corresponding variances in spite of the low signal-to-noise ratio provided in the financial markets. Modern deep reinforcement learning (DRL) offers a framework for optimizing sequential trader decisions but lacks theoretical guarantees of convergence. On the other hand, the performances on real financial trading problems are strongly affected by the goodness of the signal used to predict returns. To disentangle the effects coming from return unpredictability from those coming from algorithm un-trainability, we investigate the performance of model-free DRL traders in a market environment with different known mean-reverting factors driving the dynamics. When the framework admits an exact dynamic programming solution, we can assess the limits and capabilities of different value-based algorithms to retrieve meaningful trading signals in a data-driven manner. We consider DRL agents that leverage classical strategies to increase their performances and we show that this approach guarantees flexibility, outperforming the benchmark strategy when the price dynamics is misspecified and some original assumptions on the market environment are violated with the presence of extreme events and volatility clustering.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Is the difference between deep hedging and delta hedging a statistical arbitrage?,"The recent work of Horikawa and Nakagawa (2024) claims that under a complete market admitting statistical arbitrage, the difference between the hedging position provided by deep hedging and that of the replicating portfolio is a statistical arbitrage. This raises concerns as it entails that deep hedging can include a speculative component aimed simply at exploiting the structure of the risk measure guiding the hedging optimisation problem. We test whether such finding remains true in a GARCH-based market model, which is an illustrative case departing from complete market dynamics. We observe that the difference between deep hedging and delta hedging is a speculative overlay if the risk measure considered does not put sufficient relative weight on adverse outcomes. Nevertheless, a suitable choice of risk measure can prevent the deep hedging agent from engaging in speculation.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Improving DeFi Accessibility through Efficient Liquidity Provisioning with Deep Reinforcement Learning,"This paper applies deep reinforcement learning (DRL) to optimize liquidity provisioning in Uniswap v3, a decentralized finance (DeFi) protocol implementing an automated market maker (AMM) model with concentrated liquidity. We model the liquidity provision task as a Markov Decision Process (MDP) and train an active liquidity provider (LP) agent using the Proximal Policy Optimization (PPO) algorithm. The agent dynamically adjusts liquidity positions by using information about price dynamics to balance fee maximization and impermanent loss mitigation. We use a rolling window approach for training and testing, reflecting realistic market conditions and regime shifts. This study compares the data-driven performance of the DRL-based strategy against common heuristics adopted by small retail LP actors that do not systematically modify their liquidity positions. By promoting more efficient liquidity management, this work aims to make DeFi markets more accessible and inclusive for a broader range of participants. Through a data-driven approach to liquidity management, this work seeks to contribute to the ongoing development of more efficient and user-friendly DeFi markets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Forecasting interest rates through Vasicek and CIR models: a partitioning approach,"The aim of this paper is to propose a new methodology that allows forecasting, through Vasicek and CIR models, of future expected interest rates (for each maturity) based on rolling windows from observed financial market data. The novelty, apart from the use of those models not for pricing but for forecasting the expected rates at a given maturity, consists in an appropriate partitioning of the data sample. This allows capturing all the statistically significant time changes in volatility of interest rates, thus giving an account of jumps in market dynamics. The performance of the new approach is carried out for different term structures and is tested for both models. It is shown how the proposed methodology overcomes both the usual challenges (e.g. simulating regime switching, volatility clustering, skewed tails, etc.) as well as the new ones added by the current market environment characterized by low to negative interest rates.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Deep Reinforcement Learning Framework For Financial Portfolio Management,"In this research paper, we investigate into a paper named ""A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem"" [arXiv:1706.10059]. It is a portfolio management problem which is solved by deep learning techniques. The original paper proposes a financial-model-free reinforcement learning framework, which consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. Three different instants are used to realize this framework, namely a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). The performance is then examined by comparing to a number of recently reviewed or published portfolio-selection strategies. We have successfully replicated their implementations and evaluations. Besides, we further apply this framework in the stock market, instead of the cryptocurrency market that the original paper uses. The experiment in the cryptocurrency market is consistent with the original paper, which achieve superior returns. But it doesn't perform as well when applied in the stock market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Learning a functional control for high-frequency finance,"We use a deep neural network to generate controllers for optimal trading on high frequency data. For the first time, a neural network learns the mapping between the preferences of the trader, i.e. risk aversion parameters, and the optimal controls. An important challenge in learning this mapping is that in intraday trading, trader's actions influence price dynamics in closed loop via the market impact. The exploration--exploitation tradeoff generated by the efficient execution is addressed by tuning the trader's preferences to ensure long enough trajectories are produced during the learning phase. The issue of scarcity of financial data is solved by transfer learning: the neural network is first trained on trajectories generated thanks to a Monte-Carlo scheme, leading to a good initialization before training on historical trajectories. Moreover, to answer to genuine requests of financial regulators on the explainability of machine learning generated controls, we project the obtained ""blackbox controls"" on the space usually spanned by the closed-form solution of the stylized optimal trading problem, leading to a transparent structure. For more realistic loss functions that have no closed-form solution, we show that the average distance between the generated controls and their explainable version remains small. This opens the door to the acceptance of ML-generated controls by financial regulators.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
QTMRL: An Agent for Quantitative Trading Decision-Making Based on Multi-Indicator Guided Reinforcement Learning,"In the highly volatile and uncertain global financial markets, traditional quantitative trading models relying on statistical modeling or empirical rules often fail to adapt to dynamic market changes and black swan events due to rigid assumptions and limited generalization. To address these issues, this paper proposes QTMRL (Quantitative Trading Multi-Indicator Reinforcement Learning), an intelligent trading agent combining multi-dimensional technical indicators with reinforcement learning (RL) for adaptive and stable portfolio management. We first construct a comprehensive multi-indicator dataset using 23 years of S&P 500 daily OHLCV data (2000-2022) for 16 representative stocks across 5 sectors, enriching raw data with trend, volatility, and momentum indicators to capture holistic market dynamics. Then we design a lightweight RL framework based on the Advantage Actor-Critic (A2C) algorithm, including data processing, A2C algorithm, and trading agent modules to support policy learning and actionable trading decisions. Extensive experiments compare QTMRL with 9 baselines (e.g., ARIMA, LSTM, moving average strategies) across diverse market regimes, verifying its superiority in profitability, risk adjustment, and downside risk control. The code of QTMRL is publicly available at https://github.com/ChenJiahaoJNU/QTMRL.git","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fast Super-Paramagnetic Clustering,"We map stock market interactions to spin models to recover their hierarchical structure using a simulated annealing based Super-Paramagnetic Clustering (SPC) algorithm. This is directly compared to a modified implementation of a maximum likelihood approach we call Fast Super-Paramagnetic Clustering (f-SPC). The methods are first applied standard toy test-case problems, and then to a data-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249 days. The signal to noise ratio of stock market correlation matrices is briefly considered. Our result recover approximately clusters representative of standard economic sectors and mixed ones whose dynamics shine light on the adaptive nature of financial markets and raise concerns relating to the effectiveness of industry based static financial market classification in the world of real-time data analytics. A key result is that we show that f-SPC maximum likelihood solutions converge to ones found within the Super-Paramagnetic Phase where the entropy is maximum, and those solutions are qualitatively better for high dimensionality data-sets.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Can Machine Learning Algorithms Outperform Traditional Models for Option Pricing?,"This study investigates the application of machine learning techniques, specifically Neural Networks, Random Forests, and CatBoost for option pricing, in comparison to traditional models such as Black-Scholes and Heston Model. Using both synthetically generated data and real market option data, each model is evaluated in predicting the option price. The results show that machine learning models can capture complex, non-linear relationships in option prices and, in several cases, outperform both Black-Scholes and Heston models. These findings highlight the potential of data-driven methods to improve pricing accuracy and better reflect market dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
TABL-ABM: A Hybrid Framework for Synthetic LOB Generation,"The recent application of deep learning models to financial trading has heightened the need for high fidelity financial time series data. This synthetic data can be used to supplement historical data to train large trading models. The state-of-the-art models for the generative application often rely on huge amounts of historical data and large, complicated models. These models range from autoregressive and diffusion-based models through to architecturally simpler models such as the temporal-attention bilinear layer. Agent-based approaches to modelling limit order book dynamics can also recreate trading activity through mechanistic models of trader behaviours. In this work, we demonstrate how a popular agent-based framework for simulating intraday trading activity, the Chiarella model, can be combined with one of the most performant deep learning models for forecasting multi-variate time series, the TABL model. This forecasting model is coupled to a simulation of a matching engine with a novel method for simulating deleted order flow. Our simulator gives us the ability to test the generative abilities of the forecasting model using stylised facts. Our results show that this methodology generates realistic price dynamics however, when analysing deeper, parts of the markets microstructure are not accurately recreated, highlighting the necessity for including more sophisticated agent behaviors into the modeling framework to help account for tail events.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism,"In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multi-agent systems and traditional quantitative investment methods across diverse evaluation metrics. ContestTrade is open-sourced on GitHub at https://github.com/FinStep-AI/ContestTrade.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimal Asset Allocation For Outperforming A Stochastic Benchmark Target,"We propose a data-driven Neural Network (NN) optimization framework to determine the optimal multi-period dynamic asset allocation strategy for outperforming a general stochastic target. We formulate the problem as an optimal stochastic control with an asymmetric, distribution shaping, objective function. The proposed framework is illustrated with the asset allocation problem in the accumulation phase of a defined contribution pension plan, with the goal of achieving a higher terminal wealth than a stochastic benchmark. We demonstrate that the data-driven approach is capable of learning an adaptive asset allocation strategy directly from historical market returns, without assuming any parametric model of the financial market dynamics. Following the optimal adaptive strategy, investors can make allocation decisions simply depending on the current state of the portfolio. The optimal adaptive strategy outperforms the benchmark constant proportion strategy, achieving a higher terminal wealth with a 90% probability, a 46% higher median terminal wealth, and a significantly more right-skewed terminal wealth distribution. We further demonstrate the robustness of the optimal adaptive strategy by testing the performance of the strategy on bootstrap resampled market data, which has different distributions compared to the training data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
From Deep Filtering to Deep Econometrics,"Calculating true volatility is an essential task for option pricing and risk management. However, it is made difficult by market microstructure noise. Particle filtering has been proposed to solve this problem as it favorable statistical properties, but relies on assumptions about underlying market dynamics. Machine learning methods have also been proposed but lack interpretability, and often lag in performance. In this paper we implement the SV-PF-RNN: a hybrid neural network and particle filter architecture. Our SV-PF-RNN is designed specifically with stochastic volatility estimation in mind. We then show that it can improve on the performance of a basic particle filter.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Malliavin-Mancino estimators implemented with non-uniform fast Fourier transforms,"We implement and test kernel averaging Non-Uniform Fast Fourier Transform (NUFFT) methods to enhance the performance of correlation and covariance estimation on asynchronously sampled event-data using the Malliavin-Mancino Fourier estimator. The methods are benchmarked for Dirichlet and Fejr Fourier basis kernels. We consider test cases formed from Geometric Brownian motions to replicate synchronous and asynchronous data for benchmarking purposes. We consider three standard averaging kernels to convolve the event-data for synchronisation via over-sampling for use with the Fast Fourier Transform (FFT): the Gaussian kernel, the Kaiser-Bessel kernel, and the exponential of semi-circle kernel. First, this allows us to demonstrate the performance of the estimator with different combinations of basis kernels and averaging kernels. Second, we investigate and compare the impact of the averaging scales explicit in each averaging kernel and its relationship between the time-scale averaging implicit in the Malliavin-Mancino estimator. Third, we demonstrate the relationship between time-scale averaging based on the number of Fourier coefficients used in the estimator to a theoretical model of the Epps effect. We briefly demonstrate the methods on Trade-and-Quote (TAQ) data from the Johannesburg Stock Exchange to make an initial visualisation of the correlation dynamics for various time-scales under market microstructure.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios,"In the ever-changing and intricate landscape of financial markets, portfolio optimisation remains a formidable challenge for investors and asset managers. Conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences. To address this, we propose an innovative framework, termed Diffusion-Augmented Reinforcement Learning (DARL), which synergistically integrates Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for portfolio management. By leveraging DDPMs to generate synthetic market crash scenarios conditioned on varying stress intensities, our approach significantly enhances the robustness of training data. Empirical evaluations demonstrate that DARL outperforms traditional baselines, delivering superior risk-adjusted returns and resilience against unforeseen crises, such as the 2025 Tariff Crisis. This work offers a robust and practical methodology to bolster stress resilience in DRL-driven financial applications.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Bi-LSTM Price Prediction based on Attention Mechanism,"With the increasing enrichment and development of the financial derivatives market, the frequency of transactions is also faster and faster. Due to human limitations, algorithms and automatic trading have recently become the focus of discussion. In this paper, we propose a bidirectional LSTM neural network based on an attention mechanism, which is based on two popular assets, gold and bitcoin. In terms of Feature Engineering, on the one hand, we add traditional technical factors, and at the same time, we combine time series models to develop factors. In the selection of model parameters, we finally chose a two-layer deep learning network. According to AUC measurement, the accuracy of bitcoin and gold is 71.94% and 73.03% respectively. Using the forecast results, we achieved a return of 1089.34% in two years. At the same time, we also compare the attention Bi-LSTM model proposed in this paper with the traditional model, and the results show that our model has the best performance in this data set. Finally, we discuss the significance of the model and the experimental results, as well as the possible improvement direction in the future.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On The Calibration of Short-Term Interest Rates Through a CIR Model,"It is well known that the Cox-Ingersoll-Ross (CIR) stochastic model to study the term structure of interest rates, as introduced in 1985, is inadequate for modelling the current market environment with negative short interest rates. Moreover, the diffusion term in the rate dynamics goes to zero when short rates are small; both volatility and long-run mean do not change with time; they do not fit with the skewed (fat tails) distribution of the interest rates, etc. The aim of the present work is to suggest a new framework, which we call the CIR\# model, that well fits the term structure of short interest rates so that the market volatility structure is preserved as well as the analytical tractability of the original CIR model.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Learning the Exact SABR Model,"The SABR model is a cornerstone of interest rate volatility modeling, but its practical application relies heavily on the analytical approximation by Hagan et al., whose accuracy deteriorates for high volatility, long maturities, and out-of-the-money options, admitting arbitrage. While machine learning approaches have been proposed to overcome these limitations, they have often been limited by simplified SABR dynamics or a lack of systematic validation against the full spectrum of market conditions.   We develop a novel SABR DNN, a specialized Artificial Deep Neural Network (DNN) architecture that learns the true SABR stochastic dynamics using an unprecedented large training dataset (more than 200 million points) of interest rate Cap/Floor volatility surfaces, including very long maturities (30Y) and extreme strikes consistently with market quotations. Our dataset is obtained via high-precision unbiased Monte Carlo simulation of a special scaled shifted-SABR stochastic dynamics, which allows dimensional reduction without any loss of generality.   Our SABR DNN provides arbitrage-free calibration of real market volatility surfaces and Cap/Floor prices for any maturity and strike with negligible computational effort and without retraining across business dates. Our results fully address the gaps in the previous machine learning SABR literature in a systematic and self-consistent way, and can be extended to cover any interest rate European options in different rate tenors and currencies, thus establishing a comprehensive functional SABR framework that can be adopted for daily trading and risk management activities.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On the Hull-White model with volatility smile for Valuation Adjustments,"Affine Diffusion dynamics are frequently used for Valuation Adjustments (xVA) calculations due to their analytic tractability. However, these models cannot capture the market-implied skew and smile, which are relevant when computing xVA metrics. Hence, additional degrees of freedom are required to capture these market features. In this paper, we address this through an SDE with state-dependent coefficients. The SDE is consistent with the convex combination of a finite number of different AD dynamics. We combine Hull-White one-factor models where one model parameter is varied. We use the Randomized AD (RAnD) technique to parameterize the combination of dynamics. We refer to our SDE with state-dependent coefficients and the RAnD parametrization of the original models as the rHW model. The rHW model allows for efficient semi-analytic calibration to European swaptions through the analytic tractability of the Hull-White dynamics. We use a regression-based Monte-Carlo simulation to calculate exposures. In this setting, we demonstrate the significant effect of skew and smile on exposures and xVAs of linear and early-exercise interest rate derivatives.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Numerical Simulation of Exchange Option with Finite Liquidity: Controlled Variate Model,"In this paper we develop numerical pricing methodologies for European style Exchange Options written on a pair of correlated assets, in a market with finite liquidity. In contrast to the standard multi-asset Black-Scholes framework, trading in our market model has a direct impact on the asset's price. The price impact is incorporated into the dynamics of the first asset through a specific trading strategy, as in large trader liquidity model. Two-dimensional Milstein scheme is implemented to simulate the pair of assets prices. The option value is numerically estimated by Monte Carlo with the Margrabe option as controlled variate. Time complexity of these numerical schemes are included. Finally, we provide a deep learning framework to implement this model effectively in a production environment.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Understanding the Maker Protocol,"This paper discusses a decentralized finance (DeFi) application called MakerDAO. The Maker Protocol, built on the Ethereum blockchain, enables users to create and hold currency. Current elements of the Maker Protocol are the Dai stable coin, Maker Vaults, and Voting. MakerDAO governs the Maker Protocol by deciding on key parameters (e.g., stability fees, collateral types and rates, etc.) through the voting power of Maker (MKR) holders. The Maker Protocol is one of the largest decentralized applications (DApps) on the Ethereum blockchain and is the first decentralized finance (DeFi) application to earn significant adoption. The objective of this paper is to analyze and discuss the significance, uses, and functions of this DeFi application.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Multi-Objective reward generalization: Improving performance of Deep Reinforcement Learning for applications in single-asset trading,"We investigate the potential of Multi-Objective, Deep Reinforcement Learning for stock and cryptocurrency single-asset trading: in particular, we consider a Multi-Objective algorithm which generalizes the reward functions and discount factor (i.e., these components are not specified a priori, but incorporated in the learning process). Firstly, using several important assets (cryptocurrency pairs BTCUSD, ETHUSDT, XRPUSDT, and stock indexes AAPL, SPY, NIFTY50), we verify the reward generalization property of the proposed Multi-Objective algorithm, and provide preliminary statistical evidence showing increased predictive stability over the corresponding Single-Objective strategy. Secondly, we show that the Multi-Objective algorithm has a clear edge over the corresponding Single-Objective strategy when the reward mechanism is sparse (i.e., when non-null feedback is infrequent over time). Finally, we discuss the generalization properties with respect to the discount factor. The entirety of our code is provided in open source format.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
High-Frequency Options Trading | With Portfolio Optimization,"This paper explores the effectiveness of high-frequency options trading strategies enhanced by advanced portfolio optimization techniques, investigating their ability to consistently generate positive returns compared to traditional long or short positions on options. Utilizing SPY options data recorded in five-minute intervals over a one-month period, we calculate key metrics such as Option Greeks and implied volatility, applying the Binomial Tree model for American options pricing and the Newton-Raphson algorithm for implied volatility calculation. Investment universes are constructed based on criteria like implied volatility and Greeks, followed by the application of various portfolio optimization models, including Standard Mean-Variance and Robust Methods. Our research finds that while basic long-short strategies centered on implied volatility and Greeks generally underperform, more sophisticated strategies incorporating advanced Greeks, such as Vega and Rho, along with dynamic portfolio optimization, show potential in effectively navigating the complexities of the options market. The study highlights the importance of adaptability and responsiveness in dynamic portfolio strategies within the high-frequency trading environment, particularly under volatile market conditions. Future research could refine strategy parameters and explore less frequently traded options, offering new insights into high-frequency options trading and portfolio management.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Libor model with expiry-wise stochastic volatility and displacement,"We develop a multi-factor stochastic volatility Libor model with displacement, where each individual forward Libor is driven by its own square-root stochastic volatility process. The main advantage of this approach is that, maturity-wise, each square-root process can be calibrated to the corresponding cap(let)vola-strike panel at the market. However, since even after freezing the Libors in the drift of this model, the Libor dynamics are not affine, new affine approximations have to be developed in order to obtain Fourier based (approximate) pricing procedures for caps and swaptions. As a result, we end up with a Libor modeling package that allows for efficient calibration to a complete system of cap/swaption market quotes that performs well even in crises times, where structural breaks in vola-strike-maturity panels are typically observed.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fees in AMMs: A quantitative study,"In the ever evolving landscape of decentralized finance automated market makers (AMMs) play a key role: they provide a market place for trading assets in a decentralized manner. For so-called bluechip pairs, arbitrage activity provides a major part of the revenue generation of AMMs but also a major source of loss due to the so-called 'informed orderflow'. Finding ways to minimize those losses while still keeping uninformed trading activity alive is a major problem in the field. In this paper we will investigate the mechanics of said arbitrage and try to understand how AMMs can maximize the revenue creation or in other words minimize the losses. To that end, we model the dynamics of arbitrage activity for a concrete implementation of a pool and study its sensitivity to the choice of fee aiming to maximize the revenue for the AMM. We identify dynamical fees that mimic the directionality of the price due to asymmetric fee choices as a promising avenue to mitigate losses to toxic flow. This work is based on and extends a recent article by some of the authors.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Analytic estimation of parameters of stochastic volatility diffusion models with exponential-affine characteristic function for currency option pricing,"This dissertation develops and justifies a novel method for deriving approximate formulas to estimate two parameters in stochastic volatility diffusion models with exponentially-affine characteristic functions and single- or two-factor variance. These formulas aim to improve the accuracy of option pricing and enhance the calibration process by providing reliable initial values for local minimization algorithms. The parameters relate to the volatility of the stochastic factor in instantaneous variance dynamics and the correlation between stochastic factors and asset price dynamics.   The study comprises five chapters. Chapter one outlines the currency option market, pricing methods, and the general structure of stochastic volatility models. Chapter two derives the replication strategy dynamics and introduces a new two-factor volatility model: the OUOU model. Chapter three analyzes the distribution and surface dynamics of implied volatilities using principal component and common factor analysis. Chapter four discusses calibration methods for stochastic volatility models, particularly the Heston model, and presents the new Implied Central Moments method to estimate parameters in the Heston and Schbel-Zhu models. Extensions to two-factor models, Bates and OUOU, are also explored. Chapter five evaluates the performance of the proposed formulas on the EURUSD options market, demonstrating the superior accuracy of the new method.   The dissertation successfully meets its research objectives, expanding tools for derivative pricing and risk assessment. Key contributions include faster and more precise parameter estimation formulas and the introduction of the OUOU model - an extension of the Schbel-Zhu model with a semi-analytical valuation formula for European options, previously unexamined in the literature.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Libor at crossroads: stochastic switching detection using information theory quantifiers,"This paper studies the 28 time series of Libor rates, classified in seven maturities and four currencies), during the last 14 years. The analysis was performed using a novel technique in financial economics: the Complexity-Entropy Causality Plane. This planar representation allows the discrimination of different stochastic and chaotic regimes. Using a temporal analysis based on moving windows, this paper unveals an abnormal movement of Libor time series arround the period of the 2007 financial crisis. This alteration in the stochastic dynamics of Libor is contemporary of what press called ""Libor scandal"", i.e. the manipulation of interest rates carried out by several prime banks. We argue that our methodology is suitable as a market watch mechanism, as it makes visible the temporal redution in informational efficiency of the market.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Coupling news sentiment with web browsing data improves prediction of intra-day price dynamics,"The new digital revolution of big data is deeply changing our capability of understanding society and forecasting the outcome of many social and economic systems. Unfortunately, information can be very heterogeneous in the importance, relevance, and surprise it conveys, affecting severely the predictive power of semantic and statistical methods. Here we show that the aggregation of web users' behavior can be elicited to overcome this problem in a hard to predict complex system, namely the financial market. Specifically, our in-sample analysis shows that the combined use of sentiment analysis of news and browsing activity of users of Yahoo! Finance greatly helps forecasting intra-day and daily price changes of a set of 100 highly capitalized US stocks traded in the period 2012-2013. Sentiment analysis or browsing activity when taken alone have very small or no predictive power. Conversely, when considering a ""news signal"" where in a given time interval we compute the average sentiment of the clicked news, weighted by the number of clicks, we show that for nearly 50% of the companies such signal Granger-causes hourly price returns. Our result indicates a ""wisdom-of-the-crowd"" effect that allows to exploit users' activity to identify and weigh properly the relevant and surprising news, enhancing considerably the forecasting power of the news sentiment.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Mean-field theory of the Santa Fe model revisited: a systematic derivation from an exact BBGKY hierarchy for the zero-intelligence limit-order book model,"The Santa Fe model is an established econophysics model for describing stochastic dynamics of the limit order book from the viewpoint of the zero-intelligence approach. While its foundation was studied by combining a dimensional analysis and a mean-field theory by E. Smith et al. in Quantitative Finance 2003, their arguments are rather heuristic and lack solid mathematical foundation; indeed, their mean-field equations were derived with heuristic arguments and their solutions were not explicitly obtained. In this work, we revisit the mean-field theory of the Santa Fe model from the viewpoint of kinetic theory -- a traditional mathematical program in statistical physics. We study the exact master equation for the Santa Fe model and systematically derive the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchical equation. By applying the mean-field approximation, we derive the mean-field equation for the order-book density profile, parallel to the Boltzmann equation in conventional statistical physics. Furthermore, we obtain explicit and closed expression of the mean-field solutions. Our solutions have several implications: (1)Our scaling formulas are available for both $\to 0$ and $\to \infty$ asymptotics, where $$ is the market-order submission intensity. Particularly, the mean-field theory works very well for small $$, while its validity is partially limited for large $$. (2)The ``method of image'' solution, heuristically derived by Bouchaud-Mzard-Potters in Quantitative Finance 2002, is obtained for large $$, serving as a mathematical foundation for their heuristic arguments. (3)Finally, we point out an error in E. Smith et al. 2003 in the scaling law for the diffusion constant due to a misspecification in their dimensional analysis.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
The Shannon information of filtrations and the additional logarithmic utility of insiders,"The background for the general mathematical link between utility and information theory investigated in this paper is a simple financial market model with two kinds of small traders: less informed traders and insiders, whose extra information is represented by an enlargement of the other agents' filtration. The expected logarithmic utility increment, that is, the difference of the insider's and the less informed trader's expected logarithmic utility is described in terms of the information drift, that is, the drift one has to eliminate in order to perceive the price dynamics as a martingale from the insider's perspective. On the one hand, we describe the information drift in a very general setting by natural quantities expressing the probabilistic better informed view of the world. This, on the other hand, allows us to identify the additional utility by entropy related quantities known from information theory. In particular, in a complete market in which the insider has some fixed additional information during the entire trading interval, its utility increment can be represented by the Shannon information of his extra knowledge. For general markets, and in some particular examples, we provide estimates of maximal utility by information inequalities.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Physics-Informed Convolutional Transformer for Predicting Volatility Surface,"Predicting volatility is important for asset predicting, option pricing and hedging strategies because it cannot be directly observed in the financial market. The Black-Scholes option pricing model is one of the most widely used models by market participants. Notwithstanding, the Black-Scholes model is based on heavily criticized theoretical premises, one of which is the constant volatility assumption. The dynamics of the volatility surface is difficult to estimate. In this paper, we establish a novel architecture based on physics-informed neural networks and convolutional transformers. The performance of the new architecture is directly compared to other well-known deep-learning architectures, such as standard physics-informed neural networks, convolutional long-short term memory (ConvLSTM), and self-attention ConvLSTM. Numerical evidence indicates that the proposed physics-informed convolutional transformer network achieves a superior performance than other methods.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Fast Pricing of Energy Derivatives with Mean-reverting Jump-diffusion Processes,"Most energy and commodity markets exhibit mean-reversion and occasional distinctive price spikes, which results in demand for derivative products which protect the holder against high prices. To this end, in this paper we present exact and fast methodologies for the simulation of the spot price dynamics modeled as the exponential of the sum of an Ornstein-Uhlenbeck and an independent pure jump process, where the latter one is driven by a compound Poisson process with (bilateral) exponentially distributed jumps. These methodologies are finally applied to the pricing of Asian options, gas storages and swings under different combinations of jump-diffusion market models, and the apparent computational advantages of the proposed procedures are emphasized.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Coronavirus and oil price crash,"Coronavirus (COVID-19) creates fear and uncertainty, hitting the global economy and amplifying the financial markets volatility. The oil price reaction to COVID-19 was gradually accommodated until March 09, 2020, when, 49 days after the release of the first coronavirus monitoring report by the World Health Organization (WHO), Saudi Arabia floods the market with oil. As a result, international prices drop with more than 20% in one single day. Against this background, the purpose of this paper is to investigate the impact of COVID-19 numbers on crude oil prices, while controlling for the impact of financial volatility and the United States (US) economic policy uncertainty. Our ARDL estimation shows that the COVID-19 daily reported cases of new infections have a marginal negative impact on the crude oil prices in the long run. Nevertheless, by amplifying the financial markets volatility, COVID-19 also has an indirect effect on the recent dynamics of crude oil prices.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Intra-Horizon Expected Shortfall and Risk Structure in Models with Jumps,"The present article deals with intra-horizon risk in models with jumps. Our general understanding of intra-horizon risk is along the lines of the approach taken in Boudoukh, Richardson, Stanton and Whitelaw (2004), Rossello (2008), Bhattacharyya, Misra and Kodase (2009), Bakshi and Panayotov (2010), and Leippold and Vasiljevi (2019). In particular, we believe that quantifying market risk by strictly relying on point-in-time measures cannot be deemed a satisfactory approach in general. Instead, we argue that complementing this approach by studying measures of risk that capture the magnitude of losses potentially incurred at any time of a trading horizon is necessary when dealing with (m)any financial position(s). To address this issue, we propose an intra-horizon analogue of the expected shortfall for general profit and loss processes and discuss its key properties. Our intra-horizon expected shortfall is well-defined for (m)any popular class(es) of Lvy processes encountered when modeling market dynamics and constitutes a coherent measure of risk, as introduced in Cheridito, Delbaen and Kupper (2004). On the computational side, we provide a simple method to derive the intra-horizon risk inherent to popular Lvy dynamics. Our general technique relies on results for maturity-randomized first-passage probabilities and allows for a derivation of diffusion and single jump risk contributions. These theoretical results are complemented with an empirical analysis, where popular Lvy dynamics are calibrated to S&P 500 index data and an analysis of the resulting intra-horizon risk is presented.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep Reinforcement Learning with LLM Evaluation,"Financial bond yield forecasting is challenging due to data scarcity, nonlinear macroeconomic dependencies, and evolving market conditions. In this paper, we propose a novel framework that leverages Causal Generative Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement learning (RL) to generate high-fidelity synthetic bond yield data for four major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key macroeconomic variables, we ensure statistical fidelity by preserving essential market properties. To transform this market dependent synthetic data into actionable insights, we employ a finetuned Large Language Model (LLM) Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments, and volatility projections. We use automated, human and LLM evaluations, all of which demonstrate that our framework improves forecasting performance over existing methods, with statistical validation via predictive accuracy, MAE evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation (3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement learning-enhanced synthetic data generation achieves the least Mean Absolute Error of 0.103, demonstrating its effectiveness in replicating real-world bond market dynamics. We not only enhance data-driven trading strategies but also provides a scalable, high-fidelity synthetic financial data pipeline for risk & volatility management and investment decision-making. This work establishes a bridge between synthetic data generation, LLM driven financial forecasting, and language model evaluation, contributing to AI-driven financial decision-making.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Realized Local Volatility Surface,"For quantitative trading risk management purposes, we present a novel idea: the realized local volatility surface. Concisely, it stands for the conditional expected volatility when sudden market behaviors of the underlying occur. One is able to explore risk management usages by following the orthotical Delta-Gamma dynamic hedging framework. The realized local volatility surface is, mathematically, a generalized Wiener measure from historical prices. It is reconstructed via employing high-frequency trading market data. A Stick-Breaking Gaussian Mixture Model is fitted via Hamiltonian Monte Carlo, producing a local volatility surface with 95% credible intervals. A practically validated Bayesian nonparametric estimation workflow. Empirical results on TSLA high-frequency data illustrate its ability to capture counterfactual volatility. We also discuss its application in improving volatility-based risk management.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Constant Function Market Makers: Multi-Asset Trades via Convex Optimization,"The rise of Ethereum and other blockchains that support smart contracts has led to the creation of decentralized exchanges (DEXs), such as Uniswap, Balancer, Curve, mStable, and SushiSwap, which enable agents to trade cryptocurrencies without trusting a centralized authority. While traditional exchanges use order books to match and execute trades, DEXs are typically organized as constant function market makers (CFMMs). CFMMs accept and reject proposed trades based on the evaluation of a function that depends on the proposed trade and the current reserves of the DEX. For trades that involve only two assets, CFMMs are easy to understand, via two functions that give the quantity of one asset that must be tendered to receive a given quantity of the other, and vice versa. When more than two assets are being exchanged, it is harder to understand the landscape of possible trades. We observe that various problems of choosing a multi-asset trade can be formulated as convex optimization problems, and can therefore be reliably and efficiently solved.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Backward Stochastic PDEs related to the utility maximization problem,"We study utility maximization problem for general utility functions using dynamic programming approach. We consider an incomplete financial market model, where the dynamics of asset prices are described by an $R^d$-valued continuous semimartingale. Under some regularity assumptions we derive backward stochastic partial differential equation (BSPDE) related directly to the primal problem and show that the strategy is optimal if and only if the corresponding wealth process satisfies a certain forward-SDE. As examples the cases of power, exponential and logarithmic utilities are considered.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Game of Competition for Risk,"In this study, we present models where participants strategically select their risk levels and earn corresponding rewards, mirroring real-world competition across various sectors. Our analysis starts with a normal form game involving two players in a continuous action space, confirming the existence and uniqueness of a Nash equilibrium and providing an analytical solution. We then extend this analysis to multi-player scenarios, introducing a new numerical algorithm for its calculation. A key novelty of our work lies in using regret minimization algorithms to solve continuous games through discretization. This groundbreaking approach enables us to incorporate additional real-world factors like market frictions and risk correlations among firms. We also experimentally validate that the Nash equilibrium in our model also serves as a correlated equilibrium. Our findings illuminate how market frictions and risk correlations affect strategic risk-taking. We also explore how policy measures can impact risk-taking and its associated rewards, with our model providing broader applicability than the Diamond-Dybvig framework. We make our methodology and open-source code available at https://github.com/louisabraham/cfrgame   Finally, we contribute methodologically by advocating the use of algorithms in economics, shifting focus from finite games to games with continuous action sets. Our study provides a solid framework for analyzing strategic interactions in continuous action games, emphasizing the importance of market frictions, risk correlations, and policy measures in strategic risk-taking dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Some variation of COBRA in sequential learning setup,"This research paper introduces innovative approaches for multivariate time series forecasting based on different variations of the combined regression strategy. We use specific data preprocessing techniques which makes a radical change in the behaviour of prediction. We compare the performance of the model based on two types of hyper-parameter tuning Bayesian optimisation (BO) and Usual Grid search. Our proposed methodologies outperform all state-of-the-art comparative models. We illustrate the methodologies through eight time series datasets from three categories: cryptocurrency, stock index, and short-term load forecasting.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On Parametric Optimal Execution and Machine Learning Surrogates,"We investigate optimal order execution problems in discrete time with instantaneous price impact and stochastic resilience. First, in the setting of linear transient price impact we derive a closed-form recursion for the optimal strategy, extending the deterministic results from Obizhaeva and Wang (J Financial Markets, 2013). Second, we develop a numerical algorithm based on dynamic programming and deep learning for the case of nonlinear transient price impact as proposed by Bouchaud et al. (Quant. Finance, 2004). Specifically, we utilize an actor-critic framework that constructs two neural-network (NN) surrogates for the value function and the feedback control. The flexible scalability of NN functional approximators enables parametric learning, i.e., incorporating several model or market parameters as part of the input space. Precise calibration of price impact, resilience, etc., is known to be extremely challenging and hence it is critical to understand sensitivity of the execution policy to these parameters. Our NN learner organically scales across multiple input dimensions and is shown to accurately approximate optimal strategies across a wide range of parameter configurations. We provide a fully reproducible Jupyter Notebook with our NN implementation, which is of independent pedagogical interest, demonstrating the ease of use of NN surrogates in (parametric) stochastic control problems.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Alternative Loss Function in Evaluation of Transformer Models,"The proper design and architecture of testing machine learning models, especially in their application to quantitative finance problems, is crucial. The most important aspect of this process is selecting an adequate loss function for training, validation, estimation purposes, and hyperparameter tuning. Therefore, in this research, through empirical experiments on equity and cryptocurrency assets, we apply the Mean Absolute Directional Loss (MADL) function, which is more adequate for optimizing forecast-generating models used in algorithmic investment strategies. The MADL function results are compared between Transformer and LSTM models, and we show that in almost every case, Transformer results are significantly better than those obtained with LSTM.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Interpool: a liquidity pool designed for interoperability that mints, exchanges, and burns","The lack of proper interoperability poses a significant challenge in leveraging use cases within the blockchain industry. Unlike typical solutions that rely on third parties such as oracles and witnesses, the interpool design operates as a standalone solution that mints, exchanges, and burns (MEB) within the same liquidity pool. This MEB approach ensures that minting is backed by the locked capital supplied by liquidity providers. During the exchange process, the order of transactions in the mempool is optimized to maximize returns, effectively transforming the front-running issue into a solution that forges an external blockchain hash. This forged hash enables a novel protocol, Listrack (Listen and Track), which ensures that ultimate liquidity is always enforced through a solid burning procedure, strengthening a trustless design. Supported by Listrack, atomic swaps become feasible even outside the interpool, thereby enhancing the current design into a comprehensive interoperability solution","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy,"Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Can Artificial Intelligence Trade the Stock Market?,"The paper explores the use of Deep Reinforcement Learning (DRL) in stock market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and Proximal Policy Optimization (PPO) and compares them with Buy and Hold benchmark. It evaluates these algorithms across three currency pairs, the S&P 500 index and Bitcoin, on the daily data in the period of 2019-2023. The results demonstrate DRL's effectiveness in trading and its ability to manage risk by strategically avoiding trades in unfavorable conditions, providing a substantial edge over classical approaches, based on supervised learning in terms of risk-adjusted returns.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
A Macroscopic Portfolio Model: From Rational Agents to Bounded Rationality,"We introduce a microscopic model of interacting financial agents, where each agent is characterized by two portfolios; money invested in bonds and money invested in stocks. Furthermore, each agent is faced with an optimization problem in order to determine the optimal asset allocation. The stock price evolution is driven by the aggregated investment decision of all agents. In fact, we are faced with a differential game since all agents aim to invest optimal. Mathematically such a problem is ill posed and we introduce the concept of Nash equilibrium solutions to ensure the existence of a solution. Especially, we denote an agent who solves this Nash equilibrium exactly a rational agent. As next step we use model predictive control to approximate the control problem. This enables us to derive a precise mathematical characterization of the degree of rationality of a financial agent. This is a novel concept in portfolio optimization and can be regarded as a general approach. In a second step we consider the case of a fully myopic agent, where we can solve the optimal investment decision of investors analytically. We select the running cost to be the expected missed revenue of an agent and we assume quadratic transaction costs. More precisely the expected revenues are determined by a combination of a fundamentalist or chartist strategy. Then we derive the mean field limit of the microscopic model in order to obtain a macroscopic portfolio model. The novelty in comparison to existent macroeconomic models in literature is that our model is derived from microeconomic dynamics. The resulting portfolio model is a three dimensional ODE system which enables us to derive analytical results. Simulations reveal, that our model is able to replicate the most prominent features of financial markets, namely booms and crashes.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Advancing Financial Forecasting: A Comparative Analysis of Neural Forecasting Models N-HiTS and N-BEATS,"In the rapidly evolving field of financial forecasting, the application of neural networks presents a compelling advancement over traditional statistical models. This research paper explores the effectiveness of two specific neural forecasting models, N-HiTS and N-BEATS, in predicting financial market trends. Through a systematic comparison with conventional models, this study demonstrates the superior predictive capabilities of neural approaches, particularly in handling the non-linear dynamics and complex patterns inherent in financial time series data. The results indicate that N-HiTS and N-BEATS not only enhance the accuracy of forecasts but also boost the robustness and adaptability of financial predictions, offering substantial advantages in environments that require real-time decision-making. The paper concludes with insights into the practical implications of neural forecasting in financial markets and recommendations for future research directions.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Gaussian Process Regression for Derivative Portfolio Modeling and Application to CVA Computations,"Modeling counterparty risk is computationally challenging because it requires the simultaneous evaluation of all the trades with each counterparty under both market and credit risk. We present a multi-Gaussian process regression approach, which is well suited for OTC derivative portfolio valuation involved in CVA computation. Our approach avoids nested simulation or simulation and regression of cash flows by learning a Gaussian metamodel for the mark-to-market cube of a derivative portfolio. We model the joint posterior of the derivatives as a Gaussian process over function space, with the spatial covariance structure imposed on the risk factors. Monte-Carlo simulation is then used to simulate the dynamics of the risk factors. The uncertainty in portfolio valuation arising from the Gaussian process approximation is quantified numerically. Numerical experiments demonstrate the accuracy and convergence properties of our approach for CVA computations, including a counterparty portfolio of interest rate swaps.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Integration of Fractional Order Black-Scholes Merton with Neural Network,"This study enhances option pricing by presenting unique pricing model fractional order Black-Scholes-Merton (FOBSM) which is based on the Black-Scholes-Merton (BSM) model. The main goal is to improve the precision and authenticity of option pricing, matching them more closely with the financial landscape. The approach integrates the strengths of both the BSM and neural network (NN) with complex diffusion dynamics. This study emphasizes the need to take fractional derivatives into account when analyzing financial market dynamics. Since FOBSM captures memory characteristics in sequential data, it is better at simulating real-world systems than integer-order models. Findings reveals that in complex diffusion dynamics, this hybridization approach in option pricing improves the accuracy of price predictions. the key contribution of this work lies in the development of a novel option pricing model (FOBSM) that leverages fractional calculus and neural networks to enhance accuracy in capturing complex diffusion dynamics and memory effects in financial data.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Model of an Open, Decentralized Computational Network with Incentive-Based Load Balancing","This paper proposes a model that enables permissionless and decentralized networks for complex computations. We explore the integration and optimize load balancing in an open, decentralized computational network. Our model leverages economic incentives and reputation-based mechanisms to dynamically allocate tasks between operators and coprocessors. This approach eliminates the need for specialized hardware or software, thereby reducing operational costs and complexities. We present a mathematical model that enhances restaking processes in blockchain systems by enabling operators to delegate complex tasks to coprocessors. The model's effectiveness is demonstrated through experimental simulations, showcasing its ability to optimize reward distribution, enhance security, and improve operational efficiency.   Our approach facilitates a more flexible and scalable network through the use of economic commitments, adaptable dynamic rating models, and a coprocessor load incentivization system. Supported by experimental simulations, the model demonstrates its capability to optimize resource allocation, enhance system resilience, and reduce operational risks. This ensures significant improvements in both security and cost-efficiency for the blockchain ecosystem.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Making Leveraged Exchange-Traded Funds Work for your Portfolio,"We examine strategically incorporating broad stock market leveraged exchange-traded funds (LETFs) into investment portfolios. We demonstrate that easily understandable and implementable strategies can enhance the risk-return profile of a portfolio containing LETFs. Our analysis shows that seemingly reasonable investment strategies may result in undesirable Omega ratios, with these effects compounding across rebalancing periods. By contrast, relatively simple dynamic strategies that systematically de-risk the portfolio once gains are observed can exploit this compounding effect, taking advantage of favorable Omega ratio dynamics. Our findings suggest that LETFs represent a valuable tool for investors employing dynamic strategies, while confirming their well-documented unsuitability for passive or static approaches.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Numerical methods for an optimal order execution problem,"This paper deals with numerical solutions to an impulse control problem arising from optimal portfolio liquidation with bid-ask spread and market price impact penalizing speedy execution trades. The corresponding dynamic programming (DP) equation is a quasi-variational inequality (QVI) with solvency constraint satisfied by the value function in the sense of constrained viscosity solutions. By taking advantage of the lag variable tracking the time interval between trades, we can provide an explicit backward numerical scheme for the time discretization of the DPQVI. The convergence of this discrete-time scheme is shown by viscosity solutions arguments. An optimal quantization method is used for computing the (conditional) expectations arising in this scheme. Numerical results are presented by examining the behaviour of optimal liquidation strategies, and comparative performance analysis with respect to some benchmark execution strategies. We also illustrate our optimal liquidation algorithm on real data, and observe various interesting patterns of order execution strategies. Finally, we provide some numerical tests of sensitivity with respect to the bid/ask spread and market impact parameters.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Enhancing Trading Performance Through Sentiment Analysis with Large Language Models: Evidence from the S&P 500,"This study integrates real-time sentiment analysis from financial news, GPT-2 and FinBERT, with technical indicators and time-series models like ARIMA and ETS to optimize S&P 500 trading strategies. By merging sentiment data with momentum and trend-based metrics, including a benchmark buy-and-hold and sentiment-based approach, is evaluated through assets values and returns. Results show that combining sentiment-driven insights with traditional models improves trading performance, offering a more dynamic approach to stock trading that adapts to market changes in volatile environments.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Market Misconduct in Decentralized Finance (DeFi): Analysis, Regulatory Challenges and Policy Implications","Technological advancement drives financial innovation, reshaping the traditional finance landscape and redefining user-market interactions. The rise of blockchain and Decentralized Finance (DeFi) underscores this intertwined evolution of technology and finance. While DeFi has introduced exciting opportunities, it has also exposed the ecosystem to new forms of market misconduct. This paper aims to bridge the academic and regulatory gaps by addressing key research questions about market misconduct in DeFi. We begin by discussing how blockchain technology can potentially enable the emergence of novel forms of market misconduct. We then offer a comprehensive definition and taxonomy for understanding DeFi market misconduct. Through comparative analysis and empirical measurements, we examine the novel forms of misconduct in DeFi, shedding light on their characteristics and social impact. Subsequently, we investigate the challenges of building a tailored regulatory framework for DeFi. We identify key areas where existing regulatory frameworks may need enhancement. Finally, we discuss potential approaches that bring DeFi into the regulatory perimeter.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Time-varying neural network for stock return prediction,"We consider the problem of neural network training in a time-varying context. Machine learning algorithms have excelled in problems that do not change over time. However, problems encountered in financial markets are often time-varying. We propose the online early stopping algorithm and show that a neural network trained using this algorithm can track a function changing with unknown dynamics. We compare the proposed algorithm to current approaches on predicting monthly U.S. stock returns and show its superiority. We also show that prominent factors (such as the size and momentum effects) and industry indicators, exhibit time varying stock return predictiveness. We find that during market distress, industry indicators experience an increase in importance at the expense of firm level features. This indicates that industries play a role in explaining stock returns during periods of heightened risk.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Credit Default Swap Calibration and Equity Swap Valuation under Counterparty Risk with a Tractable Structural Model,"In this paper we develop a tractable structural model with analytical default probabilities depending on some dynamics parameters, and we show how to calibrate the model using a chosen number of Credit Default Swap (CDS) market quotes. We essentially show how to use structural models with a calibration capability that is typical of the much more tractable credit-spread based intensity models. We apply the structural model to a concrete calibration case and observe what happens to the calibrated dynamics when the CDS-implied credit quality deteriorates as the firm approaches default. Finally we provide a typical example of a case where the calibrated structural model can be used for credit pricing in a much more convenient way than a calibrated reduced form model: The pricing of counterparty risk in an equity swap.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Signature-based models: theory and calibration,"We consider asset price models whose dynamics are described by linear functions of the (time extended) signature of a primary underlying process, which can range from a (market-inferred) Brownian motion to a general multidimensional continuous semimartingale. The framework is universal in the sense that classical models can be approximated arbitrarily well and that the model's parameters can be learned from all sources of available data by simple methods. We provide conditions guaranteeing absence of arbitrage as well as tractable option pricing formulas for so-called sig-payoffs, exploiting the polynomial nature of generic primary processes. One of our main focus lies on calibration, where we consider both time-series and implied volatility surface data, generated from classical stochastic volatility models and also from S&P500 index market data. For both tasks the linearity of the model turns out to be the crucial tractability feature which allows to get fast and accurate calibrations results.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Twitter Sentiment Analysis Applied to Finance: A Case Study in the Retail Industry,"This paper presents a financial analysis over Twitter sentiment analytics extracted from listed retail brands. We investigate whether there is statistically-significant information between the Twitter sentiment and volume, and stock returns and volatility. Traditional newswires are also considered as a proxy for the market sentiment for comparative purpose. The results suggest that social media is indeed a valuable source in the analysis of the financial dynamics in the retail sector even when compared to mainstream news such as the Wall Street Journal and Dow Jones Newswires.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
On Data-Driven Drawdown Control with Restart Mechanism in Trading,"This paper extends the existing drawdown modulation control policy to include a novel restart mechanism for trading. It is known that the drawdown modulation policy guarantees the maximum percentage drawdown no larger than a prespecified drawdown limit for all time with probability one. However, when the prespecified limit is approaching in practice, such a modulation policy becomes a stop-loss order, which may miss the profitable follow-up opportunities if any. Motivated by this, we add a data-driven restart mechanism into the drawdown modulation trading system to auto-tune the performance. We find that with the restart mechanism, our policy may achieve a superior trading performance to that without the restart, even with a nonzero transaction costs setting. To support our findings, some empirical studies using equity ETF and cryptocurrency with historical price data are provided.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Conditional Density Models for Asset Pricing,"We model the dynamics of asset prices and associated derivatives by consideration of the dynamics of the conditional probability density process for the value of an asset at some specified time in the future. In the case where the price process is driven by Brownian motion, an associated ""master equation"" for the dynamics of the conditional probability density is derived and expressed in integral form. By a ""model"" for the conditional density process we mean a solution to the master equation along with the specification of (a) the initial density, and (b) the volatility structure of the density. The volatility structure is assumed at any time and for each value of the argument of the density to be a functional of the history of the density up to that time. In practice one specifies the functional modulo sufficient parametric freedom to allow for the input of additional option data apart from that implicit in the initial density. The scheme is sufficiently flexible to allow for the input of various types of data depending on the nature of the options market and the class of valuation problem being undertaken. Various examples are studied in detail, with exact solutions provided in some cases.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
OrderFusion: Encoding Orderbook for End-to-End Probabilistic Intraday Electricity Price Forecasting,"Probabilistic forecasting of intraday electricity prices is essential to manage market uncertainties. However, current methods rely heavily on domain feature extraction, which breaks the end-to-end training pipeline and limits the model's ability to learn expressive representations from the raw orderbook. Moreover, these methods often require training separate models for different quantiles, further violating the end-to-end principle and introducing the quantile crossing issue. Recent advances in time-series models have demonstrated promising performance in general forecasting tasks. However, these models lack inductive biases arising from buy-sell interactions and are thus overparameterized. To address these challenges, we propose an end-to-end probabilistic model called OrderFusion, which produces interaction-aware representations of buy-sell dynamics, hierarchically estimates multiple quantiles, and remains parameter-efficient with only 4,872 parameters. We conduct extensive experiments and ablation studies on price indices (ID1, ID2, and ID3) using three years of orderbook in high-liquidity (German) and low-liquidity (Austrian) markets. The experimental results demonstrate that OrderFusion consistently outperforms multiple competitive baselines across markets, and ablation studies highlight the contribution of its individual components. The project page is at: https://runyao-yu.github.io/OrderFusion/.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Optimal trading with online parameters revisions,"The aim of this paper is to explain how parameters adjustments can be integrated in the design or the control of automates of trading. Typically, we are interested by the online estimation of the market impacts generated by robots or single orders, and how they/the controller should react in an optimal way to the informations generated by the observation of the realized impacts. This can be formulated as an optimal impulse control problem with unknown parameters, on which a prior is given. We explain how a mix of the classical Bayesian updating rule and of optimal control techniques allows one to derive the dynamic programming equation satisfied by the corresponding value function, from which the optimal policy can be inferred. We provide an example of convergent finite difference scheme and consider typical examples of applications.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
"Sizing the Risk: Kelly, VIX, and Hybrid Approaches in Put-Writing on Index Options","This paper examines systematic put-writing strategies applied to S&P 500 Index options, with a focus on position sizing as a key determinant of long-term performance. Despite the well-documented volatility risk premium, where implied volatility exceeds realized volatility, the practical implementation of short-dated volatility-selling strategies remains underdeveloped in the literature. This study evaluates three position sizing approaches: the Kelly criterion, VIX-based volatility regime scaling, and a novel hybrid method combining both. Using SPXW options with expirations from 0 to 5 days, the analysis explores a broad design space, including moneyness levels, volatility estimators, and memory horizons. Results show that ultra-short-dated, far out-of-the-money options deliver superior risk-adjusted returns. The hybrid sizing method consistently balances return generation with robust drawdown control, particularly under low-volatility conditions such as those seen in 2024. The study offers new insights into volatility harvesting, introducing a dynamic sizing framework that adapts to shifting market regimes. It also contributes practical guidance for constructing short-dated option strategies that are robust across market environments. These findings have direct applications for institutional investors seeking to enhance portfolio efficiency through systematic exposure to volatility premia.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Convex Optimization Over Risk-Neutral Probabilities,"We consider a collection of derivatives that depend on the price of an underlying asset at expiration or maturity. The absence of arbitrage is equivalent to the existence of a risk-neutral probability distribution on the price; in particular, any risk neutral distribution can be interpreted as a certificate establishing that no arbitrage exists. We are interested in the case when there are multiple risk-neutral probabilities. We describe a number of convex optimization problems over the convex set of risk neutral price probabilities. These include computation of bounds on the cumulative distribution, VaR, CVaR, and other quantities, over the set of risk-neutral probabilities. After discretizing the underlying price, these problems become finite dimensional convex or quasiconvex optimization problems, and therefore are tractable. We illustrate our approach using real options and futures pricing data for the S&P 500 index and Bitcoin.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
Realised Volatility Forecasting: Machine Learning via Financial Word Embedding,"This study develops a financial word embedding using 15 years of business news. Our results show that this specialised language model produces more accurate results than general word embeddings, based on a financial benchmark we established. As an application, we incorporate this word embedding into a simple machine learning model to enhance the HAR model for forecasting realised volatility. This approach statistically and economically outperforms established econometric models. Using an explainable AI method, we also identify key phrases in business news that contribute significantly to volatility, offering insights into language patterns tied to market dynamics.","cat:q-fin.CP AND (cryptocurrency OR bitcoin OR blockchain OR ""market dynamics"")",0
MC-curves and aesthetic measurements for pseudospiral curve segments,"This article studies families of curves with monotonic curvature function (MC-curves) and their applications in geometric modelling and aesthetic design. Aesthetic analysis and assessment of the structure and plastic qualities of pseudospirals, which are curves with monotonic curvature function, are conducted for the first time in the field of geometric modelling from the position of technical aesthetics laws. The example of car body surface modelling with the use of aesthetics splines is given.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A mathematical design and evaluation of Bernstein-Bezier curves' shape features using the laws of technical aesthetics,"We present some notes on the definition of mathematical design as well as on the methods of mathematical modeling which are used in the process of the artistic design of the environment and its components. For the first time in the field of geometric modeling, we perform an aesthetic analysis of planar Bernstein-Bezier curves from the standpoint of the laws of technical aesthetics. The shape features of the curve segments' geometry were evaluated using the following criteria: conciseness-integrity, expressiveness, proportional consistency, compositional balance, structural organization, imagery, rationality, dynamism, scale, flexibility and harmony. In the non-Russian literature, Bernstein-Bezier curves using a monotonic curvature function (i.e., a class A Bezier curve) are considered to be fair (i.e., beautiful) curves, but their aesthetic analysis has never been performed. The aesthetic analysis performed by the authors of this work means that this is no longer the case. To confirm the conclusions of the authors' research, a survey of the ""aesthetic appropriateness"" of certain Bernstein-Bezier curve segments was conducted among 240 children, aged 14-17. The results of this survey have shown themselves to be in full accordance with the authors' results.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Medium. Permeation: SARS-COV-2 Painting Creation by Generative Model,"Airborne particles are the medium for SARS-CoV-2 to invade the human body. Light also reflects through suspended particles in the air, allowing people to see a colorful world. Impressionism is the most prominent art school that explores the spectrum of color created through color reflection of light. We find similarities of color structure and color stacking in the Impressionist paintings and the illustrations of the novel coronavirus by artists around the world. With computerized data analysis through the main tones, the way of color layout, and the way of color stacking in the paintings of the Impressionists, we train computers to draw the novel coronavirus in an Impressionist style using a Generative Adversarial Network to create our artwork ""Medium. Permeation"". This artwork is composed of 196 randomly generated viral pictures arranged in a 14 by 14 matrix to form a large-scale painting. In addition, we have developed an extended work: Gradual Change, which is presented as video art. We use Graph Neural Network to present 196 paintings of the new coronavirus to the audience one by one in a gradual manner. In front of LED TV screen, audience will find 196 virus paintings whose colors will change continuously. This large video painting symbolizes that worldwide 196 countries have been invaded by the epidemic, and every nation continuously pops up mutant viruses. The speed of vaccine development cannot keep up with the speed of virus mutation. This is also the first generative art in the world based on the common features and a metaphorical symbiosis between Impressionist art and the novel coronavirus. This work warns us of the unprecedented challenges posed by the SARS-CoV-2, implying that the world should not ignore the invisible enemy who uses air as a medium.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Samila: A Generative Art Generator,"Generative art merges creativity with computation, using algorithms to produce aesthetic works. This paper introduces Samila, a Python-based generative art library that employs mathematical functions and randomness to create visually compelling compositions. The system allows users to control the generation process through random seeds, function selections, and projection modes, enabling the exploration of randomness and artistic expression. By adjusting these parameters, artists can create diverse compositions that reflect intentionality and unpredictability. We demonstrate that Samila's outputs are uniquely determined by two random generation seeds, making regeneration nearly impossible without both. Additionally, altering the point generation functions while preserving the seed produces artworks with distinct graphical characteristics, forming a visual family. Samila serves as both a creative tool for artists and an educational resource for teaching mathematical and programming concepts. It also provides a platform for research in generative design and computational aesthetics. Future developments could include AI-driven generation and aesthetic evaluation metrics to enhance creative control and accessibility.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Deep Learning of Individual Aesthetics,"Accurate evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research. Prior work has tended to focus on feature measures of the artefact, such as symmetry, complexity and coherence. However, research models from Psychology suggest that human aesthetic experiences encapsulate factors beyond the artefact, making accurate computational models very difficult to design. The interactive genetic algorithm (IGA) circumvents the problem through human-in-the-loop, subjective evaluation of aesthetics, but is limited due to user fatigue and small population sizes. In this paper we look at how recent advances in deep learning can assist in automating personal aesthetic judgement. Using a leading artist's computer art dataset, we investigate the relationship between image measures, such as complexity, and human aesthetic evaluation. We use dimension reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in a generative system. Convolutional Neural Networks trained on the artist's prior aesthetic evaluations are used to suggest new possibilities similar or between known high quality genotype-phenotype mappings. We integrate this classification and discovery system into a software tool for evolving complex generative art and design.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Visual Indeterminacy in GAN Art,"This paper explores visual indeterminacy as a description for artwork created with Generative Adversarial Networks (GANs). Visual indeterminacy describes images which appear to depict real scenes, but, on closer examination, defy coherent spatial interpretation. GAN models seem to be predisposed to producing indeterminate images, and indeterminacy is a key feature of much modern representational art, as well as most GAN art. It is hypothesized that indeterminacy is a consequence of a powerful-but-imperfect image synthesis model that must combine general classes of objects, scenes, and textures.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Perceptual Aesthetics Measure for 3D Shapes,"While the problem of image aesthetics has been well explored, the study of 3D shape aesthetics has focused on specific manually defined features. In this paper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel data and without manually-crafted features by leveraging the strength of deep learning. We collect data from humans on their aesthetics preferences for various 3D shape classes. We take a deep convolutional 3D shape ranking approach to compute a measure that gives an aesthetics score for a 3D shape. We demonstrate our approach with various types of shapes and for applications such as aesthetics-based visualization, search, and scene composition.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Creative NFT-Copyrighted AR Face Mask Authoring Using Unity3D Editor,"In this paper, we extend well-designed 3D face masks into AR face masks and demonstrate the possibility of transforming this into an NFT-copyrighted AR face mask that helps authenticate the ownership of the AR mask user so as to improve creative control, brand identification, and ID protection. The output of this project will not only potentially validate the value of the NFT technology but also explore how to combine the NFT technology with AR technology so as to be applied to e-commerce and e-business aspects of the multimedia industry.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D View Optimization for Improving Image Aesthetics,"Achieving aesthetically pleasing photography necessitates attention to multiple factors, including composition and capture conditions, which pose challenges to novices. Prior research has explored the enhancement of photo aesthetics post-capture through 2D manipulation techniques; however, these approaches offer limited search space for aesthetics. We introduce a pioneering method that employs 3D operations to simulate the conditions at the moment of capture retrospectively. Our approach extrapolates the input image and then reconstructs the 3D scene from the extrapolated image, followed by an optimization to identify camera parameters and image aspect ratios that yield the best 3D view with enhanced aesthetics. Comparative qualitative and quantitative assessments reveal that our method surpasses traditional 2D editing techniques with superior aesthetics.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Let's Resonate! How to Elicit Improvisation and Letting Go in Interactive Digital Art,"Participatory art allows for the spectator to be a participant or a viewer able to engage actively with interactive art. Real-time technologies offer new ways to create participative artworks. We hereby investigate how to engage participation through movement in interactive digital art, and what this engagement can awaken, focusing on the ways to elicit improvisation and letting go. We analyze two Virtual Reality installations, ''InterACTE'' and ''Eve, dance is an unplaceable place,'' involving body movement, dance, creativity and the presence of an observing audience. We evaluate the premises, the setup, and the feedback of the spectators in the two installations. We propose a model following three different perspectives of resonance: 1. Inter Resonance between Spectator and Artwork, which involves curiosity, imitation, playfulness and improvisation. 2. Inner Resonance of Spectator him/herself, where embodiment and creativity contribute to the sense of being present and letting go. 3. Collective Resonance between Spectator/Artwork and Audience, which is stimulated by curiosity, and triggers motor contagion, engagement and gathering. The two analyzed examples seek to awaken open-minded communicative possibilities through the use of interactive digital artworks. Moreover, the need to recognize and develop the idea of resonance becomes increasingly important in this time of urgency to communicate, understand and support collectivity.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning Aesthetic Layouts via Visual Guidance,"We explore computational approaches for visual guidance to aid in creating aesthetically pleasing art and graphic design. Our work complements and builds on previous work that developed models for how humans look at images. Our approach comprises three steps. First, we collected a dataset of art masterpieces and labeled the visual fixations with state-of-art vision models. Second, we clustered the visual guidance templates of the art masterpieces with unsupervised learning. Third, we developed a pipeline using generative adversarial networks to learn the principles of visual guidance and that can produce aesthetically pleasing layouts. We show that the aesthetic visual guidance principles can be learned and integrated into a high-dimensional model and can be queried by the features of graphic elements. We evaluate our approach by generating layouts on various drawings and graphic designs. Moreover, our model considers the color and structure of graphic elements when generating layouts. Consequently, we believe our tool, which generates multiple aesthetic layout options in seconds, can help artists create beautiful art and graphic designs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Digital Life Project: Autonomous 3D Characters with Social Intelligence,"In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories,"Human aesthetic preferences for 3D shapes are central to industrial design, virtual reality, and consumer product development. However, most computational models of 3D aesthetics lack empirical grounding in large-scale human judgments, limiting their practical relevance. We present a large-scale study of human preferences. We collected 22,301 pairwise comparisons across five object categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon Mechanical Turk. Building on a previously published dataset~\cite{dev2020learning}, we introduce new non-linear modeling and cross-category analysis to uncover the geometric drivers of aesthetic preference. We apply the Bradley-Terry model to infer latent aesthetic scores and use Random Forests with SHAP analysis to identify and interpret the most influential geometric features (e.g., symmetry, curvature, compactness). Our cross-category analysis reveals both universal principles and domain-specific trends in aesthetic preferences. We focus on human interpretable geometric features to ensure model transparency and actionable design insights, rather than relying on black-box deep learning approaches. Our findings bridge computational aesthetics and cognitive science, providing practical guidance for designers and a publicly available dataset to support reproducibility. This work advances the understanding of 3D shape aesthetics through a human-centric, data-driven framework.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Analyzing symmetry and symmetry breaking by computational aesthetic measures,"We study creating and analyzing symmetry and broken symmetry in digital art. Our focus is not so much on computer-generating artistic images, but rather on analyzing concepts and templates for incorporating symmetry and symmetry breaking into the creation process. Taking as a starting point patterns generated algorithmically by emulating the collective feeding behavior of sand-bubbler crabs, all four types of two-dimensional symmetry are used as isometric maps. Apart from a geometric interpretation of symmetry, we also consider color as an object of symmetric transformations. Color symmetry is realized as a color permutation consistent with the isometries. Moreover, we analyze the abilities of computational aesthetic measures to serve as a metric that reflects design parameters, i.e. the type of symmetry and the degree of symmetry breaking.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations,"We developed and validated a rating scale to assess the aesthetic pleasure (or beauty) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, ""enjoyable,"" ""likable,"" ""pleasing,"" ""nice,"" and ""appealing."" Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Can AI Recognize the Style of Art? Analyzing Aesthetics through the Lens of Style Transfer,"This study investigates how artificial intelligence (AI) recognizes style through style transfer-an AI technique that generates a new image by applying the style of one image to another. Despite the considerable interest that style transfer has garnered among researchers, most efforts have focused on enhancing the quality of output images through advanced AI algorithms. In this paper, we approach style transfer from an aesthetic perspective, thereby bridging AI techniques and aesthetics. We analyze two style transfer algorithms: one based on convolutional neural networks (CNNs) and the other utilizing recent Transformer models. By comparing the images produced by each, we explore the elements that constitute the style of artworks through an aesthetic analysis of the style transfer results. We then elucidate the limitations of current style transfer techniques. Based on these limitations, we propose potential directions for future research on style transfer techniques.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances,"Digital storytelling, as an art form, has struggled with cost-quality balance. The emergence of AI-generated Content (AIGC) is considered as a potential solution for efficient digital storytelling production. However, the specific form, effects, and impacts of this fusion remain unclear, leaving the boundaries of AIGC combined with storytelling undefined. This work explores the current integration state of AIGC and digital storytelling, investigates the artistic value of their fusion in a sample project, and addresses common issues through interviews. Through our study, we conclude that AIGC, while proficient in image creation, voiceover production, and music composition, falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present, especially in complex character animations, facial expressions, and sound effects. The research objective is to increase public awareness of the current state, limitations, and challenges arising from combining AIGC and digital storytelling.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Chaotic Art: Quantum Representation and Manipulation of Color,"Due to its unique computing principles, quantum computing technology will profoundly change the spectacle of color art. Focusing on experimental exploration of color qubit representation, color channel processing, and color image generation via quantum computing, this article proposes a new technical path for color computing in quantum computing environment, by which digital color is represented, operated, and measured in quantum bits, and then restored for classical computers as computing results. This method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in Qiskit and IBM Q. By building a bridge between classical chromatics and quantum graphics, quantum computers can be used for information visualization, image processing, and more color computing tasks. Furthermore, quantum computing can be expected to facilitate new color theories and artistic concepts.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
AKN_Regie: a bridge between digital and performing arts,"In parallel with the dissemination of information technology, we note the persistence of frontiers within creative practices, in particular between the digital arts and the performing arts. Crossings of these frontiers brought to light the need for a common appropriation of digital issues. As a result of this appropriation, the AvatarStaging platform and its software dimension AKN_Regie will be described in their use to direct avatars on a mixed theatre stage. Developed with the Blueprint visual language within Epic Games' Unreal Engine, AKN_Regie offers a user interface accessible to non-programming artists. This feature will be used to describe two perspectives of appropriation of the tool: the Plugin perspective for these users and the Blueprint perspective for programming artists who want to improve the tool. These two perspectives are then completed by a C++ perspective that aligns AKN_Regie with the language with which the engine itself is programmed. The circulations between these three perspectives are finally studied by drawing on work on the ecology of collective intelligence.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
null2: Boundary-Dissolving Bodies and Architecture towards Digital Nature,"This paper presents a case study of the thematic pavilion null2 at Expo 2025 Osaka-Kansai, contrasting with the static Jomon motifs of Taro Okamoto's Tower of the Sun from Expo 1970. The study discusses Yayoi-inspired mirror motifs and dynamically transforming interactive spatial configuration of null2, where visitors become integrated as experiential content. The shift from static representation to a new ontological and aesthetic model, characterized by the visitor's body merging in real-time with architectural space at installation scale, is analyzed. Referencing the philosophical context of Expo 1970 theme 'Progress and Harmony for Mankind,' this research reconsiders the worldview articulated by null2 in Expo 2025, in which computation is naturalized and ubiquitous, through its intersection with Eastern philosophical traditions. It investigates how immersive experiences within the pavilion, grounded in the philosophical framework of Digital Nature, reinterpret traditional spatial and structural motifs of the tea room, positioning them within contemporary digital art discourse. The aim is to contextualize and document null2 as an important contemporary case study from Expo practices, considering the historical and social background in Japan from the 19th to 21st century, during which world expositions served as pivotal points for the birth of modern Japanese concept of 'fine art,' symbolic milestones of economic development, and key moments in urban and media culture formation. Furthermore, this paper academically organizes architectural techniques, computer graphics methodologies, media art practices, and theoretical backgrounds utilized in null2, highlighting the scholarly significance of preserving these as an archival document for future generations.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Saving temporary exhibitions in virtual environments: the Digital Renaissance of Ulisse Aldrovandi -- acquisition and digitisation of cultural heritage objects,"As per the objectives of Project CHANGES, particularly its thematic sub-project on the use of virtual technologies for museums and art collections, our goal was to obtain a digital twin of the temporary exhibition on Ulisse Aldrovandi called ""The Other Renaissance"", and make it accessible to users online. After a preliminary study of the exhibition, focussing on acquisition constraints and related solutions, we proceeded with the digital twin creation by acquiring, processing, modelling, optimising, exporting, and metadating the exhibition. We made hybrid use of two acquisition techniques to create new digital cultural heritage objects and environments, and we used open technologies, formats, and protocols to make available the final digital product. Here, we describe the process of collecting and curating bibliographical exhibition (meta)data and the beginning of the digital twin creation to foster its findability, accessibility, interoperability, and reusability. The creation of the digital twin is currently ongoing.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generating Pixel Art Character Sprites using GANs,"Iterating on creating pixel art character sprite sheets is essential to the game development process. However, it can take a lot of effort until the final versions containing different poses and animation clips are achieved. This paper investigates using conditional generative adversarial networks to aid the designers in creating such sprite sheets. We propose an architecture based on Pix2Pix to generate images of characters facing a target side (e.g., right) given sprites of them in a source pose (e.g., front). Experiments with small pixel art datasets yielded promising results, resulting in models with varying degrees of generalization, sometimes capable of generating images very close to the ground truth. We analyze the results through visual inspection and quantitatively with FID.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Variational Formulation of the Log-Aesthetic Surface and Development of Discrete Surface Filters,"The log-aesthetic curves include the logarithmic (equiangular) spiral, clothoid, and involute curves. Although most of them are expressed only by an integral form of the tangent vector, it is possible to interactively generate and deform them and they are expected to be utilized for practical use of industrial and graphical design. The discrete log-aesthetic filter based on the formulation of the log-aesthetic curve has successfully been introduced not to impose strong constraints on the designer's activity, to let him/her design freely and to embed the properties of the log-aesthetic curves for complicated ones with both increasing and decreasing curvature. In this paper, in order to define the log-aesthetic surface and develop surface filters based on its formulation, at first we reformulate the log-aesthetic curve with variational principle. Then we propose several new functionals to be minimized for free-form surfaces and define the log-aesthetic surface. Furthermore we propose new discrete surface filters based on the log-aesthetic surface formulation","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Interactive $G^1$ and $G^2$ Hermite Interpolation Using Extended Log-aesthetic Curves,"In the field of aesthetic design, log-aesthetic curves have a significant role to meet the high industrial requirements. In this paper, we propose a new interactive $G^1$ Hermite interpolation method based on the algorithm of Yoshida et al. with a minor boundary condition. In this novel approach, we compute an extended log-aesthetic curve segment that may include inflection point (S-shaped curve) or cusp. The curve segment is defined by its endpoints, a tangent vector at the first point, and a tangent direction at the second point. The algorithm also determines the shape parameter of the log-aesthetic curve based on the length of the first tangent that provides control over the curvature of the first point and makes the method capable of joining log-aesthetic curve segments with $G^2$ continuity.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2,"This paper introduces LAV (Latent Audio-Visual), a system that integrates EnCodec's neural audio compression with StyleGAN2's generative capabilities to produce visually dynamic outputs driven by pre-recorded audio. Unlike previous works that rely on explicit feature mappings, LAV uses EnCodec embeddings as latent representations, directly transformed into StyleGAN2's style latent space via randomly initialized linear mapping. This approach preserves semantic richness in the transformation, enabling nuanced and semantically coherent audio-visual translations. The framework demonstrates the potential of using pretrained audio compression models for artistic and computational applications.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Evaluating Machine Learning Approaches for ASCII Art Generation,"Generating structured ASCII art using computational techniques demands a careful interplay between aesthetic representation and computational precision, requiring models that can effectively translate visual information into symbolic text characters. Although Convolutional Neural Networks (CNNs) have shown promise in this domain, the comparative performance of deep learning architectures and classical machine learning methods remains unexplored. This paper explores the application of contemporary ML and DL methods to generate structured ASCII art, focusing on three key criteria: fidelity, character classification accuracy, and output quality. We investigate deep learning architectures, including Multilayer Perceptrons (MLPs), ResNet, and MobileNetV2, alongside classical approaches such as Random Forests, Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN), trained on an augmented synthetic dataset of ASCII characters. Our results show that complex neural network architectures often fall short in producing high-quality ASCII art, whereas classical machine learning classifiers, despite their simplicity, achieve performance similar to CNNs. Our findings highlight the strength of classical methods in bridging model simplicity with output quality, offering new insights into ASCII art synthesis and machine learning on image data with low dimensionality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Optimizing for Aesthetically Pleasing Quadrotor Camera Motion,"In this paper we first contribute a large scale online study (N=400) to better understand aesthetic perception of aerial video. The results indicate that it is paramount to optimize smoothness of trajectories across all keyframes. However, for experts timing control remains an essential tool. Satisfying this dual goal is technically challenging because it requires giving up desirable properties in the optimization formulation. Second, informed by this study we propose a method that optimizes positional and temporal reference fit jointly. This allows to generate globally smooth trajectories, while retaining user control over reference timings. The formulation is posed as a variable, infinite horizon, contour-following algorithm. Finally, a comparative lab study indicates that our optimization scheme outperforms the state-of-the-art in terms of perceived usability and preference of resulting videos. For novices our method produces smoother and better looking results and also experts benefit from generated timings.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Digital Restoration of Da Vinci's Sketches,"A sketch, found in one of Leonardo da Vinci's notebooks and covered by the written notes of this genius, has been recently restored. The restoration reveals a possible self-portrait of the artist, drawn when he was young. Here, we discuss the discovery of this self-portrait and the procedure used for restoration. Actually, this is a restoration performed on the digital image of the sketch, a procedure that can easily extended and applied to ancient documents for studies of art and palaeography.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Fairing of planar curves to log-aesthetic curves,"We present an algorithm to fair a given planar curve by a log-aesthetic curve (LAC). We show how a general LAC segment can be uniquely characterized by seven parameters and present a method of parametric approximation based on this fact. This work aims to provide tools to be used in reverse engineering for computer aided geometric design. Finally, we show an example of usage by applying this algorithm to the point data obtained from 3D scanning a car's roof.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Developing Creative AI to Generate Sculptural Objects,"We explore the intersection of human and machine creativity by generating sculptural objects through machine learning. This research raises questions about both the technical details of automatic art generation and the interaction between AI and people, as both artists and the audience of art. We introduce two algorithms for generating 3D point clouds and then discuss their actualization as sculpture and incorporation into a holistic art installation. Specifically, the Amalgamated DeepDream (ADD) algorithm solves the sparsity problem caused by the naive DeepDream-inspired approach and generates creative and printable point clouds. The Partitioned DeepDream (PDD) algorithm further allows us to explore more diverse 3D object creation by combining point cloud clustering algorithms and ADD.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Curvature of planar aesthetic curves,"Farin proposed a method for designing Bezier curves with monotonic curvature and torsion. Such curves are relevant in design due to their aesthetic shape. The method relies on applying a matrix M to the first edge of the control polygon of the curve in order to obtain by iteration the remaining edges. With this method, sufficient conditions on the matrix $M$ are provided, which lead to the definition of Class A curves, generalising a previous result by Mineur et al for plane curves with M being the composition of a dilatation and a rotation. However, Cao and Wang have shown counterexamples for such conditions. In this paper, we revisit Farin's idea of using the subdivision algorithm to relate the curvature at every point of the curve to the curvature at the initial point in order to produce a closed formula for the curvature of planar curves in terms of the eigenvalues of the matrix M and the seed vector for the curve, the first edge of the control polygon. Moreover, we give new conditions in order to produce planar curves with monotonic curvature. The main difference is that we do not require our conditions on the eigenvalues to be preserved under subdivision of the curve. This facilitates giving a unified derivation of the existing results and obtain more general results in the planar case.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D Generative Adversarial Network,"Automatic mesh-based shape generation is of great interest across a wide range of disciplines, from industrial design to gaming, computer graphics and various other forms of digital art. While most traditional methods focus on primitive based model generation, advances in deep learning made it possible to learn 3-dimensional geometric shape representations in an end-to-end manner. However, most current deep learning based frameworks focus on the representation and generation of voxel and point-cloud based shapes, making it not directly applicable to design and graphics communities. This study addresses the needs for automatic generation of mesh-based geometries, and propose a novel framework that utilizes signed distance function representation that generates detail preserving three-dimensional surface mesh by a deep learning based approach.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Analysis and Compilation of Normal Map Generation Techniques for Pixel Art,"Pixel art is a popular artistic style adopted in the gaming industry, and nowadays, it is often accompanied by modern rendering techniques. One example is dynamic lighting for the game sprites, for which normal mapping defines how the light interacts with the material represented by each pixel. Although there are different methods to generate normal maps for 3D games, applying them for pixel art may not yield correct results due to the style specificities. Therefore, this work compiles different normal map generation methods and study their applicability for pixel art, reducing the scarcity of existing material on the techniques and contributing to a qualitative analysis of the behavior of these methods in different case studies.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
AKN_Regie: bridging digital and performing arts,"AvatarStaging framework consists in directing avatars on a mixed theatrical stage, enabling a co-presence between the materiality of the physical actor and the virtuality of avatars controlled in real time by motion capture or specific animation players. It led to the implementation of the AKN_Regie authoring tool, programmed with the Blueprint visual language as a plugin for the Unreal Engine (UE) video game engine. The paper describes AKN_Regie main functionalities as a tool for non-programmer theatrical people. It gives insights of its implementation in the Blueprint visual language specific to UE. It details how the tool evolved along with its use in around ten theater productions. A circulation process between a nonprogramming point of view on AKN_Regie called Plugin Perspective and a programming acculturation to its development called Blueprint Perspective is discussed. Finally, a C++ Perspective is suggested to enhance the cultural appropriation of technological issues, bridging the gap between performing arts deeply involved in human materiality and avatars inviting to discover new worlds.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Line Art Correlation Matching Feature Transfer Network for Automatic Animation Colorization,"Automatic animation line art colorization is a challenging computer vision problem, since the information of the line art is highly sparse and abstracted and there exists a strict requirement for the color and style consistency between frames. Recently, a lot of Generative Adversarial Network (GAN) based image-to-image translation methods for single line art colorization have emerged. They can generate perceptually appealing results conditioned on line art images. However, these methods can not be adopted for the purpose of animation colorization because there is a lack of consideration of the in-between frame consistency. Existing methods simply input the previous colored frame as a reference to color the next line art, which will mislead the colorization due to the spatial misalignment of the previous colored frame and the next line art especially at positions where apparent changes happen. To address these challenges, we design a kind of correlation matching feature transfer model (called CMFT) to align the colored reference feature in a learnable way and integrate the model into an U-Net based generator in a coarse-to-fine manner. This enables the generator to transfer the layer-wise synchronized features from the deep semantic code to the content progressively. Extension evaluation shows that CMFT model can effectively improve the in-between consistency and the quality of colored frames especially when the motion is intense and diverse.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Audio-guided Album Cover Art Generation with Genetic Algorithms,"Over 60,000 songs are released on Spotify every day, and the competition for the listener's attention is immense. In that regard, the importance of captivating and inviting cover art cannot be underestimated, because it is deeply entangled with a song's character and the artist's identity, and remains one of the most important gateways to lead people to discover music. However, designing cover art is a highly creative, lengthy and sometimes expensive process that can be daunting, especially for non-professional artists. For this reason, we propose a novel deep-learning framework to generate cover art guided by audio features. Inspired by VQGAN-CLIP, our approach is highly flexible because individual components can easily be replaced without the need for any retraining. This paper outlines the architectural details of our models and discusses the optimization challenges that emerge from them. More specifically, we will exploit genetic algorithms to overcome bad local minima and adversarial examples. We find that our framework can generate suitable cover art for most genres, and that the visual features adapt themselves to audio feature changes. Given these results, we believe that our framework paves the road for extensions and more advanced applications in audio-guided visual generation tasks.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Full-body High-resolution Anime Generation with Progressive Structure-conditional Generative Adversarial Networks,"We propose Progressive Structure-conditional Generative Adversarial Networks (PSGAN), a new framework that can generate full-body and high-resolution character images based on structural information. Recent progress in generative adversarial networks with progressive training has made it possible to generate high-resolution images. However, existing approaches have limitations in achieving both high image quality and structural consistency at the same time. Our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training. In this paper, we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024x1024 based on target pose sequences. We also create a novel dataset containing full-body 1024x1024 high-resolution images and exact 2D pose keypoints using Unity 3D Avatar models.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CharNeRF: 3D Character Generation from Concept Art,"3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Training on Art Composition Attributes to Influence CycleGAN Art Generation,"I consider how to influence CycleGAN, image-to-image translation, by using additional constraints from a neural network trained on art composition attributes. I show how I trained the the Art Composition Attributes Network (ACAN) by incorporating domain knowledge based on the rules of art evaluation and the result of applying each art composition attribute to apple2orange image translation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Designing color symmetry in stigmergic art,"Color symmetry is an extension of symmetry imposed by isometric transformations and means that the colors of geometrical objects are assigned according to the symmetry properties of the objects. A color symmetry permutes the coloring of the objects consistently with their symmetry group. We apply this concept to bio-inspired generative art. Therefore, we interpret the geometrical objects as motifs that may repeat themselves with a symmetry-consistent coloring. The motifs are obtained by design principles from stigmergy. We discuss a design procedure and present visual results.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Exploring Fungal Morphology Simulation and Dynamic Light Containment from a Graphics Generation Perspective,"Fungal simulation and control are considered crucial techniques in Bio-Art creation. However, coding algorithms for reliable fungal simulations have posed significant challenges for artists. This study equates fungal morphology simulation to a two-dimensional graphic time-series generation problem. We propose a zero-coding, neural network-driven cellular automaton. Fungal spread patterns are learned through an image segmentation model and a time-series prediction model, which then supervise the training of neural network cells, enabling them to replicate real-world spreading behaviors. We further implemented dynamic containment of fungal boundaries with lasers. Synchronized with the automaton, the fungus successfully spreads into pre-designed complex shapes in reality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"Universal software platform for visualizing class F curves, log-aesthetic curves and development of applied CAD systems","This article describes the capabilities of a universal software platform for visualizing class F curves and developing specialized applications for CAD systems based on Microsoft Excel VBA, the software complex FairCurveModeler, and computer algebra systems. Additionally, it demonstrates the use of a software platform for visualizing functional and log-aesthetic curves integrated with CAD Fusion360. The value of the curves is evident in visualizing the qualitative geometry of the product shape in industrial design. Moreover, the requirements for the characteristics of class F curves are emphasized to form a visual purity of shape in industrial design and to provide a positive emotional perception of the visual image of the product by a person.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Creativity of Text-to-Image Generation,"Text-guided synthesis of images has made a giant leap towards becoming a mainstream phenomenon. With text-to-image generation systems, anybody can create digital images and artworks. This provokes the question of whether text-to-image generation is creative. This paper expounds on the nature of human creativity involved in text-to-image art (so-called ""AI art"") with a specific focus on the practice of prompt engineering. The paper argues that the current product-centered view of creativity falls short in the context of text-to-image generation. A case exemplifying this shortcoming is provided and the importance of online communities for the creative ecosystem of text-to-image art is highlighted. The paper provides a high-level summary of this online ecosystem drawing on Rhodes' conceptual four P model of creativity. Challenges for evaluating the creativity of text-to-image generation and opportunities for research on text-to-image generation in the field of Human-Computer Interaction (HCI) are discussed.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Interactive digital storytelling: bringing cultural heritage in a classroom,"Interactive digital storytelling becomes a popular choice for information presentation in many fields. Its application spans from media industry and business information visualization, through digital cultural heritage, serious games, education, to contemporary theater and visual arts. The benefits of this form of multimedia presentation in education are generally recognized, and several studies exploring and supporting the opinion are conducted. In addition to discussing the benefits, we wanted to address the challenges in introducing interactive digital storytelling and serious games in the classroom. The challenge of inherent ambiguity of edutainment, due to opposing features of education and entertainment is augmented with different viewpoints of multidisciplinary team members. We specifically address the opposing views on artistic liberty, at one side, and technical constraints and historic facts, on the other side. In this paper we present the first findings related to these questions and to initiate furthering discussions in this area.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MetaHead: An Engine to Create Realistic Digital Head,"Collecting and labeling training data is one important step for learning-based methods because the process is time-consuming and biased. For face analysis tasks, although some generative models can be used to generate face data, they can only achieve a subset of generation diversity, reconstruction accuracy, 3D consistency, high-fidelity visual quality, and easy editability. One recent related work is the graphics-based generative method, but it can only render low realism head with high computation cost. In this paper, we propose MetaHead, a unified and full-featured controllable digital head engine, which consists of a controllable head radiance field(MetaHead-F) to super-realistically generate or reconstruct view-consistent 3D controllable digital heads and a generic top-down image generation framework LabelHead to generate digital heads consistent with the given customizable feature labels. Experiments validate that our controllable digital head engine achieves the state-of-the-art generation visual quality and reconstruction accuracy. Moreover, the generated labeled data can assist real training data and significantly surpass the labeled data generated by graphics-based methods in terms of training effect.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Continuation of Famous Art with AI: A Conditional Adversarial Network Inpainting Approach,"Much of the state-of-the-art in image synthesis inspired by real artwork are either entirely generative by filtered random noise or inspired by the transfer of style. This work explores the application of image inpainting to continue famous artworks and produce generative art with a Conditional GAN. During the training stage of the process, the borders of images are cropped, leaving only the centre. An inpainting GAN is then tasked with learning to reconstruct the original image from the centre crop by way of minimising both adversarial and absolute difference losses, which are analysed by both their Frchet Inception Distances and manual observations which are presented. Once the network is trained, images are then resized rather than cropped and presented as input to the generator. Following the learning process, the generator then creates new images by continuing from the edges of the original piece. Three experiments are performed with datasets of 4766 landscape paintings (impressionism and romanticism), 1167 Ukiyo-e works from the Japanese Edo period, and 4968 abstract artworks. Results show that geometry and texture (including canvas and paint) as well as scenery such as sky, clouds, water, land (including hills and mountains), grass, and flowers are implemented by the generator when extending real artworks. In the Ukiyo-e experiments, it was observed that features such as written text were generated even in cases where the original image did not have any, due to the presence of an unpainted border within the input image.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
State of the Art in Dense Monocular Non-Rigid 3D Reconstruction,"3D reconstruction of deformable (or non-rigid) scenes from a set of monocular 2D image observations is a long-standing and actively researched area of computer vision and graphics. It is an ill-posed inverse problem, since -- without additional prior assumptions -- it permits infinitely many solutions leading to accurate projection to the input 2D images. Non-rigid reconstruction is a foundational building block for downstream applications like robotics, AR/VR, or visual content creation. The key advantage of using monocular cameras is their omnipresence and availability to the end users as well as their ease of use compared to more sophisticated camera set-ups such as stereo or multi-view systems. This survey focuses on state-of-the-art methods for dense non-rigid 3D reconstruction of various deformable objects and composite scenes from monocular videos or sets of monocular views. It reviews the fundamentals of 3D reconstruction and deformation modeling from 2D image observations. We then start from general methods -- that handle arbitrary scenes and make only a few prior assumptions -- and proceed towards techniques making stronger assumptions about the observed objects and types of deformations (e.g. human faces, bodies, hands, and animals). A significant part of this STAR is also devoted to classification and a high-level comparison of the methods, as well as an overview of the datasets for training and evaluation of the discussed techniques. We conclude by discussing open challenges in the field and the social aspects associated with the usage of the reviewed methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Methodology for intelligent injection point location based on geometric algorithms and discrete topologies for virtual digital twin environments,"This article presents an innovative methodology for locating injection points in injection-molded parts using intelligent models with geometric algorithms for discrete topologies. The first algorithm calculates the center of mass of the discrete model based on the center of mass of each triangular facet in the system, ensuring uniform molten plastic distribution during mold cavity filling. Two sub-algorithms intelligently evaluate the geometry and optimal injection point location. The first sub-algorithm generates a geometric matrix based on a two-dimensional nodal quadrature adapted to the part's bounding box. The second sub-algorithm projects the nodal matrix and associated circular areas orthogonally on the part's surface along the demolding direction. The optimal injection point location is determined by minimizing the distance to the center of mass from the first algorithm's result. This novel methodology has been validated through rheological simulations in six case studies with complex geometries. The results demonstrate uniform and homogeneous molten plastic distribution with minimal pressure loss during the filling phase. Importantly, this methodology does not require expert intervention, reducing time and costs associated with manual injection mold feed system design. It is also adaptable to various design environments and virtual twin systems, not tied to specific CAD software. The validated results surpass the state of the art, offering an agile alternative for digital twin applications in new product design environments, reducing dependence on experts, facilitating designer training, and ultimately cutting costs","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Seeding Diversity into AI Art,"This paper argues that generative art driven by conformance to a visual and/or semantic corpus lacks the necessary criteria to be considered creative. Among several issues identified in the literature, we focus on the fact that generative adversarial networks (GANs) that create a single image, in a vacuum, lack a concept of novelty regarding how their product differs from previously created ones. We envision that an algorithm that combines the novelty preservation mechanisms in evolutionary algorithms with the power of GANs can deliberately guide its creative process towards output that is both good and novel. In this paper, we use recent advances in image generation based on semantic prompts using OpenAI's CLIP model, interrupting the GAN's iterative process with short cycles of evolutionary divergent search. The results of evolution are then used to continue the GAN's iterative process; we hypothesise that this intervention will lead to more novel outputs. Testing our hypothesis using novelty search with local competition, a quality-diversity evolutionary algorithm that can increase visual diversity while maintaining quality in the form of adherence to the semantic prompt, we explore how different notions of visual diversity can affect both the process and the product of the algorithm. Results show that even a simplistic measure of visual diversity can help counter a drift towards similar images caused by the GAN. This first experiment opens a new direction for introducing higher intentionality and a more nuanced drive for GANs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
State of the Art on Neural Rendering,"Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Missing Data Imputation GAN for Character Sprite Generation,"Creating and updating pixel art character sprites with many frames spanning different animations and poses takes time and can quickly become repetitive. However, that can be partially automated to allow artists to focus on more creative tasks. In this work, we concentrate on creating pixel art character sprites in a target pose from images of them facing other three directions. We present a novel approach to character generation by framing the problem as a missing data imputation task. Our proposed generative adversarial networks model receives the images of a character in all available domains and produces the image of the missing pose. We evaluated our approach in the scenarios with one, two, and three missing images, achieving similar or better results to the state-of-the-art when more images are available. We also evaluate the impact of the proposed changes to the base architecture.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ScribGen: Generating Scribble Art Through Metaheuristics,"Art has long been a medium for individuals to engage with the world. Scribble art, a form of abstract visual expression, features spontaneous, gestural strokes made with pens or brushes. These dynamic and expressive compositions, created quickly and impulsively, reveal intricate patterns and hidden meanings upon closer inspection. While scribble art is often associated with spontaneous expression and experimentation, it can also be planned and intentional. Some artists use scribble techniques as a starting point for their creative process, exploring the possibilities of line, shape, and texture before refining their work into more polished compositions. From ancient cave paintings to modern abstract sketches and doodles, scribble art has evolved with civilizations, reflecting diverse artistic movements and cultural influences. This evolution highlights its universal appeal, transcending language and cultural barriers and connecting people through the shared experience of creating art.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art,"Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels and sounds, to trees and weapons. A large amount of game content is comprised of graphical assets, such as clouds, buildings or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs, and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 200 accepted papers. This paper explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Recent Trends in 3D Reconstruction of General Non-Rigid Scenes,"Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem. This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is concluded with a discussion of the remaining limitations and open challenges.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Fractal Art Generation using GPUs,"Fractal image generation algorithms exhibit extreme parallelizability. Using general purpose graphics processing unit (GPU) programming to implement escape-time algorithms for Julia sets of functions,parallel methods generate visually attractive fractal images much faster than traditional methods. Vastly improved speeds are achieved using this method of computation, which allow real-time generation and display of images. A comparison is made between sequential and parallel implementations of the algorithm. An application created by the authors demonstrates using the increased speed to create dynamic imaging of fractals where the user may explore paths of parameter values corresponding to a given function's Mandelbrot set. Examples are given of artistic and mathematical insights gained by experiencing fractals interactively and from the ability to sample the parameter space quickly and comprehensively.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
State of the Art on Diffusion Models for Visual Computing,"The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SD-$$XL: Generating Low-Resolution Quantized Imagery via Score Distillation,"Low-resolution quantized imagery, such as pixel art, is seeing a revival in modern applications ranging from video game graphics to digital design and fabrication, where creativity is often bound by a limited palette of elemental units. Despite their growing popularity, the automated generation of quantized images from raw inputs remains a significant challenge, often necessitating intensive manual input. We introduce SD-$$XL, an approach for producing quantized images that employs score distillation sampling in conjunction with a differentiable image generator. Our method enables users to input a prompt and optionally an image for spatial conditioning, set any desired output size $H \times W$, and choose a palette of $n$ colors or elements. Each color corresponds to a distinct class for our generator, which operates on an $H \times W \times n$ tensor. We adopt a softmax approach, computing a convex sum of elements, thus rendering the process differentiable and amenable to backpropagation. We show that employing Gumbel-softmax reparameterization allows for crisp pixel art effects. Unique to our method is the ability to transform input images into low-resolution, quantized versions while retaining their key semantic features. Our experiments validate SD-$$XL's performance in creating visually pleasing and faithful representations, consistently outperforming the current state-of-the-art. Furthermore, we showcase SD-$$XL's practical utility in fabrication through its applications in interlocking brick mosaic, beading and embroidery design.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Enhancing Vehicular Networks with Generative AI: Opportunities and Challenges,"In the burgeoning field of intelligent transportation systems, the integration of Generative Artificial Intelligence (AI) into vehicular networks presents a transformative potential for the automotive industry. This paper explores the innovative applications of generative AI in enhancing communication protocols, optimizing traffic management, and bolstering security frameworks within vehicular networks. By examining current technologies and recent advancements, we identify key challenges such as scalability, real-time data processing, and security vulnerabilities that come with AI integration. Additionally, we propose novel applications and methodologies that leverage generative AI to simulate complex network scenarios, generate adaptive communication schemes, and enhance predictive capabilities for traffic conditions. This study not only reviews the state of the art but also highlights significant opportunities where generative AI can lead to groundbreaking improvements in vehicular network efficiency and safety. Through this comprehensive exploration, our findings aim to guide future research directions and foster a deeper understanding of generative AI's role in the next generation of vehicular technologies.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Towards Reliable Human Evaluations in Gesture Generation: Insights from a Community-Driven State-of-the-Art Benchmark,"We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models -- each trained by its original authors -- across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies -- enabling new evaluations without model reimplementation required -- alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Interactive Optimization of Generative Image Modeling using Sequential Subspace Search and Content-based Guidance,"Generative image modeling techniques such as GAN demonstrate highly convincing image generation result. However, user interaction is often necessary to obtain the desired results. Existing attempts add interactivity but require either tailored architectures or extra data. We present a human-in-the-optimization method that allows users to directly explore and search the latent vector space of generative image modeling. Our system provides multiple candidates by sampling the latent vector space, and the user selects the best blending weights within the subspace using multiple sliders. In addition, the user can express their intention through image editing tools. The system samples latent vectors based on inputs and presents new candidates to the user iteratively. An advantage of our formulation is that one can apply our method to arbitrary pre-trained model without developing specialized architecture or data. We demonstrate our method with various generative image modeling applications, and show superior performance in a comparative user study with prior art iGAN.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Potential UAV Landing Sites Detection through Digital Elevation Models Analysis,"In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs) potential landing site detection using terrain information through identification of flat areas, is presented. The algorithm utilizes digital elevation models (DEM) that represent the height distribution of an area. Flat areas which constitute appropriate landing zones for UAVs in normal or emergency situations result by thresholding the image gradient magnitude of the digital surface model (DSM). The proposed technique also uses connected components evaluation on the thresholded gradient image in order to discover connected regions of sufficient size for landing. Moreover, man-made structures and vegetation areas are detected and excluded from the potential landing sites. Quantitative performance evaluation of the proposed landing site detection algorithm in a number of areas on real world and synthetic datasets, accompanied by a comparison with a state-of-the-art algorithm, proves its efficiency and superiority.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans,"Large Language Model (LLM)-driven digital humans have sparked a series of recent studies on co-speech gesture generation systems. However, existing approaches struggle with real-time synthesis and long-text comprehension. This paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel multi-modal framework for real-time 3D gesture generation. Our method incorporates three modules: 1) a cross-modal attention mechanism to achieve precise temporal alignment between speech and gestures; 2) a long-context autoregressive model with a sliding window mechanism for effective sequence modeling; 3) a large-scale gesture matching system that constructs an atomic action library and enables real-time retrieval. Additionally, we develop a lightweight pipeline implemented in the Unreal Engine for experimentation. Our approach achieves real-time inference at 120 fps and maintains a per-sentence latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive subjective and objective evaluations on the ZEGGS, and BEAT datasets demonstrate that our model outperforms current state-of-the-art methods. TRiMM enhances the speed of co-speech gesture generation while ensuring gesture quality, enabling LLM-driven digital humans to respond to speech in real time and synthesize corresponding gestures. Our code is available at https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"The Bricklayer Ecosystem - Art, Math, and Code","This paper describes the Bricklayer Ecosystem - a freely-available online educational ecosystem created for people of all ages and coding backgrounds. Bricklayer is designed in accordance with a ""low-threshold infinite ceiling"" philosophy and has been successfully used to teach coding to primary school students, middle school students, university freshmen, and in-service secondary math teachers. Bricklayer programs are written in the functional programming language SML and, when executed, create 2D and 3D artifacts. These artifacts can be viewed using a variety of third-party tools such as LEGO Digital Designer (LDD), LDraw, Minecraft clients, Brickr, as well as STereoLithography viewers.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies,"We present a fully automatic system that can produce high-fidelity, photo-realistic 3D digital human heads with a consumer RGB-D selfie camera. The system only needs the user to take a short selfie RGB-D video while rotating his/her head, and can produce a high quality head reconstruction in less than 30 seconds. Our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the state-of-the-art. Specifically, given the input video a two-stage frame selection procedure is first employed to select a few high-quality frames for reconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM) fitting algorithm is applied to recover facial geometries from multiview RGB-D data, which takes advantages of a powerful 3DMM basis constructed with extensive data generation and perturbation. Our 3DMM has much larger expressive capacities than conventional 3DMM, allowing us to recover more accurate facial geometry using merely linear basis. For reflectance synthesis, we present a hybrid approach that combines parametric fitting and CNNs to synthesize high-resolution albedo/normal maps with realistic hair/pore/wrinkle details. Results show that our system can produce faithful 3D digital human faces with extremely realistic details. The main code and the newly constructed 3DMM basis is publicly available.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,"We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Deep Illumination: Approximating Dynamic Global Illumination with Generative Adversarial Network,"We present Deep Illumination, a novel machine learning technique for approximating global illumination (GI) in real-time applications using a Conditional Generative Adversarial Network. Our primary focus is on generating indirect illumination and soft shadows with offline rendering quality at interactive rates. Inspired from recent advancement in image-to-image translation problems using deep generative convolutional networks, we introduce a variant of this network that learns a mapping from Gbuffers (depth map, normal map, and diffuse map) and direct illumination to any global illumination solution. Our primary contribution is showing that a generative model can be used to learn a density estimation from screen space buffers to an advanced illumination model for a 3D environment. Once trained, our network can approximate global illumination for scene configurations it has never encountered before within the environment it was trained on. We evaluate Deep Illumination through a comparison with both a state of the art real-time GI technique (VXGI) and an offline rendering GI technique (path tracing). We show that our method produces effective GI approximations and is also computationally cheaper than existing GI techniques. Our technique has the potential to replace existing precomputed and screen-space techniques for producing global illumination effects in dynamic scenes with physically-based rendering quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition,"We introduce SynSE, a novel syntactically guided generative approach for Zero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined generative embedding spaces constrained within and across the involved modalities (visual, language). The inter-modal constraints are defined between action sequence embedding and embeddings of Parts of Speech (PoS) tagged words in the corresponding action description. We deploy SynSE for the task of skeleton-based action sequence recognition. Our design choices enable SynSE to generalize compositionally, i.e., recognize sequences whose action descriptions contain words not encountered during training. We also extend our approach to the more challenging Generalized Zero-Shot Learning (GZSL) problem via a confidence-based gating mechanism. We are the first to present zero-shot skeleton action recognition results on the large-scale NTU-60 and NTU-120 skeleton action datasets with multiple splits. Our results demonstrate SynSE's state of the art performance in both ZSL and GZSL settings compared to strong baselines on the NTU-60 and NTU-120 datasets. The code and pretrained models are available at https://github.com/skelemoa/synse-zsl","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MagicClay: Sculpting Meshes With Generative Neural Fields,"The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work. Triangular meshes, on the other hand, are the representation of choice for most geometry related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization. To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing. Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently. Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched. Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster. In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF. Using an implemented prototype, we demonstrate superior generated geometry compared to the state-of-the-art, and novel consistent control, allowing sequential prompt-based edits to the same mesh for the first time.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Text-to-Vector Generation with Neural Path Representation,"Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D Shape Generation with Grid-based Implicit Functions,"Previous approaches to generate shapes in a 3D setting train a GAN on the latent space of an autoencoder (AE). Even though this produces convincing results, it has two major shortcomings. As the GAN is limited to reproduce the dataset the AE was trained on, we cannot reuse a trained AE for novel data. Furthermore, it is difficult to add spatial supervision into the generation process, as the AE only gives us a global representation. To remedy these issues, we propose to train the GAN on grids (i.e. each cell covers a part of a shape). In this representation each cell is equipped with a latent vector provided by an AE. This localized representation enables more expressiveness (since the cell-based latent vectors can be combined in novel ways) as well as spatial control of the generation process (e.g. via bounding boxes). Our method outperforms the current state of the art on all established evaluation measures, proposed for quantitatively evaluating the generative capabilities of GANs. We show limitations of these measures and propose the adaptation of a robust criterion from statistical analysis as an alternative.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Enigma of Complexity,"In this paper we examine the concept of complexity as it applies to generative art and design. Complexity has many different, discipline specific definitions, such as complexity in physical systems (entropy), algorithmic measures of information complexity and the field of ""complex systems"". We apply a series of different complexity measures to three different generative art datasets and look at the correlations between complexity and individual aesthetic judgement by the artist (in the case of two datasets) or the physically measured complexity of 3D forms. Our results show that the degree of correlation is different for each set and measure, indicating that there is no overall ""better"" measure. However, specific measures do perform well on individual datasets, indicating that careful choice can increase the value of using such measures. We conclude by discussing the value of direct measures in generative and evolutionary art, reinforcing recent findings from neuroimaging and psychology which suggest human aesthetic judgement is informed by many extrinsic factors beyond the measurable properties of the object being judged.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Machine Learning for Data-Driven Movement Generation: a Review of the State of the Art,"The rise of non-linear and interactive media such as video games has increased the need for automatic movement animation generation. In this survey, we review and analyze different aspects of building automatic movement generation systems using machine learning techniques and motion capture data. We cover topics such as high-level movement characterization, training data, features representation, machine learning models, and evaluation methods. We conclude by presenting a discussion of the reviewed literature and outlining the research gaps and remaining challenges for future work.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
DressCode: Autoregressively Sewing and Generating Garments from Text Guidance,"Apparel's significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D Generative Model Latent Disentanglement via Local Eigenprojection,"Designing realistic digital humans is extremely complex. Most data-driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural-network-based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state-of-the-art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation,"Dance, as an art form, fundamentally hinges on the precise synchronization with musical beats. However, achieving aesthetically pleasing dance sequences from music is challenging, with existing methods often falling short in controllability and beat alignment. To address these shortcomings, this paper introduces Beat-It, a novel framework for beat-specific, key pose-guided dance generation. Unlike prior approaches, Beat-It uniquely integrates explicit beat awareness and key pose guidance, effectively resolving two main issues: the misalignment of generated dance motions with musical beats, and the inability to map key poses to specific beats, critical for practical choreography. Our approach disentangles beat conditions from music using a nearest beat distance representation and employs a hierarchical multi-condition fusion mechanism. This mechanism seamlessly integrates key poses, beats, and music features, mitigating condition conflicts and offering rich, multi-conditioned guidance for dance generation. Additionally, a specially designed beat alignment loss ensures the generated dance movements remain in sync with the designated beats. Extensive experiments confirm Beat-It's superiority over existing state-of-the-art methods in terms of beat alignment and motion controllability.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Art of Food: Meal Image Synthesis from Ingredients,"In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual descriptions of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by a generative neural networks (GANs) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers. In contrast, meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. We propose a method that first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Extensive experiments show our model is able to generate meal image corresponding to the ingredients, which could be used to augment existing dataset for solving other computational food analysis problems.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Fast Auto-Differentiable Digitally Reconstructed Radiographs for Solving Inverse Problems in Intraoperative Imaging,"The use of digitally reconstructed radiographs (DRRs) to solve inverse problems such as slice-to-volume registration and 3D reconstruction is well-studied in preoperative settings. In intraoperative imaging, the utility of DRRs is limited by the challenges in generating them in real-time and supporting optimization procedures that rely on repeated DRR synthesis. While immense progress has been made in accelerating the generation of DRRs through algorithmic refinements and GPU implementations, DRR-based optimization remains slow because most DRR generators do not offer a straightforward way to obtain gradients with respect to the imaging parameters. To make DRRs interoperable with gradient-based optimization and deep learning frameworks, we have reformulated Siddon's method, the most popular ray-tracing algorithm used in DRR generation, as a series of vectorized tensor operations. We implemented this vectorized version of Siddon's method in PyTorch, taking advantage of the library's strong automatic differentiation engine to make this DRR generator fully differentiable with respect to its parameters. Additionally, using GPU-accelerated tensor computation enables our vectorized implementation to achieve rendering speeds equivalent to state-of-the-art DRR generators implemented in CUDA and C++. We illustrate the resulting method in the context of slice-to-volume registration. Moreover, our simulations suggest that the loss landscapes for the slice-to-volume registration problem are convex in the neighborhood of the optimal solution, and gradient-based registration promises a much faster solution than prevailing gradient-free optimization strategies. The proposed DRR generator enables fast computer vision algorithms to support image guidance in minimally invasive procedures. Our implementation is publically available at https://github.com/v715/DiffDRR.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Max Bense as a Visionary: from Entropy to the Dialectics of Programmed Images,"In 1960 in Stuttgart, Max Bense published the book Programming the Beautiful [Programmierung des Sch{}nen]. Bense looks in cybernetics for scientific concepts and instigates the thought of programming in the field of literature. His information aesthetics influences a whole generation of scientists and artists - including the Stuttgart Circle, which takes hold of the new aesthetics to carry out the first programmed artistic images. Is Max Bense a visionary? How is he revolutionizing the world of images? The article discusses the cybernetics that inspired Bense: a science of probability that contrasts with the principles of Newtonian physics. Moreover, in the sixties, Max Bense, together with Elisabeth Walther, launched the experimental magazine Rot, which devoted its pages to the concrete poetry and the first computer-generated images of Georg Nees. As Frieder Nake defends through his pioneering work and theory, these images oppose the visible and the computable. This dialectic opens to a critical thinking on the algorithmic image in art and science.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
BuildingBlock: A Hybrid Approach for Structured Building Generation,"Three-dimensional building generation is vital for applications in gaming, virtual reality, and digital twins, yet current methods face challenges in producing diverse, structured, and hierarchically coherent buildings. We propose BuildingBlock, a hybrid approach that integrates generative models, procedural content generation (PCG), and large language models (LLMs) to address these limitations. Specifically, our method introduces a two-phase pipeline: the Layout Generation Phase (LGP) and the Building Construction Phase (BCP).   LGP reframes box-based layout generation as a point-cloud generation task, utilizing a newly constructed architectural dataset and a Transformer-based diffusion model to create globally consistent layouts. With LLMs, these layouts are extended into rule-based hierarchical designs, seamlessly incorporating component styles and spatial structures.   The BCP leverages these layouts to guide PCG, enabling local-customizable, high-quality structured building generation. Experimental results demonstrate BuildingBlock's effectiveness in generating diverse and hierarchically structured buildings, achieving state-of-the-art results on multiple benchmarks, and paving the way for scalable and intuitive architectural workflows.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation,"Scene generation is crucial to many computer graphics applications. Recent advances in generative AI have streamlined sketch-to-image workflows, easing the workload for artists and designers in creating scene concept art. However, these methods often struggle for complex scenes with multiple detailed objects, sometimes missing small or uncommon instances. In this paper, we propose a Training-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after reviewing the entire cross-attention mechanism. This scheme revitalizes the existing ControlNet model, enabling effective handling of multi-instance generations, involving prompt balance, characteristics prominence, and dense tuning. Specifically, this approach enhances keyword representation via the prompt balance module, reducing the risk of missing critical instances. It also includes a characteristics prominence module that highlights TopK indices in each channel, ensuring essential features are better represented based on token sketches. Additionally, it employs dense tuning to refine contour details in the attention map, compensating for instance-related regions. Experiments validate that our triplet tuning approach substantially improves the performance of existing sketch-to-image models. It consistently generates detailed, multi-instance 2D images, closely adhering to the input prompts and enhancing visual quality in complex multi-instance scenes. Code is available at https://github.com/chaos-sun/t3s2s.git.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Walk Before You Dance: High-fidelity and Editable Dance Synthesis via Generative Masked Motion Prior,"Recent advances in dance generation have enabled the automatic synthesis of 3D dance motions. However, existing methods still face significant challenges in simultaneously achieving high realism, precise dance-music synchronization, diverse motion expression, and physical plausibility. To address these limitations, we propose a novel approach that leverages a generative masked text-to-motion model as a distribution prior to learn a probabilistic mapping from diverse guidance signals, including music, genre, and pose, into high-quality dance motion sequences. Our framework also supports semantic motion editing, such as motion inpainting and body part modification. Specifically, we introduce a multi-tower masked motion model that integrates a text-conditioned masked motion backbone with two parallel, modality-specific branches: a music-guidance tower and a pose-guidance tower. The model is trained using synchronized and progressive masked training, which allows effective infusion of the pretrained text-to-motion prior into the dance synthesis process while enabling each guidance branch to optimize independently through its own loss function, mitigating gradient interference. During inference, we introduce classifier-free logits guidance and pose-guided token optimization to strengthen the influence of music, genre, and pose signals. Extensive experiments demonstrate that our method sets a new state of the art in dance generation, significantly advancing both the quality and editability over existing approaches. Project Page available at https://foram-s1.github.io/DanceMosaic/","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Electronic Visualisation in Chemistry: From Alchemy to Art,"Chemists now routinely use software as part of their work. For example, virtual chemistry allows chemical reactions to be simulated. In particular, a selection of software is available for the visualisation of complex 3-dimensional molecular structures. Many of these are very beautiful in their own right. As well as being included as illustrations in academic papers, such visualisations are often used on the covers of chemistry journals as artistically decorative and attractive motifs. Chemical images have also been used as the basis of artworks in exhibitions. This paper explores the development of the relationship of chemistry, art, and IT. It covers some of the increasingly sophisticated software used to generate these projections (e.g., UCSF Chimera) and their progressive use as a visual art form.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Gaussian Shell Maps for Efficient 3D Human Generation,"Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Quand rechercher c'est faire des vagues : Dans et {} partir des images algorithmiques,"In Search of the Wave is a computer-generated film made in 2013, highlighting the computation of images through computer simulation, and through text and voice. Originating from a screening of the film at the Gustave Eiffel University, the article presents a reflection on research-creation in and from algorithmic images. Fundamentally, what is it in this research-creation -- especially in research on algorithmic imagery -- that can be set in motion? Without fully distinguishing between what would be research on one hand and creation on the other, we focus on characterizing forms, aesthetics, or theories that contribute to possible shifts. The inventory of these possibilities is precisely the challenge of the text: from mathematics to image and visualization, from the birth of generative aesthetics to the coding related to pioneering works (recoding), or from indexing new aesthetics to new forms of critical production.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CTR-Driven Advertising Image Generation with Multimodal Large Language Models,"In web data, advertising images are crucial for capturing user attention and improving advertising effectiveness. Most existing methods generate background for products primarily focus on the aesthetic quality, which may fail to achieve satisfactory online performance. To address this limitation, we explore the use of Multimodal Large Language Models (MLLMs) for generating advertising images by optimizing for Click-Through Rate (CTR) as the primary objective. Firstly, we build targeted pre-training tasks, and leverage a large-scale e-commerce multimodal dataset to equip MLLMs with initial capabilities for advertising image generation tasks. To further improve the CTR of generated images, we propose a novel reward model to fine-tune pre-trained MLLMs through Reinforcement Learning (RL), which can jointly utilize multimodal features and accurately reflect user click preferences. Meanwhile, a product-centric preference optimization strategy is developed to ensure that the generated background content aligns with the product characteristics after fine-tuning, enhancing the overall relevance and effectiveness of the advertising images. Extensive experiments have demonstrated that our method achieves state-of-the-art performance in both online and offline metrics. Our code and pre-trained models are publicly available at: https://github.com/Chenguoz/CAIG.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation,"We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ART-Owen Scrambling,"We present a novel algorithm for implementing Owen-scrambling, combining the generation and distribution of the scrambling bits in a single self-contained compact process. We employ a context-free grammar to build a binary tree of symbols, and equip each symbol with a scrambling code that affects all descendant nodes. We nominate the grammar of adaptive regular tiles (ART) derived from the repetition-avoiding Thue-Morse word, and we discuss its potential advantages and shortcomings. Our algorithm has many advantages, including random access to samples, fixed time complexity, GPU friendliness, and scalability to any memory budget. Further, it provides two unique features over known methods: it admits optimization, and it is invertible, enabling screen-space scrambling of the high-dimensional Sobol sampler.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Freeform Islamic Geometric Patterns,Islamic geometric patterns are a rich and venerable ornamental tradition. Many classic designs feature periodic arrangements of rosettes: star shapes surrounded by rings of hexagonal petals. We present a new technique for generating 'freeform' compositions of rosettes: finite designs that freely mix rosettes of unusual sizes while retaining the aesthetics of traditional patterns. We use a circle packing as a scaffolding for developing a patch of polygons and fill each polygon with a motif based on established constructions from Islamic art.,"cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering,"Automatically generating realistic musical performance motion can greatly enhance digital media production, often involving collaboration between professionals and musicians. However, capturing the intricate body, hand, and finger movements required for accurate musical performances is challenging. Existing methods often fall short due to the complex mapping between audio and motion, typically requiring additional inputs like scores or MIDI data. In this work, we present SyncViolinist, a multi-stage end-to-end framework that generates synchronized violin performance motion solely from audio input. Our method overcomes the challenge of capturing both global and fine-grained performance features through two key modules: a bowing/fingering module and a motion generation module. The bowing/fingering module extracts detailed playing information from the audio, which the motion generation module uses to create precise, coordinated body motions reflecting the temporal granularity and nature of the violin performance. We demonstrate the effectiveness of SyncViolinist with significantly improved qualitative and quantitative results from unseen violin performance audio, outperforming state-of-the-art methods. Extensive subjective evaluations involving professional violinists further validate our approach. The code and dataset are available at https://github.com/Kakanat/SyncViolinist.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning to Generate Diverse Dance Motions with Transformer,"With the ongoing pandemic, virtual concerts and live events using digitized performances of musicians are getting traction on massive multiplayer online worlds. However, well choreographed dance movements are extremely complex to animate and would involve an expensive and tedious production process. In addition to the use of complex motion capture systems, it typically requires a collaborative effort between animators, dancers, and choreographers. We introduce a complete system for dance motion synthesis, which can generate complex and highly diverse dance sequences given an input music sequence. As motion capture data is limited for the range of dance motions and styles, we introduce a massive dance motion data set that is created from YouTube videos. We also present a novel two-stream motion transformer generative model, which can generate motion sequences with high flexibility. We also introduce new evaluation metrics for the quality of synthesized dance motions, and demonstrate that our system can outperform state-of-the-art methods. Our system provides high-quality animations suitable for large crowds for virtual concerts and can also be used as reference for professional animation pipelines. Most importantly, we show that vast online videos can be effective in training dance motion models.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches,"2D animation is a common factor in game development, used for characters, effects and background art. It involves work that takes both skill and time, but parts of which are repetitive and tedious. Automated animation approaches exist, but are designed without animators in mind. The focus is heavily on real-life video, which follows strict laws of how objects move, and does not account for the stylistic movement often present in 2D animation. We propose a problem formulation that more closely adheres to the standard workflow of animation. We also demonstrate a model, SketchBetween, which learns to map between keyframes and sketched in-betweens to rendered sprite animations. We demonstrate that our problem formulation provides the required information for the task and that our model outperforms an existing method.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Accessible Color Sequences for Data Visualization,"Color sequences, ordered sets of colors for data visualization, that balance aesthetics with accessibility considerations are presented. In order to model aesthetic preference, data were collected with an online survey, and the results were used to train a machine-learning model. To ensure accessibility, this model was combined with minimum-perceptual-distance constraints, including for simulated color-vision deficiencies, as well as with minimum-lightness-distance constraints for grayscale printing, maximum-lightness constraints for maintaining contrast with a white background, and scores from a color-saliency model for ease of use of the colors in verbal and written descriptions. Optimal color sequences containing six, eight, and ten colors were generated using the data-driven aesthetic-preference model and accessibility constraints. Due to the balance of aesthetics and accessibility considerations, the resulting color sequences can serve as reasonable defaults in data-plotting codes, e.g., for use in scatter plots and line plots.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG,"This paper proposes ""3Dify,"" a procedural 3D computer graphics (3D-CG) generation framework utilizing Large Language Models (LLMs). The framework enables users to generate 3D-CG content solely through natural language instructions. 3Dify is built upon Dify, an open-source platform for AI application development, and incorporates several state-of-the-art LLM-related technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). For 3D-CG generation support, 3Dify automates the operation of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not support MCP-based interaction, the framework employs the Computer-Using Agent (CUA) method to automate Graphical User Interface (GUI) operations. Moreover, to enhance image generation quality, 3Dify allows users to provide feedback by selecting preferred images from multiple candidates. The LLM then learns variable patterns from these selections and applies them to subsequent generations. Furthermore, 3Dify supports the integration of locally deployed LLMs, enabling users to utilize custom-developed models and to reduce both time and monetary costs associated with external API calls by leveraging their own computational resources.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Text-guided Image-and-Shape Editing and Generation: A Short Survey,"Image and shape editing are ubiquitous among digital artworks. Graphics algorithms facilitate artists and designers to achieve desired editing intents without going through manually tedious retouching. In the recent advance of machine learning, artists' editing intents can even be driven by text, using a variety of well-trained neural networks. They have seen to be receiving an extensive success on such as generating photorealistic images, artworks and human poses, stylizing meshes from text, or auto-completion given image and shape priors. In this short survey, we provide an overview over 50 papers on state-of-the-art (text-guided) image-and-shape generation techniques. We start with an overview on recent editing algorithms in the introduction. Then, we provide a comprehensive review on text-guided editing techniques for 2D and 3D independently, where each of its sub-section begins with a brief background introduction. We also contextualize editing algorithms under recent implicit neural representations. Finally, we conclude the survey with the discussion over existing methods and potential research ideas.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning to Synthesize Graphics Programs for Geometric Artworks,"Creating and understanding art has long been a hallmark of human ability. When presented with finished digital artwork, professional graphic artists can intuitively deconstruct and replicate it using various drawing tools, such as the line tool, paint bucket, and layer features, including opacity and blending modes. While most recent research in this field has focused on art generation, proposing a range of methods, these often rely on the concept of artwork being represented as a final image. To bridge the gap between pixel-level results and the actual drawing process, we present an approach that treats a set of drawing tools as executable programs. This method predicts a sequence of steps to achieve the final image, allowing for understandable and resolution-independent reproductions under the usage of a set of drawing commands. Our experiments demonstrate that our program synthesizer, Art2Prog, can comprehensively understand complex input images and reproduce them using high-quality executable programs. The experimental results evidence the potential of machines to grasp higher-level information from images and generate compact program-level descriptions.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
String Art: Circle Drawing Using Straight Lines,"An algorithm to generate the locus of a circle using the intersection points of straight lines is proposed. The pixels on the circle are plotted independent of one another and the operations involved in finding the locus of the circle from the intersection of straight lines are parallelizable. Integer only arithmetic and algorithmic optimizations are used for speedup. The proposed algorithm makes use of an envelope to form a parabolic arc which is consequent transformed into a circle. The use of parabolic arcs for the transformation results in higher pixel errors as the radius of the circle to be drawn increases. At its current state, the algorithm presented may be suitable only for generating circles for string art.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models,"Diffusion models have shown impressive results in text-to-image synthesis. Using massive datasets of captioned images, diffusion models learn to generate raster images of highly diverse objects and scenes. However, designers frequently use vector representations of images like Scalable Vector Graphics (SVGs) for digital icons or art. Vector graphics can be scaled to any size, and are compact. We show that a text-conditioned diffusion model trained on pixel representations of images can be used to generate SVG-exportable vector graphics. We do so without access to large datasets of captioned SVGs. By optimizing a differentiable vector graphics rasterizer, our method, VectorFusion, distills abstract semantic knowledge out of a pretrained diffusion model. Inspired by recent text-to-3D work, we learn an SVG consistent with a caption using Score Distillation Sampling. To accelerate generation and improve fidelity, VectorFusion also initializes from an image sample. Experiments show greater quality than prior work, and demonstrate a range of styles including pixel art and sketches. See our project webpage at https://ajayj.com/vectorfusion .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Cartographic Relief Shading with Neural Networks,"Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
BSP-Net: Generating Compact Meshes via Binary Space Partitioning,"Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives. Code is available at https://github.com/czq142857/BSP-NET-original.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation,"The current state-of-the-art video generative models can produce commercial-grade videos with highly realistic details. However, they still struggle to coherently present multiple sequential events in the stories specified by the prompts, which is foreseeable an essential capability for future long video generation scenarios. For example, top T2V generative models still fail to generate a video of the short simple story 'how to put an elephant into a refrigerator.' While existing detail-oriented benchmarks primarily focus on fine-grained metrics like aesthetic quality and spatial-temporal consistency, they fall short of evaluating models' abilities to handle event-level story presentation. To address this gap, we introduce StoryEval, a story-oriented benchmark specifically designed to assess text-to-video (T2V) models' story-completion capabilities. StoryEval features 423 prompts spanning 7 classes, each representing short stories composed of 2-4 consecutive events. We employ advanced vision-language models, such as GPT-4V and LLaVA-OV-Chat-72B, to verify the completion of each event in the generated videos, applying a unanimous voting method to enhance reliability. Our methods ensure high alignment with human evaluations, and the evaluation of 11 models reveals its challenge, with none exceeding an average story-completion rate of 50%. StoryEval provides a new benchmark for advancing T2V models and highlights the challenges and opportunities in developing next-generation solutions for coherent story-driven video generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling,"Generating full-body human gestures based on speech signals remains challenges on quality and speed. Existing approaches model different body regions such as body, legs and hands separately, which fail to capture the spatial interactions between them and result in unnatural and disjointed movements. Additionally, their autoregressive/diffusion-based pipelines show slow generation speed due to dozens of inference steps. To address these two challenges, we propose GestureLSM, a flow-matching-based approach for Co-Speech Gesture Generation with spatial-temporal modeling. Our method i) explicitly model the interaction of tokenized body regions through spatial and temporal attention, for generating coherent full-body gestures. ii) introduce the flow matching to enable more efficient sampling by explicitly modeling the latent velocity space. To overcome the suboptimal performance of flow matching baseline, we propose latent shortcut learning and beta distribution time stamp sampling during training to enhance gesture synthesis quality and accelerate inference. Combining the spatial-temporal modeling and improved flow matching-based framework, GestureLSM achieves state-of-the-art performance on BEAT2 while significantly reducing inference time compared to existing methods, highlighting its potential for enhancing digital humans and embodied agents in real-world applications. Project Page: https://andypinxinliu.github.io/GestureLSM","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Fast Unsupervised Scheme for Polygonal Approximation,"This paper proposes a fast and unsupervised scheme for the polygonal approximation of a closed digital curve. It is demonstrated that the approximation scheme is faster than state-of-the-art approximation and is competitive with Rosin's measure and aesthetic aspects. The scheme comprises of three phases: initial segmentation, iterative vertex insertion, iterative merging, and vertex adjustment. The initial segmentation is used to detect sharp turns, that is, vertices that seemingly have high curvature. It is likely that some of the important vertices with low curvature might have been missed in the first phase; therefore, iterative vertex insertion is used to add vertices in a region where the curvature changes slowly but steadily. The initial phase may pick up some undesirable vertices, and thus merging is used to eliminate redundant vertices. Finally, vertex adjustment was used to enhance the aesthetic appearance of the approximation. The quality of the approximations was measured using the Rosin's method. The robustness of the proposed scheme with respect to geometric transformation was observed.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models,"Coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as Digital Twin City, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as twins from 3D dense reconstructions. However, these models typically lack realistic texture relative to the real building or scene, making them unsuitable for vivid display or direct reference. In this paper, we present TwinTex, the first automatic texture mapping framework to generate a photo-realistic texture for a piece-wise planar proxy. Our method addresses most challenges occurring in such twin texture generation. Specifically, for each primitive plane, we first select a small set of photos with greedy heuristics considering photometric quality, perspective quality and facade texture completeness. Then, different levels of line features (LoLs) are extracted from the set of selected photos to generate guidance for later steps. With LoLs, we employ optimization algorithms to align texture with geometry from local to global. Finally, we fine-tune a diffusion model with a multi-mask initialization component and a new dataset to inpaint the missing region. Experimental results on many buildings, indoor scenes and man-made objects of varying complexity demonstrate the generalization ability of our algorithm. Our approach surpasses state-of-the-art texture mapping methods in terms of high-fidelity quality and reaches a human-expert production level with much less effort. Project page: https://vcc.tech/research/2023/TwinTex.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Wrought Iron Beauty of Poncelet Loci,"We've built a web-based tool for the real-time interaction with loci of Poncelet triangle families. Our initial goals were to facilitate exploratory detection of geometric properties of such families. During frequent walks in my neighborhood, it appeared to me Poncelet loci shared a palette of motifs with those found in wrought iron gates at the entrance of many a residential building. As a result, I started to look at Poncelet loci aesthetically, a kind of generative art. Features were gradually added to the tool with the sole purpose of beautifying the output. Hundreds of interesting loci were subsequently collected into an online ""gallery"", with some further enhanced by a graphic designer. We will tour some of these byproducts here. An interesting question is if Poncelet loci could serve as the basis for future metalwork and/or architectural designs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Improving the Physics of Video Generation with VJEPA-2 Reward Signal,"This is a short technical report describing the winning entry of the PhysicsIQ Challenge, presented at the Perception Test Workshop at ICCV 2025. State-of-the-art video generative models exhibit severely limited physical understanding, and often produce implausible videos. The Physics IQ benchmark has shown that visual realism does not imply physics understanding. Yet, intuitive physics understanding has shown to emerge from SSL pretraining on natural videos. In this report, we investigate whether we can leverage SSL-based video world models to improve the physics plausibility of video generative models. In particular, we build ontop of the state-of-the-art video generative model MAGI-1 and couple it with the recently introduced Video Joint Embedding Predictive Architecture 2 (VJEPA-2) to guide the generation process. We show that by leveraging VJEPA-2 as reward signal, we can improve the physics plausibility of state-of-the-art video generative models by ~6%.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition,"The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
EDGE: Editable Dance Generation From Music,"Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MAGMA: Music Aligned Generative Motion Autodecoder,"Mapping music to dance is a challenging problem that requires spatial and temporal coherence along with a continual synchronization with the music's progression. Taking inspiration from large language models, we introduce a 2-step approach for generating dance using a Vector Quantized-Variational Autoencoder (VQ-VAE) to distill motion into primitives and train a Transformer decoder to learn the correct sequencing of these primitives. We also evaluate the importance of music representations by comparing naive music feature extraction using Librosa to deep audio representations generated by state-of-the-art audio compression algorithms. Additionally, we train variations of the motion generator using relative and absolute positional encodings to determine the effect on generated motion quality when generating arbitrarily long sequence lengths. Our proposed approach achieve state-of-the-art results in music-to-motion generation benchmarks and enables the real-time generation of considerably longer motion sequences, the ability to chain multiple motion sequences seamlessly, and easy customization of motion sequences to meet style requirements.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"State-of-the-Art in the Architecture, Methods and Applications of StyleGAN","Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Artistic control over the glitch in AI-generated motion capture,"Artificial intelligence (AI) models are prevalent today and provide a valuable tool for artists. However, a lesser-known artifact that comes with AI models that is not always discussed is the glitch. Glitches occur for various reasons; sometimes, they are known, and sometimes they are a mystery. Artists who use AI models to generate art might not understand the reason for the glitch but often want to experiment and explore novel ways of augmenting the output of the glitch. This paper discusses some of the questions artists have when leveraging the glitch in AI art production. It explores the unexpected positive outcomes produced by glitches in the specific context of motion capture and performance art.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Neural Garment Dynamics via Manifold-Aware Transformers,"Data driven and learning based solutions for modeling dynamic garments have significantly advanced, especially in the context of digital humans. However, existing approaches often focus on modeling garments with respect to a fixed parametric human body model and are limited to garment geometries that were seen during training. In this work, we take a different approach and model the dynamics of a garment by exploiting its local interactions with the underlying human body. Specifically, as the body moves, we detect local garment-body collisions, which drive the deformation of the garment. At the core of our approach is a mesh-agnostic garment representation and a manifold-aware transformer network design, which together enable our method to generalize to unseen garment and body geometries. We evaluate our approach on a wide variety of garment types and motion sequences and provide competitive qualitative and quantitative results with respect to the state of the art.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MIDGET: Music Conditioned 3D Dance Generation,"In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Visualization of Age Distributions as Elements of Medical Data-Stories,"In various fields, including medicine, age distributions are crucial. Despite widespread media coverage of health topics, there remains a need to enhance health communication. Narrative medical visualization is promising for improving information comprehension and retention. This study explores the most effective ways to present age distributions of diseases through narrative visualizations. We conducted a thorough analysis of existing visualizations, held workshops with a broad audience, and reviewed relevant literature. From this, we identified design choices focusing on comprehension, aesthetics, engagement, and memorability. We specifically tested three pictogram variants: pictograms as bars, stacked pictograms, and annotations. After evaluating 18 visualizations with 72 participants and three expert reviews, we determined that annotations were most effective for comprehension and aesthetics. However, traditional bar charts were preferred for engagement, and other variants were more memorable. The study provides a set of design recommendations based on these insights.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
PointInverter: Point Cloud Reconstruction and Editing via a Generative Model with Shape Priors,"In this paper, we propose a new method for mapping a 3D point cloud to the latent space of a 3D generative adversarial network. Our generative model for 3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point cloud generator. We derive an efficient way to encode an input 3D point cloud to the latent space of the SP-GAN. Our point cloud encoder can resolve the point ordering issue during inversion, and thus can determine the correspondences between points in the generated 3D point cloud and those in the canonical sphere used by the generator. We show that our method outperforms previous GAN inversion methods for 3D point clouds, achieving state-of-the-art results both quantitatively and qualitatively. Our code is available at https://github.com/hkust-vgd/point_inverter.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D Neural Field Generation using Triplane Diffusion,"Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generative Adversarial Networks for photo to Hayao Miyazaki style cartoons,"This paper takes on the problem of transferring the style of cartoon images to real-life photographic images by implementing previous work done by CartoonGAN. We trained a Generative Adversial Network(GAN) on over 60 000 images from works by Hayao Miyazaki at Studio Ghibli. To evaluate our results, we conducted a qualitative survey comparing our results with two state-of-the-art methods. 117 survey results indicated that our model on average outranked state-of-the-art methods on cartoon-likeness.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Efficient Geometry-aware 3D Generative Adversarial Networks,"Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Logo Generation Using Regional Features: A Faster R-CNN Approach to Generative Adversarial Networks,"In this paper we introduce Local Logo Generative Adversarial Network (LL-GAN) that uses regional features extracted from Faster R-CNN for logo generation. We demonstrate the strength of this approach by training the framework on a small style-rich dataset of real heavy metal logos to generate new ones. LL-GAN achieves Inception Score of 5.29 and Frechet Inception Distance of 223.94, improving on state-of-the-art models StyleGAN2 and Self-Attention GAN.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Photo2Relief: Let Human in the Photograph Stand Out,"In this paper, we propose a technique for making humans in photographs protrude like reliefs. Unlike previous methods which mostly focus on the face and head, our method aims to generate art works that describe the whole body activity of the character. One challenge is that there is no ground-truth for supervised deep learning. We introduce a sigmoid variant function to manipulate gradients tactfully and train our neural networks by equipping with a loss function defined in gradient domain. The second challenge is that actual photographs often across different light conditions. We used image-based rendering technique to address this challenge and acquire rendering images and depth data under different lighting conditions. To make a clear division of labor in network modules, a two-scale architecture is proposed to create high-quality relief from a single photograph. Extensive experimental results on a variety of scenes show that our method is a highly effective solution for generating digital 2.5D artwork from photographs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
VertexRegen: Mesh Generation with Continuous Level of Detail,"We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Golden interpolation,"For the classic aesthetic interpolation problem, we propose an entirely new thought: apply the golden section. For how to apply the golden section to interpolation methods, we present three examples: the golden step interpolation, the golden piecewise linear interpolation and the golden curve interpolation, which respectively deal with the applications of golden section in the interpolation of degree 0, 1, and 2 in the plane. In each example, we present our basic ideas, the specific methods, comparative examples and applications, and relevant criteria. And it is worth mentioning that for aesthetics, we propose two novel concepts: the golden cuspidal hill and the golden domed hill. This paper aims to provide the reference for the combination of golden section and interpolation, and stimulate more and better related researches.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Neurosymbolic Models for Computer Graphics,"Procedural models (i.e. symbolic programs that output visual data) are a historically-popular method for representing graphics content: vegetation, buildings, textures, etc. They offer many advantages: interpretable design parameters, stochastic variations, high-quality outputs, compact representation, and more. But they also have some limitations, such as the difficulty of authoring a procedural model from scratch. More recently, AI-based methods, and especially neural networks, have become popular for creating graphic content. These techniques allow users to directly specify desired properties of the artifact they want to create (via examples, constraints, or objectives), while a search, optimization, or learning algorithm takes care of the details. However, this ease of use comes at a cost, as it's often hard to interpret or manipulate these representations. In this state-of-the-art report, we summarize research on neurosymbolic models in computer graphics: methods that combine the strengths of both AI and symbolic programs to represent, generate, and manipulate visual data. We survey recent work applying these techniques to represent 2D shapes, 3D shapes, and materials & textures. Along the way, we situate each prior work in a unified design space for neurosymbolic models, which helps reveal underexplored areas and opportunities for future research.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MG-Gen: Single Image to Motion Graphics Generation,"We introduce MG-Gen, a framework that generates motion graphics directly from a single raster image. MG-Gen decompose a single raster image into layered structures represented as HTML, generate animation scripts for each layer, and then render them into a video. Experiments confirm MG-Gen generates dynamic motion graphics while preserving text readability and fidelity to the input conditions, whereas state-of-the-art image-to-video generation methods struggle with them. The code is available at https://github.com/CyberAgentAILab/MG-GEN.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model,"The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Voronoi Grid-Shell Structures,"We introduce a framework for the generation of grid-shell structures that is based on Voronoi diagrams and allows us to design tessellations that achieve excellent static performances. We start from an analysis of stress on the input surface and we use the resulting tensor field to induce an anisotropic non-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation under the same metric. The resulting mesh is hex-dominant and made of cells with a variable density, which depends on the amount of stress, and anisotropic shape, which depends on the direction of maximum stress. This mesh is further optimized taking into account symmetry and regularity of cells to improve aesthetics. We demonstrate that our grid-shells achieve better static performances with respect to quad-based grid shells, while offering an innovative and aesthetically pleasing look.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Neural-Polyptych: Content Controllable Painting Recreation for Diverse Genres,"To bridge the gap between artists and non-specialists, we present a unified framework, Neural-Polyptych, to facilitate the creation of expansive, high-resolution paintings by seamlessly incorporating interactive hand-drawn sketches with fragments from original paintings. We have designed a multi-scale GAN-based architecture to decompose the generation process into two parts, each responsible for identifying global and local features. To enhance the fidelity of semantic details generated from users' sketched outlines, we introduce a Correspondence Attention module utilizing our Reference Bank strategy. This ensures the creation of high-quality, intricately detailed elements within the artwork. The final result is achieved by carefully blending these local elements while preserving coherent global consistency. Consequently, this methodology enables the production of digital paintings at megapixel scale, accommodating diverse artistic expressions and enabling users to recreate content in a controlled manner. We validate our approach to diverse genres of both Eastern and Western paintings. Applications such as large painting extension, texture shuffling, genre switching, mural art restoration, and recomposition can be successfully based on our framework.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Text-Guided Vector Graphics Customization,"Vector graphics are widely used in digital art and valued by designers for their scalability and layer-wise topological properties. However, the creation and editing of vector graphics necessitate creativity and design expertise, leading to a time-consuming process. In this paper, we propose a novel pipeline that generates high-quality customized vector graphics based on textual prompts while preserving the properties and layer-wise information of a given exemplar SVG. Our method harnesses the capabilities of large pre-trained text-to-image models. By fine-tuning the cross-attention layers of the model, we generate customized raster images guided by textual prompts. To initialize the SVG, we introduce a semantic-based path alignment method that preserves and transforms crucial paths from the exemplar SVG. Additionally, we optimize path parameters using both image-level and vector-level losses, ensuring smooth shape deformation while aligning with the customized raster image. We extensively evaluate our method using multiple metrics from vector-level, image-level, and text-level perspectives. The evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality. The project page is https://intchous.github.io/SVGCustomization.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Design Characterization for Black-and-White Textures in Visualization,"We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Engineering Sketch Generation for Computer-Aided Design,"Engineering sketches form the 2D basis of parametric Computer-Aided Design (CAD), the foremost modeling paradigm for manufactured objects. In this paper we tackle the problem of learning based engineering sketch generation as a first step towards synthesis and composition of parametric CAD models. We propose two generative models, CurveGen and TurtleGen, for engineering sketch generation. Both models generate curve primitives without the need for a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3D CAD modeling operations. We find in our perceptual evaluation using human subjects that both CurveGen and TurtleGen produce more realistic engineering sketches when compared with the current state-of-the-art for engineering sketch generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Robust Voxelization and Visualization by Improved Tetrahedral Mesh Generation,"When obtaining interior 3D voxel data from triangular meshes, most existing methods fail to handle low quality meshes which happens to take up a big portion on the internet. In this work we present a robust voxelization method that is based on tetrahedral mesh generation within a user defined error bound. Comparing to other tetrahedral mesh generation methods, our method produces much higher quality tetrahedral meshes as the intermediate outcome, which allows us to utilize a faster voxelization algorithm that is based on a stronger assumption. We show the results comparing to various methods including the state-of-the-art. Our contribution includes a framework which takes triangular mesh as an input and produces voxelized data, a proof to an unproved algorithm that performs better than the state-of-the-art, and various experiments including parallelization built on the GPU and CPU. We further tested our method on various dataset including Princeton ModelNet and Thingi10k to show the robustness of the framework, where near 100% availability is achieved, while others can only achieve around 50%.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Realistic Image Generation using Region-phrase Attention,"The Generative Adversarial Network (GAN) has recently been applied to generate synthetic images from text. Despite significant advances, most current state-of-the-art algorithms are regular-grid region based; when attention is used, it is mainly applied between individual regular-grid regions and a word. These approaches are sufficient to generate images that contain a single object in its foreground, such as a ""bird"" or ""flower"". However, natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image. Therefore, the regular-grid based image attention weights may not necessarily concentrate on the intended foreground region(s), which in turn, results in an unnatural looking image. Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not necessarily provide a full visual context unless they are applied together. For this reason, in our paper, we proposed a novel method in which we introduced an additional set of attentions between true-grid regions and word phrases. The true-grid region is derived using a set of auxiliary bounding boxes. These auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases. Word phrases are derived from analysing Part-of-Speech (POS) results. We perform experiments on this novel network architecture using the Microsoft Common Objects in Context (MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a short sentence description. Our proposed approach is capable of generating more realistic images compared with the current state-of-the-art algorithms.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GAUDI: A Neural Architect for Immersive 3D Scene Generation,"We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera. We tackle this challenging problem with a scalable yet powerful approach, where we first optimize a latent representation that disentangles radiance fields and camera poses. This latent representation is then used to learn a generative model that enables both unconditional and conditional generation of 3D scenes. Our model generalizes previous works that focus on single objects by removing the assumption that the camera pose distribution can be shared across samples. We show that GAUDI obtains state-of-the-art performance in the unconditional generative setting across multiple datasets and allows for conditional generation of 3D scenes given conditioning variables like sparse image observations or text that describes the scene.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Leveraging virtual technologies to enhance museums and art collections: insights from project CHANGES,"We investigated the use of virtual technologies to digitise and enhance cultural heritage (CH), aligning with Open Science and FAIR principles. Through case studies in museums, we developed reproducible workflows, 3D models, and tools fostering accessibility, inclusivity, and sustainability of CH. Applications include interdisciplinary research, educational innovation, and CH preservation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
KNN-Diffusion: Image Generation via Large-Scale Retrieval,"Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled. In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model without any text, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images only (without paired text data)","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Neural Head Avatars from Monocular RGB Videos,"We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generative Layout Modeling using Constraint Graphs,"We propose a new generative model for layout generation. We generate layouts in three steps. First, we generate the layout elements as nodes in a layout graph. Second, we compute constraints between layout elements as edges in the layout graph. Third, we solve for the final layout using constrained optimization. For the first two steps, we build on recent transformer architectures. The layout optimization implements the constraints efficiently. We show three practical contributions compared to the state of the art: our work requires no user input, produces higher quality layouts, and enables many novel capabilities for conditional layout generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GVP: Generative Volumetric Primitives,"Advances in 3D-aware generative models have pushed the boundary of image synthesis with explicit camera control. To achieve high-resolution image synthesis, several attempts have been made to design efficient generators, such as hybrid architectures with both 3D and 2D components. However, such a design compromises multiview consistency, and the design of a pure 3D generator with high resolution is still an open problem. In this work, we present Generative Volumetric Primitives (GVP), the first pure 3D generative model that can sample and render 512-resolution images in real-time. GVP jointly models a number of volumetric primitives and their spatial information, both of which can be efficiently generated via a 2D convolutional network. The mixture of these primitives naturally captures the sparsity and correspondence in the 3D volume. The training of such a generator with a high degree of freedom is made possible through a knowledge distillation technique. Experiments on several datasets demonstrate superior efficiency and 3D consistency of GVP over the state-of-the-art.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model,"Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Boosting 3D Object Generation through PBR Materials,"Automatic 3D content creation has gained increasing attention recently, due to its potential in various applications such as video games, film industry, and AR/VR. Recent advancements in diffusion models and multimodal models have notably improved the quality and efficiency of 3D object generation given a single RGB image. However, 3D objects generated even by state-of-the-art methods are still unsatisfactory compared to human-created assets. Considering only textures instead of materials makes these methods encounter challenges in photo-realistic rendering, relighting, and flexible appearance editing. And they also suffer from severe misalignment between geometry and high-frequency texture details. In this work, we propose a novel approach to boost the quality of generated 3D objects from the perspective of Physics-Based Rendering (PBR) materials. By analyzing the components of PBR materials, we choose to consider albedo, roughness, metalness, and bump maps. For albedo and bump maps, we leverage Stable Diffusion fine-tuned on synthetic data to extract these values, with novel usages of these fine-tuned models to obtain 3D consistent albedo UV and bump UV for generated objects. In terms of roughness and metalness maps, we adopt a semi-automatic process to provide room for interactive adjustment, which we believe is more practical. Extensive experiments demonstrate that our model is generally beneficial for various state-of-the-art generation methods, significantly boosting the quality and realism of their generated 3D objects, with natural relighting effects and substantially improved geometry.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SpaText: Spatio-Textual Representation for Controllable Image Generation,"Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Interconnected virtual space and Theater. Practice as research on theater stage in the era of the network,"Since 2014, we have been conducting experiments based on a multidisciplinary collaboration between specialists in theatrical staging and researchers in virtual reality, digital art, and video games. This team focused its work on the similarities and differencesthat exist between real physical actors (actor-performers) and virtual digital actors (avatars). From this multidisciplinary approach, experimental research-creation projects have emerged and rely on a physical actor playing with the image of an avatar, controlled by another physical actor via the intermediary of a low-cost motion-capture system. In the first part of the paper, we will introduce the scenographic design on which our presentation is based, and the modifications we have made in relation to our previous work. Next, in the second section, we will discuss in detail the impact of augmenting the player's game using an avatar, compared to the scenic limitations of the theatrical stage. In part three of the paper, we will discuss the software-related aspects of the project, focusing on exchanges between the different components of our design and describing the algorithms enabling us to utilize the real-time movement of a player via various capture devices. To conclude, we will examine in detail how our experimental system linking physical actors and avatars profoundly alters the nature of collaboration between directors, actors, and digital artists in terms of actor/avatar direction.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
BulletGen: Improving 4D Reconstruction with Bullet-Time Generation,"Transforming casually captured, monocular videos into fully immersive dynamic experiences is a highly ill-posed task, and comes with significant challenges, e.g., reconstructing unseen regions, and dealing with the ambiguity in monocular depth estimation. In this work we introduce BulletGen, an approach that takes advantage of generative models to correct errors and complete missing information in a Gaussian-based dynamic scene representation. This is done by aligning the output of a diffusion-based video generation model with the 4D reconstruction at a single frozen ""bullet-time"" step. The generated frames are then used to supervise the optimization of the 4D Gaussian model. Our method seamlessly blends generative content with both static and dynamic scene components, achieving state-of-the-art results on both novel-view synthesis, and 2D/3D tracking tasks.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint,"Diffusion models have been popular for point cloud generation tasks. Existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution. However, the reverse diffusion process can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties. We propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation. Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CanvoX: High-resolution VR Painting in Large Volumetric Canvas,"With virtual reality, digital painting on 2D canvases is now being extended to 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as tools that pave the way to a new form of art - 3D emmersive painting. Current 3D painting systems are only a start, emitting textured triangular geometries. In this paper, we advance this new art of 3D painting to 3D volumetric painting that enables an artist to draw a huge scene with full control of spatial color fields. Inspired by the fact that 2D paintings often use vast space to paint background and small but detailed space for foreground, we claim that supporting a large canvas in varying detail is essential for 3D painting. In order to help artists focus and audiences to navigate the large canvas space, we provide small artist-defined areas, called rooms, that serve as beacons for artist-suggested scales, spaces, locations for intended appreciation view of the painting. Artists and audiences can easily transport themselves between different rooms. Technically, our canvas is represented as an array of deep octrees of depth 24 or higher, built on CPU for volume painting and on GPU for volume rendering using accurate ray casting. In CPU side, we design an efficient iterative algorithm to refine or coarsen octree, as a result of volumetric painting strokes, at highly interactive rates, and update the corresponding GPU textures. Then we use GPU-based ray casting algorithms to render the volumetric painting result. We explore precision issues stemming from ray-casting the octree of high depth, and provide a new analysis and verification. From our experimental results as well as the positive feedback from the participating artists, we strongly believe that our new 3D volume painting system can open up a new possibility for VR-driven digital art medium to professional artists as well as to novice users.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GPU Programming - Speeding Up the 3D Surface Generator VESTA,"The novel ""Volume-Enclosing Surface exTraction Algorithm"" (VESTA) generates triangular isosurfaces from computed tomography volumetric images and/or three-dimensional (3D) simulation data. Here, we present various benchmarks for GPU-based code implementations of both VESTA and the current state-of-the-art Marching Cubes Algorithm (MCA). One major result of this study is that VESTA runs significantly faster than the MCA.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D Primitives Gpgpu Generation for Volume Visualization in 3D Graphics Systems,"This article discusses the study of 3D graphic volume primitive computer system generation (3D segments) based on General Purpose Graphics Processing Unit (GPGPU) technology for 3D volume visualization systems. It is based on the general method of Volume 3D primitive generation and an algorithm for the voxelization of 3D lines, previously proposed and studied by the authors. We considered the Compute Unified Device Architect (CUDA) implementation of a parametric method for generating 3D line segments and characteristics of generation on modern Graphics Processing Units. Experiments on the test bench showed the relative inefficiency of generating a single 3D line segment and the efficiency of generating both fixed and arbitrary length of 3D segments on a Graphics Processing Unit (GPU). Experimental studies have proven the effectiveness and the quality of produced solutions by our method, when compared to existing state-of-the-art approaches.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models,"We present a new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models. Our method operates on a top-down image-based representation, and inserts objects iteratively into the scene by predicting their category, location, orientation and size with separate neural network modules. Our pipeline naturally supports automatic completion of partial scenes, as well as synthesis of complete scenes. Our method is significantly faster than the previous image-based method and generates result that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Layered-Garment Net: Generating Multiple Implicit Garment Layers from a Single Image,"Recent research works have focused on generating human models and garments from their 2D images. However, state-of-the-art researches focus either on only a single layer of the garment on a human model or on generating multiple garment layers without any guarantee of the intersection-free geometric relationship between them. In reality, people wear multiple layers of garments in their daily life, where an inner layer of garment could be partially covered by an outer one. In this paper, we try to address this multi-layer modeling problem and propose the Layered-Garment Net (LGN) that is capable of generating intersection-free multiple layers of garments defined by implicit function fields over the body surface, given the person's near front-view image. With a special design of garment indication fields (GIF), we can enforce an implicit covering relationship between the signed distance fields (SDF) of different layers to avoid self-intersections among different garment surfaces and the human body. Experiments demonstrate the strength of our proposed LGN framework in generating multi-layer garments as compared to state-of-the-art methods. To the best of our knowledge, LGN is the first research work to generate intersection-free multiple layers of garments on the human body from a single image.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Rewriting a Deep Generative Model,"A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation","3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis. Our project page is available at https://weizheliu.github.io/NeuSDFusion/ .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Character Animation in AR: Character Animation in AR: a mobile application development study,"Digital preservation of the cultural heritages is one of the major applications of various computer graphics and vision algorithms. The advancement in the AR/VR technologies is giving the cultural heritage preservation an interesting spin due to its immense visualization ability. The use of these technologies to digitally recreate heritage sites and art is becoming a popular trend. A project, called Indian Digital Heritage (IDH), for recreating the heritage site of Hampi, Karnataka during Vijaynagara empire ($1336$ - $1646$ CE) has been initiated by the Department of Science and Technology (DST) few years back. Immense work on surveying the site, collecting geographical and historic information about the life in Hampi, creating 3D models for buildings and people of Hampi and many other related tasks has been undertaken by various participants of this project. A major part of this project is to make tourists visiting Hampi visualize the life of people in ancient Hampi through any handy device. With such a requirement, the mobile AR based platform becomes a natural choice for developing any application for this purpose. We contributed to the project by developing an AR based mobile application to recreate a scene from Virupaksha Bazaar of Hampi with two components - author scene with augmented virtual contents at any scale, and visualize the same scene by reaching the physical location of augmentation. We develop an interactive application for the purpose of digitally recreating ancient Hampi. Though the focus of this work is not creating any static or dynamic content from scratch, it shows an interesting application of the content created in a real world scenario.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation,"We present a GAN-based Transformer for general action-conditioned 3D human motion generation, including not only single-person actions but also multi-person interactive actions. Our approach consists of a powerful Action-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Process latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superiority in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alternately modeling temporal correlations and human interactions with Transformer encoders. To further facilitate research on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat behaviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion representations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step towards a general human motion generator.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion,"In recent years, video generation has become a prominent generative tool and has drawn significant attention. However, there is little consideration in audio-to-video generation, though audio contains unique qualities like temporal semantics and magnitude. Hence, we propose The Power of Sound (TPoS) model to incorporate audio input that includes both changeable temporal semantics and magnitude. To generate video frames, TPoS utilizes a latent stable diffusion model with textual semantic information, which is then guided by the sequential audio embedding from our pretrained Audio Encoder. As a result, this method produces audio reactive video contents. We demonstrate the effectiveness of TPoS across various tasks and compare its results with current state-of-the-art techniques in the field of audio-to-video generation. More examples are available at https://ku-vai.github.io/TPoS/","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Quality Metric Guided Portrait Line Drawing Generation from Unpaired Training Data,"Face portrait line drawing is a unique style of art which is highly abstract and expressive. However, due to its high semantic constraints, many existing methods learn to generate portrait drawings using paired training data, which is costly and time-consuming to obtain. In this paper, we propose a novel method to automatically transform face photos to portrait drawings using unpaired training data with two new features; i.e., our method can (1) learn to generate high quality portrait drawings in multiple styles using a single network and (2) generate portrait drawings in a ""new style"" unseen in the training data. To achieve these benefits, we (1) propose a novel quality metric for portrait drawings which is learned from human perception, and (2) introduce a quality loss to guide the network toward generating better looking portrait drawings. We observe that existing unpaired translation methods such as CycleGAN tend to embed invisible reconstruction information indiscriminately in the whole drawings due to significant information imbalance between the photo and portrait drawing domains, which leads to important facial features missing. To address this problem, we propose a novel asymmetric cycle mapping that enforces the reconstruction information to be visible and only embedded in the selected facial regions. Along with localized discriminators for important facial regions, our method well preserves all important facial features in the generated drawings. Generator dissection further explains that our model learns to incorporate face semantic information during drawing generation. Extensive experiments including a user study show that our model outperforms state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GenHMR: Generative Human Mesh Recovery,"Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
DiffListener: Discrete Diffusion Model for Listener Generation,"The listener head generation (LHG) task aims to generate natural nonverbal listener responses based on the speaker's multimodal cues. While prior work either rely on limited modalities (e.g. audio and facial information) or employ autoregressive approaches which have limitations such as accumulating prediction errors. To address these limitations, we propose DiffListener, a discrete diffusion based approach for non-autoregressive listener head generation. Our model takes the speaker's facial information, audio, and text as inputs, additionally incorporating facial differential information to represent the temporal dynamics of expressions and movements. With this explicit modeling of facial dynamics, DiffListener can generate coherent reaction sequences in a non-autoregressive manner. Through comprehensive experiments, DiffListener demonstrates state-of-the-art performance in both quantitative and qualitative evaluations. The user study shows that DiffListener generates natural context-aware listener reactions that are well synchronized with the speaker. The code and demo videos are available in https://siyeoljung.github.io/DiffListener","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Few-shot Compositional Font Generation with Dual Memory,"Generating a new font library is a very labor-intensive and time-consuming job for glyph-rich scripts. Despite the remarkable success of existing font generation methods, they have significant drawbacks; they require a large number of reference images to generate a new font set, or they fail to capture detailed styles with only a few samples. In this paper, we focus on compositional scripts, a widely used letter system in the world, where each glyph can be decomposed by several components. By utilizing the compositionality of compositional scripts, we propose a novel font generation framework, named Dual Memory-augmented Font Generation Network (DM-Font), which enables us to generate a high-quality font library with only a few samples. We employ memory components and global-context awareness in the generator to take advantage of the compositionality. In the experiments on Korean-handwriting fonts and Thai-printing fonts, we observe that our method generates a significantly better quality of samples with faithful stylization compared to the state-of-the-art generation methods quantitatively and qualitatively. Source code is available at https://github.com/clovaai/dmfont.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Neural Wavelet-domain Diffusion for 3D Shape Generation,"This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Text to 3D Scene Generation with Rich Lexical Grounding,"The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Texture for Colors: Natural Representations of Colors Using Variable Bit-Depth Textures,"Numerous methods have been proposed to transform color and grayscale images to their single bit-per-pixel binary counterparts. Commonly, the goal is to enhance specific attributes of the original image to make it more amenable for analysis. However, when the resulting binarized image is intended for human viewing, aesthetics must also be considered. Binarization techniques, such as half-toning, stippling, and hatching, have been widely used for modeling the original image's intensity profile. We present an automated method to transform an image to a set of binary textures that represent not only the intensities, but also the colors of the original. The foundation of our method is information preservation: creating a set of textures that allows for the reconstruction of the original image's colors solely from the binarized representation. We present techniques to ensure that the textures created are not visually distracting, preserve the intensity profile of the images, and are natural in that they map sets of colors that are perceptually similar to patterns that are similar. The approach uses deep-neural networks and is entirely self-supervised; no examples of good vs. bad binarizations are required. The system yields aesthetically pleasing binary images when tested on a variety of image sources.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Talking-head Generation with Rhythmic Head Motion,"When people deliver a speech, they naturally move heads, and this rhythmic head motion conveys prosodic information. However, generating a lip-synced video while moving head naturally is challenging. While remarkably successful, existing works either generate still talkingface videos or rely on landmark/video frames as sparse/dense mapping guidance to generate head movements, which leads to unrealistic or uncontrollable video synthesis. To overcome the limitations, we propose a 3D-aware generative network along with a hybrid embedding module and a non-linear composition module. Through modeling the head motion and facial expressions1 explicitly, manipulating 3D animation carefully, and embedding reference images dynamically, our approach achieves controllable, photo-realistic, and temporally coherent talking-head videos with natural head movements. Thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available on https://github.com/ lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation,"We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
OpenCOLE: Towards Reproducible Automatic Graphic Design Generation,"Automatic generation of graphic designs has recently received considerable attention. However, the state-of-the-art approaches are complex and rely on proprietary datasets, which creates reproducibility barriers. In this paper, we propose an open framework for automatic graphic design called OpenCOLE, where we build a modified version of the pioneering COLE and train our model exclusively on publicly available datasets. Based on GPT4V evaluations, our model shows promising performance comparable to the original COLE. We release the pipeline and training results to encourage open development.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Sketch2Cloth: Sketch-based 3D Garment Generation with Unsigned Distance Fields,"3D model reconstruction from a single image has achieved great progress with the recent deep generative models. However, the conventional reconstruction approaches with template mesh deformation and implicit fields have difficulty in reconstructing non-watertight 3D mesh models, such as garments. In contrast to image-based modeling, the sketch-based approach can help users generate 3D models to meet the design intentions from hand-drawn sketches. In this study, we propose Sketch2Cloth, a sketch-based 3D garment generation system using the unsigned distance fields from the user's sketch input. Sketch2Cloth first estimates the unsigned distance function of the target 3D model from the sketch input, and extracts the mesh from the estimated field with Marching Cubes. We also provide the model editing function to modify the generated mesh. We verified the proposed Sketch2Cloth with quantitative evaluations on garment generation and editing with a state-of-the-art approach.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Automatic Generation of Constrained Furniture Layouts,"Efficient authoring of vast virtual environments hinges on algorithms that are able to automatically generate content while also being controllable. We propose a method to automatically generate furniture layouts for indoor environments. Our method is simple, efficient, human-interpretable and amenable to a wide variety of constraints. We model the composition of rooms into classes of objects and learn joint (co-occurrence) statistics from a database of training layouts. We generate new layouts by performing a sequence of conditional sampling steps, exploiting the statistics learned from the database. The generated layouts are specified as 3D object models, along with their positions and orientations. We show they are of equivalent perceived quality to the training layouts, and compare favorably to a state-of-the-art method. We incorporate constraints using a general mechanism -- rejection sampling -- which provides great flexibility at the cost of extra computation. We demonstrate the versatility of our method by applying a wide variety of constraints relevant to real-world applications.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MeshArt: Generating Articulated Meshes with Structure-Guided Transformers,"Articulated 3D object generation is fundamental for creating realistic, functional, and interactable virtual assets which are not simply static. We introduce MeshArt, a hierarchical transformer-based approach to generate articulated 3D meshes with clean, compact geometry, reminiscent of human-crafted 3D models. We approach articulated mesh generation in a part-by-part fashion across two stages. First, we generate a high-level articulation-aware object structure; then, based on this structural information, we synthesize each part's mesh faces. Key to our approach is modeling both articulation structures and part meshes as sequences of quantized triangle embeddings, leading to a unified hierarchical framework with transformers for autoregressive generation. Object part structures are first generated as their bounding primitives and articulation modes; a second transformer, guided by these articulation structures, then generates each part's mesh triangles. To ensure coherency among generated parts, we introduce structure-guided conditioning that also incorporates local part mesh connectivity. MeshArt shows significant improvements over state of the art, with 57.1% improvement in structure coverage and a 209-point improvement in mesh generation FID.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Geometry-Based Layout Generation with Hyper-Relations AMONG Objects,"Recent studies show increasing demands and interests in automatically generating layouts, while there is still much room for improving the plausibility and robustness. In this paper, we present a data-driven layout framework without model formulation and loss term optimization. We achieve and organize priors directly based on samples from datasets instead of sampling probabilistic models. Therefore, our method enables expressing and generating mathematically inexpressible relations among three or more objects. Subsequently, a non-learning geometric algorithm attempts arranging objects plausibly considering constraints such as walls, windows, etc. Experiments would show our generated layouts outperform the state-of-art and our framework is competitive to human designers.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning to Hallucinate Face Images via Component Generation and Enhancement,"We propose a two-stage method for face hallucination. First, we generate facial components of the input image using CNNs. These components represent the basic facial structures. Second, we synthesize fine-grained facial structures from high resolution training images. The details of these structures are transferred into facial components for enhancement. Therefore, we generate facial components to approximate ground truth global appearance in the first stage and enhance them through recovering details in the second stage. The experiments demonstrate that our method performs favorably against state-of-the-art methods","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements,"Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging. Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description. We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning. SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. These programs are generalized into compact, editable meta-programs. When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects. Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples. Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
TreEnhance: A Tree Search Method For Low-Light Image Enhancement,"In this paper we present TreEnhance, an automatic method for low-light image enhancement capable of improving the quality of digital images. The method combines tree search theory, and in particular the Monte Carlo Tree Search (MCTS) algorithm, with deep reinforcement learning. Given as input a low-light image, TreEnhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it. During the training phase, the method repeatedly alternates two main phases: a generation phase, where a modified version of MCTS explores the space of image editing operations and selects the most promising sequence, and an optimization phase, where the parameters of a neural network, implementing the enhancement policy, are updated.   Two different inference solutions are proposed for the enhancement of new images: one is based on MCTS and is more accurate but more time and memory consuming; the other directly applies the learned policy and is faster but slightly less precise. As a further contribution, we propose a guided search strategy that ""reverses"" the enhancement procedure that a photo editor applied to a given input image. Unlike other methods from the state of the art, TreEnhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning. We tested the method on two datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good results from both a qualitative and a quantitative point of view.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"Procedural Generation and Rendering of Realistic, Navigable Forest Environments: An Open-Source Tool","Simulation of forest environments has applications from entertainment and art creation to commercial and scientific modelling. Due to the unique features and lighting in forests, a forest-specific simulator is desirable, however many current forest simulators are proprietary or highly tailored to a particular application. Here we review several areas of procedural generation and rendering specific to forest generation, and utilise this to create a generalised, open-source tool for generating and rendering interactive, realistic forest scenes. The system uses specialised L-systems to generate trees which are distributed using an ecosystem simulation algorithm. The resulting scene is rendered using a deferred rendering pipeline, a Blinn-Phong lighting model with real-time leaf transparency and post-processing lighting effects. The result is a system that achieves a balance between high natural realism and visual appeal, suitable for tasks including training computer vision algorithms for autonomous robots and visual media generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generative AI Framework for 3D Object Generation in Augmented Reality,"This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Sketch-Guided Scene Image Generation,"Text-to-image models are showcasing the impressive ability to create high-quality and diverse generative images. Nevertheless, the transition from freehand sketches to complex scene images remains challenging using diffusion models. In this study, we propose a novel sketch-guided scene image generation framework, decomposing the task of scene image scene generation from sketch inputs into object-level cross-domain generation and scene-level image construction. We employ pre-trained diffusion models to convert each single object drawing into an image of the object, inferring additional details while maintaining the sparse sketch structure. In order to maintain the conceptual fidelity of the foreground during scene generation, we invert the visual features of object images into identity embeddings for scene generation. In scene-level image construction, we generate the latent representation of the scene image using the separated background prompts, and then blend the generated foreground objects according to the layout of the sketch input. To ensure the foreground objects' details remain unchanged while naturally composing the scene image, we infer the scene image on the blended latent representation using a global prompt that includes the trained identity tokens. Through qualitative and quantitative experiments, we demonstrate the ability of the proposed approach to generate scene images from hand-drawn sketches surpasses the state-of-the-art approaches.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-Driven Human Motion Generation,"Achieving high-fidelity and temporally smooth 3D human motion generation remains a challenge, particularly within resource-constrained environments. We introduce FlowMotion, a novel method leveraging Conditional Flow Matching (CFM). FlowMotion incorporates a training objective within CFM that focuses on more accurately predicting target motion in 3D human motion generation, resulting in enhanced generation fidelity and temporal smoothness while maintaining the fast synthesis times characteristic of flow-matching-based methods. FlowMotion achieves state-of-the-art jitter performance, achieving the best jitter in the KIT dataset and the second-best jitter in the HumanML3D dataset, and a competitive FID value in both datasets. This combination provides robust and natural motion sequences, offering a promising equilibrium between generation quality and temporal naturalness.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN,"We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both GAN inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Responsible Disclosure of Generative Models Using Scalable Fingerprinting,"Over the past years, deep generative models have achieved a new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than $10^{38}$ identifiable models. Experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution. Code and models are available at https://github.com/ningyu1991/ScalableGANFingerprints .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Towards Photo-Realistic Virtual Try-On by Adaptively Generating$\leftrightarrow$Preserving Image Content,"Image visual try-on aims at transferring a target clothing image onto a reference person, and has become a hot topic in recent years. Prior arts usually focus on preserving the character of a clothing image (e.g. texture, logo, embroidery) when warping it to arbitrary human pose. However, it remains a big challenge to generate photo-realistic try-on images when large occlusions and human poses are presented in the reference person. To address this issue, we propose a novel visual try-on network, namely Adaptive Content Generating and Preserving Network (ACGPN). In particular, ACGPN first predicts semantic layout of the reference image that will be changed after try-on (e.g. long sleeve shirt$\rightarrow$arm, arm$\rightarrow$jacket), and then determines whether its image content needs to be generated or preserved according to the predicted semantic layout, leading to photo-realistic try-on and rich clothing details. ACGPN generally involves three major modules. First, a semantic layout generation module utilizes semantic segmentation of the reference image to progressively predict the desired semantic layout after try-on. Second, a clothes warping module warps clothing images according to the generated semantic layout, where a second-order difference constraint is introduced to stabilize the warping process during training. Third, an inpainting module for content fusion integrates all information (e.g. reference image, semantic layout, warped clothes) to adaptively produce each semantic part of human body. In comparison to the state-of-the-art methods, ACGPN can generate photo-realistic images with much better perceptual quality and richer fine-details.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically Structured Sequences,"Autoregressive models have proven to be very powerful in NLP text generation tasks and lately have gained popularity for image generation as well. However, they have seen limited use for the synthesis of 3D shapes so far. This is mainly due to the lack of a straightforward way to linearize 3D data as well as to scaling problems with the length of the resulting sequences when describing complex shapes. In this work we address both of these problems. We use octrees as a compact hierarchical shape representation that can be sequentialized by traversal ordering. Moreover, we introduce an adaptive compression scheme, that significantly reduces sequence lengths and thus enables their effective generation with a transformer, while still allowing fully autoregressive sampling and parallel training. We demonstrate the performance of our model by comparing against the state-of-the-art in shape generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Wish You Were Here: Context-Aware Human Generation,"We present a novel method for inserting objects, specifically humans, into existing images, such that they blend in a photorealistic manner, while respecting the semantic context of the scene. Our method involves three subnetworks: the first generates the semantic map of the new person, given the pose of the other persons in the scene and an optional bounding box specification. The second network renders the pixels of the novel person and its blending mask, based on specifications in the form of multiple appearance components. A third network refines the generated face in order to match those of the target person. Our experiments present convincing high-resolution outputs in this novel and challenging application domain. In addition, the three networks are evaluated individually, demonstrating for example, state of the art results in pose transfer benchmarks.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generative Novel View Synthesis with 3D-Aware Diffusion Models,"We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding,"The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI. VisionGPT-3D provides a versatile multimodal framework building upon the strengths of multimodal foundation models. It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse multimodal inputs such as text prompts.   Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse,"While motion generation has made substantial progress, its practical application remains constrained by dataset diversity and scale, limiting its ability to handle out-of-distribution scenarios. To address this, we propose a simple and effective baseline, RMD, which enhances the generalization of motion generation through retrieval-augmented techniques. Unlike previous retrieval-based methods, RMD requires no additional training and offers three key advantages: (1) the external retrieval database can be flexibly replaced; (2) body parts from the motion database can be reused, with an LLM facilitating splitting and recombination; and (3) a pre-trained motion diffusion model serves as a prior to improve the quality of motions obtained through retrieval and direct combination. Without any training, RMD achieves state-of-the-art performance, with notable advantages on out-of-distribution data.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
UV-free Texture Generation with Denoising and Geodesic Heat Diffusions,"Seams, distortions, wasted UV space, vertex-duplication, and varying resolution over the surface are the most prominent issues of the standard UV-based texturing of meshes. These issues are particularly acute when automatic UV-unwrapping techniques are used. For this reason, instead of generating textures in automatically generated UV-planes like most state-of-the-art methods, we propose to represent textures as coloured point-clouds whose colours are generated by a denoising diffusion probabilistic model constrained to operate on the surface of 3D objects. Our sampling and resolution agnostic generative model heavily relies on heat diffusion over the surface of the meshes for spatial communication between points. To enable processing of arbitrarily sampled point-cloud textures and ensure long-distance texture consistency we introduce a fast re-sampling of the mesh spectral properties used during the heat diffusion and introduce a novel heat-diffusion-based self-attention mechanism. Our code and pre-trained models are available at github.com/simofoti/UV3-TeD.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Quantum Brush: A quantum computing-based tool for digital painting,"We present Quantum Brush, an open-source digital painting tool that harnesses quantum computing to generate novel artistic expressions. The tool includes four different brushes that translate strokes into unique quantum algorithms, each highlighting a different way in which quantum effects can produce novel aesthetics. Each brush is designed to be compatible with the current noisy intermediate-scale quantum (NISQ) devices, as demonstrated by executing them on IQM's Sirius device.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation,"In poster design, content-aware layout generation is crucial for automatically arranging visual-textual elements on the given image. With limited training data, existing work focused on image-centric enhancement. However, this neglects the diversity of layouts and fails to cope with shape-variant elements or diverse design intents in generalized settings. To this end, we proposed a layout-centric approach that leverages layout knowledge implicit in large language models (LLMs) to create posters for omnifarious purposes, hence the name PosterO. Specifically, it structures layouts from datasets as trees in SVG language by universal shape, design intent vectorization, and hierarchical node representation. Then, it applies LLMs during inference to predict new layout trees by in-context learning with intent-aligned example selection. After layout trees are generated, we can seamlessly realize them into poster designs by editing the chat with LLMs. Extensive experimental results have demonstrated that PosterO can generate visually appealing layouts for given images, achieving new state-of-the-art performance across various benchmarks. To further explore PosterO's abilities under the generalized settings, we built PStylish7, the first dataset with multi-purpose posters and various-shaped elements, further offering a challenging test for advanced research.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction,"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Efficient and Scalable Chinese Vector Font Generation via Component Composition,"Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters. Recent advances remain limited to generating a small set of characters with simple structure. In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components. Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components. To achieve this, we collect a large-scale dataset that contains over \textit{90K} Chinese characters with their components and layout information. Upon the dataset, we propose a simple yet effective framework based on spatial transformer networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the Bzier curves, resulting in Chinese characters in vector format. Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and zero-shot font extension.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising,"We demonstrate generating HDR images using the concerted action of multiple black-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR generative diffusion models is vital as, first, there is no sufficiently large HDR image dataset available to re-train them, and, second, even if it was, re-training such models is impossible for most compute budgets. Instead, we seek inspiration from the HDR image capture literature that traditionally fuses sets of LDR images, called ""exposure brackets'', to produce a single HDR image. We operate multiple denoising processes to generate multiple LDR brackets that together form a valid HDR result. The key to making this work is to introduce a consistency term into the diffusion process to couple the brackets such that they agree across the exposure range they share while accounting for possible differences due to the quantization error. We demonstrate state-of-the-art unconditional and conditional or restoration-type (LDR2HDR) generative modeling results, yet in HDR.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing,"Image smoothing is a fundamental procedure in applications of both computer vision and graphics. The required smoothing properties can be different or even contradictive among different tasks. Nevertheless, the inherent smoothing nature of one smoothing operator is usually fixed and thus cannot meet the various requirements of different applications. In this paper, we first introduce the truncated Huber penalty function which shows strong flexibility under different parameter settings. A generalized framework is then proposed with the introduced truncated Huber penalty function. When combined with its strong flexibility, our framework is able to achieve diverse smoothing natures where contradictive smoothing behaviors can even be achieved. It can also yield the smoothing behavior that can seldom be achieved by previous methods, and superior performance is thus achieved in challenging cases. These together enable our framework capable of a range of applications and able to outperform the state-of-the-art approaches in several tasks, such as image detail enhancement, clip-art compression artifacts removal, guided depth map restoration, image texture removal, etc. In addition, an efficient numerical solution is provided and its convergence is theoretically guaranteed even the optimization framework is non-convex and non-smooth. A simple yet effective approach is further proposed to reduce the computational cost of our method while maintaining its performance. The effectiveness and superior performance of our approach are validated through comprehensive experiments in a range of applications. Our code is available at https://github.com/wliusjtu/Generalized-Smoothing-Framework.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Facial Reenactment Through a Personalized Generator,"In recent years, the role of image generative models in facial reenactment has been steadily increasing. Such models are usually subject-agnostic and trained on domain-wide datasets. The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination. Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual. In this paper, we propose a novel method for facial reenactment using a personalized generator. We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera. Images synthesized by the personalized generator are guaranteed to preserve identity. The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions. To this end, we locate the desired frames in the latent space of the personalized generator using carefully designed latent optimization. Through extensive evaluation, we demonstrate state-of-the-art performance for facial reenactment. Furthermore, we show that since our reenactment takes place in a semantic latent space, it can be semantically edited and stylized in post-processing.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk,"We introduce Mesh Silksong, a compact and efficient mesh representation tailored to generate the polygon mesh in an auto-regressive manner akin to silk weaving. Existing mesh tokenization methods always produce token sequences with repeated vertex tokens, wasting the network capability. Therefore, our approach tokenizes mesh vertices by accessing each mesh vertice only once, reduces the token sequence's redundancy by 50\%, and achieves a state-of-the-art compression rate of approximately 22\%. Furthermore, Mesh Silksong produces polygon meshes with superior geometric properties, including manifold topology, watertight detection, and consistent face normals, which are critical for practical applications. Experimental results demonstrate the effectiveness of our approach, showcasing not only intricate mesh generation but also significantly improved geometric integrity.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis,"We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ChartPointFlow for Topology-Aware 3D Point Cloud Generation,"A point cloud serves as a representation of the surface of a three-dimensional (3D) shape. Deep generative models have been adapted to model their variations typically using a map from a ball-like set of latent variables. However, previous approaches did not pay much attention to the topological structure of a point cloud, despite that a continuous map cannot express the varying numbers of holes and intersections. Moreover, a point cloud is often composed of multiple subparts, and it is also difficult to express. In this study, we propose ChartPointFlow, a flow-based generative model with multiple latent labels for 3D point clouds. Each label is assigned to points in an unsupervised manner. Then, a map conditioned on a label is assigned to a continuous subset of a point cloud, similar to a chart of a manifold. This enables our proposed model to preserve the topological structure with clear boundaries, whereas previous approaches tend to generate blurry point clouds and fail to generate holes. The experimental results demonstrate that ChartPointFlow achieves state-of-the-art performance in terms of generation and reconstruction compared with other point cloud generators. Moreover, ChartPointFlow divides an object into semantic subparts using charts, and it demonstrates superior performance in case of unsupervised segmentation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Recent Advance in 3D Object and Scene Generation: A Survey,"In recent years, the demand for 3D content has grown exponentially with intelligent upgrading of interactive media, extended reality (XR), and Metaverse industries. In order to overcome the limitation of traditional manual modeling approaches, such as labor-intensive workflows and prolonged production cycles, revolutionary advances have been achieved through the convergence of novel 3D representation paradigms and artificial intelligence generative technologies. In this survey, we conduct a systematically review of the cutting-edge achievements in static 3D object and scene generation, as well as establish a comprehensive technical framework through systematic categorization. Specifically, we initiate our analysis with mainstream 3D object representations, followed by in-depth exploration of two principal technical pathways in object generation: data-driven supervised learning methods and deep generative model-based approaches. Regarding scene generation, we focus on three dominant paradigms: layout-guided compositional synthesis, 2D prior-based scene generation, and rule-driven modeling. Finally, we critically examine persistent challenges in 3D generation and propose potential research directions for future investigation. This survey aims to provide readers with a structured understanding of state-of-the-art 3D generation technologies while inspiring researchers to undertake more exploration in this domain.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
AutoSimulate: (Quickly) Learning Synthetic Data Generation,"Simulation is increasingly being used for generating large labelled datasets in many machine learning problems. Recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task, usually relying on REINFORCE-like gradient estimators. However these approaches are very expensive as they treat the entire data generation, model training, and validation pipeline as a black-box and require multiple costly objective evaluations at each iteration. We propose an efficient alternative for optimal synthetic data generation, based on a novel differentiable approximation of the objective. This allows us to optimize the simulator, which may be non-differentiable, requiring only one objective evaluation at each iteration with a little overhead. We demonstrate on a state-of-the-art photorealistic renderer that the proposed method finds the optimal data distribution faster (up to $50\times$), with significantly reduced training data generation (up to $30\times$) and better accuracy ($+8.7\%$) on real-world test datasets than previous methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation,"Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
FlexPainter: Flexible and Multi-View Consistent Texture Generation,"Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce \textbf{FlexPainter}, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Faster Projected GAN: Towards Faster Few-Shot Image Generation,"In order to solve the problems of long training time, large consumption of computing resources and huge parameter amount of GAN network in image generation, this paper proposes an improved GAN network model, which is named Faster Projected GAN, based on Projected GAN. The proposed network is mainly focuses on the improvement of generator of Projected GAN. By introducing depth separable convolution (DSC), the number of parameters of the Projected GAN is reduced, the training speed is accelerated, and memory is saved. Experimental results show that on ffhq-1k, art-painting, Landscape and other few-shot image datasets, a 20% speed increase and a 15% memory saving are achieved. At the same time, FID loss is less or no loss, and the amount of model parameters is better controlled. At the same time, significant training speed improvement has been achieved in the small sample image generation task of special scenes such as earthquake scenes with few public datasets.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN,"The research topic of sketch-to-portrait generation has witnessed a boost of progress with deep learning techniques. The recently proposed StyleGAN architectures achieve state-of-the-art generation ability but the original StyleGAN is not friendly for sketch-based creation due to its unconditional generation nature. To address this issue, we propose a direct conditioning strategy to better preserve the spatial information under the StyleGAN framework. Specifically, we introduce Spatially Conditioned StyleGAN (SC-StyleGAN for short), which explicitly injects spatial constraints to the original StyleGAN generation process. We explore two input modalities, sketches and semantic maps, which together allow users to express desired generation results more precisely and easily. Based on SC-StyleGAN, we present DrawingInStyles, a novel drawing interface for non-professional users to easily produce high-quality, photo-realistic face images with precise control, either from scratch or editing existing ones. Qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion,"Controllable, high-fidelity mesh editing remains a significant challenge in 3D content creation. Existing generative methods often struggle with complex geometries and fail to produce detailed results. We propose CraftMesh, a novel framework for high-fidelity generative mesh manipulation via Poisson Seamless Fusion. Our key insight is to decompose mesh editing into a pipeline that leverages the strengths of 2D and 3D generative models: we edit a 2D reference image, then generate a region-specific 3D mesh, and seamlessly fuse it into the original model. We introduce two core techniques: Poisson Geometric Fusion, which utilizes a hybrid SDF/Mesh representation with normal blending to achieve harmonious geometric integration, and Poisson Texture Harmonization for visually consistent texture blending. Experimental results demonstrate that CraftMesh outperforms state-of-the-art methods, delivering superior global consistency and local detail in complex editing tasks.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
OmniControl: Control Any Joint at Any Time for Human Motion Generation,"We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Auto-Regressive Diffusion for Generating 3D Human-Object Interactions,"Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging field with applications in animation, video games, virtual reality, and robotics. A key challenge in HOI generation is maintaining interaction consistency in long sequences. Existing Text-to-Motion-based approaches, such as discrete motion tokenization, cannot be directly applied to HOI generation due to limited data in this domain and the complexity of the modality. To address the problem of interaction consistency in long sequences, we propose an autoregressive diffusion model (ARDHOI) that predicts the next continuous token. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE) to learn a physically plausible space of continuous HOI tokens, thereby ensuring that generated human-object motions are realistic and natural. For generating sequences autoregressively, we develop a Mamba-based context encoder to capture and maintain consistent sequential actions. Additionally, we implement an MLP-based denoiser to generate the subsequent token conditioned on the encoded context. Our model has been evaluated on the OMOMO and BEHAVE datasets, where it outperforms existing state-of-the-art methods in terms of both performance and inference speed. This makes ARDHOI a robust and efficient solution for text-driven HOI tasks","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
OctFusion: Octree-based Diffusion Models for 3D Shape Generation,"Diffusion models have emerged as a popular method for 3D generation. However, it is still challenging for diffusion models to efficiently generate diverse and high-quality 3D shapes. In this paper, we introduce OctFusion, which can generate 3D shapes with arbitrary resolutions in 2.5 seconds on a single Nvidia 4090 GPU, and the extracted meshes are guaranteed to be continuous and manifold. The key components of OctFusion are the octree-based latent representation and the accompanying diffusion models. The representation combines the benefits of both implicit neural representations and explicit spatial octrees and is learned with an octree-based variational autoencoder. The proposed diffusion model is a unified multi-scale U-Net that enables weights and computation sharing across different octree levels and avoids the complexity of widely used cascaded diffusion schemes. We verify the effectiveness of OctFusion on the ShapeNet and Objaverse datasets and achieve state-of-the-art performances on shape generation tasks. We demonstrate that OctFusion is extendable and flexible by generating high-quality color fields for textured mesh generation and high-quality 3D shapes conditioned on text prompts, sketches, or category labels. Our code and pre-trained models are available at https://github.com/octree-nn/octfusion.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Shape Conditioned Human Motion Generation with Diffusion Model,"Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and muscles, considering solely the skeleton for generation may neglect their inherent interdependency, which can limit the variability and precision of the generated results. To address this issue, we propose a Shape-conditioned Motion Diffusion model (SMD), which enables the generation of motion sequences directly in mesh format, conditioned on a specified target mesh. In SMD, the input meshes are transformed into spectral coefficients using graph Laplacian, to efficiently represent meshes. Subsequently, we propose a Spectral-Temporal Autoencoder (STAE) to leverage cross-temporal dependencies within the spectral domain. Extensive experimental evaluations show that SMD not only produces vivid and realistic motions but also achieves competitive performance in text-to-motion and action-to-motion tasks when compared to state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Unsupervised multi-modal Styled Content Generation,"The emergence of deep generative models has recently enabled the automatic generation of massive amounts of graphical content, both in 2D and in 3D. Generative Adversarial Networks (GANs) and style control mechanisms, such as Adaptive Instance Normalization (AdaIN), have proved particularly effective in this context, culminating in the state-of-the-art StyleGAN architecture. While such models are able to learn diverse distributions, provided a sufficiently large training set, they are not well-suited for scenarios where the distribution of the training data exhibits a multi-modal behavior. In such cases, reshaping a uniform or normal distribution over the latent space into a complex multi-modal distribution in the data domain is challenging, and the generator might fail to sample the target distribution well. Furthermore, existing unsupervised generative models are not able to control the mode of the generated samples independently of the other visual attributes, despite the fact that they are typically disentangled in the training data.   In this paper, we introduce UMMGAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that UMMGAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generating 360 Video is What You Need For a 3D Scene,"Generating 3D scenes is still a challenging task due to the lack of readily available scene data. Most existing methods only produce partial scenes and provide limited navigational freedom. We introduce a practical and scalable solution that uses 360 video as an intermediate scene representation, capturing the full-scene context and ensuring consistent visual content throughout the generation. We propose WorldPrompter, a generative pipeline that synthesizes traversable 3D scenes from text prompts. WorldPrompter incorporates a conditional 360 panoramic video generator, capable of producing a 128-frame video that simulates a person walking through and capturing a virtual environment. The resulting video is then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor, enabling a true walkable experience within the 3D scene. Experiments demonstrate that our panoramic video generation model, trained with a mix of image and video data, achieves convincing spatial and temporal consistency for static scenes. This is validated by an average COLMAP matching rate of 94.6\%, allowing for high-quality panoramic Gaussian splat reconstruction and improved navigation throughout the scene. Qualitative and quantitative results also show it outperforms the state-of-the-art 360 video generators and 3D scene generation models.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Matrix-3D: Omnidirectional Explorable 3D World Generation,"Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis,"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what generative models have learned inside the deep generative representations and how photo-realistic images are able to be composed of the layer-wise stochasticity introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results further suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
IGG: Image Generation Informed by Geodesic Dynamics in Deformation Spaces,"Generative models have recently gained increasing attention in image generation and editing tasks. However, they often lack a direct connection to object geometry, which is crucial in sensitive domains such as computational anatomy, biology, and robotics. This paper presents a novel framework for Image Generation informed by Geodesic dynamics (IGG) in deformation spaces. Our IGG model comprises two key components: (i) an efficient autoencoder that explicitly learns the geodesic path of image transformations in the latent space; and (ii) a latent geodesic diffusion model that captures the distribution of latent representations of geodesic deformations conditioned on text instructions. By leveraging geodesic paths, our method ensures smooth, topology-preserving, and interpretable deformations, capturing complex variations in image structures while maintaining geometric consistency. We validate the proposed IGG on plant growth data and brain magnetic resonance imaging (MRI). Experimental results show that IGG outperforms the state-of-the-art image generation/editing models with superior performance in generating realistic, high-quality images with preserved object topology and reduced artifacts. Our code is publicly available at https://github.com/nellie689/IGG.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning Generative Models of Shape Handles,"We present a generative model to synthesize 3D shapes as sets of handles -- lightweight proxies that approximate the original 3D shape -- for applications in interactive editing, shape parsing, and building compact 3D representations. Our model can generate handle sets with varying cardinality and different types of handles (Figure 1). Key to our approach is a deep architecture that predicts both the parameters and existence of shape handles, and a novel similarity measure that can easily accommodate different types of handles, such as cuboids or sphere-meshes. We leverage the recent advances in semantic 3D annotation as well as automatic shape summarizing techniques to supervise our approach. We show that the resulting shape representations are intuitive and achieve superior quality than previous state-of-the-art. Finally, we demonstrate how our method can be used in applications such as interactive shape editing, completion, and interpolation, leveraging the latent space learned by our model to guide these tasks. Project page: http://mgadelha.me/shapehandles.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality,"High-quality environment lighting is essential for creating immersive mobile augmented reality (AR) experiences. However, achieving visually coherent estimation for mobile AR is challenging due to several key limitations in AR device sensing capabilities, including low camera FoV and limited pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. In this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality, diverse environment maps in the format of 360 HDR images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the output aligns with the physical environment's visual context and color appearance. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. Through a combination of quantitative and qualitative evaluations, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. For example, CleAR achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. CleAR produces lighting estimates of comparable or better quality in just 3.2 seconds -- over 110X faster than state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Random-phase Gaussian Wave Splatting for Computer-generated Holography,"Holographic near-eye displays offer ultra-compact form factors for virtual and augmented reality systems, but rely on advanced computer-generated holography (CGH) algorithms to convert 3D scenes into interference patterns that can be displayed on spatial light modulators (SLMs). Gaussian Wave Splatting (GWS) has recently emerged as a powerful CGH paradigm that allows for the conversion of Gaussians, a state-of-the-art neural 3D representation, into holograms. However, GWS assumes smooth-phase distributions over the Gaussian primitives, limiting their ability to model view-dependent effects and reconstruct accurate defocus blur, and severely under-utilizing the space-bandwidth product of the SLM. In this work, we propose random-phase GWS (GWS-RP) to improve bandwidth utilization, which has the effect of increasing eyebox size, reconstructing accurate defocus blur and parallax, and supporting time-multiplexed rendering to suppress speckle artifacts.   At the core of GWS-RP are (1) a fundamentally new wavefront compositing procedure and (2) an alpha-blending scheme specifically designed for random-phase Gaussian primitives, ensuring physically correct color reconstruction and robust occlusion handling. Additionally, we present the first formally derived algorithm for applying random phase to Gaussian primitives, grounded in rigorous statistical optics analysis and validated through practical near-eye display applications. Through extensive simulations and experimental validations, we demonstrate that these advancements, collectively with time-multiplexing, uniquely enables full-bandwith light field CGH that supports accurate accurate parallax and defocus, yielding state-of-the-art image quality and perceptually faithful 3D holograms for next-generation near-eye displays.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StableMaterials: Enhancing Diversity in Material Generation via Semi-Supervised Learning,"We introduce StableMaterials, a novel approach for generating photorealistic physical-based rendering (PBR) materials that integrate semi-supervised learning with Latent Diffusion Models (LDMs). Our method employs adversarial training to distill knowledge from existing large-scale image generation models, minimizing the reliance on annotated data and enhancing the diversity in generation. This distillation approach aligns the distribution of the generated materials with that of image textures from an SDXL model, enabling the generation of novel materials that are not present in the initial training dataset. Furthermore, we employ a diffusion-based refiner model to improve the visual quality of the samples and achieve high-resolution generation. Finally, we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps. We detail the architecture and training process of StableMaterials, the integration of semi-supervised training within existing LDM frameworks and show the advantages of our approach. Comparative evaluations with state-of-the-art methods show the effectiveness of StableMaterials, highlighting its potential applications in computer graphics and beyond. StableMaterials is publicly available at https://gvecchio.com/stablematerials.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Multi-Criteria Assessment of Shape Quality in CAD Systems of the Future,"Unlike many other works, where authors are usually focused on one or two quality criteria, the current manuscript, which is a generalization of the article [35] published in Russian, offers a multi-criteria approach to the assessment of the shape quality of curves that constitute component parts of the surfaces used for the computer modelling of object shapes in various types of design. Based on the analysis of point particle motion along a curved path, requirements for the quality of functional curves are proposed: a high order of smoothness, a minimum number of curvature extrema, minimization of the maximum value of curvature and its variation rate, minimization of the potential energy of the curve, and aesthetic analysis from the standpoint of the laws of technical aesthetics. The authors do not set themselves the task of giving a simple and precise mathematical definition of such curves. On the contrary, this category can include various curves that meet certain quality criteria, the refinement and addition of which is possible in the near future. Engineering practice shows that quality criteria can change over time, which does not diminish the need to develop multi-criteria methods for assessing the quality of geometric shapes. Technical issues faced during edge rounding in 3D models that affect the quality of industrial design product shape have been reviewed as an example of the imperfection of existing CAD systems.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
LumiGAN: Unconditional Generation of Relightable 3D Human Faces,"Unsupervised learning of 3D human faces from unstructured 2D image data is an active research area. While recent works have achieved an impressive level of photorealism, they commonly lack control of lighting, which prevents the generated assets from being deployed in novel environments. To this end, we introduce LumiGAN, an unconditional Generative Adversarial Network (GAN) for 3D human faces with a physically based lighting module that enables relighting under novel illumination at inference time. Unlike prior work, LumiGAN can create realistic shadow effects using an efficient visibility formulation that is learned in a self-supervised manner. LumiGAN generates plausible physical properties for relightable faces, including surface normals, diffuse albedo, and specular tint without any ground truth data. In addition to relightability, we demonstrate significantly improved geometry generation compared to state-of-the-art non-relightable 3D GANs and notably better photorealism than existing relightable GANs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SViM3D: Stable Video Material Diffusion for Single Image 3D Generation,"We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D generation on ImageNet,"Existing 3D-from-2D generators are typically designed for well-curated single-category datasets, where all the objects have (approximately) the same scale, 3D location, and orientation, and the camera always points to the center of the scene. This makes them inapplicable to diverse, in-the-wild datasets of non-alignable scenes rendered from arbitrary camera poses. In this work, we develop a 3D generator with Generic Priors (3DGP): a 3D synthesis framework with more general assumptions about the training data, and show that it scales to very challenging datasets, like ImageNet. Our model is based on three new ideas. First, we incorporate an inaccurate off-the-shelf depth estimator into 3D GAN training via a special depth adaptation module to handle the imprecision. Then, we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training. Finally, we extend the recent ideas of transferring knowledge from pre-trained classifiers into GANs for patch-wise trained models by employing a simple distillation-based technique on top of the discriminator. It achieves more stable training than the existing methods and speeds up the convergence by at least 40%. We explore our model on four datasets: SDIP Dogs 256x256, SDIP Elephants 256x256, LSUN Horses 256x256, and ImageNet 256x256, and demonstrate that 3DGP outperforms the recent state-of-the-art in terms of both texture and geometry quality. Code and visualizations: https://snap-research.github.io/3dgp.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Assessing Image Quality Using a Simple Generative Representation,"Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as perceived by a human observer. Current state-of-the-art techniques are based on deep representations trained in discriminative manner. Such representations may ignore visually important features, if they are not predictive of class labels. Recent generative models successfully learn low-dimensional representations using auto-encoding and have been argued to preserve better visual features. Here we leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer,"In the field of 3D point cloud generation, numerous 3D generative models have demonstrated the ability to generate diverse and realistic 3D shapes. However, the majority of these approaches struggle to generate controllable 3D point cloud shapes that meet user-specific requirements, hindering the large-scale application of 3D point cloud generation. To address the challenge of lacking control in 3D point cloud generation, we are the first to propose controlling the generation of point clouds by shape structures that comprise part existences and part adjacency relationships. We manually annotate the adjacency relationships between the segmented parts of point cloud shapes, thereby constructing a StructureGraph representation. Based on this StructureGraph representation, we introduce StrucADT, a novel structure-controllable point cloud generation model, which consists of StructureGraphNet module to extract structure-aware latent features, cCNF Prior module to learn the distribution of the latent features controlled by the part adjacency, and Diffusion Transformer module conditioned on the latent features and part adjacency to generate structure-consistent point cloud shapes. Experimental results demonstrate that our structure-controllable 3D point cloud generation method produces high-quality and diverse point cloud shapes, enabling the generation of controllable point clouds based on user-specified shape structures and achieving state-of-the-art performance in controllable point cloud generation on the ShapeNet dataset.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation","This paper presents a new approach for 3D shape generation, inversion, and manipulation, through a direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a diffusion-based generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details. Further, we may jointly train an encoder network to learn a latent space for inverting shapes, allowing us to enable a rich variety of whole-shape and region-aware shape manipulations. Both quantitative and qualitative experimental results manifest the compelling shape generation, inversion, and manipulation capabilities of our approach over the state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Visual Object Networks: Image Generation with Disentangled 3D Representation,"Recent progress in deep generative models has led to tremendous breakthroughs in image generation. However, while existing models can synthesize photorealistic images, they lack an understanding of our underlying 3D world. We present a new generative model, Visual Object Networks (VON), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel our image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches to generate natural images. The VON not only generates images that are more realistic than state-of-the-art 2D image synthesis methods, but also enables many 3D operations such as changing the viewpoint of a generated image, editing of shape and texture, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars,"Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control,"Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments. Project page: https://collaborativevideodiffusion.github.io/.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SketchGen: Generating Constrained CAD Sketches,"Computer-aided design (CAD) is the most widely used modeling approach for technical design. The typical starting point in these designs is 2D sketches which can later be extruded and combined to obtain complex three-dimensional assemblies. Such sketches are typically composed of parametric primitives, such as points, lines, and circular arcs, augmented with geometric constraints linking the primitives, such as coincidence, parallelism, or orthogonality. Sketches can be represented as graphs, with the primitives as nodes and the constraints as edges. Training a model to automatically generate CAD sketches can enable several novel workflows, but is challenging due to the complexity of the graphs and the heterogeneity of the primitives and constraints. In particular, each type of primitive and constraint may require a record of different size and parameter types. We propose SketchGen as a generative model based on a transformer architecture to address the heterogeneity problem by carefully designing a sequential language for the primitives and constraints that allows distinguishing between different primitive or constraint types and their parameters, while encouraging our model to re-use information across related parameters, encoding shared structure. A particular highlight of our work is the ability to produce primitives linked via constraints that enables the final output to be further regularized via a constraint solver. We evaluate our model by demonstrating constraint prediction for given sets of primitives and full sketch generation from scratch, showing that our approach significantly out performs the state-of-the-art in CAD sketch generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Automatic Camera Trajectory Control with Enhanced Immersion for Virtual Cinematography,"User-generated cinematic creations are gaining popularity as our daily entertainment, yet it is a challenge to master cinematography for producing immersive contents. Many existing automatic methods focus on roughly controlling predefined shot types or movement patterns, which struggle to engage viewers with the circumstances of the actor. Real-world cinematographic rules show that directors can create immersion by comprehensively synchronizing the camera with the actor. Inspired by this strategy, we propose a deep camera control framework that enables actor-camera synchronization in three aspects, considering frame aesthetics, spatial action, and emotional status in the 3D virtual stage. Following rule-of-thirds, our framework first modifies the initial camera placement to position the actor aesthetically. This adjustment is facilitated by a self-supervised adjustor that analyzes frame composition via camera projection. We then design a GAN model that can adversarially synthesize fine-grained camera movement based on the physical action and psychological state of the actor, using an encoder-decoder generator to map kinematics and emotional variables into camera trajectories. Moreover, we incorporate a regularizer to align the generated stylistic variances with specific emotional categories and intensities. The experimental results show that our proposed method yields immersive cinematic videos of high quality, both quantitatively and qualitatively. Live examples can be found in the supplementary video.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Large Scale Image Completion via Co-Modulated Generative Adversarial Networks,"Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects,"The recent availability and adaptability of text-to-image models has sparked a new era in many related domains that benefit from the learned text priors as well as high-quality and fast generation capabilities, one of which is texture generation for 3D objects. Although recent texture generation methods achieve impressive results by using text-to-image networks, the combination of global consistency, quality, and speed, which is crucial for advancing texture generation to real-world applications, remains elusive. To that end, we introduce Meta 3D TextureGen: a new feedforward method comprised of two sequential networks aimed at generating high-quality and globally consistent textures for arbitrary geometries of any complexity degree in less than 20 seconds. Our method achieves state-of-the-art results in quality and speed by conditioning a text-to-image model on 3D semantics in 2D space and fusing them into a complete and high-resolution UV texture map, as demonstrated by extensive qualitative and quantitative evaluations. In addition, we introduce a texture enhancement network that is capable of up-scaling any texture by an arbitrary ratio, producing 4k pixel resolution textures.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ROGR: Relightable 3D Objects using Generative Relighting,"We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation,"We present PrimDiffusion, the first diffusion-based framework for 3D human generation. Devising diffusion models for 3D human generation is difficult due to the intensive computational cost of 3D representations and the articulated topology of 3D humans. To tackle these challenges, our key insight is operating the denoising diffusion process directly on a set of volumetric primitives, which models the human body as a number of small volumes with radiance and kinematic information. This volumetric primitives representation marries the capacity of volumetric representations with the efficiency of primitive-based rendering. Our PrimDiffusion framework has three appealing properties: 1) compact and expressive parameter space for the diffusion model, 2) flexible 3D representation that incorporates human prior, and 3) decoder-free rendering for efficient novel-view and novel-pose synthesis. Extensive experiments validate that PrimDiffusion outperforms state-of-the-art methods in 3D human generation. Notably, compared to GAN-based methods, our PrimDiffusion supports real-time rendering of high-quality 3D humans at a resolution of $512\times512$ once the denoising process is done. We also demonstrate the flexibility of our framework on training-free conditional generation such as texture transfer and 3D inpainting.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CASteer: Steering Diffusion Models for Controllable Generation,"Diffusion models have transformed image generation, yet controlling their outputs to reliably erase undesired concepts remains challenging. Existing approaches usually require task-specific training and struggle to generalize across both concrete (e.g., objects) and abstract (e.g., styles) concepts. We propose CASteer (Cross-Attention Steering), a training-free framework for concept erasure in diffusion models using steering vectors to influence hidden representations dynamically. CASteer precomputes concept-specific steering vectors by averaging neural activations from images generated for each target concept. During inference, it dynamically applies these vectors to suppress undesired concepts only when they appear, ensuring that unrelated regions remain unaffected. This selective activation enables precise, context-aware erasure without degrading overall image quality. This approach achieves effective removal of harmful or unwanted content across a wide range of visual concepts, all without model retraining. CASteer outperforms state-of-the-art concept erasure techniques while preserving unrelated content and minimizing unintended effects. Pseudocode is provided in the supplementary.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models,"The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar that is consistent with the text description and can be readily articulated using the deformation model of the 3DMM. We show that our diffusion-based articulated head avatars outperform state-of-the-art approaches for this task. The latter are typically based on CLIP, which is known to provide limited diversity of generation and accuracy for 3D object generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction,"Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Dream-in-Style: Text-to-3D Generation Using Stylized Score Distillation,"We present a method to generate 3D objects in styles. Our method takes a text prompt and a style reference image as input and reconstructs a neural radiance field to synthesize a 3D model with the content aligning with the text prompt and the style following the reference image. To simultaneously generate the 3D object and perform style transfer in one go, we propose a stylized score distillation loss to guide a text-to-3D optimization process to output visually plausible geometry and appearance. Our stylized score distillation is based on a combination of an original pretrained text-to-image model and its modified sibling with the key and value features of self-attention layers manipulated to inject styles from the reference image. Comparisons with state-of-the-art methods demonstrated the strong visual performance of our method, further supported by the quantitative results from our user study.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MyStyle: A Personalized Generative Prior,"We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generating coherent spontaneous speech and gesture from text,"Embodied human communication encompasses both verbal (speech) and non-verbal information (e.g., gesture and head movements). Recent advances in machine learning have substantially improved the technologies for generating synthetic versions of both of these types of data: On the speech side, text-to-speech systems are now able to generate highly convincing, spontaneous-sounding speech using unscripted speech audio as the source material. On the motion side, probabilistic motion-generation methods can now synthesise vivid and lifelike speech-driven 3D gesticulation. In this paper, we put these two state-of-the-art technologies together in a coherent fashion for the first time. Concretely, we demonstrate a proof-of-concept system trained on a single-speaker audio and motion-capture dataset, that is able to generate both speech and full-body gestures together from text input. In contrast to previous approaches for joint speech-and-gesture generation, we generate full-body gestures from speech synthesis trained on recordings of spontaneous speech from the same person as the motion-capture data. We illustrate our results by visualising gesture spaces and text-speech-gesture alignments, and through a demonstration video at https://simonalexanderson.github.io/IVA2020 .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Multi-View Depth Consistent Image Generation Using Generative AI Models: Application on Architectural Design of University Buildings,"In the early stages of architectural design, shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs. Generative artificial intelligence (AI) provides a promising solution to automate this transformation, but ensuring multi-view consistency remains a significant challenge. To solve this issue, we propose a novel three-stage consistent image generation framework using generative AI models to generate architectural designs from shoebox model representations. The proposed method enhances state-of-the-art image generation diffusion models to generate multi-view consistent architectural images. We employ ControlNet as the backbone and optimize it to accommodate multi-view inputs of architectural shoebox models captured from predefined perspectives. To ensure stylistic and structural consistency across multi-view images, we propose an image space loss module that incorporates style loss, structural loss and angle alignment loss. We then use depth estimation method to extract depth maps from the generated multi-view images. Finally, we use the paired data of the architectural images and depth maps as inputs to improve the multi-view consistency via the depth-aware 3D attention module. Experimental results demonstrate that the proposed framework can generate multi-view architectural images with consistent style and structural coherence from shoebox model inputs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Curved Diffusion: A Generative Model With Optical Geometry Control,"State-of-the-art diffusion models can generate highly realistic images based on various conditioning like text, segmentation, and depth. However, an essential aspect often overlooked is the specific camera geometry used during image capture. The influence of different optical systems on the final scene appearance is frequently overlooked. This study introduces a framework that intimately integrates a text-to-image diffusion model with the particular lens geometry used in image rendering. Our method is based on a per-pixel coordinate conditioning method, enabling the control over the rendering geometry. Notably, we demonstrate the manipulation of curvature properties, achieving diverse visual effects, such as fish-eye, panoramic views, and spherical texturing using a single diffusion model.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning,"Generating artistic portraits is a challenging problem in computer vision. Existing portrait stylization models that generate good quality results are based on Image-to-Image Translation and require abundant data from both source and target domains. However, without enough data, these methods would result in overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits generation model with a novel contrastive transfer learning strategy. We adapt a pretrained StyleGAN in the source domain to a target artistic domain with no more than 10 artistic faces. To reduce overfitting to the few training examples, we introduce a novel Cross-Domain Triplet loss which explicitly encourages the target instances generated from different latent codes to be distinguishable. We propose a new encoder which embeds real faces into Z+ space and proposes a dual-path training strategy to better cope with the adapted decoder and eliminate the artifacts. Extensive qualitative, quantitative comparisons and a user study show our method significantly outperforms state-of-the-arts under 10-shot and 1-shot settings and generates high quality artistic portraits. The code will be made publicly available.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
View Generalization for Single Image Textured 3D Models,"Humans can easily infer the underlying 3D geometry and texture of an object only from a single 2D image. Current computer vision methods can do this, too, but suffer from view generalization problems - the models inferred tend to make poor predictions of appearance in novel views. As for generalization problems in machine learning, the difficulty is balancing single-view accuracy (cf. training error; bias) with novel view accuracy (cf. test error; variance). We describe a class of models whose geometric rigidity is easily controlled to manage this tradeoff. We describe a cycle consistency loss that improves view generalization (roughly, a model from a generated view should predict the original view well). View generalization of textures requires that models share texture information, so a car seen from the back still has headlights because other cars have headlights. We describe a cycle consistency loss that encourages model textures to be aligned, so as to encourage sharing. We compare our method against the state-of-the-art method and show both qualitative and quantitative improvements.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CASIM: Composite Aware Semantic Injection for Text to Motion Generation,"Recent advances in generative modeling and tokenization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. However, effectively leveraging textual information for conditional motion generation remains an open challenge. We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (CASIM), comprising a composite-aware semantic encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. Experiments on HumanML3D and KIT benchmarks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. Qualitative analyses further highlight the superiority of our composite-aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
TexPainter: Generative Mesh Texturing with Multi-view Consistency,"The recent success of pre-trained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3D meshes in the wild. However, these models are trained in the screen space, while converting them to a multi-view consistent texture image poses a major obstacle to the output quality. In this paper, we propose a novel method to enforce multi-view consistency. Our method is based on the observation that latent space in a pre-trained diffusion model is noised separately for each camera view, making it difficult to achieve multi-view consistency by directly manipulating the latent codes. Based on the celebrated Denoising Diffusion Implicit Models (DDIM) scheme, we propose to use an optimization-based color-fusion to enforce consistency and indirectly modify the latent codes by gradient back-propagation. Our method further relaxes the sequential dependency assumption among the camera views. By evaluating on a series of general 3D models, we find our simple approach improves consistency and overall quality of the generated textures as compared to competing state-of-the-arts. Our implementation is available at: https://github.com/Quantuman134/TexPainter","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Imperative vs. Declarative Programming Paradigms for Open-Universe Scene Generation,"Current methods for generating 3D scene layouts from text predominantly follow a declarative paradigm, where a Large Language Model (LLM) specifies high-level constraints that are then resolved by a separate solver. This paper challenges that consensus by introducing a more direct, imperative approach. We task an LLM with generating a step-by-step program that iteratively places each object relative to those already in the scene. This paradigm simplifies the underlying scene specification language, enabling the creation of more complex, varied, and highly structured layouts that are difficult to express declaratively. To improve the robustness, we complement our method with a novel, LLM-free error correction mechanism that operates directly on the generated code, iteratively adjusting parameters within the program to resolve collisions and other inconsistencies. In forced-choice perceptual studies, human participants overwhelmingly preferred our imperative layouts, choosing them over those from two state-of-the-art declarative systems 82% and 94% of the time, demonstrating the significant potential of this alternative paradigm. Finally, we present a simple automated evaluation metric for 3D scene layout generation that correlates strongly with human judgment.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation,"Biometric face morphing poses a critical challenge to identity verification systems, undermining their security and robustness. To address this issue, we propose WaFusion, a novel framework combining wavelet decomposition and diffusion models to generate high-quality, realistic morphed face images efficiently. WaFusion leverages the structural details captured by wavelet transforms and the generative capabilities of diffusion models, producing face morphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and WVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art methods, producing high-resolution morphs with fewer artifacts. Our framework excels across key biometric metrics, including the Attack Presentation Classification Error Rate (APCER), Bona Fide Presentation Classification Error Rate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in biometric morph generation, offering a cutting-edge and efficient solution to enhance biometric security systems.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents,"We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models,"We propose a novel framework COLLAGE for generating collaborative agent-object-agent interactions by leveraging large language models (LLMs) and hierarchical motion-specific vector-quantized variational autoencoders (VQ-VAEs). Our model addresses the lack of rich datasets in this domain by incorporating the knowledge and reasoning abilities of LLMs to guide a generative diffusion model. The hierarchical VQ-VAE architecture captures different motion-specific characteristics at multiple levels of abstraction, avoiding redundant concepts and enabling efficient multi-resolution representation. We introduce a diffusion model that operates in the latent space and incorporates LLM-generated motion planning cues to guide the denoising process, resulting in prompt-specific motion generation with greater control and diversity. Experimental results on the CORE-4D, and InterHuman datasets demonstrate the effectiveness of our approach in generating realistic and diverse collaborative human-object-human interactions, outperforming state-of-the-art methods. Our work opens up new possibilities for modeling complex interactions in various domains, such as robotics, graphics and computer vision.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Gaussian Wave Splatting for Computer-Generated Holography,"State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computer-generated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Freehand Sketch Generation from Mechanical Components,"Drawing freehand sketches of mechanical components on multimedia devices for AI-based engineering modeling has become a new trend. However, its development is being impeded because existing works cannot produce suitable sketches for data-driven research. These works either generate sketches lacking a freehand style or utilize generative models not originally designed for this task resulting in poor effectiveness. To address this issue, we design a two-stage generative framework mimicking the human sketching behavior pattern, called MSFormer, which is the first time to produce humanoid freehand sketches tailored for mechanical components. The first stage employs Open CASCADE technology to obtain multi-view contour sketches from mechanical components, filtering perturbing signals for the ensuing generation process. Meanwhile, we design a view selector to simulate viewpoint selection tasks during human sketching for picking out information-rich sketches. The second stage translates contour sketches into freehand sketches by a transformer-based generator. To retain essential modeling features as much as possible and rationalize stroke distribution, we introduce a novel edge-constraint stroke initialization. Furthermore, we utilize a CLIP vision encoder and a new loss function incorporating the Hausdorff distance to enhance the generalizability and robustness of the model. Extensive experiments demonstrate that our approach achieves state-of-the-art performance for generating freehand sketches in the mechanical domain. Project page: https://mcfreeskegen.github.io .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation,"We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors,"Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication,"Full-body gestures play a pivotal role in natural interactions and are crucial for achieving effective communication. Nevertheless, most existing studies primarily focus on the gesture generation of speakers, overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them. This paper innovatively proposes an Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication. For the first time, we integrate the full-body gestures of listeners into the generation framework. By devising a novel inter-diffusion mechanism, this model can accurately capture the complex interaction patterns between speakers and listeners during communication. In the model construction process, based on the advanced diffusion model architecture, we innovatively introduce interaction conditions and the GAN model to increase the denoising step size. As a result, when generating gesture sequences, the model can not only dynamically generate based on the speaker's speech information but also respond in realtime to the listener's feedback, enabling synergistic interaction between the two. Abundant experimental results demonstrate that compared with the current state-of-the-art gesture generation methods, the model we proposed has achieved remarkable improvements in the naturalness, coherence, and speech-gesture synchronization of the generated gestures. In the subjective evaluation experiments, users highly praised the generated interaction scenarios, believing that they are closer to real life human communication situations. Objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators, providing more powerful support for effective communication.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GPN: Generative Point-based NeRF,"Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StyleSwap: Style-Based Generator Empowers Robust Face Swapping,"Numerous attempts have been made to the task of person-agnostic face swapping given its wide applications. While existing methods mostly rely on tedious network and loss designs, they still struggle in the information balancing between the source and target faces, and tend to produce visible artifacts. In this work, we introduce a concise and effective framework named StyleSwap. Our core idea is to leverage a style-based generator to empower high-fidelity and robust face swapping, thus the generator's advantage can be adopted for optimizing identity similarity. We identify that with only minimal modifications, a StyleGAN2 architecture can successfully handle the desired information from both source and target. Additionally, inspired by the ToRGB layers, a Swapping-Driven Mask Branch is further devised to improve information blending. Furthermore, the advantage of StyleGAN inversion can be adopted. Particularly, a Swapping-Guided ID Inversion strategy is proposed to optimize identity similarity. Extensive experiments validate that our framework generates high-quality face swapping results that outperform state-of-the-art methods both qualitatively and quantitatively.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SVGBuilder: Component-Based Colored SVG Generation with Text-Guided Autoregressive Transformers,"Scalable Vector Graphics (SVG) are essential XML-based formats for versatile graphics, offering resolution independence and scalability. Unlike raster images, SVGs use geometric shapes and support interactivity, animation, and manipulation via CSS and JavaScript. Current SVG generation methods face challenges related to high computational costs and complexity. In contrast, human designers use component-based tools for efficient SVG creation. Inspired by this, SVGBuilder introduces a component-based, autoregressive model for generating high-quality colored SVGs from textual input. It significantly reduces computational overhead and improves efficiency compared to traditional methods. Our model generates SVGs up to 604 times faster than optimization-based approaches. To address the limitations of existing SVG datasets and support our research, we introduce ColorSVG-100K, the first large-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset fills the gap in color information for SVG generation models and enhances diversity in model training. Evaluation against state-of-the-art models demonstrates SVGBuilder's superior performance in practical applications, highlighting its efficiency and quality in generating complex SVG graphics.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?,"Recent advancements in implicit 3D representations and generative models have markedly propelled the field of 3D object generation forward. However, it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls, which is crucial in fields like industrial design and manufacturing. To bridge this gap, we introduce a framework that employs Large Language Models (LLMs) to generate text-driven 3D shapes, manipulating 3D software via program synthesis. We present 3D-PreMise, a dataset specifically tailored for 3D parametric modeling of industrial shapes, designed to explore state-of-the-art LLMs within our proposed pipeline. Our work reveals effective generation strategies and delves into the self-correction capabilities of LLMs using a visual interface. Our work highlights both the potential and limitations of LLMs in 3D parametric modeling for industrial applications.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A General Framework for Bilateral and Mean Shift Filtering,"We present a generalization of the bilateral filter that can be applied to feature-preserving smoothing of signals on images, meshes, and other domains within a single unified framework. Our discretization is competitive with state-of-the-art smoothing techniques in terms of both accuracy and speed, is easy to implement, and has parameters that are straightforward to understand. Unlike previous bilateral filters developed for meshes and other irregular domains, our construction reduces exactly to the image bilateral on rectangular domains and comes with a rigorous foundation in both the smooth and discrete settings. These guarantees allow us to construct unconditionally convergent mean-shift schemes that handle a variety of extremely noisy signals. We also apply our framework to geometric edge-preserving effects like feature enhancement and show how it is related to local histogram techniques.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
VQ-SGen: A Vector Quantized Stroke Representation for Creative Sketch Generation,"This paper presents VQ-SGen, a novel algorithm for high-quality creative sketch generation. Recent approaches have framed the task as pixel-based generation either as a whole or part-by-part, neglecting the intrinsic and contextual relationships among individual strokes, such as the shape and spatial positioning of both proximal and distant strokes. To overcome these limitations, we propose treating each stroke within a sketch as an entity and introducing a vector-quantized (VQ) stroke representation for fine-grained sketch generation. Our method follows a two-stage framework - in stage one, we decouple each stroke's shape and location information to ensure the VQ representation prioritizes stroke shape learning. In stage two, we feed the precise and compact representation into an auto-decoding Transformer to incorporate stroke semantics, positions, and shapes into the generation process. By utilizing tokenized stroke representation, our approach generates strokes with high fidelity and facilitates novel applications, such as text or class label conditioned generation and sketch completion. Comprehensive experiments demonstrate our method surpasses existing state-of-the-art techniques on the CreativeSketch dataset, underscoring its effectiveness.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image Generation,"Training audio-to-image generative models requires an abundance of diverse audio-visual pairs that are semantically aligned. Such data is almost always curated from in-the-wild videos, given the cross-modal semantic correspondence that is inherent to them. In this work, we hypothesize that insisting on the absolute need for ground truth audio-visual correspondence, is not only unnecessary, but also leads to severe restrictions in scale, quality, and diversity of the data, ultimately impairing its use in the modern generative models. That is, we propose a scalable image sonification framework where instances from a variety of high-quality yet disjoint uni-modal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern vision-language models. To demonstrate the efficacy of this approach, we use our sonified images to train an audio-to-image generative model that performs competitively against state-of-the-art. Finally, through a series of ablation studies, we exhibit several intriguing auditory capabilities like semantic mixing and interpolation, loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
RefAdGen: High-Fidelity Advertising Image Generation,"The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generating Holistic 3D Human Motion from Speech,"This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code will be released for research purposes at https://talkshow.is.tue.mpg.de.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward,"In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos,"Estimating video depth in open-world scenarios is challenging due to the diversity of videos in appearance, content motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. The generalization ability to open-world videos is achieved by training the video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that can process extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020","Co-speech gestures, gestures that accompany speech, play an important role in human communication. Automatic co-speech gesture generation is thus a key enabling technology for embodied conversational agents (ECAs), since humans expect ECAs to be capable of multi-modal communication. Research into gesture generation is rapidly gravitating towards data-driven methods. Unfortunately, individual research efforts in the field are difficult to compare: there are no established benchmarks, and each study tends to use its own dataset, motion visualisation, and evaluation methodology. To address this situation, we launched the GENEA Challenge, a gesture-generation challenge wherein participating teams built automatic gesture-generation systems on a common dataset, and the resulting systems were evaluated in parallel in a large, crowdsourced user study using the same motion-rendering pipeline. Since differences in evaluation outcomes between systems now are solely attributable to differences between the motion-generation methods, this enables benchmarking recent approaches against one another in order to get a better impression of the state of the art in the field. This paper reports on the purpose, design, results, and implications of our challenge.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data,"Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.   Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race. Code and models are available at https://github.com/ningyu1991/ArtificialGANFingerprints .","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Method for Training-free Person Image Picture Generation,"The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Character Image Feature Encoder extracts only the relevant image features, rather than information about the model's composition or movements. In addition, the Character Image Feature Encoder can be adapted to different models after training. The proposed model can be conveniently incorporated into the Stable Diffusion generation process without modifying the model's ontology or used in combination with Stable Diffusion as a joint model.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Disentangled Generation and Aggregation for Robust Radiance Fields,"The utilization of the triplane-based radiance fields has gained attention in recent years due to its ability to effectively disentangle 3D scenes with a high-quality representation and low computation cost. A key requirement of this method is the precise input of camera poses. However, due to the local update property of the triplane, a similar joint estimation as previous joint pose-NeRF optimization works easily results in local minima. To this end, we propose the Disentangled Triplane Generation module to introduce global feature context and smoothness into triplane learning, which mitigates errors caused by local updating. Then, we propose the Disentangled Plane Aggregation to mitigate the entanglement caused by the common triplane feature aggregation during camera pose updating. In addition, we introduce a two-stage warm-start training strategy to reduce the implicit constraints caused by the triplane generator. Quantitative and qualitative results demonstrate that our proposed method achieves state-of-the-art performance in novel view synthesis with noisy or unknown camera poses, as well as efficient convergence of optimization. Project page: https://gaohchen.github.io/DiGARR/.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
"Quark: Real-time, High-resolution, and General Neural View Synthesis","We present a novel neural algorithm for performing high-quality, high-resolution, real-time novel view synthesis. From a sparse set of input RGB images or videos streams, our network both reconstructs the 3D scene and renders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our feed-forward network generalizes across a wide variety of datasets and scenes and produces state-of-the-art quality for a real-time method. Our quality approaches, and in some cases surpasses, the quality of some of the top offline methods. In order to achieve these results we use a novel combination of several key concepts, and tie them together into a cohesive and effective algorithm. We build on previous works that represent the scene using semi-transparent layers and use an iterative learned render-and-refine approach to improve those layers. Instead of flat layers, our method reconstructs layered depth maps (LDMs) that efficiently represent scenes with complex depth and occlusions. The iterative update steps are embedded in a multi-scale, UNet-style architecture to perform as much compute as possible at reduced resolution. Within each update step, to better aggregate the information from multiple input views, we use a specialized Transformer-based network component. This allows the majority of the per-input image processing to be performed in the input image space, as opposed to layer space, further increasing efficiency. Finally, due to the real-time nature of our reconstruction and rendering, we dynamically create and discard the internal 3D geometry for each frame, generating the LDM for each view. Taken together, this produces a novel and effective algorithm for view synthesis. Through extensive evaluation, we demonstrate that we achieve state-of-the-art quality at real-time rates. Project page: https://quark-3d.github.io/","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization,"In recent years, 3D generation has made great strides in both academia and industry. However, generating 3D scenes from a single RGB image remains a significant challenge, as current approaches often struggle to ensure both object generation quality and scene coherence in multi-object scenarios. To overcome these limitations, we propose a novel three-stage framework for 3D scene generation with explicit geometric representations and high-quality textural details via single image-guided model generation and spatial layout optimization. Our method begins with an image instance segmentation and inpainting phase, which recovers missing details of occluded objects in the input images, thereby achieving complete generation of foreground 3D assets. Subsequently, our approach captures the spatial geometry of reference image by constructing pseudo-stereo viewpoint for camera parameter estimation and scene depth inference, while employing a model selection strategy to ensure optimal alignment between the 3D assets generated in the previous step and the input. Finally, through model parameterization and minimization of the Chamfer distance between point clouds in 3D and 2D space, our approach optimizes layout parameters to produce an explicit 3D scene representation that maintains precise alignment with input guidance image. Extensive experiments on multi-object scene image sets have demonstrated that our approach not only outperforms state-of-the-art methods in terms of geometric accuracy and texture fidelity of individual generated 3D models, but also has significant advantages in scene layout synthesis.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics,"Head and gaze dynamics are crucial in expressive 3D facial animation for conveying emotion and intention. However, existing methods frequently address facial components in isolation, overlooking the intricate coordination between gaze, head motion, and speech. The scarcity of high-quality gaze-annotated datasets hinders the development of data-driven models capable of capturing realistic, personalized gaze control. To address these challenges, we propose StyGazeTalk, an audio-driven method that generates synchronized gaze and head motion styles. We extract speaker-specific motion traits from gaze-head sequences with a multi-layer LSTM structure incorporating a style encoder, enabling the generation of diverse animation styles. We also introduce a high-precision multimodal dataset comprising eye-tracked gaze, audio, head pose, and 3D facial parameters, providing a valuable resource for training and evaluating head and gaze control models. Experimental results demonstrate that our method generates realistic, temporally coherent, and style-aware head-gaze motions, significantly advancing the state-of-the-art in audio-driven facial animation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Automatic Graphics Program Generation using Attention-Based Hierarchical Decoder,"Recent progress on deep learning has made it possible to automatically transform the screenshot of Graphic User Interface (GUI) into code by using the encoder-decoder framework. While the commonly adopted image encoder (e.g., CNN network), might be capable of extracting image features to the desired level, interpreting these abstract image features into hundreds of tokens of code puts a particular challenge on the decoding power of the RNN-based code generator. Considering the code used for describing GUI is usually hierarchically structured, we propose a new attention-based hierarchical code generation model, which can describe GUI images in a finer level of details, while also being able to generate hierarchically structured code in consistency with the hierarchical layout of the graphic elements in the GUI. Our model follows the encoder-decoder framework, all the components of which can be trained jointly in an end-to-end manner. The experimental results show that our method outperforms other current state-of-the-art methods on both a publicly available GUI-code dataset as well as a dataset established by our own.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Make-A-Texture: Fast Shape-Aware Texture Generation in 3 Seconds,"We present Make-A-Texture, a new framework that efficiently synthesizes high-resolution texture maps from textual prompts for given 3D geometries. Our approach progressively generates textures that are consistent across multiple viewpoints with a depth-aware inpainting diffusion model, in an optimized sequence of viewpoints determined by an automatic view selection algorithm.   A significant feature of our method is its remarkable efficiency, achieving a full texture generation within an end-to-end runtime of just 3.07 seconds on a single NVIDIA H100 GPU, significantly outperforming existing methods. Such an acceleration is achieved by optimizations in the diffusion model and a specialized backprojection method. Moreover, our method reduces the artifacts in the backprojection phase, by selectively masking out non-frontal faces, and internal faces of open-surfaced objects.   Experimental results demonstrate that Make-A-Texture matches or exceeds the quality of other state-of-the-art methods. Our work significantly improves the applicability and practicality of texture generation models for real-world 3D content creation, including interactive creation and text-guided texture editing.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging,"With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
pylustrator: Code generation for reproducible figures for publication,"One major challenge in science is to make all results potentially reproducible. Thus, along with the raw data, every step from basic processing of the data, evaluation, to the generation of the figures, has to be documented as clearly as possible. While there are many programming libraries that cover the basic processing and plotting steps (e.g. Matplotlib in Python), no library yet addresses the reproducible composing of single plots into meaningful figures for publication. Thus, up to now it is still state-of-the-art to generate publishable figures using image-processing or vector-drawing software leading to unwanted alterations of the presented data in the worst case and to figure quality reduction in the best case. Pylustrator a open source library based on the Matplotlib aims to fill this gap and provides a tool to easily generate the code necessary to compose publication figures from single plots. It provides a graphical user interface where the user can interactively compose the figures. All changes are tracked and converted to code that is automatically integrated into the calling script file. Thus, this software provides the missing link from raw data to the complete plot published in scientific journals and thus contributes to the transparency of the complete evaluation procedure.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Learning to generate line drawings that convey geometry and semantics,"This paper presents an unpaired method for creating line drawings from photographs. Current methods often rely on high quality paired datasets to generate line drawings. However, these datasets often have limitations due to the subjects of the drawings belonging to a specific domain, or in the amount of data collected. Although recent work in unsupervised image-to-image translation has shown much progress, the latest methods still struggle to generate compelling line drawings. We observe that line drawings are encodings of scene information and seek to convey 3D shape and semantic meaning. We build these observations into a set of objectives and train an image translation to map photographs into line drawings. We introduce a geometry loss which predicts depth information from the image features of a line drawing, and a semantic loss which matches the CLIP features of a line drawing with its corresponding photograph. Our approach outperforms state-of-the-art unpaired image translation and line drawing generation methods on creating line drawings from arbitrary photographs. For code and demo visit our webpage carolineec.github.io/informative_drawings","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Chest X-ray Inpainting with Deep Generative Models,"Generative adversarial networks have been successfully applied to inpainting in natural images. However, the current state-of-the-art models have not yet been widely adopted in the medical imaging domain. In this paper, we investigate the performance of three recently published deep learning based inpainting models: context encoders, semantic image inpainting, and the contextual attention model, applied to chest x-rays, as the chest exam is the most commonly performed radiological procedure. We train these generative models on 1.2M 128 $\times$ 128 patches from 60K healthy x-rays, and learn to predict the center 64 $\times$ 64 region in each patch. We test the models on both the healthy and abnormal radiographs. We evaluate the results by visual inspection and comparing the PSNR scores. The outputs of the models are in most cases highly realistic. We show that the methods have potential to enhance and detect abnormalities. In addition, we perform a 2AFC observer study and show that an experienced human observer performs poorly in detecting inpainted regions, particularly those generated by the contextual attention model.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SDM-NET: Deep Generative Network for Structured Deformable Mesh,"We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details. Through extensive experiments and comparisons with the state-of-the-art deep generative models of shapes, we demonstrate the superiority of SDM-NET in generating meshes with visual quality, flexible topology, and meaningful structures, which benefit shape interpolation and other subsequently modeling tasks.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
IntrinsicEdit: Precise generative image manipulation in intrinsic space,"Generative diffusion models have advanced image editing with high-quality results and intuitive interfaces such as prompts and semantic drawing. However, these interfaces lack precise control, and the associated methods typically specialize on a single editing task. We introduce a versatile, generative workflow that operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision for a range of editing operations. Building atop the RGB-X diffusion framework, we address key challenges of identity preservation and intrinsic-channel entanglement. By incorporating exact diffusion inversion and disentangled channel manipulation, we enable precise, efficient editing with automatic resolution of global illumination effects -- all without additional data collection or model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including color and texture adjustments, object insertion and removal, global relighting, and their combinations.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment,"Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-modal task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, we propose a method called MACS to conduct multi-source audio-to-image generation. To our best knowledge, this is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, effective image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 out of the 21 evaluation indexes on all tasks and delivers superior visual quality.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling,"The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation,"We present a caricature generation framework based on shape and style manipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically creates a realistic and detailed caricature from an input photo with optional controls on shape exaggeration degree and color stylization type. The key component of our method is shape exaggeration blocks that are used for modulating coarse layer feature maps of StyleGAN to produce desirable caricature shape exaggerations. We first build a layer-mixed StyleGAN for photo-to-caricature style conversion by swapping fine layers of the StyleGAN for photos to the corresponding layers of the StyleGAN trained to generate caricatures. Given an input photo, the layer-mixed model produces detailed color stylization for a caricature but without shape exaggerations. We then append shape exaggeration blocks to the coarse layers of the layer-mixed model and train the blocks to create shape exaggerations while preserving the characteristic appearances of the input. Experimental results show that our StyleCariGAN generates realistic and detailed caricatures compared to the current state-of-the-art methods. We demonstrate StyleCariGAN also supports other StyleGAN-based image manipulations, such as facial expression control.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Various Types of Aesthetic Curves,"The research on developing planar curves to produce visually pleasing products (ranges from electric appliances to car body design) and indentifying/modifying planar curves for special purposes namely for railway design, highway design and robot trajectories have been progressing since 1970s. The pattern of research in this field of study has branched to five major groups namely curve synthesis, fairing process, improvement in control of natural spiral, construction of new type of planar curves and, natural spiral fitting & approximation techniques. The purpose of is this paper is to briefly review recent progresses in Computer Aided Geometric Design (CAGD) focusing on the topics states above.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
AI-powered Contextual 3D Environment Generation: A Systematic Review,"The generation of high-quality 3D environments is crucial for industries such as gaming, virtual reality, and cinema, yet remains resource-intensive due to the reliance on manual processes. This study performs a systematic review of existing generative AI techniques for 3D scene generation, analyzing their characteristics, strengths, limitations, and potential for improvement. By examining state-of-the-art approaches, it presents key challenges such as scene authenticity and the influence of textual inputs. Special attention is given to how AI can blend different stylistic domains while maintaining coherence, the impact of training data on output quality, and the limitations of current models. In addition, this review surveys existing evaluation metrics for assessing realism and explores how industry professionals incorporate AI into their workflows. The findings of this study aim to provide a comprehensive understanding of the current landscape and serve as a foundation for future research on AI-driven 3D content generation. Key findings include that advanced generative architectures enable high-quality 3D content creation at a high computational cost, effective multi-modal integration techniques like cross-attention and latent space alignment facilitate text-to-3D tasks, and the quality and diversity of training data combined with comprehensive evaluation metrics are critical to achieving scalable, robust 3D scene generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections,"In this work, we present SceneDreamer, an unconditional generative model for unbounded 3D scenes, which synthesizes large-scale 3D landscapes from random noise. Our framework is learned from in-the-wild 2D image collections only, without any 3D annotations. At the core of SceneDreamer is a principled learning paradigm comprising 1) an efficient yet expressive 3D scene representation, 2) a generative scene parameterization, and 3) an effective renderer that can leverage the knowledge from 2D images. Our approach begins with an efficient bird's-eye-view (BEV) representation generated from simplex noise, which includes a height field for surface elevation and a semantic field for detailed scene semantics. This BEV scene representation enables 1) representing a 3D scene with quadratic complexity, 2) disentangled geometry and semantics, and 3) efficient training. Moreover, we propose a novel generative neural hash grid to parameterize the latent space based on 3D positions and scene semantics, aiming to encode generalizable features across various scenes. Lastly, a neural volumetric renderer, learned from 2D image collections through adversarial training, is employed to produce photorealistic images. Extensive experiments demonstrate the effectiveness of SceneDreamer and superiority over state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Normal Image Manipulation for Bas-relief Generation with Hybrid Styles,"We introduce a normal-based bas-relief generation and stylization method which is motivated by the recent advancement in this topic. Creating bas-relief from normal images has successfully facilitated bas-relief modeling in image space. However, the use of normal images in previous work is often restricted to certain type of operations only. This paper is intended to extend normal-based methods and construct bas-reliefs from normal images in a versatile way. Our method can not only generate a new normal image by combining various frequencies of existing normal images and details transferring, but also build bas-reliefs from a single RGB image and its edge-based sketch image. In addition, we introduce an auxiliary function to represent a smooth base surface and generate a layered global shape. To integrate above considerations into our framework, we formulate the bas- relief generation as a variational problem which can be solved by a screened Poisson equation. Some advantages of our method are that it expands the bas-relief shape space and generates diversified styles of results, and that it is capable of transferring details from one region to other regions. Our method is easy to implement, and produces good-quality bas-relief models. We experiment our method on a range of normal images and it compares favorably to other popular classic and state-of-the-art methods.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching,"The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Advances in Neural Rendering,"Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis with Rendering,"Driving simulators play a large role in developing and testing new intelligent vehicle systems. The visual fidelity of the simulation is critical for building vision-based algorithms and conducting human driver experiments. Low visual fidelity breaks immersion for human-in-the-loop driving experiments. Conventional computer graphics pipelines use detailed 3D models, meshes, textures, and rendering engines to generate 2D images from 3D scenes. These processes are labor-intensive, and they do not generate photorealistic imagery. Here we introduce a hybrid generative neural graphics pipeline for improving the visual fidelity of driving simulations. Given a 3D scene, we partially render only important objects of interest, such as vehicles, and use generative adversarial processes to synthesize the background and the rest of the image. To this end, we propose a novel image formation strategy to form 2D semantic images from 3D scenery consisting of simple object models without textures. These semantic images are then converted into photorealistic RGB images with a state-of-the-art Generative Adversarial Network (GAN) trained on real-world driving scenes. This replaces repetitiveness with randomly generated but photorealistic surfaces. Finally, the partially-rendered and GAN synthesized images are blended with a blending GAN. We show that the photorealism of images generated with the proposed method is more similar to real-world driving datasets such as Cityscapes and KITTI than conventional approaches. This comparison is made using semantic retention analysis and Frechet Inception Distance (FID) measurements.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation,"Recently, the field of text-guided 3D scene generation has garnered significant attention. High-quality generation that aligns with physical realism and high controllability is crucial for practical 3D scene applications. However, existing methods face fundamental limitations: (i) difficulty capturing complex relationships between multiple objects described in the text, (ii) inability to generate physically plausible scene layouts, and (iii) lack of controllability and extensibility in compositional scenes. In this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian Splatting (3DGS) to facilitate high-quality, physically consistent compositional scene generation guided by text. Specifically, given a text prompt, we convert it into a directed scene graph and adaptively adjust the density and layout of the initial compositional 3D Gaussians. Subsequently, dynamic camera adjustments are made based on the training focal point to ensure entity-level generation quality. Finally, by extracting directed dependencies from the scene graph, we tailor physical and layout energy to ensure both realism and flexibility. Comprehensive experiments demonstrate that LayoutDreamer outperforms other compositional scene generation quality and semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA) performance in the multiple objects generation metric of T3Bench.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision,"Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image,"In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Previous methods based on Score Distillation Sampling (SDS) can produce diversified 3D results by distilling 3D knowledge from large 2D diffusion models, but they usually suffer from long per-case optimization time with inconsistent issues. Recent works address the problem and generate better 3D results either by finetuning a multi-view diffusion model or training a fast feed-forward model. However, they still lack intricate textures and complex geometries due to inconsistency and limited generated resolution. To simultaneously achieve high fidelity, consistency, and efficiency in single image-to-3D, we propose a novel framework Unique3D that includes a multi-view diffusion model with a corresponding normal diffusion model to generate multi-view images with their normal maps, a multi-level upscale process to progressively improve the resolution of generated orthographic multi-views, as well as an instant and consistent mesh reconstruction algorithm called ISOMER, which fully integrates the color and geometric priors into mesh results. Extensive experiments demonstrate that our Unique3D significantly outperforms other image-to-3D baselines in terms of geometric and textural details.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation,"In-betweening human motion generation aims to synthesize intermediate motions that transition between user-specified keyframes. In addition to maintaining smooth transitions, a crucial requirement of this task is to generate diverse motion sequences. It is still challenging to maintain diversity, particularly when it is necessary for the motions within a generated batch sampling to differ meaningfully from one another due to complex motion dynamics. In this paper, we propose a novel method, termed the Multi-Criteria Guidance with In-Betweening Motion Model (MCG-IMM), for in-betweening human motion generation. A key strength of MCG-IMM lies in its plug-and-play nature: it enhances the diversity of motions generated by pretrained models without introducing additional parameters This is achieved by providing a sampling process of pretrained generative models with multi-criteria guidance. Specifically, MCG-IMM reformulates the sampling process of pretrained generative model as a multi-criteria optimization problem, and introduces an optimization process to explore motion sequences that satisfy multiple criteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play multi-criteria guidance is compatible with different families of generative models, including denoised diffusion probabilistic models, variational autoencoders, and generative adversarial networks. Experiments on four popular human motion datasets demonstrate that MCG-IMM consistently state-of-the-art methods in in-betweening motion generation task.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
RelitLRM: Generative Relightable Radiance for Large Reconstruction Models,"We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relit-lrm.github.io/.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation,"The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, a real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference processes of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
SeqTex: Generate Mesh Textures in Video Sequence,"Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech,"We present ZeroEGGS, a neural network framework for speech-driven gesture generation with zero-shot style control by example. This means style can be controlled via only a short example motion clip, even for motion styles unseen during training. Our model uses a Variational framework to learn a style embedding, making it easy to modify style through latent space manipulation or blending and scaling of style embeddings. The probabilistic nature of our framework further enables the generation of a variety of outputs given the same input, addressing the stochastic nature of gesture motion. In a series of experiments, we first demonstrate the flexibility and generalizability of our model to new speakers and styles. In a user study, we then show that our model outperforms previous state-of-the-art techniques in naturalness of motion, appropriateness for speech, and style portrayal. Finally, we release a high-quality dataset of full-body gesture motion including fingers, with speech, spanning across 19 different styles.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing,"3D scene generation plays a crucial role in gaming, artistic creation, virtual reality and many other domains. However, current 3D scene design still relies heavily on extensive manual effort from creators, and existing automated methods struggle to generate open-domain scenes or support flexible editing. As a result, generating 3D worlds directly from text has garnered increasing attention. In this paper, we introduce HOLODECK 2.0, an advanced vision-language-guided framework for 3D world generation with support for interactive scene editing based on human feedback. HOLODECK 2.0 can generate diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and cyberpunk styles) that exhibit high semantic fidelity to fine-grained input descriptions, suitable for both indoor and open-domain environments. HOLODECK 2.0 leverages vision-language models (VLMs) to identify and parse the objects required in a scene and generates corresponding high-quality assets via state-of-the-art 3D generative models. It then iteratively applies spatial constraints derived from the VLMs to achieve semantically coherent and physically plausible layouts. Human evaluations and CLIP-based assessments demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely aligned with detailed textual descriptions, consistently outperforming baselines across indoor and open-domain scenarios. Additionally, we provide editing capabilities that flexibly adapt to human feedback, supporting layout refinement and style-consistent object edits. Finally, we present a practical application of HOLODECK 2.0 in procedural game modeling, generating visually rich and immersive environments, potentially boosting efficiency.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MeshMosaic: Scaling Artist Mesh Generation via Local-to-Global Assembly,"Scaling artist-designed meshes to high triangle numbers remains challenging for autoregressive generative models. Existing transformer-based methods suffer from long-sequence bottlenecks and limited quantization resolution, primarily due to the large number of tokens required and constrained quantization granularity. These issues prevent faithful reproduction of fine geometric details and structured density patterns. We introduce MeshMosaic, a novel local-to-global framework for artist mesh generation that scales to over 100K triangles--substantially surpassing prior methods, which typically handle only around 8K faces. MeshMosaic first segments shapes into patches, generating each patch autoregressively and leveraging shared boundary conditions to promote coherence, symmetry, and seamless connectivity between neighboring regions. This strategy enhances scalability to high-resolution meshes by quantizing patches individually, resulting in more symmetrical and organized mesh density and structure. Extensive experiments across multiple public datasets demonstrate that MeshMosaic significantly outperforms state-of-the-art methods in both geometric fidelity and user preference, supporting superior detail representation and practical mesh generation for real-world applications.","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
MOSPA: Human Motion Generation Driven by Spatial Audio,"Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse, realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control,"We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/","cat:cs.GR AND (""digital art"" OR NFT OR ""generative art"" OR aesthetics)",0
Intrusion Detection in Internet of Things using Convolutional Neural Networks,"Internet of Things (IoT) has become a popular paradigm to fulfil needs of the industry such as asset tracking, resource monitoring and automation. As security mechanisms are often neglected during the deployment of IoT devices, they are more easily attacked by complicated and large volume intrusion attacks using advanced techniques. Artificial Intelligence (AI) has been used by the cyber security community in the past decade to automatically identify such attacks. However, deep learning methods have yet to be extensively explored for Intrusion Detection Systems (IDS) specifically for IoT. Most recent works are based on time sequential models like LSTM and there is short of research in CNNs as they are not naturally suited for this problem. In this article, we propose a novel solution to the intrusion attacks against IoT devices using CNNs. The data is encoded as the convolutional operations to capture the patterns from the sensors data along time that are useful for attacks detection by CNNs. The proposed method is integrated with two classical CNNs: ResNet and EfficientNet, where the detection performance is evaluated. The experimental results show significant improvement in both true positive rate and false positive rate compared to the baseline using LSTM.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing IoT Security: A Novel Feature Engineering Approach for ML-Based Intrusion Detection Systems,"The integration of Internet of Things (IoT) applications in our daily lives has led to a surge in data traffic, posing significant security challenges. IoT applications using cloud and edge computing are at higher risk of cyberattacks because of the expanded attack surface from distributed edge and cloud services, the vulnerability of IoT devices, and challenges in managing security across interconnected systems leading to oversights. This led to the rise of ML-based solutions for intrusion detection systems (IDSs), which have proven effective in enhancing network security and defending against diverse threats. However, ML-based IDS in IoT systems encounters challenges, particularly from noisy, redundant, and irrelevant features in varied IoT datasets, potentially impacting its performance. Therefore, reducing such features becomes crucial to enhance system performance and minimize computational costs. This paper focuses on improving the effectiveness of ML-based IDS at the edge level by introducing a novel method to find a balanced trade-off between cost and accuracy through the creation of informative features in a two-tier edge-user IoT environment. A hybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming algorithm is utilized for this purpose. Three IoT intrusion detection datasets, namely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the proposed approach.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Exploring Security Economics in IoT Standardization Efforts,"The Internet of Things (IoT) propagates the paradigm of interconnecting billions of heterogeneous devices by various manufacturers. To enable IoT applications, the communication between IoT devices follows specifications defined by standard developing organizations. In this paper, we present a case study that investigates disclosed insecurities of the popular IoT standard ZigBee, and derive general lessons about security economics in IoT standardization efforts. We discuss the motivation of IoT standardization efforts that are primarily driven from an economic perspective, in which large investments in security are not considered necessary since the consumers do not reward them. Success at the market is achieved by being quick-to-market, providing functional features and offering easy integration for complementors. Nevertheless, manufacturers should not only consider economic reasons but also see their responsibility to protect humans and technological infrastructures from being threatened by insecure IoT products. In this context, we propose a number of recommendations to strengthen the security design in future IoT standardization efforts, ranging from the definition of a precise security model to the enforcement of an update policy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Tactical Edge IoT in Defense and National Security,"The deployment of Internet of Things (IoT) systems in Defense and National Security faces some limitations that can be addressed with Edge Computing approaches. The Edge Computing and IoT paradigms combined bring potential benefits, since they confront the limitations of traditional centralized cloud computing approaches, which enable easy scalability, real-time applications or mobility support, but whose use poses certain risks in aspects like cybersecurity. This chapter identifies scenarios in which Defense and National Security can leverage Commercial Off-The-Shelf (COTS) Edge IoT capabilities to deliver greater survivability to warfighters or first responders, while lowering costs and increasing operational efficiency and effectiveness. In addition, it presents the general design of a Tactical Edge IoT communications architecture, it identifies the open challenges for a widespread adoption and provides research guidelines and some recommendations for enabling cost-effective Edge IoT for Defense and National Security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Internet Service Providers' and Individuals' Attitudes, Barriers, and Incentives to Secure IoT","Internet Service Providers (ISPs) and individual users of Internet of Things (IoT) play a vital role in securing IoT. However, encouraging them to do so is hard. Our study investigates ISPs' and individuals' attitudes towards the security of IoT, the obstacles they face, and their incentives to keep IoT secure, drawing evidence from Japan.   Due to the complex interactions of the stakeholders, we follow an iterative methodology where we present issues and potential solutions to our stakeholders in turn. For ISPs, we survey 27 ISPs in Japan, followed by a workshop with representatives from government and 5 ISPs. Based on the findings from this, we conduct semi-structured interviews with 20 participants followed by a more quantitative survey with 328 participants. We review these results in a second workshop with representatives from government and 7 ISPs. The appreciation of challenges by each party has lead to findings that are supported by all stakeholders.   Securing IoT devices is neither users' nor ISPs' priority. Individuals are keen on more interventions both from the government as part of regulation and from ISPs in terms of filtering malicious traffic. Participants are willing to pay for enhanced monitoring and filtering. While ISPs do want to help users, there appears to be a lack of effective technology to aid them. ISPs would like to see more public recognition for their efforts, but internally they struggle with executive buy-in and effective means to communicate with their customers. The majority of barriers and incentives are external to ISPs and individuals, demonstrating the complexity of keeping IoT secure and emphasizing the need for relevant stakeholders in the IoT ecosystem to work in tandem.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks,"The rapid expansion of the Internet of Things (IoT) has revolutionized modern industries by enabling smart automation and real time connectivity. However, this evolution has also introduced complex cybersecurity challenges due to the heterogeneous, resource constrained, and distributed nature of these environments. To address these challenges, this research presents CST AFNet, a novel dual attention based deep learning framework specifically designed for robust intrusion detection in IoT networks. The model integrates multi scale Convolutional Neural Networks (CNNs) for spatial feature extraction, Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal dependencies, and a dual attention mechanism, channel and temporal attention, to enhance focus on critical patterns in the data. The proposed method was trained and evaluated on the Edge IIoTset dataset, a comprehensive and realistic benchmark containing more than 2.2 million labeled instances spanning 15 attack types and benign traffic, collected from a seven layer industrial testbed. Our proposed model achieves outstanding accuracy for both 15 attack types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover, this model demonstrates exceptional performance with macro averaged precision, recall, and F1 score all above 99.3 percent. Experimental results show that CST AFNet achieves superior detection accuracy, significantly outperforming traditional deep learning models. The findings confirm that CST AFNet is a powerful and scalable solution for real time cyber threat detection in complex IoT and IIoT environments, paving the way for more secure, intelligent, and adaptive cyber physical systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Hybrid Deep Learning Anomaly Detection Framework for Intrusion Detection,"Cyber intrusion attacks that compromise the users' critical and sensitive data are escalating in volume and intensity, especially with the growing connections between our daily life and the Internet. The large volume and high complexity of such intrusion attacks have impeded the effectiveness of most traditional defence techniques. While at the same time, the remarkable performance of the machine learning methods, especially deep learning, in computer vision, had garnered research interests from the cyber security community to further enhance and automate intrusion detections. However, the expensive data labeling and limitation of anomalous data make it challenging to train an intrusion detector in a fully supervised manner. Therefore, intrusion detection based on unsupervised anomaly detection is an important feature too. In this paper, we propose a three-stage deep learning anomaly detection based network intrusion attack detection framework. The framework comprises an integration of unsupervised (K-means clustering), semi-supervised (GANomaly) and supervised learning (CNN) algorithms. We then evaluated and showed the performance of our implemented framework on three benchmark datasets: NSL-KDD, CIC-IDS2018, and TON_IoT.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing Fog-to-Things Environment Using Intrusion Detection System Based On Ensemble Learning,"The growing interest in the Internet of Things (IoT) applications is associated with an augmented volume of security threats. In this vein, the Intrusion detection systems (IDS) have emerged as a viable solution for the detection and prevention of malicious activities. Unlike the signature-based detection approaches, machine learning-based solutions are a promising means for detecting unknown attacks. However, the machine learning models need to be accurate enough to reduce the number of false alarms. More importantly, they need to be trained and evaluated on realistic datasets such that their efficacy can be validated on real-time deployments. Many solutions proposed in the literature are reported to have high accuracy but are ineffective in real applications due to the non-representativity of the dataset used for training and evaluation of the underlying models. On the other hand, some of the existing solutions overcome these challenges but yield low accuracy which hampers their implementation for commercial tools. These solutions are majorly based on single learners and are therefore directly affected by the intrinsic limitations of each learning algorithm. The novelty of this paper is to use the most realistic dataset available for intrusion detection called NSL-KDD, and combine multiple learners to build ensemble learners that increase the accuracy of the detection. Furthermore, a deployment architecture in a fog-to-things environment that employs two levels of classifications is proposed. In such architecture, the first level performs an anomaly detection which reduces the latency of the classification substantially, while the second level, executes attack classifications, enabling precise prevention measures. Finally, the experimental results demonstrate the effectiveness of the proposed IDS in comparison with the other state-of-the-arts on the NSL-KDD dataset.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Federated Learning for Malware Detection in IoT Devices,"This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware affecting IoT devices is presented. N-BaIoT, a dataset modeling network traffic of several real IoT devices while affected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware affecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The first one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participant's privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered. The baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to different attacks, even with a single adversary. The performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios. These functions provide a significant improvement against malicious participants, but more efforts are still needed to make federated approaches robust.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Traffic-aware Patching for Cyber Security in Mobile IoT,"The various types of communication technologies and mobility features in Internet of Things (IoT) on the one hand enable fruitful and attractive applications, but on the other hand facilitates malware propagation, thereby raising new challenges on handling IoT-empowered malware for cyber security. Comparing with the malware propagation control scheme in traditional wireless networks where nodes can be directly repaired and secured, in IoT, compromised end devices are difficult to be patched. Alternatively, blocking malware via patching intermediate nodes turns out to be a more feasible and practical solution. Specifically, patching intermediate nodes can effectively prevent the proliferation of malware propagation by securing infrastructure links and limiting malware propagation to local device-to-device dissemination. This article proposes a novel traffic-aware patching scheme to select important intermediate nodes to patch, which applies to the IoT system with limited patching resources and response time constraint. Experiments on real-world trace datasets in IoT networks are conducted to demonstrate the advantage of the proposed traffic-aware patching scheme in alleviating malware propagation.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Online Feature Ranking for Intrusion Detection Systems,"Many current approaches to the design of intrusion detection systems apply feature selection in a static, non-adaptive fashion. These methods often neglect the dynamic nature of network data which requires to use adaptive feature selection techniques. In this paper, we present a simple technique based on incremental learning of support vector machines in order to rank the features in real time within a streaming model for network data. Some illustrative numerical experiments with two popular benchmark datasets show that our approach allows to adapt to the changes in normal network behaviour and novel attack patterns which have not been experienced before.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Active Learning for Wireless IoT Intrusion Detection,"Internet of Things (IoT) is becoming truly ubiquitous in our everyday life, but it also faces unique security challenges. Intrusion detection is critical for the security and safety of a wireless IoT network. This paper discusses the human-in-the-loop active learning approach for wireless intrusion detection. We first present the fundamental challenges against the design of a successful Intrusion Detection System (IDS) for wireless IoT network. We then briefly review the rudimentary concepts of active learning and propose its employment in the diverse applications of wireless intrusion detection. Experimental example is also presented to show the significant performance improvement of the active learning method over traditional supervised learning approach. While machine learning techniques have been widely employed for intrusion detection, the application of human-in-the-loop machine learning that leverages both machine and human intelligence to intrusion detection of IoT is still in its infancy. We hope this article can assist the readers in understanding the key concepts of active learning and spur further research in this area.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
RTFM: How hard are IoT platform providers making it for their developers?,"Internet of Things (IoT) devices routinely have security issues, but are the platform designers providing enough support to IoT developers for them to easily implement security features for their platforms? We surveyed the documentation, code and guidance from nine IoT manufacturers to look at what guidance they provided for implementing three security features required by several security standards (secure boot, device identity keys and unique per device passwords). We find that more needs to be done to support developers if we want them to adopt security features -- especially in the face of incoming legislation that will require developers to implement them.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A review of Federated Learning in Intrusion Detection Systems for IoT,"Intrusion detection systems are evolving into intelligent systems that perform data analysis searching for anomalies in their environment. The development of deep learning technologies opened the door to build more complex and effective threat detection models. However, training those models may be computationally infeasible in most Internet of Things devices. Current approaches rely on powerful centralized servers that receive data from all their parties -- violating basic privacy constraints and substantially affecting response times and operational costs due to the huge communication overheads. To mitigate these issues, Federated Learning emerged as a promising approach where different agents collaboratively train a shared model, neither exposing training data to others nor requiring a compute-intensive centralized infrastructure. This paper focuses on the application of Federated Learning approaches in the field of Intrusion Detection. Both technologies are described in detail and current scientific progress is reviewed and categorized. Finally, the paper highlights the limitations present in recent works and presents some future directions for this technology.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Toward Autonomous and Efficient Cybersecurity: A Multi-Objective AutoML-based Intrusion Detection System,"With increasingly sophisticated cybersecurity threats and rising demand for network automation, autonomous cybersecurity mechanisms are becoming critical for securing modern networks. The rapid expansion of Internet of Things (IoT) systems amplifies these challenges, as resource-constrained IoT devices demand scalable and efficient security solutions. In this work, an innovative Intrusion Detection System (IDS) utilizing Automated Machine Learning (AutoML) and Multi-Objective Optimization (MOO) is proposed for autonomous and optimized cyber-attack detection in modern networking environments. The proposed IDS framework integrates two primary innovative techniques: Optimized Importance and Percentage-based Automated Feature Selection (OIP-AutoFS) and Optimized Performance, Confidence, and Efficiency-based Combined Algorithm Selection and Hyperparameter Optimization (OPCE-CASH). These components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency. This work presents the first IDS framework that integrates all four AutoML stages and employs multi-objective optimization to jointly optimize detection effectiveness, efficiency, and confidence for deployment in resource-constrained systems. Experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed MOO-AutoML IDS outperforms state-of-the-art IDSs, establishing a new benchmark for autonomous, efficient, and optimized security for networks. Designed to support IoT and edge environments with resource constraints, the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Cyber Situation Awareness with Active Learning for Intrusion Detection,"Intrusion detection has focused primarily on detecting cyberattacks at the event-level. Since there is such a large volume of network data and attacks are minimal, machine learning approaches have focused on improving accuracy and reducing false positives, but this has frequently resulted in overfitting. In addition, the volume of intrusion detection alerts is large and creates fatigue in the human analyst who must review them. This research addresses the problems associated with event-level intrusion detection and the large volumes of intrusion alerts by applying active learning and cyber situation awareness. This paper includes the results of two experiments using the UNSW-NB15 dataset. The first experiment evaluated sampling approaches for querying the oracle, as part of active learning. It then trained a Random Forest classifier using the samples and evaluated its results. The second experiment applied cyber situation awareness by aggregating the detection results of the first experiment and calculating the probability that a computer system was part of a cyberattack. This research showed that moving the perspective of event-level alerts to the probability that a computer system was part of an attack improved the accuracy of detection and reduced the volume of alerts that a human analyst would need to review.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Road Context-aware Intrusion Detection System for Autonomous Cars,"Security is of primary importance to vehicles. The viability of performing remote intrusions onto the in-vehicle network has been manifested. In regard to unmanned autonomous cars, limited work has been done to detect intrusions for them while existing intrusion detection systems (IDSs) embrace limitations against strong adversaries. In this paper, we consider the very nature of autonomous car and leverage the road context to build a novel IDS, named Road context-aware IDS (RAIDS). When a computer-controlled car is driving through continuous roads, road contexts and genuine frames transmitted on the car's in-vehicle network should resemble a regular and intelligible pattern. RAIDS hence employs a lightweight machine learning model to extract road contexts from sensory information (e.g., camera images and distance sensor values) that are used to generate control signals for maneuvering the car. With such ongoing road context, RAIDS validates corresponding frames observed on the in-vehicle network. Anomalous frames that substantially deviate from road context will be discerned as intrusions. We have implemented a prototype of RAIDS with neural networks, and conducted experiments on a Raspberry Pi with extensive datasets and meaningful intrusion cases. Evaluations show that RAIDS significantly outperforms state-of-the-art IDS without using road context by up to 99.9% accuracy and short response time.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Program Analysis of Commodity IoT Applications for Security and Privacy: Challenges and Opportunities,"Recent advances in Internet of Things (IoT) have enabled myriad domains such as smart homes, personal monitoring devices, and enhanced manufacturing. IoT is now pervasive---new applications are being used in nearly every conceivable environment, which leads to the adoption of device-based interaction and automation. However, IoT has also raised issues about the security and privacy of these digitally augmented spaces. Program analysis is crucial in identifying those issues, yet the application and scope of program analysis in IoT remains largely unexplored by the technical community. In this paper, we study privacy and security issues in IoT that require program-analysis techniques with an emphasis on identified attacks against these systems and defenses implemented so far. Based on a study of five IoT programming platforms, we identify the key insights that result from research efforts in both the program analysis and security communities and relate the efficacy of program-analysis techniques to security and privacy issues. We conclude by studying recent IoT analysis systems and exploring their implementations. Through these explorations, we highlight key challenges and opportunities in calibrating for the environments in which IoT systems will be used.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Clustered Federated Learning Architecture for Network Anomaly Detection in Large Scale Heterogeneous IoT Networks,"There is a growing trend of cyberattacks against Internet of Things (IoT) devices; moreover, the sophistication and motivation of those attacks is increasing. The vast scale of IoT, diverse hardware and software, and being typically placed in uncontrolled environments make traditional IT security mechanisms such as signature-based intrusion detection and prevention systems challenging to integrate. They also struggle to cope with the rapidly evolving IoT threat landscape due to long delays between the analysis and publication of the detection rules. Machine learning methods have shown faster response to emerging threats; however, model training architectures like cloud or edge computing face multiple drawbacks in IoT settings, including network overhead and data isolation arising from the large scale and heterogeneity that characterizes these networks.   This work presents an architecture for training unsupervised models for network intrusion detection in large, distributed IoT and Industrial IoT (IIoT) deployments. We leverage Federated Learning (FL) to collaboratively train between peers and reduce isolation and network overhead problems. We build upon it to include an unsupervised device clustering algorithm fully integrated into the FL pipeline to address the heterogeneity issues that arise in FL settings. The architecture is implemented and evaluated using a testbed that includes various emulated IoT/IIoT devices and attackers interacting in a complex network topology comprising 100 emulated devices, 30 switches and 10 routers. The anomaly detection models are evaluated on real attacks performed by the testbed's threat actors, including the entire Mirai malware lifecycle, an additional botnet based on the Merlin command and control server and other red-teaming tools performing scanning activities and multiple attacks targeting the emulated devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Lack of Systematic Approach to Security of IoT Context Sharing Platforms,IoT context-sharing platforms are an essential component of today's interconnected IoT deployments with their security affecting the entire deployment and the critical infrastructure adopting IoT. We report on a lack of systematic approach to the security of IoT context-sharing platforms and propose the need for a methodological and systematic alternative to evaluate the existing solutions and develop `secure-by-design' solutions. We have identified the key components of a generic IoT context-sharing platform and propose using MITRE ATT&CK for threat modelling of such platforms.,"cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A New Clustering Approach for Anomaly Intrusion Detection,Recent advances in technology have made our work easier compare to earlier times. Computer network is growing day by day but while discussing about the security of computers and networks it has always been a major concerns for organizations varying from smaller to larger enterprises. It is true that organizations are aware of the possible threats and attacks so they always prepare for the safer side but due to some loopholes attackers are able to make attacks. Intrusion detection is one of the major fields of research and researchers are trying to find new algorithms for detecting intrusions. Clustering techniques of data mining is an interested area of research for detecting possible intrusions and attacks. This paper presents a new clustering approach for anomaly intrusion detection by using the approach of K-medoids method of clustering and its certain modifications. The proposed algorithm is able to achieve high detection rate and overcomes the disadvantages of K-means algorithm.,"cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
HBFL: A Hierarchical Blockchain-based Federated Learning Framework for a Collaborative IoT Intrusion Detection,"The continuous strengthening of the security posture of IoT ecosystems is vital due to the increasing number of interconnected devices and the volume of sensitive data shared. The utilisation of Machine Learning (ML) capabilities in the defence against IoT cyber attacks has many potential benefits. However, the currently proposed frameworks do not consider data privacy, secure architectures, and/or scalable deployments of IoT ecosystems. In this paper, we propose a hierarchical blockchain-based federated learning framework to enable secure and privacy-preserved collaborative IoT intrusion detection. We highlight and demonstrate the importance of sharing cyber threat intelligence among inter-organisational IoT networks to improve the model's detection capabilities. The proposed ML-based intrusion detection framework follows a hierarchical federated learning architecture to ensure the privacy of the learning process and organisational data. The transactions (model updates) and processes will run on a secure immutable ledger, and the conformance of executed tasks will be verified by the smart contract. We have tested our solution and demonstrated its feasibility by implementing it and evaluating the intrusion detection performance using a key IoT data set. The outcome is a securely designed ML-based intrusion detection system capable of detecting a wide range of malicious activities while preserving data privacy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security Update Labels: Establishing Economic Incentives for Security Patching of IoT Consumer Products,"With the expansion of the Internet of Things (IoT), the number of security incidents due to insecure and misconfigured IoT devices is increasing. Especially on the consumer market, manufacturers focus on new features and early releases at the expense of a comprehensive security strategy. Hence, experts have started calling for regulation of the IoT consumer market, while policymakers are seeking for suitable regulatory approaches. We investigate how manufacturers can be incentivized to increase sustainable security efforts for IoT products. We propose mandatory security update labels that inform consumers during buying decisions about the willingness of the manufacturer to provide security updates in the future. Mandatory means that the labels explicitly state when security updates are not guaranteed. We conducted a user study with more than 1,400 participants to assess the importance of security update labels for the consumer choice by means of a conjoint analysis. The results show that the availability of security updates (until which date the updates are guaranteed) accounts for 8% to 35% impact on overall consumers' choice, depending on the perceived security risk of the product category. For products with a high perceived security risk, this availability is twice as important as other high-ranked product attributes. Moreover, provisioning time for security updates (how quickly the product will be patched after a vulnerability is discovered) additionally accounts for 7% to 25% impact on consumers' choices. The proposed labels are intuitively understood by consumers, do not require product assessments by third parties before release, and have a potential to incentivize manufacturers to provide sustainable security support.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
FedMADE: Robust Federated Learning for Intrusion Detection in IoT Networks Using a Dynamic Aggregation Method,"The rapid proliferation of Internet of Things (IoT) devices across multiple sectors has escalated serious network security concerns. This has prompted ongoing research in Machine Learning (ML)-based Intrusion Detection Systems (IDSs) for cyber-attack classification. Traditional ML models require data transmission from IoT devices to a centralized server for traffic analysis, raising severe privacy concerns. To address this issue, researchers have studied Federated Learning (FL)-based IDSs that train models across IoT devices while keeping their data localized. However, the heterogeneity of data, stemming from distinct vulnerabilities of devices and complexity of attack vectors, poses a significant challenge to the effectiveness of FL models. While current research focuses on adapting various ML models within the FL framework, they fail to effectively address the issue of attack class imbalance among devices, which significantly degrades the classification accuracy of minority attacks. To overcome this challenge, we introduce FedMADE, a novel dynamic aggregation method, which clusters devices by their traffic patterns and aggregates local models based on their contributions towards overall performance. We evaluate FedMADE against other FL algorithms designed for non-IID data and observe up to 71.07% improvement in minority attack classification accuracy. We further show that FedMADE is robust to poisoning attacks and incurs only a 4.7% (5.03 seconds) latency overhead in each communication round compared to FedAvg, without increasing the computational load of IoT devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
CIoTA: Collaborative IoT Anomaly Detection via Blockchain,"Due to their rapid growth and deployment, Internet of things (IoT) devices have become a central aspect of our daily lives. However, they tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can help us secure the IoT devices. However, an anomaly detection model must be trained for a long time in order to capture all benign behaviors. This approach is vulnerable to adversarial attacks since all observations are assumed to be benign while training the anomaly detection model.   In this paper, we propose CIoTA, a lightweight framework that utilizes the blockchain concept to perform distributed and collaborative anomaly detection for devices with limited resources. CIoTA uses blockchain to incrementally update a trusted anomaly detection model via self-attestation and consensus among IoT devices. We evaluate CIoTA on our own distributed IoT simulation platform, which consists of 48 Raspberry Pis, to demonstrate CIoTA's ability to enhance the security of each device and the security of the network as a whole.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security and Privacy Management of IoT Using Quantum Computing,"The convergence of the Internet of Things (IoT) and quantum computing is redefining the security paradigm of interconnected digital systems. Classical cryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and Advanced Encryption Standard (AES) have long provided the foundation for securing IoT communication. However, the emergence of quantum algorithms such as Shor's and Grover's threatens to render these techniques vulnerable, necessitating the development of quantum-resilient alternatives. This chapter examines the implications of quantum computing for IoT security and explores strategies for building cryptographically robust systems in the post-quantum era. It presents an overview of Post-Quantum Cryptographic (PQC) families, including lattice-based, code-based, hash-based, and multivariate approaches, analyzing their potential for deployment in resource-constrained IoT environments. In addition, quantum-based methods such as Quantum Key Distribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed for their ability to enhance confidentiality and privacy through physics-based security guarantees. The chapter also highlights issues of privacy management, regulatory compliance, and standardization, emphasizing the need for collaborative efforts across academia, industry, and governance. Overall, it provides a comprehensive perspective on security IoT ecosystems against quantum threats and ensures resilience in the next generation of intelligent networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing IoT Security with CNN and LSTM-Based Intrusion Detection Systems,"Protecting Internet of things (IoT) devices against cyber attacks is imperative owing to inherent security vulnerabilities. These vulnerabilities can include a spectrum of sophisticated attacks that pose significant damage to both individuals and organizations. Employing robust security measures like intrusion detection systems (IDSs) is essential to solve these problems and protect IoT systems from such attacks. In this context, our proposed IDS model consists on a combination of convolutional neural network (CNN) and long short-term memory (LSTM) deep learning (DL) models. This fusion facilitates the detection and classification of IoT traffic into binary categories, benign and malicious activities by leveraging the spatial feature extraction capabilities of CNN for pattern recognition and the sequential memory retention of LSTM for discerning complex temporal dependencies in achieving enhanced accuracy and efficiency. In assessing the performance of our proposed model, the authors employed the new CICIoT2023 dataset for both training and final testing, while further validating the model's performance through a conclusive testing phase utilizing the CICIDS2017 dataset. Our proposed model achieves an accuracy rate of 98.42%, accompanied by a minimal loss of 0.0275. False positive rate(FPR) is equally important, reaching 9.17% with an F1-score of 98.57%. These results demonstrate the effectiveness of our proposed CNN-LSTM IDS model in fortifying IoT environments against potential cyber threats.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Generative Adversarial Networks-Driven Cyber Threat Intelligence Detection Framework for Securing Internet of Things,"While the benefits of 6G-enabled Internet of Things (IoT) are numerous, providing high-speed, low-latency communication that brings new opportunities for innovation and forms the foundation for continued growth in the IoT industry, it is also important to consider the security challenges and risks associated with the technology. In this paper, we propose a two-stage intrusion detection framework for securing IoTs, which is based on two detectors. In the first stage, we propose an adversarial training approach using generative adversarial networks (GAN) to help the first detector train on robust features by supplying it with adversarial examples as validation sets. Consequently, the classifier would perform very well against adversarial attacks. Then, we propose a deep learning (DL) model for the second detector to identify intrusions. We evaluated the proposed approach's efficiency in terms of detection accuracy and robustness against adversarial attacks. Experiment results with a new cyber security dataset demonstrate the effectiveness of the proposed methodology in detecting both intrusions and persistent adversarial examples with a weighted avg of 96%, 95%, 95%, and 95% for precision, recall, f1-score, and accuracy, respectively.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection,"The last decades have been characterized by unprecedented technological advances, many of them powered by modern technologies such as Artificial Intelligence (AI) and Machine Learning (ML). The world has become more digitally connected than ever, but we face major challenges. One of the most significant is cybercrime, which has emerged as a global threat to governments, businesses, and civil societies. The pervasiveness of digital technologies combined with a constantly shifting technological foundation has created a complex and powerful playground for cybercriminals, which triggered a surge in demand for intelligent threat detection systems based on machine and deep learning. This paper investigates AI-based cyber threat detection to protect our modern digital ecosystems. The primary focus is on evaluating ML-based classifiers and ensembles for anomaly-based malware detection and network intrusion detection and how to integrate those models in the context of network security, mobile security, and IoT security. The discussion highlights the challenges when deploying and integrating AI-enabled cybersecurity solutions into existing enterprise systems and IT infrastructures, including options to overcome those challenges. Finally, the paper provides future research directions to further increase the security and resilience of our modern digital industries, infrastructures, and ecosystems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Introduction to IoT,"The Internet of Things has rapidly transformed the 21st century, enhancing decision-making processes and introducing innovative consumer services such as pay-as-you-use models. The integration of smart devices and automation technologies has revolutionized every aspect of our lives, from health services to the manufacturing industry, and from the agriculture sector to mining. Alongside the positive aspects, it is also essential to recognize the significant safety, security, and trust concerns in this technological landscape. This chapter serves as a comprehensive guide for newcomers interested in the IoT domain, providing a foundation for making future contributions. Specifically, it discusses the overview, historical evolution, key characteristics, advantages, architectures, taxonomy of technologies, and existing applications in major IoT domains. In addressing prevalent issues and challenges in designing and deploying IoT applications, the chapter examines security threats across architectural layers, ethical considerations, user privacy concerns, and trust-related issues. This discussion equips researchers with a solid understanding of diverse IoT aspects, providing a comprehensive understanding of IoT technology along with insights into the extensive potential and impact of this transformative field.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine Learning-Assisted Intrusion Detection for Enhancing Internet of Things Security,"Attacks against the Internet of Things (IoT) are rising as devices, applications, and interactions become more networked and integrated. The increase in cyber-attacks that target IoT networks poses a considerable vulnerability and threat to the privacy, security, functionality, and availability of critical systems, which leads to operational disruptions, financial losses, identity thefts, and data breaches. To efficiently secure IoT devices, real-time detection of intrusion systems is critical, especially those using machine learning to identify threats and mitigate risks and vulnerabilities. This paper investigates the latest research on machine learning-based intrusion detection strategies for IoT security, concentrating on real-time responsiveness, detection accuracy, and algorithm efficiency. Key studies were reviewed from all well-known academic databases, and a taxonomy was provided for the existing approaches. This review also highlights existing research gaps and outlines the limitations of current IoT security frameworks to offer practical insights for future research directions and developments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Comparative Analysis of Machine Learning Algorithms for Intrusion Detection in Edge-Enabled IoT Networks,"A significant increase in the number of interconnected devices and data communication through wireless networks has given rise to various threats, risks and security concerns. Internet of Things (IoT) applications is deployed in almost every field of daily life, including sensitive environments. The edge computing paradigm has complemented IoT applications by moving the computational processing near the data sources. Among various security models, Machine Learning (ML) based intrusion detection is the most conceivable defense mechanism to combat the anomalous behavior in edge-enabled IoT networks. The ML algorithms are used to classify the network traffic into normal and malicious attacks. Intrusion detection is one of the challenging issues in the area of network security. The research community has proposed many intrusion detection systems. However, the challenges involved in selecting suitable algorithm(s) to provide security in edge-enabled IoT networks exist. In this paper, a comparative analysis of conventional machine learning classification algorithms has been performed to categorize the network traffic on NSL-KDD dataset using Jupyter on Pycharm tool. It can be observed that Multi-Layer Perception (MLP) has dependencies between input and output and relies more on network configuration for intrusion detection. Therefore, MLP can be more appropriate for edge-based IoT networks with a better training time of 1.2 seconds and testing accuracy of 79%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Online Self-Supervised Deep Learning for Intrusion Detection Systems,"This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Deep Learning (DL) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known {machine learning and deep learning} models, showing that this SSID framework is very useful and advantageous as an accurate and online learning DL-based IDS for IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Burstiness of Intrusion Detection Process: Empirical Evidence and a Modeling Approach,"We analyze sets of intrusion detection records observed on the networks of several large, nonresidential organizations protected by a form of intrusion detection and prevention service. Our analyses reveal that the process of intrusion detection in these networks exhibits a significant degree of burstiness as well as strong memory, with burstiness and memory properties that are comparable to those of natural processes driven by threshold effects, but different from bursty human activities. We explore time-series models of these observable network security incidents based on partially observed data using a hidden Markov model with restricted hidden states, which we fit using Markov Chain Monte Carlo techniques. We examine the output of the fitted model with respect to its statistical properties and demonstrate that the model adequately accounts for intrinsic ""bursting"" within observed network incidents as a result of alternation between two or more stochastic processes. While our analysis does not lead directly to new detection capabilities, the practical implications of gaining better understanding of the observed burstiness are significant, and include opportunities for quantifying a network's risks and defensive efforts.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
EDIMA: Early Detection of IoT Malware Network Activity Using Machine Learning Techniques,"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning/infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Are Trees Really Green? A Detection Approach of IoT Malware Attacks,"Nowadays, the Internet of Things (IoT) is widely employed, and its usage is growing exponentially because it facilitates remote monitoring, predictive maintenance, and data-driven decision making, especially in the healthcare and industrial sectors. However, IoT devices remain vulnerable due to their resource constraints and difficulty in applying security patches. Consequently, various cybersecurity attacks are reported daily, such as Denial of Service, particularly in IoT-driven solutions. Most attack detection methodologies are based on Machine Learning (ML) techniques, which can detect attack patterns. However, the focus is more on identification rather than considering the impact of ML algorithms on computational resources. This paper proposes a green methodology to identify IoT malware networking attacks based on flow privacy-preserving statistical features. In particular, the hyperparameters of three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are optimized based on energy consumption and test-time performance in terms of Matthew's Correlation Coefficient. Our results show that models maintain high performance and detection accuracy while consistently reducing power usage in terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion Detection Systems are suitable for IoT and other resource-constrained devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Intrusion Detection in IoT Networks Using Hyperdimensional Computing: A Case Study on the NSL-KDD Dataset,"The rapid expansion of Internet of Things (IoT) networks has introduced new security challenges, necessitating efficient and reliable methods for intrusion detection. In this study, a detection framework based on hyperdimensional computing (HDC) is proposed to identify and classify network intrusions using the NSL-KDD dataset, a standard benchmark for intrusion detection systems. By leveraging the capabilities of HDC, including high-dimensional representation and efficient computation, the proposed approach effectively distinguishes various attack categories such as DoS, probe, R2L, and U2R, while accurately identifying normal traffic patterns. Comprehensive evaluations demonstrate that the proposed method achieves an accuracy of 99.54%, significantly outperforming conventional intrusion detection techniques, making it a promising solution for IoT network security. This work emphasizes the critical role of robust and precise intrusion detection in safeguarding IoT systems against evolving cyber threats.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Comparative Analysis of Machine Learning Techniques for IoT Intrusion Detection,"The digital transformation faces tremendous security challenges. In particular, the growing number of cyber-attacks targeting Internet of Things (IoT) systems restates the need for a reliable detection of malicious network activity. This paper presents a comparative analysis of supervised, unsupervised and reinforcement learning techniques on nine malware captures of the IoT-23 dataset, considering both binary and multi-class classification scenarios. The developed models consisted of Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), Isolation Forest (iForest), Local Outlier Factor (LOF) and a Deep Reinforcement Learning (DRL) model based on a Double Deep Q-Network (DDQN), adapted to the intrusion detection context. The most reliable performance was achieved by LightGBM. Nonetheless, iForest displayed good anomaly detection results and the DRL model demonstrated the possible benefits of employing this methodology to continuously improve the detection. Overall, the obtained results indicate that the analyzed techniques are well suited for IoT intrusion detection.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Safeguarding the IoT from Malware Epidemics: A Percolation Theory Approach,"The upcoming Internet of things (IoT) is foreseen to encompass massive numbers of connected devices, smart objects, and cyber-physical systems. Due to the large-scale and massive deployment of devices, it is deemed infeasible to safeguard 100% of the devices with state-of-the-art security countermeasures. Hence, large-scale IoT has inevitable loopholes for network intrusion and malware infiltration. Even worse, exploiting the high density of devices and direct wireless connectivity, malware infection can stealthily propagate through susceptible (i.e., unsecured) devices and form an epidemic outbreak without being noticed to security administration. A malware outbreak enables adversaries to compromise large population of devices, which can be exploited to launch versatile cyber and physical malicious attacks. In this context, we utilize spatial firewalls, to safeguard the IoT from malware outbreak. In particular, spatial firewalls are computationally capable devices equipped with state-of-the-art security and anti-malware programs that are spatially deployed across the network to filter the wireless traffic in order to detect and thwart malware propagation. Using tools from percolation theory, we prove that there exists a critical density of spatial firewalls beyond which malware outbreak is impossible. This, in turns, safeguards the IoT from malware epidemics regardless of the infection/treatment rates. To this end, a tractable upper bound for the critical density of spatial firewalls is obtained. Furthermore, we characterize the relative communications ranges of the spatial firewalls and IoT devices to ensure secure network connectivity. The percentage of devices secured by the firewalls is also characterized.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
CoAP-DoS: An IoT Network Intrusion Dataset,"The need for secure Internet of Things (IoT) devices is growing as IoT devices are becoming more integrated into vital networks. Many systems rely on these devices to remain available and provide reliable service. Denial of service attacks against IoT devices are a real threat due to the fact these low power devices are very susceptible to denial-of-service attacks. Machine learning enabled network intrusion detection systems are effective at identifying new threats, but they require a large amount of data to work well. There are many network traffic data sets but very few that focus on IoT network traffic. Within the IoT network data sets there is a lack of CoAP denial of service data. We propose a novel data set covering this gap. We develop a new data set by collecting network traffic from real CoAP denial of service attacks and compare the data on multiple different machine learning classifiers. We show that the data set is effective on many classifiers.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Analyzing Adversarial Attacks Against Deep Learning for Intrusion Detection in IoT Networks,"Adversarial attacks have been widely studied in the field of computer vision but their impact on network security applications remains an area of open research. As IoT, 5G and AI continue to converge to realize the promise of the fourth industrial revolution (Industry 4.0), security incidents and events on IoT networks have increased. Deep learning techniques are being applied to detect and mitigate many of such security threats against IoT networks. Feedforward Neural Networks (FNN) have been widely used for classifying intrusion attacks in IoT networks. In this paper, we consider a variant of the FNN known as the Self-normalizing Neural Network (SNN) and compare its performance with the FNN for classifying intrusion attacks in an IoT network. Our analysis is performed using the BoT-IoT dataset from the Cyber Range Lab of the center of UNSW Canberra Cyber. In our experimental results, the FNN outperforms the SNN for intrusion detection in IoT networks based on multiple performance metrics such as accuracy, precision, and recall as well as multi-classification metrics such as Cohen's Kappa score. However, when tested for adversarial robustness, the SNN demonstrates better resilience against the adversarial samples from the IoT dataset, presenting a promising future in the quest for safer and more secure deep learning in IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Optimizing Malware Detection in IoT Networks: Leveraging Resource-Aware Distributed Computing for Enhanced Security,"In recent years, networked IoT systems have revolutionized connectivity, portability, and functionality, offering a myriad of advantages. However, these systems are increasingly targeted by adversaries due to inherent security vulnerabilities and limited computational and storage resources. Malicious applications, commonly known as malware, pose a significant threat to IoT devices and networks. While numerous malware detection techniques have been proposed, existing approaches often overlook the resource constraints inherent in IoT environments, assuming abundant resources for detection tasks. This oversight is compounded by ongoing workloads such as sensing and on-device computations, further diminishing available resources for malware detection. To address these challenges, we present a novel resource- and workload-aware malware detection framework integrated with distributed computing for IoT networks. Our approach begins by analyzing available resources for malware detection using a lightweight regression model. Depending on resource availability, ongoing workload executions, and communication costs, the malware detection task is dynamically allocated either on-device or offloaded to neighboring IoT nodes with sufficient resources. To safeguard data integrity and user privacy, rather than transferring the entire malware detection task, the classifier is partitioned and distributed across multiple nodes, and subsequently integrated at the parent node for comprehensive malware detection. Experimental analysis demonstrates the efficacy of our proposed technique, achieving a remarkable speed-up of 9.8x compared to on-device inference, while maintaining a high malware detection accuracy of 96.7%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"New Frontiers in IoT: Networking, Systems, Reliability, and Security Challenges","The field of IoT has blossomed and is positively influencing many application domains. In this paper, we bring out the unique challenges this field poses to research in computer systems and networking. The unique challenges arise from the unique characteristics of IoT systems such as the diversity of application domains where they are used and the increasingly demanding protocols they are being called upon to run (such as, video and LIDAR processing) on constrained resources (on-node and network). We show how these open challenges can benefit from foundations laid in other areas, such as, 5G cellular protocols, ML model reduction, and device-edge-cloud offloading. We then discuss the unique challenges for reliability, security, and privacy posed by IoT systems due to their salient characteristics which include heterogeneity of devices and protocols, dependence on the physical environment, and the close coupling with humans. We again show how the open research challenges benefit from reliability, security, and privacy advancements in other areas. We conclude by providing a vision for a desirable end state for IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Secure and Robust Scheme for Sharing Confidential Information in IoT Systems,"In Internet of Things (IoT) systems with security demands, there is often a need to distribute sensitive information (such as encryption keys, digital signatures, or login credentials, etc.) among the devices, so that it can be retrieved for confidential purposes at a later moment. However, this information cannot be entrusted to any one device, since the failure of that device or an attack on it will jeopardize the security of the entire network. Even if the information is divided among devices, there is still the danger that an attacker can compromise a group of devices and expose the sensitive information. In this work, we design and implement a secure and robust scheme to enable the distribution of sensitive information in IoT networks. The proposed approach has two important properties: (1) it uses Threshold Secret Sharing (TSS) to split the information into pieces distributed among all devices in the system - and so the information can only be retrieved collaboratively by groups of devices; and (2) it ensures the privacy and integrity of the information, even when attackers hijack a large number of devices and use them in concert - specifically, all the compromised devices can be identified, the confidentiality of information is kept, and authenticity of the secret can be guaranteed.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
PhishChain: A Decentralized and Transparent System to Blacklist Phishing URLs,"Blacklists are a widely-used Internet security mechanism to protect Internet users from financial scams, malicious web pages and other cyber attacks based on blacklisted URLs. In this demo, we introduce PhishChain, a transparent and decentralized system to blacklisting phishing URLs. At present, public/private domain blacklists, such as PhishTank, CryptoScamDB, and APWG, are maintained by a centralized authority, but operate in a crowd sourcing fashion to create a manually verified blacklist periodically. In addition to being a single point of failure, the blacklisting process utilized by such systems is not transparent. We utilize the blockchain technology to support transparency and decentralization, where no single authority is controlling the blacklist and all operations are recorded in an immutable distributed ledger. Further, we design a page rank based truth discovery algorithm to assign a phishing score to each URL based on crowd sourced assessment of URLs. As an incentive for voluntary participation, we assign skill points to each user based on their participation in URL verification.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
AI-based Two-Stage Intrusion Detection for Software Defined IoT Networks,"Software Defined Internet of Things (SD-IoT) Networks profits from centralized management and interactive resource sharing which enhances the efficiency and scalability of IoT applications. But with the rapid growth in services and applications, it is vulnerable to possible attacks and faces severe security challenges. Intrusion detection has been widely used to ensure network security, but classical detection means are usually signature-based or explicit-behavior-based and fail to detect unknown attacks intelligently, which are hard to satisfy the requirements of SD-IoT Networks. In this paper, we propose an AI-based two-stage intrusion detection empowered by software defined technology. It flexibly captures network flows with a globle view and detects attacks intelligently through applying AI algorithms. We firstly leverage Bat algorithm with swarm division and Differential Mutation to select typical features. Then, we exploit Random forest through adaptively altering the weights of samples using weighted voting mechanism to classify flows. Evaluation results prove that the modified intelligent algorithms select more important features and achieve superior performance in flow classification. It is also verified that intelligent intrusion detection shows better accuracy with lower overhead comparied with existing solutions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Ask the Experts: What Should Be on an IoT Privacy and Security Label?,"Information about the privacy and security of Internet of Things (IoT) devices is not readily available to consumers who want to consider it before making purchase decisions. While legislators have proposed adding succinct, consumer accessible, labels, they do not provide guidance on the content of these labels. In this paper, we report on the results of a series of interviews and surveys with privacy and security experts, as well as consumers, where we explore and test the design space of the content to include on an IoT privacy and security label. We conduct an expert elicitation study by following a three-round Delphi process with 22 privacy and security experts to identify the factors that experts believed are important for consumers when comparing the privacy and security of IoT devices to inform their purchase decisions. Based on how critical experts believed each factor is in conveying risk to consumers, we distributed these factors across two layers---a primary layer to display on the product package itself or prominently on a website, and a secondary layer available online through a web link or a QR code. We report on the experts' rationale and arguments used to support their choice of factors. Moreover, to study how consumers would perceive the privacy and security information specified by experts, we conducted a series of semi-structured interviews with 15 participants, who had purchased at least one IoT device (smart home device or wearable). Based on the results of our expert elicitation and consumer studies, we propose a prototype privacy and security label to help consumers make more informed IoT-related purchase decisions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Intrusion Detection Systems for Smart Home IoT Devices: Experimental Comparison Study,"Smart homes are one of the most promising applications of the emerging Internet of Things (IoT) technology. With the growing number of IoT related devices such as smart thermostats, smart fridges, smart speaker, smart light bulbs and smart locks, smart homes promise to make our lives easier and more comfortable. However, the increased deployment of such smart devices brings an increase in potential security risks and home privacy breaches. In order to overcome such risks, Intrusion Detection Systems are presented as pertinent tools that can provide network-level protection for smart devices deployed in home environments. These systems monitor the network activities of the smart home-connected de-vices and focus on alerting suspicious or malicious activity. They also can deal with detected abnormal activities by hindering the impostors in accessing the victim devices. However, the employment of such systems in the context of a smart home can be challenging due to the devices hardware limitations, which may restrict their ability to counter the existing and emerging attack vectors. Therefore, this paper proposes an experimental comparison between the widely used open-source NIDSs namely Snort, Suricata and Bro IDS to find the most appropriate one for smart homes in term of detection accuracy and resources consumption including CP and memory utilization. Experimental Results show that Suricata is the best performing NIDS for smart homes","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
LBDMIDS: LSTM Based Deep Learning Model for Intrusion Detection Systems for IoT Networks,"In the recent years, we have witnessed a huge growth in the number of Internet of Things (IoT) and edge devices being used in our everyday activities. This demands the security of these devices from cyber attacks to be improved to protect its users. For years, Machine Learning (ML) techniques have been used to develop Network Intrusion Detection Systems (NIDS) with the aim of increasing their reliability/robustness. Among the earlier ML techniques DT performed well. In the recent years, Deep Learning (DL) techniques have been used in an attempt to build more reliable systems. In this paper, a Deep Learning enabled Long Short Term Memory (LSTM) Autoencoder and a 13-feature Deep Neural Network (DNN) models were developed which performed a lot better in terms of accuracy on UNSW-NB15 and Bot-IoT datsets. Hence we proposed LBDMIDS, where we developed NIDS models based on variants of LSTMs namely, stacked LSTM and bidirectional LSTM and validated their performance on the UNSW\_NB15 and BoT\-IoT datasets. This paper concludes that these variants in LBDMIDS outperform classic ML techniques and perform similarly to the DNN models that have been suggested in the past.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Machine and Deep Learning for IoT Security and Privacy: Applications, Challenges, and Future Directions","The integration of the Internet of Things (IoT) connects a number of intelligent devices with a minimum of human interference that can interact with one another. IoT is rapidly emerging in the areas of computer science. However, new security problems were posed by the cross-cutting design of the multidisciplinary elements and IoT systems involved in deploying such schemes. Ineffective is the implementation of security protocols, i.e., authentication, encryption, application security, and access network for IoT systems and their essential weaknesses in security. Current security approaches can also be improved to protect the IoT environment effectively. In recent years, deep learning (DL)/ machine learning (ML) has progressed significantly in various critical implementations. Therefore, DL/ML methods are essential to turn IoT systems protection from simply enabling safe contact between IoT systems to intelligence systems in security. This review aims to include an extensive analysis of ML systems and state-of-the-art developments in DL methods to improve enhanced IoT device protection methods. On the other hand, various new insights in machine and deep learning for IoT Securities illustrate how it could help future research. IoT protection risks relating to emerging or essential threats are identified, as well as future IoT device attacks and possible threats associated with each surface. We then carefully analyze DL and ML IoT protection approaches and present each approach's benefits, possibilities, and weaknesses. This review discusses a number of potential challenges and limitations. The future works, recommendations, and suggestions of DL/ML in IoT security are also included.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Secure IoT access at scale using blockchains and smart contracts,"Blockchains and smart contracts are an emerging, promising technology, that has received considerable attention. We use the blockchain technology, and in particular Ethereum, to implement a large-scale event-based Internet of Things (IoT) control system. We argue that the distributed nature of the ""ledger,"" as well as, Ethereum's capability of parallel execution of replicated ""smart contracts"", provide the sought after automation, generality, flexibility, resilience, and high availability. We design a realistic blockchain-based IoT architecture, using existing technologies while by taking into consideration the characteristics and limitations of IoT devices and applications. Furthermore, we leverage blockchain's immutability and Ethereum's support for custom tokens to build a robust and efficient token-based access control mechanism. Our evaluation shows that our solution is viable and offers significant security and usability advantages.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Tree-based Intelligent Intrusion Detection System in Internet of Vehicles,"The use of autonomous vehicles (AVs) is a promising technology in Intelligent Transportation Systems (ITSs) to improve safety and driving efficiency. Vehicle-to-everything (V2X) technology enables communication among vehicles and other infrastructures. However, AVs and Internet of Vehicles (IoV) are vulnerable to different types of cyber-attacks such as denial of service, spoofing, and sniffing attacks. In this paper, an intelligent intrusion detection system (IDS) is proposed based on tree-structure machine learning models. The results from the implementation of the proposed intrusion detection system on standard data sets indicate that the system has the ability to identify various cyber-attacks in the AV networks. Furthermore, the proposed ensemble learning and feature selection approaches enable the proposed system to achieve high detection rate and low computational cost simultaneously.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Is there a Trojan! : Literature survey and critical evaluation of the latest ML based modern intrusion detection systems in IoT environments,"IoT as a domain has grown so much in the last few years that it rivals that of the mobile network environments in terms of data volumes as well as cybersecurity threats. The confidentiality and privacy of data within IoT environments have become very important areas of security research within the last few years. More and more security experts are interested in designing robust IDS systems to protect IoT environments as a supplement to the more traditional security methods. Given that IoT devices are resource-constrained and have a heterogeneous protocol stack, most traditional intrusion detection approaches don't work well within these schematic boundaries. This has led security researchers to innovate at the intersection of Machine Learning and IDS to solve the shortcomings of non-learning based IDS systems in the IoT ecosystem.   Despite various ML algorithms already having high accuracy with IoT datasets, we can see a lack of sufficient production grade models. This survey paper details a comprehensive summary of the latest learning-based approaches used in IoT intrusion detection systems, and conducts a thorough critical review of these systems, potential pitfalls in ML pipelines, challenges from an ML perspective, and discusses future research scope and recommendations.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection,"The proliferation of Internet of Things (IoT) devices has expanded the attack surface, necessitating efficient intrusion detection systems (IDSs) for network protection. This paper presents FLARE, a feature-based lightweight aggregation for robust evaluation of IoT intrusion detection to address the challenges of securing IoT environments through feature aggregation techniques. FLARE utilizes a multilayered processing approach, incorporating session, flow, and time-based sliding-window data aggregation to analyze network behavior and capture vital features from IoT network traffic data. We perform extensive evaluations on IoT data generated from our laboratory experimental setup to assess the effectiveness of the proposed aggregation technique. To classify attacks in IoT IDS, we employ four supervised learning models and two deep learning models. We validate the performance of these models in terms of accuracy, precision, recall, and F1-score. Our results reveal that incorporating the FLARE aggregation technique as a foundational step in feature engineering, helps lay a structured representation, and enhances the performance of complex end-to-end models, making it a crucial step in IoT IDS pipeline. Our findings highlight the potential of FLARE as a valuable technique to improve performance and reduce computational costs of end-to-end IDS implementations, thereby fostering more robust IoT intrusion detection systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Security: On-Chip Secure Deletion Scheme using ECC Modulation in IoT Appliances,"NAND flash memory-based IoT devices inherently suffer from data retention issues. In IoT security, these retention issues are significant and require a robust solution for secure deletion. Secure deletion methods can be categorized into off-chip and on-chip schemes. Off-chip secure deletion schemes, based on block-level erasure operations, are unable to perform real-time trim operations. Consequently, they are vulnerable to hacking threats. On the other hand, on-chip secure deletion schemes enable real-time trim operations by performing deletion on a page-by-page basis. However, the on-chip scheme introduces a challenge of program disturbance for neighboring page data. The proposed on-chip deletion scheme tackles this problem by utilizing ECC code modulation through a partial program operation. This approach significantly reduces the program disturbance issue associated with neighboring page data. Moreover, the proposed code modulation secure deletion scheme allows for real-time verification of the deletion of original data.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for Intrusion Detection and Security Research,"In this paper, a dataset of IoT network traffic is presented. Our dataset was generated by utilising the Gotham testbed, an emulated large-scale Internet of Things (IoT) network designed to provide a realistic and heterogeneous environment for network security research. The testbed includes 78 emulated IoT devices operating on various protocols, including MQTT, CoAP, and RTSP. Network traffic was captured in Packet Capture (PCAP) format using tcpdump, and both benign and malicious traffic were recorded. Malicious traffic was generated through scripted attacks, covering a variety of attack types, such as Denial of Service (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and various stages of Command and Control (C&C) communication. The data were subsequently processed in Python for feature extraction using the Tshark tool, and the resulting data was converted to Comma Separated Values (CSV) format and labelled. The data repository includes the raw network traffic in PCAP format and the processed labelled data in CSV format. Our dataset was collected in a distributed manner, where network traffic was captured separately for each IoT device at the interface between the IoT gateway and the device. Our dataset was collected in a distributed manner, where network traffic was separately captured for each IoT device at the interface between the IoT gateway and the device. With its diverse traffic patterns and attack scenarios, this dataset provides a valuable resource for developing Intrusion Detection Systems and security mechanisms tailored to complex, large-scale IoT environments. The dataset is publicly available at Zenodo.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Who Let the Smart Toaster Hack the House? An Investigation into the Security Vulnerabilities of Consumer IoT Devices,"For smart homes to be safe homes, they must be designed with security in mind. Yet, despite the widespread proliferation of connected digital technologies in the home environment, there is a lack of research evaluating the security vulnerabilities and potential risks present within these systems. Our research presents a comprehensive methodology for conducting systematic IoT security attacks, intercepting network traffic and evaluating the security risks of smart home devices. We perform hundreds of automated experiments using 11 popular commercial IoT devices when deployed in a testbed, exposed to a series of real deployed attacks (flooding, port scanning and OS scanning). Our findings indicate that these devices are vulnerable to security attacks and our results are relevant to the security research community, device engineers and the users who rely on these technologies in their daily lives.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
LCCDE: A Decision-Based Ensemble Framework for Intrusion Detection in The Internet of Vehicles,"Modern vehicles, including autonomous vehicles and connected vehicles, have adopted an increasing variety of functionalities through connections and communications with other vehicles, smart devices, and infrastructures. However, the growing connectivity of the Internet of Vehicles (IoV) also increases the vulnerabilities to network attacks. To protect IoV systems against cyber threats, Intrusion Detection Systems (IDSs) that can identify malicious cyber-attacks have been developed using Machine Learning (ML) approaches. To accurately detect various types of attacks in IoV networks, we propose a novel ensemble IDS framework named Leader Class and Confidence Decision Ensemble (LCCDE). It is constructed by determining the best-performing ML model among three advanced ML algorithms (XGBoost, LightGBM, and CatBoost) for every class or type of attack. The class leader models with their prediction confidence values are then utilized to make accurate decisions regarding the detection of various types of cyber-attacks. Experiments on two public IoV security datasets (Car-Hacking and CICIDS2017 datasets) demonstrate the effectiveness of the proposed LCCDE for intrusion detection on both intra-vehicle and external networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
How Memory-Safe is IoT? Assessing the Impact of Memory-Protection Solutions for Securing Wireless Gateways,"The rapid development of the Internet of Things (IoT) has enabled novel user-centred applications, including many in safety-critical areas such as healthcare, smart environment security, and emergency response systems. The diversity in IoT manufacturers, standards, and devices creates a combinatorial explosion of such deployment scenarios, leading to increased security and safety threats due to the difficulty of managing such heterogeneity. In almost every IoT deployment, wireless gateways are crucial for interconnecting IoT devices and providing services, yet they are vulnerable to external threats and serve as key entry points for large-scale IoT attacks. Memory-based vulnerabilities are among the most serious threats in software, with no universal solution yet available. Legacy memory protection mechanisms, such as canaries, RELRO, NX, and Fortify, have enhanced memory safety but remain insufficient for comprehensive protection. Emerging technologies like ARM-MTE, CHERI, and Rust are based on more universal and robust Secure-by-Design (SbD) memory safety principles, yet each entails different trade-offs in hardware or code modifications. Given the challenges of balancing security levels with associated overheads in IoT systems, this paper explores the impact of memory safety on the IoT domain through an empirical large-scale analysis of memory-related vulnerabilities in modern wireless gateways. Our results show that memory vulnerabilities constitute the majority of IoT gateway threats, underscoring the necessity for SbD solutions, with the choice of memory-protection technology depending on specific use cases and associated overheads.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Two Phase Authentication and VPN Based Secured Communication for IoT Home Networks,"With the advancement of technology, devices, which are considered non-traditional in terms of internet capabilities, are now being embedded in microprocessors to communicate and these devices are known as IoT devices. This technology has enabled household devices to have the ability to communicate with the internet and a network comprising of such device can create a home IoT network. Such IoT devices are resource constrained and lack high-level security protocols. Thus, security becomes a major issue for such network systems. One way to secure the networks is through reliable authentication protocols and data transfer mechanism. As the household devices are controllable by the users remotely, they are accessed over the internet. Therefore, there should also be a method to make the communication over the internet between IoT devices and the users more secured. This paper proposes a two-phase authentication protocol for authentication purposes and a VPN based secure channel creation for the communication of the devices in the network. Furthermore, the paper discusses the Elliptic Curve Cryptography as a viable alternative to RSA for a more efficient Key exchange mechanism for low-powered IoT devices in the network.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing IoT Malware Detection through Adaptive Model Parallelism and Resource Optimization,"The widespread integration of IoT devices has greatly improved connectivity and computational capabilities, facilitating seamless communication across networks. Despite their global deployment, IoT devices are frequently targeted for security breaches due to inherent vulnerabilities. Among these threats, malware poses a significant risk to IoT devices. The lack of built-in security features and limited resources present challenges for implementing effective malware detection techniques on IoT devices. Moreover, existing methods assume access to all device resources for malware detection, which is often not feasible for IoT devices deployed in critical real-world scenarios. To overcome this challenge, this study introduces a novel approach to malware detection tailored for IoT devices, leveraging resource and workload awareness inspired by model parallelism. Initially, the device assesses available resources for malware detection using a lightweight regression model. Based on resource availability, ongoing workload, and communication costs, the malware detection task is dynamically allocated either on-device or offloaded to neighboring IoT nodes with sufficient resources. To uphold data integrity and user privacy, instead of transferring the entire malware detection task, the classifier is divided and distributed across multiple nodes, then integrated at the parent node for detection. Experimental results demonstrate that this proposed technique achieves a significant speedup of 9.8 x compared to on-device inference, while maintaining a high malware detection accuracy of 96.7%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection,"The Internet of Things (IoT), with its high degree of interconnectivity and limited computational resources, is particularly vulnerable to a wide range of cyber threats. Intrusion detection systems (IDS) have been extensively studied to enhance IoT security, and machine learning-based IDS (ML-IDS) show considerable promise for detecting malicious activity. However, their effectiveness is often constrained by poor adaptability to emerging threats and the issue of catastrophic forgetting during continuous learning. To address these challenges, we propose CITADEL, a self-supervised continual learning framework designed to extract robust representations from benign data while preserving long-term knowledge through optimized memory consolidation mechanisms. CITADEL integrates a tabular-to-image transformation module, a memory-aware masked autoencoder for self-supervised representation learning, and a novelty detection component capable of identifying anomalies without dependence on labeled attack data. Our design enables the system to incrementally adapt to emerging behaviors while retaining its ability to detect previously observed threats. Experiments on multiple intrusion datasets demonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based lifelong anomaly detector (VLAD) in key detection and retention metrics, highlighting its effectiveness in dynamic IoT environments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing,"As the number of IoT devices has increased rapidly, IoT botnets have exploited the vulnerabilities of IoT devices. However, it is still challenging to detect the initial intrusion on IoT devices prior to massive attacks. Recent studies have utilized power side-channel information to identify this intrusion behavior on IoT devices but still lack accurate models in real-time for ubiquitous botnet detection.   We proposed the first online intrusion detection system called DeepAuditor for IoT devices via power auditing. To develop the real-time system, we proposed a lightweight power auditing device called Power Auditor. We also designed a distributed CNN classifier for online inference in a laboratory setting. In order to protect data leakage and reduce networking redundancy, we then proposed a privacy-preserved inference protocol via Packed Homomorphic Encryption and a sliding window protocol in our system. The classification accuracy and processing time were measured, and the proposed classifier outperformed a baseline classifier, especially against unseen patterns. We also demonstrated that the distributed CNN design is secure against any distributed components. Overall, the measurements were shown to the feasibility of our real-time distributed system for intrusion detection on IoT devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
History of malware,In past three decades almost everything has changed in the field of malware and malware analysis. From malware created as proof of some security concept and malware created for financial gain to malware created to sabotage infrastructure. In this work we will focus on history and evolution of malware and describe most important malwares.,"cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Decentralised Trustworthy Collaborative Intrusion Detection System for IoT,"Intrusion Detection Systems (IDS) have been the industry standard for securing IoT networks against known attacks. To increase the capability of an IDS, researchers proposed the concept of blockchain-based Collaborative-IDS (CIDS), wherein blockchain acts as a decentralised platform allowing collaboration between CIDS nodes to share intrusion related information, such as intrusion alarms and detection rules. However, proposals in blockchain-based CIDS overlook the importance of continuous evaluation of the trustworthiness of each node and generally work based on the assumption that the nodes are always honest. In this paper, we propose a decentralised CIDS that emphasises the importance of building trust between CIDS nodes. In our proposed solution, each CIDS node exchanges detection rules to help other nodes detect new types of intrusion. Our architecture offloads the trust computation to the blockchain and utilises a decentralised storage to host the shared trustworthy detection rules, ensuring scalability. Our implementation in a lab-scale testbed shows that the our solution is feasible and performs within the expected benchmarks of the Ethereum platform.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Lightweight CNN-BiLSTM based Intrusion Detection Systems for Resource-Constrained IoT Devices,"Intrusion Detection Systems (IDSs) have played a significant role in detecting and preventing cyber-attacks within traditional computing systems. It is not surprising that the same technology is being applied to secure Internet of Things (IoT) networks from cyber threats. The limited computational resources available on IoT devices make it challenging to deploy conventional computing-based IDSs. The IDSs designed for IoT environments must also demonstrate high classification performance, utilize low-complexity models, and be of a small size. Despite significant progress in IoT-based intrusion detection, developing models that both achieve high classification performance and maintain reduced complexity remains challenging. In this study, we propose a hybrid CNN architecture composed of a lightweight CNN and bidirectional LSTM (BiLSTM) to enhance the performance of IDS on the UNSW-NB15 dataset. The proposed model is specifically designed to run onboard resource-constrained IoT devices and meet their computation capability requirements. Despite the complexity of designing a model that fits the requirements of IoT devices and achieves higher accuracy, our proposed model outperforms the existing research efforts in the literature by achieving an accuracy of 97.28\% for binary classification and 96.91\% for multiclassification.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things,"This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
MPInspector: A Systematic and Automatic Approach for Evaluating the Security of IoT Messaging Protocols,"Facilitated by messaging protocols (MP), many home devices are connected to the Internet, bringing convenience and accessibility to customers. However, most deployed MPs on IoT platforms are fragmented and are not implemented carefully to support secure communication. To the best of our knowledge, there is no systematic solution to perform automatic security checks on MP implementations yet.   To bridge the gap, we present MPInspector, the first automatic and systematic solution for vetting the security of MP implementations. MPInspector combines model learning with formal analysis and operates in three stages: (a) using parameter semantics extraction and interaction logic extraction to automatically infer the state machine of an MP implementation, (b) generating security properties based on meta properties and the state machine, and (c) applying automatic property based formal verification to identify property violations. We evaluate MPInspector on three popular MPs, including MQTT, CoAP and AMQP, implemented on nine leading IoT platforms. It identifies 252 property violations, leveraging which we further identify eleven types of attacks under two realistic attack scenarios. In addition, we demonstrate that MPInspector is lightweight (the average overhead of end-to-end analysis is ~4.5 hours) and effective with a precision of 100% in identifying property violations.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine Learning Based Intrusion Detection Systems for IoT Applications,"Internet of Things (IoT) and its applications are the most popular research areas at present. The characteristics of IoT on one side make it easily applicable to real-life applications, whereas on the other side expose it to cyber threats. Denial of Service (DoS) is one of the most catastrophic attacks against IoT. In this paper, we investigate the prospects of using machine learning classification algorithms for securing IoT against DoS attacks. A comprehensive study is carried on the classifiers which can advance the development of anomaly-based intrusion detection systems (IDSs). Performance assessment of classifiers is done in terms of prominent metrics and validation methods. Popular datasets CIDDS-001, UNSW-NB15, and NSL-KDD are used for benchmarking classifiers. Friedman and Nemenyi tests are employed to analyze the significant differences among classifiers statistically. In addition, Raspberry Pi is used to evaluate the response time of classifiers on IoT specific hardware. We also discuss a methodology for selecting the best classifier as per application requirements. The main goals of this study are to motivate IoT security researchers for developing IDSs using ensemble learning, and suggesting appropriate methods for statistical assessment of classifier's performance.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks,"In recent years, the evolution of machine learning techniques has significantly impacted the field of intrusion detection, particularly within the context of the Internet of Things (IoT). As IoT networks expand, the need for robust security measures to counteract potential threats has become increasingly critical. This paper introduces a hybrid Intrusion Detection System (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs) with the XGBoost algorithm. Our proposed IDS leverages the unique capabilities of KANs, which utilize learnable activation functions to model complex relationships within data, alongside the powerful ensemble learning techniques of XGBoost, known for its high performance in classification tasks. This hybrid approach not only enhances the detection accuracy but also improves the interpretability of the model, making it suitable for dynamic and intricate IoT environments. Experimental evaluations demonstrate that our hybrid IDS achieves an impressive detection accuracy exceeding 99% in distinguishing between benign and malicious activities. Additionally, we were able to achieve F1 scores, precision, and recall that exceeded 98%. Furthermore, we conduct a comparative analysis against traditional Multi-Layer Perceptron (MLP) networks, assessing performance metrics such as Precision, Recall, and F1-score. The results underscore the efficacy of integrating KANs with XGBoost, highlighting the potential of this innovative approach to significantly strengthen the security framework of IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Clustering based opcode graph generation for malware variant detection,"Malwares are the key means leveraged by threat actors in the cyber space for their attacks. There is a large array of commercial solutions in the market and significant scientific research to tackle the challenge of the detection and defense against malwares. At the same time, attackers also advance their capabilities in creating polymorphic and metamorphic malwares to make it increasingly challenging for existing solutions. To tackle this issue, we propose a methodology to perform malware detection and family attribution. The proposed methodology first performs the extraction of opcodes from malwares in each family and constructs their respective opcode graphs. We explore the use of clustering algorithms on the opcode graphs to detect clusters of malwares within the same malware family. Such clusters can be seen as belonging to different sub-family groups. Opcode graph signatures are built from each detected cluster. Hence, for each malware family, a group of signatures is generated to represent the family. These signatures are used to classify an unknown sample as benign or belonging to one the malware families. We evaluate our methodology by performing experiments on a dataset consisting of both benign files and malware samples belonging to a number of different malware families and comparing the results to existing approach.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT,"The rapid expansion of the Internet of Things (IoT) has introduced significant security challenges, necessitating efficient and adaptive Intrusion Detection Systems (IDS). Traditional IDS models often overlook the temporal characteristics of network traffic, limiting their effectiveness in early threat detection. We propose a Transformer-based Early Intrusion Detection System (EIDS) that incorporates dynamic temporal positional encodings to enhance detection accuracy while maintaining computational efficiency. By leveraging network flow timestamps, our approach captures both sequence structure and timing irregularities indicative of malicious behaviour. Additionally, we introduce a data augmentation pipeline to improve model robustness. Evaluated on the CICIoT2023 dataset, our method outperforms existing models in both accuracy and earliness. We further demonstrate its real-time feasibility on resource-constrained IoT devices, achieving low-latency inference and minimal memory footprint.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Autonomous Low Power IoT System Architecture for Cybersecurity Monitoring,"Network security morning (NSM) is essential for any cybersecurity system, where the average cost of a cyber attack is 1.1 million. No matter how secure a system, it will eventually fail without proper and continuous monitoring. No wonder that the cybersecurity market is expected to grow up to $170.4 billion in 2022. However, the majority of legacy industries do not invest in NSM implementation until it is too late due to the initial and operation costs and static unutilized resources. Thus, this paper proposes a novel dynamic Internet of things (IoT) architecture for an industrial NSM that features a low installation and operation cost, low power consumption, intelligent organization behavior, and environmentally friendly operation. As a case study, the system is implemented in a mid-range oil a gas manufacturing facility in the southern states with more than 300 machines and servers over three remote locations and a production plant that features a challenging atmosphere condition. The proposed system successfully shows a significant saving (>65%) in power consumption, acquires one-tenth of the installation cost, develops an intelligent operation expert system tool as well as saves the environment from more than 500mg of CO2 pollution per hour, promoting green IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Optimized IoT Intrusion Detection using Machine Learning Technique,"An application of software known as an Intrusion Detection System (IDS) employs machine algorithms to identify network intrusions. Selective logging, safeguarding privacy, reputation-based defense against numerous attacks, and dynamic response to threats are a few of the problems that intrusion identification is used to solve. The biological system known as IoT has seen a rapid increase in high dimensionality and information traffic. Self-protective mechanisms like intrusion detection systems (IDSs) are essential for defending against a variety of attacks. On the other hand, the functional and physical diversity of IoT IDS systems causes significant issues. These attributes make it troublesome and unrealistic to completely use all IoT elements and properties for IDS self-security. For peculiarity-based IDS, this study proposes and implements a novel component selection and extraction strategy (our strategy). A five-ML algorithm model-based IDS for machine learning-based networks with proper hyperparamater tuning is presented in this paper by examining how the most popular feature selection methods and classifiers are combined, such as K-Nearest Neighbors (KNN) Classifier, Decision Tree (DT) Classifier, Random Forest (RF) Classifier, Gradient Boosting Classifier, and Ada Boost Classifier. The Random Forest (RF) classifier had the highest accuracy of 99.39%. The K-Nearest Neighbor (KNN) classifier exhibited the lowest performance among the evaluated models, achieving an accuracy of 94.84%. This study's models have a significantly higher performance rate than those used in previous studies, indicating that they are more reliable.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Poster Abstract: Towards Scalable and Trustworthy Decentralized Collaborative Intrusion Detection System for IoT,"An Intrusion Detection System (IDS) aims to alert users of incoming attacks by deploying a detector that monitors network traffic continuously. As an effort to increase detection capabilities, a set of independent IDS detectors typically work collaboratively to build intelligence of holistic network representation, which is referred to as Collaborative Intrusion Detection System (CIDS). However, developing an effective CIDS, particularly for the IoT ecosystem raises several challenges. Recent trends and advances in blockchain technology, which provides assurance in distributed trust and secure immutable storage, may contribute towards the design of effective CIDS. In this poster abstract, we present our ongoing work on a decentralized CIDS for IoT, which is based on blockchain technology. We propose an architecture that provides accountable trust establishment, which promotes incentives and penalties, and scalable intrusion information storage by exchanging bloom filters. We are currently implementing a proof-of-concept of our modular architecture in a local test-bed and evaluate its effectiveness in detecting common attacks in IoT networks and the associated overhead.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Federated Deep Learning for Intrusion Detection in IoT Networks,"The vast increase of Internet of Things (IoT) technologies and the ever-evolving attack vectors have increased cyber-security risks dramatically. A common approach to implementing AI-based Intrusion Detection systems (IDSs) in distributed IoT systems is in a centralised manner. However, this approach may violate data privacy and prohibit IDS scalability. Therefore, intrusion detection solutions in IoT ecosystems need to move towards a decentralised direction. Federated Learning (FL) has attracted significant interest in recent years due to its ability to perform collaborative learning while preserving data confidentiality and locality. Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic data distribution conditions. To that end, we design an experiment representative of the real world and evaluate the performance of an FL-based IDS. For our experiments, we rely on TON-IoT, a realistic IoT network traffic dataset, associating each IP address with a single FL client. Additionally, we explore pre-training and investigate various aggregation methods to mitigate the impact of data heterogeneity. Lastly, we benchmark our approach against a centralised solution. The comparison shows that the heterogeneous nature of the data has a considerable negative impact on the model's performance when trained in a distributed manner. However, in the case of a pre-trained initial global FL model, we demonstrate a performance improvement of over 20% (F1-score) compared to a randomly initiated global model.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Phishsense-1B: A Technical Perspective on an AI-Powered Phishing Detection Model,"Phishing is a persistent cybersecurity threat in today's digital landscape. This paper introduces Phishsense-1B, a refined version of the Llama-Guard-3-1B model, specifically tailored for phishing detection and reasoning. This adaptation utilizes Low-Rank Adaptation (LoRA) and the GuardReasoner finetuning methodology. We outline our LoRA-based fine-tuning process, describe the balanced dataset comprising phishing and benign emails, and highlight significant performance improvements over the original model. Our findings indicate that Phishsense-1B achieves an impressive 97.5% accuracy on a custom dataset and maintains strong performance with 70% accuracy on a challenging real-world dataset. This performance notably surpasses both unadapted models and BERT-based detectors. Additionally, we examine current state-of-the-art detection methods, compare prompt-engineering with fine-tuning strategies, and explore potential deployment scenarios.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Malware Detection in IOT Systems Using Machine Learning Techniques,"Malware detection in IoT environments necessitates robust methodologies. This study introduces a CNN-LSTM hybrid model for IoT malware identification and evaluates its performance against established methods. Leveraging K-fold cross-validation, the proposed approach achieved 95.5% accuracy, surpassing existing methods. The CNN algorithm enabled superior learning model construction, and the LSTM classifier exhibited heightened accuracy in classification. Comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model, highlighting its potential for enhancing IoT security. The study advocates for future exploration of SVMs as alternatives, emphasizes the need for distributed detection strategies, and underscores the importance of predictive analyses for a more powerful IOT security. This research serves as a platform for developing more resilient security measures in IoT ecosystems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Effective Intrusion Detection in Highly Imbalanced IoT Networks with Lightweight S2CGAN-IDS,"Since the advent of the Internet of Things (IoT), exchanging vast amounts of information has increased the number of security threats in networks. As a result, intrusion detection based on deep learning (DL) has been developed to achieve high throughput and high precision. Unlike general deep learning-based scenarios, IoT networks contain benign traffic far more than abnormal traffic, with some rare attacks. However, most existing studies have been focused on sacrificing the detection rate of the majority class in order to improve the detection rate of the minority class in class-imbalanced IoT networks. Although this way can reduce the false negative rate of minority classes, it both wastes resources and reduces the credibility of the intrusion detection systems. To address this issue, we propose a lightweight framework named S2CGAN-IDS. The proposed framework leverages the distribution characteristics of network traffic to expand the number of minority categories in both data space and feature space, resulting in a substantial increase in the detection rate of minority categories while simultaneously ensuring the detection precision of majority categories. To reduce the impact of sparsity on the experiments, the CICIDS2017 numeric dataset is utilized to demonstrate the effectiveness of the proposed method. The experimental results indicate that our proposed approach outperforms the superior method in both Precision and Recall, particularly with a 10.2% improvement in the F1-score.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
NURSE: eNd-UseR IoT malware detection tool for Smart homEs,"Traditional techniques to detect malware infections were not meant to be used by the end-user and current malware removal tools and security software cannot handle the heterogeneity of IoT devices. In this paper, we design, develop and evaluate a tool, called NURSE, to fill this information gap, i.e., enabling end-users to detect IoT-malware infections in their home networks. NURSE follows a modular approach to analyze IoT traffic as captured by means of an ARP spoofing technique which does not require any network modification or specific hardware. Thus, NURSE provides zero-configuration IoT traffic analysis within everybody's reach. After testing NURSE in 83 different IoT network scenarios with a wide variety of IoT device types, results show that NURSE identifies malware-infected IoT devices with high accuracy (86.7%) using device network behavior and contacted destinations.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection,"The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empirical results prominently underscore the framework's efficacy, boasting an impressive 88\% detection rate for previously unseen attacks when compared against existing approaches and recent advancements. Future work will perform extensive experimentation across various openness levels and attack scenarios, further strengthening the adaptability and performance of our proposed solution in safeguarding IoT environments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"A Review of Performance, Energy and Privacy of Intrusion Detection Systems for IoT","Internet of Things (IoT) is a disruptive technology with applications across diverse domains such as transportation and logistics systems, smart grids, smart homes, connected vehicles, and smart cities. Alongside the growth of these infrastructures, the volume and variety of attacks on these infrastructures has increased highlighting the significance of distinct protection mechanisms. Intrusion detection is one of the distinguished protection mechanisms with notable recent efforts made to establish effective intrusion detection for IoT and IoV. However, unique characteristics of such infrastructures including battery power, bandwidth and processors overheads, and the network dynamics can influence the operation of an intrusion detection system. This paper presents a comprehensive study of existing intrusion detection systems for IoT systems including emerging systems such as Internet of Vehicles (IoV). The paper analyzes existing systems in three aspects: computational overhead, energy consumption and privacy implications. Based on a rigorous analysis of the existing intrusion detection approaches, the paper also identifies open challenges for an effective and collaborative design of intrusion detection system for resource-constrained IoT system in general and its applications such as IoV. These efforts are envisaged to highlight state of the art with respect to intrusion detection for IoT and open challenges requiring specific efforts to achieve efficient intrusion detection within these systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Lightweight Classification of IoT Malware based on Image Recognition,"The Internet of Things (IoT) is an extension of the traditional Internet, which allows a very large number of smart devices, such as home appliances, network cameras, sensors and controllers to connect to one another to share information and improve user experiences. Current IoT devices are typically micro-computers for domain-specific computations rather than traditional functionspecific embedded devices. Therefore, many existing attacks, targeted at traditional computers connected to the Internet, may also be directed at IoT devices. For example, DDoS attacks have become very common in IoT environments, as these environments currently lack basic security monitoring and protection mechanisms, as shown by the recent Mirai and Brickerbot IoT botnets. In this paper, we propose a novel light-weight approach for detecting DDos malware in IoT environments.We firstly extract one-channel gray-scale images converted from binaries, and then utilize a lightweight convolutional neural network for classifying IoT malware families. The experimental results show that the proposed system can achieve 94.0% accuracy for the classification of goodware and DDoS malware, and 81.8% accuracy for the classification of goodware and two main malware families.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Malware Squid: A Novel IoT Malware Traffic Analysis Framework using Convolutional Neural Network and Binary Visualisation,"Internet of Things devices have seen a rapid growth and popularity in recent years with many more ordinary devices gaining network capability and becoming part of the ever growing IoT network. With this exponential growth and the limitation of resources, it is becoming increasingly harder to protect against security threats such as malware due to its evolving faster than the defence mechanisms can handle with. The traditional security systems are not able to detect unknown malware as they use signature-based methods. In this paper, we aim to address this issue by introducing a novel IoT malware traffic analysis approach using neural network and binary visualisation. The prime motivation of the proposed approach is to faster detect and classify new malware (zero-day malware). The experiment results show that our method can satisfy the accuracy requirement of practical application.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Intrusion Detection using Network Traffic Profiling and Machine Learning for IoT,"The rapid increase in the use of IoT devices brings many benefits to the digital society, ranging from improved efficiency to higher productivity. However, the limited resources and the open nature of these devices make them vulnerable to various cyber threats. A single compromised device can have an impact on the whole network and lead to major security and physical damages. This paper explores the potential of using network profiling and machine learning to secure IoT against cyber-attacks. The proposed anomaly-based intrusion detection solution dynamically and actively profiles and monitors all networked devices for the detection of IoT device tampering attempts as well as suspicious network transactions. Any deviation from the defined profile is considered to be an attack and is subject to further analysis. Raw traffic is also passed on to the machine learning classifier for examination and identification of potential attacks. Performance assessment of the proposed methodology is conducted on the Cyber-Trust testbed using normal and malicious network traffic. The experimental results show that the proposed anomaly detection system delivers promising results with an overall accuracy of 98.35% and 0.98% of false-positive alarms.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Feature-Oriented IoT Malware Analysis: Extraction, Classification, and Future Directions","As IoT devices continue to proliferate, their reliability is increasingly constrained by security concerns. In response, researchers have developed diverse malware analysis techniques to detect and classify IoT malware. These techniques typically rely on extracting features at different levels from IoT applications, giving rise to a wide range of feature extraction methods. However, current approaches still face significant challenges when applied in practice. This survey provides a comprehensive review of feature extraction techniques for IoT malware analysis from multiple perspectives. We first examine static and dynamic feature extraction methods, followed by hybrid approaches. We then explore feature representation strategies based on graph learning. Finally, we compare the strengths and limitations of existing techniques, highlight open challenges, and outline promising directions for future research.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Secure Back-up and Restore for Resource-Constrained IoT based on Nanotechnology,"With the emergence of IoT (Internet of things), huge amounts of sensitive data are being processed and transmitted everyday in edge devices with little to no security. Due to their aggressive power management schemes, it is a common and necessary technique to make a back-up of their program states and other necessary data in a non-volatile memory (NVM) before going to sleep or low power mode. However, this memory is often left unprotected as adding robust security measures tends to be expensive for these resource constrained systems. In this paper, we propose a lightweight security system for NVM during low power mode. This security architecture uses the memristor, an emerging nanoscale device which is used to build hardware security primitives like PUF (physical unclonable function) based encryption-decryption, true random number generators (TRNG), and memory integrity checking. A reliability enhancement technique for this PUF is also proposed which shows how this system would work even with less-than-100\% reliable PUF responses. Together, with all these techniques, we have established a dual layer security protocol (data encryption+integrity check) which provides reasonable security to an embedded processor while being very lightweight in terms of area, power, and computation time. A complete system design is demonstrated with 65$n$m CMOS and emerging memristive technology. With this, we have provided a detailed and accurate estimation of resource overhead. Analysis of the security of the whole system is also provided.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
BLEND: Efficient and blended IoT data storage and communication with application layer security,"Many IoT use cases demand both secure storage and secure communication. Resource-constrained devices cannot afford having one set of crypto protocols for storage and another for communication. Lightweight application layer security standards are being developed for IoT communication. Extending these protocols for secure storage can significantly reduce communication latency and local processing.   We present BLEND, combining secure storage and communication by storing IoT data as pre-computed encrypted network packets. Unlike local methods, BLEND not only eliminates separate crypto for secure storage needs, but also eliminates a need for real-time crypto operations, reducing the communication latency significantly. Our evaluation shows that compared with a local solution, BLEND reduces send latency from 630 microseconds to 110 microseconds per packet. BLEND enables PKI based key management while being sufficiently lightweight for IoT. BLEND doesn't need modifications to communication standards used when extended for secure storage, and can therefore preserve underlying protocols' security guarantees.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Binary and Multi-Class Intrusion Detection in IoT Using Standalone and Hybrid Machine and Deep Learning Models,"Maintaining security in IoT systems depends on intrusion detection since these networks' sensitivity to cyber-attacks is growing. Based on the IoT23 dataset, this study explores the use of several Machine Learning (ML) and Deep Learning (DL) along with the hybrid models for binary and multi-class intrusion detection. The standalone machine and deep learning models like Random Forest (RF), Extreme Gradient Boosting (XGBoost), Artificial Neural Network (ANN), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Convolutional Neural Network (CNN) were used. Furthermore, two hybrid models were created by combining machine learning techniques: RF, XGBoost, AdaBoost, KNN, and SVM and these hybrid models were voting based hybrid classifier. Where one is for binary, and the other one is for multi-class classification. These models vi were tested using precision, recall, accuracy, and F1-score criteria and compared the performance of each model. This work thoroughly explains how hybrid, standalone ML and DL techniques could improve IDS (Intrusion Detection System) in terms of accuracy and scalability in IoT (Internet of Things).","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Evaluation of an Anomaly Detector for Routers using Parameterizable Malware in an IoT Ecosystem,"This work explores the evaluation of a machine learning anomaly detector using custom-made parameterizable malware in an Internet of Things (IoT) Ecosystem. It is assumed that the malware has infected, and resides on, the Linux router that serves other devices on the network, as depicted in Figure 1. This IoT Ecosystem was developed as a testbed to evaluate the efficacy of a behavior-based anomaly detector. The malware consists of three types of custom-made malware: ransomware, cryptominer, and keylogger, which all have exfiltration capabilities to the network. The parameterization of the malware gives the malware samples multiple degrees of freedom, specifically relating to the rate and size of data exfiltration. The anomaly detector uses feature sets crafted from system calls and network traffic, and uses a Support Vector Machine (SVM) for behavioral-based anomaly detection. The custom-made malware is used to evaluate the situations where the SVM is effective, as well as the situations where it is not effective.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An Adaptive End-to-End IoT Security Framework Using Explainable AI and LLMs,"The exponential growth of the Internet of Things (IoT) has significantly increased the complexity and volume of cybersecurity threats, necessitating the development of advanced, scalable, and interpretable security frameworks. This paper presents an innovative, comprehensive framework for real-time IoT attack detection and response that leverages Machine Learning (ML), Explainable AI (XAI), and Large Language Models (LLM). By integrating XAI techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) with a model-independent architecture, we ensure our framework's adaptability across various ML algorithms. Additionally, the incorporation of LLMs enhances the interpretability and accessibility of detection decisions, providing system administrators with actionable, human-understandable explanations of detected threats. Our end-to-end framework not only facilitates a seamless transition from model development to deployment but also represents a real-world application capability that is often lacking in existing research. Based on our experiments with the CIC-IOT-2023 dataset \cite{neto2023ciciot2023}, Gemini and OPENAI LLMS demonstrate unique strengths in attack mitigation: Gemini offers precise, focused strategies, while OPENAI provides extensive, in-depth security measures. Incorporating SHAP and LIME algorithms within XAI provides comprehensive insights into attack detection, emphasizing opportunities for model improvement through detailed feature analysis, fine-tuning, and the adaptation of misclassifications to enhance accuracy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
SETTI: A Self-supervised Adversarial Malware Detection Architecture in an IoT Environment,"In recent years, malware detection has become an active research topic in the area of Internet of Things (IoT) security. The principle is to exploit knowledge from large quantities of continuously generated malware. Existing algorithms practice available malware features for IoT devices and lack real-time prediction behaviors. More research is thus required on malware detection to cope with real-time misclassification of the input IoT data. Motivated by this, in this paper we propose an adversarial self-supervised architecture for detecting malware in IoT networks, SETTI, considering samples of IoT network traffic that may not be labeled. In the SETTI architecture, we design three self-supervised attack techniques, namely Self-MDS, GSelf-MDS and ASelf-MDS. The Self-MDS method considers the IoT input data and the adversarial sample generation in real-time. The GSelf-MDS builds a generative adversarial network model to generate adversarial samples in the self-supervised structure. Finally, ASelf-MDS utilizes three well-known perturbation sample techniques to develop adversarial malware and inject it over the self-supervised architecture. Also, we apply a defence method to mitigate these attacks, namely adversarial self-supervised training to protect the malware detection architecture against injecting the malicious samples. To validate the attack and defence algorithms, we conduct experiments on two recent IoT datasets: IoT23 and NBIoT. Comparison of the results shows that in the IoT23 dataset, the Self-MDS method has the most damaging consequences from the attacker's point of view by reducing the accuracy rate from 98% to 74%. In the NBIoT dataset, the ASelf-MDS method is the most devastating algorithm that can plunge the accuracy rate from 98% to 77%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting,"The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Autonomous Cybersecurity: An Intelligent AutoML Framework for Autonomous Intrusion Detection,"The rapid evolution of mobile networks from 5G to 6G has necessitated the development of autonomous network management systems, such as Zero-Touch Networks (ZTNs). However, the increased complexity and automation of these networks have also escalated cybersecurity risks. Existing Intrusion Detection Systems (IDSs) leveraging traditional Machine Learning (ML) techniques have shown effectiveness in mitigating these risks, but they often require extensive manual effort and expert knowledge. To address these challenges, this paper proposes an Automated Machine Learning (AutoML)-based autonomous IDS framework towards achieving autonomous cybersecurity for next-generation networks. To achieve autonomous intrusion detection, the proposed AutoML framework automates all critical procedures of the data analytics pipeline, including data pre-processing, feature engineering, model selection, hyperparameter tuning, and model ensemble. Specifically, it utilizes a Tabular Variational Auto-Encoder (TVAE) method for automated data balancing, tree-based ML models for automated feature selection and base model learning, Bayesian Optimization (BO) for hyperparameter optimization, and a novel Optimized Confidence-based Stacking Ensemble (OCSE) method for automated model ensemble. The proposed AutoML-based IDS was evaluated on two public benchmark network security datasets, CICIDS2017 and 5G-NIDD, and demonstrated improved performance compared to state-of-the-art cybersecurity methods. This research marks a significant step towards fully autonomous cybersecurity in next-generation networks, potentially revolutionizing network security applications.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT,"In critical IoT environments, such as smart homes and industrial systems, effective Intrusion Detection Systems (IDS) are essential for ensuring security. However, developing robust IDS solutions remains a significant challenge. Traditional machine learning-based IDS models typically require large datasets, but data sharing is often limited due to privacy and security concerns. Federated Learning (FL) presents a promising alternative by enabling collaborative model training without sharing raw data. Despite its advantages, FL still faces key challenges, such as data heterogeneity (non-IID data) and high energy and computation costs, particularly for resource constrained IoT devices. To address these issues, this paper proposes OptiFLIDS, a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption. It also incorporates a customized aggregation method to better handle pruned models that differ due to non-IID data distributions. Experiments conducted on three recent IoT IDS datasets, TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong detection performance while improving energy efficiency, making it well-suited for deployment in real-world IoT environments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices,"The rapid expansion of the Internet of Things (IoT) has intensified security challenges, notably from Distributed Denial of Service (DDoS) attacks launched by compromised, resource-constrained devices. Traditional defenses are often ill-suited for the IoT paradigm, creating a need for lightweight, high-performance, edge-based solutions. This paper presents the design, implementation, and evaluation of an IoT security framework that leverages the extended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for in-kernel mitigation of DDoS attacks. The system uses a rate-based detection algorithm to identify and block malicious traffic at the earliest stage of the network stack. The framework is evaluated using both Docker-based simulations and real-world deployment on a Raspberry Pi 4, showing over 97% mitigation effectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected, and system stability is preserved even under attack. These results confirm that eBPF/XDP provides a viable and highly efficient solution for hardening IoT edge devices against volumetric network attacks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT,"The novel Internet of Things (IoT) paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications, increasing systems' complexity, and the need for reliability and autonomy. In this context, both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered. Although machine-based Service Level Agreements (SLA) have been recently leveraged to establish and share policies in Cloud-based scenarios, and also in the IoT context, the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task. To tackle this problem, we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in IoT, according to the user needs. Through the use of a Reinforcement Learning based solution, a user agent, inside the environment, is trained to choose the best smart objects granting access to the target services. Moreover, the solution is designed to guarantee deadline requirements and user security and privacy needs. Finally, to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network,"The exponential expansion of IoT and 5G-Advanced applications has enlarged the attack surface for DDoS, malware, and zero-day intrusions. We propose an intrusion detection system that fuses a convolutional neural network (CNN), a bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch captures local and gated cross-feature interactions, while the AE emphasizes reconstruction-based anomaly sensitivity. Training occurs across edge devices without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC 99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced error rates with high precision and recall. Average inference time is approximately 0.0476 ms per sample on our test hardware, which is well within the less than 10 ms URLLC budget, supporting edge deployment. We also discuss explainability, drift tolerance, and FL considerations for compliant, scalable 5G-Advanced IoT security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Phishing - A Growing Threat to E-Commerce,"In today's business environment, it is difficult to imagine a workplace without access to the web, yet a variety of email born viruses, spyware, adware, Trojan horses, phishing attacks, directory harvest attacks, DoS attacks, and other threats combine to attack businesses and customers. This paper is an attempt to review phishing - a constantly growing and evolving threat to Internet based commercial transactions. Various phishing approaches that include vishing, spear phishng, pharming, keyloggers, malware, web Trojans, and others will be discussed. This paper also highlights the latest phishing analysis made by Anti-Phishing Working Group (APWG) and Korean Internet Security Center.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IMCDCF: An Incremental Malware Detection Approach Using Hidden Markov Models,"The popularity of dynamic malware analysis has grown significantly, as it enables analysts to observe the behavior of executing samples, thereby enhancing malware detection and classification decisions. With the continuous increase in new malware variants, there is an urgent need for an automated malware analysis engine capable of accurately identifying malware samples. In this paper, we provide a brief overview of malware detection and classification methodologies. Moreover, we introduce a novel framework tailored for the dynamic analysis environment, called the Incremental Malware Detection and Classification Framework (IMDCF). IMDCF offers a comprehensive solution for general-purpose malware detection and classification, achieving an accuracy rate of 96.49% while maintaining a simple architecture.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing IoT Apps with Fine-grained Control of Information Flows,"Internet of Things is growing rapidly, with many connected devices now available to consumers. With this growth, the IoT apps that manage the devices from smartphones raise significant security concerns. Typically, these apps are secured via sensitive credentials such as email and password that need to be validated through specific servers, thus requiring permissions to access the Internet. Unfortunately, even when developers are well-intentioned, such apps can be non-trivial to secure so as to guarantee that user's credentials do not leak to unauthorized servers on the Internet. For example, if the app relies on third-party libraries, as many do, those libraries can potentially capture and leak sensitive credentials. Bugs in the applications can also result in exploitable vulnerabilities that leak credentials. This paper presents our work in-progress on a prototype that enables developers to control how information flows within the app from sensitive UI data to specific servers. We extend FlowFence to enforce fine-grained information flow policies on sensitive UI data.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet of Vehicles,"Modern vehicles, including connected vehicles and autonomous vehicles, nowadays involve many electronic control units connected through intra-vehicle networks to implement various functionalities and perform actions. Modern vehicles are also connected to external networks through vehicle-to-everything technologies, enabling their communications with other vehicles, infrastructures, and smart devices. However, the improving functionality and connectivity of modern vehicles also increase their vulnerabilities to cyber-attacks targeting both intra-vehicle and external networks due to the large attack surfaces. To secure vehicular networks, many researchers have focused on developing intrusion detection systems (IDSs) that capitalize on machine learning methods to detect malicious cyber-attacks. In this paper, the vulnerabilities of intra-vehicle and external networks are discussed, and a multi-tiered hybrid IDS that incorporates a signature-based IDS and an anomaly-based IDS is proposed to detect both known and unknown attacks on vehicular networks. Experimental results illustrate that the proposed system can detect various types of known attacks with 99.99% accuracy on the CAN-intrusion-dataset representing the intra-vehicle network data and 99.88% accuracy on the CICIDS2017 dataset illustrating the external vehicular network data. For the zero-day attack detection, the proposed system achieves high F1-scores of 0.963 and 0.800 on the above two datasets, respectively. The average processing time of each data packet on a vehicle-level machine is less than 0.6 ms, which shows the feasibility of implementing the proposed system in real-time vehicle systems. This emphasizes the effectiveness and efficiency of the proposed IDS.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Deep Image: A precious image based deep learning method for online malware detection in IoT Environment,"The volume of malware and the number of attacks in IoT devices are rising everyday, which encourages security professionals to continually enhance their malware analysis tools. Researchers in the field of cyber security have extensively explored the usage of sophisticated analytics and the efficiency of malware detection. With the introduction of new malware kinds and attack routes, security experts confront considerable challenges in developing efficient malware detection and analysis solutions. In this paper, a different view of malware analysis is considered and the risk level of each sample feature is computed, and based on that the risk level of that sample is calculated. In this way, a criterion is introduced that is used together with accuracy and FPR criteria for malware analysis in IoT environment. In this paper, three malware detection methods based on visualization techniques called the clustering approach, the probabilistic approach, and the deep learning approach are proposed. Then, in addition to the usual machine learning criteria namely accuracy and FPR, a proposed criterion based on the risk of samples has also been used for comparison, with the results showing that the deep learning approach performed better in detecting malware","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Can Feature Engineering Help Quantum Machine Learning for Malware Detection?,"With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An accurate IoT Intrusion Detection Framework using Apache Spark,"The internet has caused tremendous changes since its appearance in the 1980s, and now, the Internet of Things (IoT) seems to be doing the same. The potential of IoT has made it the center of attention for many people, but, where some see an opportunity to contribute, others may see IoT networks as a target to be exploited. The high number of IoT devices makes them the perfect setup for staging denial-of-service attacks (DoS) that can have devastating consequences. This renders the need for cybersecurity measures such as intrusion detection systems (IDSs) evident. The aim of this paper is to build an IDS using the big data platform, Apache Spark. Apache Spark was used along with its ML library (MLlib) and the BoT-IoT dataset. The IDS was then tested and evaluated based on F-Measure (f1), as was the standard when evaluating imbalanced data. Two rounds of tests were performed, a partial dataset for minimizing bias, and the full BoT-IoT dataset for exploring big data and ML capabilities in a security setting. For the partial dataset, the Random Forest algorithm had the highest performance for binary classification at an average f1 measure of 99.7%, as well as 99.6% for main category classification, and an 88.5% f1 measure for sub category classification. As for the complete dataset, the Decision Tree algorithm scored the highest f1 measures for all conducted tests; 97.9% for binary classification, 79% for main category classification, and 77% for sub category classification.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
LEMDA: A Novel Feature Engineering Method for Intrusion Detection in IoT Systems,"Intrusion detection systems (IDS) for the Internet of Things (IoT) systems can use AI-based models to ensure secure communications. IoT systems tend to have many connected devices producing massive amounts of data with high dimensionality, which requires complex models. Complex models have notorious problems such as overfitting, low interpretability, and high computational complexity. Adding model complexity penalty (i.e., regularization) can ease overfitting, but it barely helps interpretability and computational efficiency. Feature engineering can solve these issues; hence, it has become critical for IDS in large-scale IoT systems to reduce the size and dimensionality of data, resulting in less complex models with excellent performance, smaller data storage, and fast detection. This paper proposes a new feature engineering method called LEMDA (Light feature Engineering based on the Mean Decrease in Accuracy). LEMDA applies exponential decay and an optional sensitivity factor to select and create the most informative features. The proposed method has been evaluated and compared to other feature engineering methods using three IoT datasets and four AI/ML models. The results show that LEMDA improves the F1 score performance of all the IDS models by an average of 34% and reduces the average training and detection times in most cases.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Anti-Phishing Training (Still) Does Not Work: A Large-Scale Reproduction of Phishing Training Inefficacy Grounded in the NIST Phish Scale,"Social engineering attacks delivered via email, commonly known as phishing, represent a persistent cybersecurity threat leading to significant organizational incidents and data breaches. Although many organizations train employees on phishing, often mandated by compliance requirements, the real-world effectiveness of this training remains debated. To contribute to evidence-based cybersecurity policy, we conducted a large-scale reproduction study (N = 12,511) at a US-based financial technology firm. Our experimental design refined prior work by comparing training modalities in operational environments, validating NIST's standardized phishing difficulty measurement, and introducing novel organizational-level temporal resilience metrics. Echoing prior work, training interventions showed no significant main effects on click rates (p=0.450) or reporting rates (p=0.417), with negligible effect sizes. However, we found that the NIST Phish Scale predicted user behavior, with click rates increasing from 7.0% for easy lures to 15.0% for hard lures. Our organizational-level resilience result was mixed: 36-55% of campaigns achieved ""inoculation"" patterns where reports preceded clicks, but training did not significantly improve organizational-level temporal protection. In summary, our results confirm the ineffectiveness of current phishing training approaches while offering a refined study design for future work.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Novel Dimension Reduction Scheme for Intrusion Detection Systems in IoT Environments,"Internet of Things (IoT) brings new challenges to the security solutions of computer networks. So far, intrusion detection system (IDS) is one of the effective security tools, but the vast amount of data that is generated by heterogeneous protocols and ""things"" alongside the constrained resources of the hosts, make some of the present IDS schemes defeated. To grant IDSs the ability of working in the IoT environments, in this paper, we propose a new distributed dimension reduction scheme which addresses the limited resources challenge. A novel autoencoder (AE) designed, and it learns to generate a latent space. Then, the constrained hosts/probes use the generated weights to lower the dimension with a single operation. The compressed data is transferred to a central IDS server to verify the traffic type. This scheme aims to lower the needed bandwidth to transfer data by compressing it and also reduce the overhead of the compression task in the hosts. The proposed scheme is evaluated on three well-known network traffic datasets (UNSW-NB15, TON\_IoT20 and NSL-KDD), and the results show that we can have a 3-dimensional latent space (about 90\% compression) without any remarkable fall in IDS detection accuracy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Discovering and Understanding the Security Hazards in the Interactions between IoT Devices, Mobile Apps, and Clouds on Smart Home Platforms","A smart home connects tens of home devices to the Internet, where an IoT cloud runs various home automation applications. While bringing unprecedented convenience and accessibility, it also introduces various security hazards to users. Prior research studied smart home security from several aspects. However, we found that the complexity of the interactions among the participating entities (i.e., devices, IoT clouds, and mobile apps) has not yet been systematically investigated. In this work, we conducted an in-depth analysis of five widely-used smart home platforms. Combining firmware analysis, network traffic interception, and blackbox testing, we reverse-engineered the details of the interactions among the participating entities. Based on the details, we inferred three legitimate state transition diagrams for the three entities, respectively. Using these state machines as a reference model, we identified a set of unexpected state transitions. To confirm and trigger the unexpected state transitions, we implemented a set of phantom devices to mimic a real device. By instructing the phantom devices to intervene in the normal entity-entity interactions, we have discovered several new vulnerabilities and a spectrum of attacks against real-world smart home platforms.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Digital Twins and Blockchain for IoT Management,"Security and privacy are primary concerns in IoT management. Security breaches in IoT resources, such as smart sensors, can leak sensitive data and compromise the privacy of individuals. Effective IoT management requires a comprehensive approach to prioritize access security and data privacy protection. Digital twins create virtual representations of IoT resources. Blockchain adds decentralization, transparency, and reliability to IoT systems. This research integrates digital twins and blockchain to manage access to IoT data streaming. Digital twins are used to encapsulate data access and view configurations. Access is enabled on digital twins, not on IoT resources directly. Trust structures programmed as smart contracts are the ones that manage access to digital twins. Consequently, IoT resources are not exposed to third parties, and access security breaches can be prevented. Blockchain has been used to validate digital twins and store their configuration. The research presented in this paper enables multitenant access and customization of data streaming views and abstracts the complexity of data access management. This approach provides access and configuration security and data privacy protection.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Open Set Dandelion Network for IoT Intrusion Detection,"As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based target membership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-the-art baseline methods by 16.9%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Dependable Intrusion Detection System for IoT: A Deep Transfer Learning-based Approach,"Security concerns for IoT applications have been alarming because of their widespread use in different enterprise systems. The potential threats to these applications are constantly emerging and changing, and therefore, sophisticated and dependable defense solutions are necessary against such threats. With the rapid development of IoT networks and evolving threat types, the traditional machine learning-based IDS must update to cope with the security requirements of the current sustainable IoT environment. In recent years, deep learning, and deep transfer learning have progressed and experienced great success in different fields and have emerged as a potential solution for dependable network intrusion detection. However, new and emerging challenges have arisen related to the accuracy, efficiency, scalability, and dependability of the traditional IDS in a heterogeneous IoT setup. This manuscript proposes a deep transfer learning-based dependable IDS model that outperforms several existing approaches. The unique contributions include effective attribute selection, which is best suited to identify normal and attack scenarios for a small amount of labeled data, designing a dependable deep transfer learning-based ResNet model, and evaluating considering real-world data. To this end, a comprehensive experimental performance evaluation has been conducted. Extensive analysis and performance evaluation show that the proposed model is robust, more efficient, and has demonstrated better performance, ensuring dependability.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks,"The increasing use of Internet of Things (IoT) devices has led to a rise in security related concerns regarding IoT Networks. The surveillance cameras in IoT networks are vulnerable to security threats such as brute force and zero-day attacks which can lead to unauthorized access by hackers and potential spying on the users activities. Moreover, these cameras can be targeted by Denial of Service (DOS) attacks, which will make it unavailable for the user. The proposed AI based framework will leverage machine learning algorithms to analyze network traffic and detect anomalous behavior, allowing for quick detection and response to potential intrusions. The framework will be trained and evaluated using real-world datasets to learn from past security incidents and improve its ability to detect potential intrusion.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Malware Detection Architecture using a Novel Channel Boosted and Squeezed CNN,"Interaction between devices, people, and the Internet has given birth to a new digital communication model, the Internet of Things (IoT). The seamless network of these smart devices is the core of this IoT model. However, on the other hand, integrating smart devices to constitute a network introduces many security challenges. These connected devices have created a security blind spot, where cybercriminals can easily launch an attack to compromise the devices using malware proliferation techniques. Therefore, malware detection is considered a lifeline for the survival of IoT devices against cyberattacks. This study proposes a novel IoT Malware Detection Architecture (iMDA) using squeezing and boosting dilated convolutional neural network (CNN). The proposed architecture exploits the concepts of edge and smoothing, multi-path dilated convolutional operations, channel squeezing, and boosting in CNN. Edge and smoothing operations are employed with split-transform-merge (STM) blocks to extract local structure and minor contrast variation in the malware images. STM blocks performed multi-path dilated convolutional operations, which helped recognize the global structure of malware patterns. Additionally, channel squeezing and merging helped to get the prominent reduced and diverse feature maps, respectively. Channel squeezing and boosting are applied with the help of STM block at the initial, middle and final levels to capture the texture variation along with the depth for the sake of malware pattern hunting. The proposed architecture has shown substantial performance compared with the customized CNN models. The proposed iMDA has achieved Accuracy: 97.93%, F1-Score: 0.9394, Precision: 0.9864, MCC: 0. 8796, Recall: 0.8873, AUC-PR: 0.9689 and AUC-ROC: 0.9938.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Adversarial Realism and Robust Learning for IoT Intrusion Detection and Classification,"The Internet of Things (IoT) faces tremendous security challenges. Machine learning models can be used to tackle the growing number of cyber-attack variations targeting IoT systems, but the increasing threat posed by adversarial attacks restates the need for reliable defense strategies. This work describes the types of constraints required for a realistic adversarial cyber-attack example and proposes a methodology for a trustworthy adversarial robustness analysis with a realistic adversarial evasion attack vector. The proposed methodology was used to evaluate three supervised algorithms, Random Forest (RF), Extreme Gradient Boosting (XGB), and Light Gradient Boosting Machine (LGBM), and one unsupervised algorithm, Isolation Forest (IFOR). Constrained adversarial examples were generated with the Adaptative Perturbation Pattern Method (A2PM), and evasion attacks were performed against models created with regular and adversarial training. Even though RF was the least affected in binary classification, XGB consistently achieved the highest accuracy in multi-class classification. The obtained results evidence the inherent susceptibility of tree-based algorithms and ensembles to adversarial evasion attacks and demonstrates the benefits of adversarial training and a security by design approach for a more robust IoT network intrusion detection and cyber-attack classification.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Federated Learning for Intrusion Detection in IoT Security: A Hybrid Ensemble Approach,"Critical role of Internet of Things (IoT) in various domains like smart city, healthcare, supply chain and transportation has made them the target of malicious attacks. Past works in this area focused on centralized Intrusion Detection System (IDS), assuming the existence of a central entity to perform data analysis and identify threats. However, such IDS may not always be feasible, mainly due to spread of data across multiple sources and gathering at central node can be costly. Also, the earlier works primarily focused on improving True Positive Rate (TPR) and ignored the False Positive Rate (FPR), which is also essential to avoid unnecessary downtime of the systems. In this paper, we first present an architecture for IDS based on hybrid ensemble model, named PHEC, which gives improved performance compared to state-of-the-art architectures. We then adapt this model to a federated learning framework that performs local training and aggregates only the model parameters. Next, we propose Noise-Tolerant PHEC in centralized and federated settings to address the label-noise problem. The proposed idea uses classifiers using weighted convex surrogate loss functions. Natural robustness of KNN classifier towards noisy data is also used in the proposed architecture. Experimental results on four benchmark datasets drawn from various security attacks show that our model achieves high TPR while keeping FPR low on noisy and clean data. Further, they also demonstrate that the hybrid ensemble models achieve performance in federated settings close to that of the centralized settings.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Non-Intrusive Machine Learning Solution for Malware Detection and Data Theft Classification in Smartphones,"Smartphones contain information that is more sensitive and personal than those found on computers and laptops. With an increase in the versatility of smartphone functionality, more data has become vulnerable and exposed to attackers. Successful mobile malware attacks could steal a user's location, photos, or even banking information. Due to a lack of post-attack strategies firms also risk going out of business due to data theft. Thus, there is a need besides just detecting malware intrusion in smartphones but to also identify the data that has been stolen to assess, aid in recovery and prevent future attacks. In this paper, we propose an accessible, non-intrusive machine learning solution to not only detect malware intrusion but also identify the type of data stolen for any app under supervision. We do this with Android usage data obtained by utilising publicly available data collection framework- SherLock. We test the performance of our architecture for multiple users on real-world data collected using the same framework. Our architecture exhibits less than 9% inaccuracy in detecting malware and can classify with 83% certainty on the type of data that is being stolen.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Harnessing the Power of Decision Trees to Detect IoT Malware,"Due to its simple installation and connectivity, the Internet of Things (IoT) is susceptible to malware attacks. Being able to operate autonomously. As IoT devices have become more prevalent, they have become the most tempting targets for malware. Weak, guessable, or hard-coded passwords, and a lack of security measures contribute to these vulnerabilities along with insecure network connections and outdated update procedures. To understand IoT malware, current methods and analysis ,using static methods, are ineffective. The field of deep learning has made great strides in recent years due to their tremendous data mining, learning, and expression capabilities, cybersecurity has enjoyed tremendous growth in recent years. As a result, malware analysts will not have to spend as much time analyzing malware. In this paper, we propose a novel detection and analysis method that harnesses the power and simplicity of decision trees. The experiments are conducted using a real word dataset, MaleVis which is a publicly available dataset. Based on the results, we show that our proposed approach outperforms existing state-of-the-art solutions in that it achieves 97.23% precision and 95.89% recall in terms of detection and classification. A specificity of 96.58%, F1-score of 96.40%, an accuracy of 96.43.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Review of Intrusion Detection Systems and Their Evaluation in the IoT,"Intrusion Detection Systems (IDS) are key components for securing critical infrastructures, capable of detecting malicious activities on networks or hosts. The procedure of implementing a IDS for Internet of Things (IoT) networks is not without challenges due to the variability of these systems and specifically the difficulty in accessing data. The specifics of these very constrained devices render the design of an IDS capable of dealing with the varied attacks a very challenging problem and a very active research subject. In the current state of literature, a number of approaches have been proposed to improve the efficiency of intrusion detection, catering to some of these limitations, such as resource constraints and mobility. In this article, we review works on IDS specifically for these kinds of devices from 2008 to 2018, collecting a total of 51 different IDS papers. We summarise the current themes of the field, summarise the techniques employed to train and deploy the IDSs and provide a qualitative evaluations of these approaches. While these works provide valuable insights and solutions for sub-parts of these constraints, we discuss the limitations of these solutions as a whole, in particular what kinds of attacks these approaches struggle to detect and the setup limitations that are unique to this kind of system. We find that although several paper claim novelty of their approach little inter paper comparisons have been made, that there is a dire need for sharing of datasets and almost no shared code repositories, consequently raising the need for a thorough comparative evaluation.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles,"Modern vehicles, including autonomous vehicles and connected vehicles, are increasingly connected to the external world, which enables various functionalities and services. However, the improving connectivity also increases the attack surfaces of the Internet of Vehicles (IoV), causing its vulnerabilities to cyber-threats. Due to the lack of authentication and encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs) are essential approaches to protect modern vehicle systems from network attacks. In this paper, a transfer learning and ensemble learning-based IDS is proposed for IoV systems using convolutional neural networks (CNNs) and hyper-parameter optimization techniques. In the experiments, the proposed IDS has demonstrated over 99.25% detection rates and F1-scores on two well-known public benchmark IoV security datasets: the Car-Hacking dataset and the CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for cyber-attack detection in both intra-vehicle and external vehicular networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
TLS Beyond the Broker: Enforcing Fine-grained Security and Trust in Publish/Subscribe Environments for IoT,"Message queuing brokers are a fundamental building block of the Internet of Things, commonly used to store and forward messages from publishing clients to subscribing clients. Often a single trusted broker offers secured (e.g. TLS) and unsecured connections but relays messages regardless of their inbound and outbound protection. Such mixed mode is facilitated for the sake of efficiency since TLS is quite a burden for MQTT implementations on class-0 IoT devices. Such a broker thus transparently interconnects securely and insecurely connected devices; we argue that such mixed mode operation can actually be a significant security problem: Clients can only control the security level of their own connection to the broker, but they cannot enforce any protection towards other clients. We describe an enhancement of such a publish/subscribe mechanism to allow for enforcing specified security levels of publishers or subscribers by only forwarding messages via connections which satisfy the desired security levels. For example, a client publishing a message over a secured channel can instruct the broker to forward the message exclusively to subscribers that are securely connected. We prototypically implemented our solution for the MQTT protocol and provide detailed overhead measurements.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Lotto: Utilizing IoT Devices in Brute-Force Attacks,"The number of IoT devices in use is increasing rapidly and so is the number of IoT applications. As in any new technology, the rapid development means rapid increase in security threats and attack surfaces. IoT security has proven to be challenging throughout the past few years. However, another challenging task is to prevent IoT devices from becoming a tool used by malicious attackers to break into other systems. In this paper, we present a conceptual design in which IoT devices are used as tools in brute-force attacks to break encryption keys of block ciphers. The proposed design shows that with adequate number of IoT devices employed in the attack, the attack can succeed in breaking large-key block ciphers.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs),"The exponential growth of the Internet of Things (IoT) has led to the emergence of substantial security concerns, with IoT networks becoming the primary target for cyberattacks. This study examines the potential of Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine learning models for intrusion detection in IoT networks. The study demonstrates that KANs, which employ learnable activation functions, outperform traditional MLPs and achieve competitive accuracy compared to state-of-the-art models such as Random Forest and XGBoost, while offering superior interpretability for intrusion detection in IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery,"The use of Machine Learning has become a significant part of malware detection efforts due to the influx of new malware, an ever changing threat landscape, and the ability of Machine Learning methods to discover meaningful distinctions between malicious and benign software. Antivirus vendors have also begun to widely utilize malware classifiers based on dynamic and static malware analysis features. Therefore, a malware author might make evasive binary modifications against Machine Learning models as part of the malware development life cycle to execute an attack successfully. This makes the studying of possible classifier evasion strategies an essential part of cyber defense against malice. To this extent, we stage a grey box setup to analyze a scenario where the malware author does not know the target classifier algorithm, and does not have access to decisions made by the classifier, but knows the features used in training. In this experiment, a malicious actor trains a surrogate model using the EMBER-2018 dataset to discover binary mutations that cause an instance to be misclassified via a Monte Carlo tree search. Then, mutated malware is sent to the victim model that takes the place of an antivirus API to test whether it can evade detection.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Lightweight IoT Malware Detection Solution Using CNN Classification,"Internet of Things (IoT) is becoming more frequently used in more applications as the number of connected devices is in a rapid increase. More connected devices result in bigger challenges in terms of scalability, maintainability and most importantly security especially when it comes to 5G networks. The security aspect of IoT devices is an infant field, which is why it is our focus in this paper. Multiple IoT device manufacturers do not consider securing the devices they produce for different reasons like cost reduction or to avoid using energy-harvesting components. Such potentially malicious devices might be exploited by the adversary to do multiple harmful attacks. Therefore, we developed a system that can recognize malicious behavior of a specific IoT node on the network. Through convolutional neural network and monitoring, we were able to provide malware detection for IoT using a central node that can be installed within the network. The achievement shows how such models can be generalized and applied easily to any network while clearing out any stigma regarding deep learning techniques.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Adversarial Samples on Android Malware Detection Systems for IoT Systems,"Many IoT(Internet of Things) systems run Android systems or Android-like systems. With the continuous development of machine learning algorithms, the learning-based Android malware detection system for IoT devices has gradually increased. However, these learning-based detection models are often vulnerable to adversarial samples. An automated testing framework is needed to help these learning-based malware detection systems for IoT devices perform security analysis. The current methods of generating adversarial samples mostly require training parameters of models and most of the methods are aimed at image data. To solve this problem, we propose a \textbf{t}esting framework for \textbf{l}earning-based \textbf{A}ndroid \textbf{m}alware \textbf{d}etection systems(TLAMD) for IoT Devices. The key challenge is how to construct a suitable fitness function to generate an effective adversarial sample without affecting the features of the application. By introducing genetic algorithms and some technical improvements, our test framework can generate adversarial samples for the IoT Android Application with a success rate of nearly 100\% and can perform black-box testing on the system.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Understanding Security in Smart City Domains From the ANT-centric Perspective,"A city is a large human settlement that serves the people who live there, and a smart city is a concept of how cities might better serve their residents through new forms of technology. In this paper, we focus on four major smart city domains according to Maslow's hierarchy of needs: smart utility, smart transportation, smart homes, and smart healthcare. Numerous IoT applications have been developed to achieve the intelligence that we desire in our smart domains, ranging from personal gadgets such as health trackers and smart watches to large-scale industrial IoT systems such as nuclear and energy management systems. However, many of the existing smart city IoT solutions can be made better by considering the suitability of their security strategies. Inappropriate system security designs generally occur in two scenarios: first, system designers recognize the importance of security but are unsure of where, when, or how to implement it; and second, system designers try to fit traditional security designs to meet the smart city security context. Thus, the objective of this paper is to provide application designers with the missing security link they may need to improve their security designs. By evaluating the specific context of each smart city domain and the context-specific security requirements, we aim to provide directions on when, where, and how they should implement security strategies and the possible security challenges they need to consider. In addition, we present a new perspective on security issues in smart cities from a data-centric viewpoint by referring to the reference architecture, the Activity-Network-Things (ANT)-centric architecture, built upon the concept of ""security in a zero-trust environment"". By doing so, we reduce the security risks posed by new system interactions or unanticipated user behaviors while avoiding the hassle of regularly upgrading security models.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Malware Network Traffic Classification using Visual Representation and Deep Learning,"With the increase of IoT devices and technologies coming into service, Malware has risen as a challenging threat with increased infection rates and levels of sophistication. Without strong security mechanisms, a huge amount of sensitive data is exposed to vulnerabilities, and therefore, easily abused by cybercriminals to perform several illegal activities. Thus, advanced network security mechanisms that are able of performing a real-time traffic analysis and mitigation of malicious traffic are required. To address this challenge, we are proposing a novel IoT malware traffic analysis approach using deep learning and visual representation for faster detection and classification of new malware (zero-day malware). The detection of malicious network traffic in the proposed approach works at the package level, significantly reducing the time of detection with promising results due to the deep learning technologies used. To evaluate our proposed method performance, a dataset is constructed which consists of 1000 pcap files of normal and malware traffic that are collected from different network traffic sources. The experimental results of Residual Neural Network (ResNet50) are very promising, providing a 94.50% accuracy rate for detection of malware traffic.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Optimization of Lightweight Malware Detection Models For AIoT Devices,"Malware intrusion is problematic for Internet of Things (IoT) and Artificial Intelligence of Things (AIoT) devices as they often reside in an ecosystem of connected devices, such as a smart home. If any devices are infected, the whole ecosystem can be compromised. Although various Machine Learning (ML) models are deployed to detect malware and network intrusion, generally speaking, robust high-accuracy models tend to require resources not found in all IoT devices, compared to less robust models defined by weak learners. In order to combat this issue, Fadhilla proposed a meta-learner ensemble model comprised of less robust prediction results inherent with weak learner ML models to produce a highly robust meta-learning ensemble model. The main problem with the prior research is that it cannot be deployed in low-end AIoT devices due to the limited resources comprising processing power, storage, and memory (the required libraries quickly exhaust low-end AIoT devices' resources.) Hence, this research aims to optimize the proposed super learner meta-learning ensemble model to make it viable for low-end AIoT devices. We show the library and ML model memory requirements associated with each optimization stage and emphasize that optimization of current ML models is necessitated for low-end AIoT devices. Our results demonstrate that we can obtain similar accuracy and False Positive Rate (FPR) metrics from high-end AIoT devices running the derived ML model, with a lower inference duration and smaller memory footprint.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving Intrusion Detection in Industrial IoT,"Industrial Internet of Things (IIoT) systems have become integral to smart manufacturing, yet their growing connectivity has also exposed them to significant cybersecurity threats. Traditional intrusion detection systems (IDS) often rely on centralized architectures that raise concerns over data privacy, latency, and single points of failure. In this work, we propose a novel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for privacy-preserving intrusion detection tailored for IIoT environments. Our architecture combines federated learning (FL) to ensure decentralized model training with blockchain technology to guarantee data integrity, trust, and tamper resistance across IIoT nodes. We design a lightweight intrusion detection model collaboratively trained using FL across edge devices without exposing sensitive data. A smart contract-enabled blockchain system records model updates and anomaly scores to establish accountability. Experimental evaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior performance of our framework, achieving 97.3% accuracy while reducing communication overhead by 41% compared to baseline centralized methods. Our approach ensures privacy, scalability, and robustness-critical for secure industrial operations. The proposed FL-BCID system provides a promising solution for enhancing trust and privacy in modern IIoT security architectures.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An Intelligent Mechanism for Monitoring and Detecting Intrusions in IoT Devices,"The current amount of IoT devices and their limitations has come to serve as a motivation for malicious entities to take advantage of such devices and use them for their own gain. To protect against cyberattacks in IoT devices, Machine Learning techniques can be applied to Intrusion Detection Systems. Moreover, privacy related issues associated with centralized approaches can be mitigated through Federated Learning. This work proposes a Host-based Intrusion Detection Systems that leverages Federated Learning and Multi-Layer Perceptron neural networks to detected cyberattacks on IoT devices with high accuracy and enhancing data privacy protection.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Botnet Detection Using an Economic Deep Learning Model,"The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Internet of Things Security Research: A Rehash of Old Ideas or New Intellectual Challenges?,"The Internet of Things (IoT) is a new computing paradigm that spans wearable devices, homes, hospitals, cities, transportation, and critical infrastructure. Building security into this new computing paradigm is a major technical challenge today. However, what are the security problems in IoT that we can solve using existing security principles? And, what are the new problems and challenges in this space that require new security mechanisms? This article summarizes the intellectual similarities and differences between classic information technology security research and IoT security research.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A New Generic Taxonomy on Hybrid Malware Detection Technique,"Malware is a type of malicious program that replicate from host machine and propagate through network. It has been considered as one type of computer attack and intrusion that can do a variety of malicious activity on a computer. This paper addresses the current trend of malware detection techniques and identifies the significant criteria in each technique to improve malware detection in Intrusion Detection System (IDS). Several existing techniques are analyzing from 48 various researches and the capability criteria of malware detection technique have been reviewed. From the analysis, a new generic taxonomy of malware detection technique have been proposed named Hybrid Malware Detection Technique (Hybrid MDT) which consists of Hybrid Signature and Anomaly detection technique and Hybrid Specification based and Anomaly detection technique to complement the weaknesses of the existing malware detection technique in detecting known and unknown attack as well as reducing false alert before and during the intrusion occur.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security Framework for IoT Devices against Cyber-Attacks,"Internet of Things (IoT) is the interconnection of heterogeneous smart devices through the Internet with diverse application areas. The huge number of smart devices and the complexity of networks has made it impossible to secure the data and communication between devices. Various conventional security controls are insufficient to prevent numerous attacks against these information-rich devices. Along with enhancing existing approaches, a peripheral defence, Intrusion Detection System (IDS), proved efficient in most scenarios. However, conventional IDS approaches are unsuitable to mitigate continuously emerging zero-day attacks. Intelligent mechanisms that can detect unfamiliar intrusions seems a prospective solution. This article explores popular attacks against IoT architecture and its relevant defence mechanisms to identify an appropriate protective measure for different networking practices and attack categories. Besides, a security framework for IoT architecture is provided with a list of security enhancement techniques.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Adaptive Bi-Recommendation and Self-Improving Network for Heterogeneous Domain Adaptation-Assisted IoT Intrusion Detection,"As Internet of Things devices become prevalent, using intrusion detection to protect IoT from malicious intrusions is of vital importance. However, the data scarcity of IoT hinders the effectiveness of traditional intrusion detection methods. To tackle this issue, in this paper, we propose the Adaptive Bi-Recommendation and Self-Improving Network (ABRSI) based on unsupervised heterogeneous domain adaptation (HDA). The ABRSI transfers enrich intrusion knowledge from a data-rich network intrusion source domain to facilitate effective intrusion detection for data-scarce IoT target domains. The ABRSI achieves fine-grained intrusion knowledge transfer via adaptive bi-recommendation matching. Matching the bi-recommendation interests of two recommender systems and the alignment of intrusion categories in the shared feature space form a mutual-benefit loop. Besides, the ABRSI uses a self-improving mechanism, autonomously improving the intrusion knowledge transfer from four ways. A hard pseudo label voting mechanism jointly considers recommender system decision and label relationship information to promote more accurate hard pseudo label assignment. To promote diversity and target data participation during intrusion knowledge transfer, target instances failing to be assigned with a hard pseudo label will be assigned with a probabilistic soft pseudo label, forming a hybrid pseudo-labelling strategy. Meanwhile, the ABRSI also makes soft pseudo-labels globally diverse and individually certain. Finally, an error knowledge learning mechanism is utilised to adversarially exploit factors that causes detection ambiguity and learns through both current and previous error knowledge, preventing error knowledge forgetfulness. Holistically, these mechanisms form the ABRSI model that boosts IoT intrusion detection accuracy via HDA-assisted intrusion knowledge transfer.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Security Techniques Based on Machine Learning,"Internet of things (IoT) that integrate a variety of devices into networks to provide advanced and intelligent services have to protect user privacy and address attacks such as spoofing attacks, denial of service attacks, jamming and eavesdropping. In this article, we investigate the attack model for IoT systems, and review the IoT security solutions based on machine learning techniques including supervised learning, unsupervised learning and reinforcement learning. We focus on the machine learning based IoT authentication, access control, secure offloading and malware detection schemes to protect data privacy. In this article, we discuss the challenges that need to be addressed to implement these machine learning based security schemes in practical IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A systematic review of metaheuristics-based and machine learning-driven intrusion detection systems in IoT,"The widespread adoption of the Internet of Things (IoT) has raised a new challenge for developers since it is prone to known and unknown cyberattacks due to its heterogeneity, flexibility, and close connectivity. To defend against such security breaches, researchers have focused on building sophisticated intrusion detection systems (IDSs) using machine learning (ML) techniques. Although these algorithms notably improve detection performance, they require excessive computing power and resources, which are crucial issues in IoT networks considering the recent trends of decentralized data processing and computing systems. Consequently, many optimization techniques have been incorporated with these ML models. Specifically, a special category of optimizer adopted from the behavior of living creatures and different aspects of natural phenomena, known as metaheuristic algorithms, has been a central focus in recent years and brought about remarkable results. Considering this vital significance, we present a comprehensive and systematic review of various applications of metaheuristics algorithms in developing a machine learning-based IDS, especially for IoT. A significant contribution of this study is the discovery of hidden correlations between these optimization techniques and machine learning models integrated with state-of-the-art IoT-IDSs. In addition, the effectiveness of these metaheuristic algorithms in different applications, such as feature selection, parameter or hyperparameter tuning, and hybrid usages are separately analyzed. Moreover, a taxonomy of existing IoT-IDSs is proposed. Furthermore, we investigate several critical issues related to such integration. Our extensive exploration ends with a discussion of promising optimization algorithms and technologies that can enhance the efficiency of IoT-IDSs.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Hybrid Machine Learning Models for Intrusion Detection in IoT: Leveraging a Real-World IoT Dataset,"The rapid growth of the Internet of Things (IoT) has revolutionized industries, enabling unprecedented connectivity and functionality. However, this expansion also increases vulnerabilities, exposing IoT networks to increasingly sophisticated cyberattacks. Intrusion Detection Systems (IDS) are crucial for mitigating these threats, and recent advancements in Machine Learning (ML) offer promising avenues for improvement. This research explores a hybrid approach, combining several standalone ML models such as Random Forest (RF), XGBoost, K-Nearest Neighbors (KNN), and AdaBoost, in a voting-based hybrid classifier for effective IoT intrusion detection. This ensemble method leverages the strengths of individual algorithms to enhance accuracy and address challenges related to data complexity and scalability. Using the widely-cited IoT-23 dataset, a prominent benchmark in IoT cybersecurity research, we evaluate our hybrid classifiers for both binary and multi-class intrusion detection problems, ensuring a fair comparison with existing literature. Results demonstrate that our proposed hybrid models, designed for robustness and scalability, outperform standalone approaches in IoT environments. This work contributes to the development of advanced, intelligent IDS frameworks capable of addressing evolving cyber threats.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Developer-Friendly Library for Smart Home IoT Privacy-Preserving Traffic Obfuscation,"The number and variety of Internet-connected devices have grown enormously in the past few years, presenting new challenges to security and privacy. Research has shown that network adversaries can use traffic rate metadata from consumer IoT devices to infer sensitive user activities. Shaping traffic flows to fit distributions independent of user activities can protect privacy, but this approach has seen little adoption due to required developer effort and overhead bandwidth costs. Here, we present a Python library for IoT developers to easily integrate privacy-preserving traffic shaping into their products. The library replaces standard networking functions with versions that automatically obfuscate device traffic patterns through a combination of payload padding, fragmentation, and randomized cover traffic. Our library successfully preserves user privacy and requires approximately 4 KB/s overhead bandwidth for IoT devices with low send rates or high latency tolerances. This overhead is reasonable given normal Internet speeds in American homes and is an improvement on the bandwidth requirements of existing solutions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Security: An End-to-End View and Case Study,"In this paper, we present an end-to-end view of IoT security and privacy and a case study. Our contribution is three-fold. First, we present our end-to-end view of an IoT system and this view can guide risk assessment and design of an IoT system. We identify 10 basic IoT functionalities that are related to security and privacy. Based on this view, we systematically present security and privacy requirements in terms of IoT system, software, networking and big data analytics in the cloud. Second, using the end-to-end view of IoT security and privacy, we present a vulnerability analysis of the Edimax IP camera system. We are the first to exploit this system and have identified various attacks that can fully control all the cameras from the manufacturer. Our real-world experiments demonstrate the effectiveness of the discovered attacks and raise the alarms again for the IoT manufacturers. Third, such vulnerabilities found in the exploit of Edimax cameras and our previous exploit of Edimax smartplugs can lead to another wave of Mirai attacks, which can be either botnets or worm attacks. To systematically understand the damage of the Mirai malware, we model propagation of the Mirai and use the simulations to validate the modeling. The work in this paper raises the alarm again for the IoT device manufacturers to better secure their products in order to prevent malware attacks like Mirai.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
The Malware as a Service ecosystem,"The goal of this chapter is to illuminate the operational frameworks, key actors, and significant cybersecurity implications of the Malware as a Service (MaaS) ecosystem. Highlighting the transformation of malware proliferation into a service-oriented model, the chapter discusses how MaaS democratises access to sophisticated cyberattack capabilities, enabling even those with minimal technical knowledge to execute catastrophic cyberattacks. The discussion extends to the roles within the MaaS ecosystem, including malware developers, affiliates, initial access brokers, and the essential infrastructure providers that support these nefarious activities. The study emphasises the profound challenges MaaS poses to traditional cybersecurity defences, rendered ineffective against the constantly evolving and highly adaptable threats generated by MaaS platforms. With the increase in malware sophistication, there is a parallel call for a paradigm shift in defensive strategies, advocating for dynamic analysis, behavioural detection, and the integration of AI and machine learning techniques. By exploring the intricacies of the MaaS ecosystem, including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime, the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals. The ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the MaaS model.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Voting Classifier-based Intrusion Detection for IoT Networks,"Internet of Things (IoT) is transforming human lives by paving the way for the management of physical devices on the edge. These interconnected IoT objects share data for remote accessibility and can be vulnerable to open attacks and illegal access. Intrusion detection methods are commonly used for the detection of such kinds of attacks but with these methods, the performance/accuracy is not optimal. This work introduces a novel intrusion detection approach based on an ensemble-based voting classifier that combines multiple traditional classifiers as a base learner and gives the vote to the predictions of the traditional classifier in order to get the final prediction. To test the effectiveness of the proposed approach, experiments are performed on a set of seven different IoT devices and tested for binary attack classification and multi-class attack classification. The results illustrate prominent accuracies on Global Positioning System (GPS) sensors and weather sensors to 96% and 97% and for other machine learning algorithms to 85% and 87%, respectively. Furthermore, comparison with other traditional machine learning methods validates the superiority of the proposed algorithm.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Characterizing IoT Software Update Practices,"Software updates are critical for ensuring systems remain free of bugs and vulnerabilities while they are in service. While many Internet of Things (IoT) devices are capable of outlasting desktops and mobile phones, their software update practices are not yet well understood, despite a large body of research aiming to create new methodologies for keeping IoT devices up to date. This paper discusses efforts towards characterizing the IoT software update landscape through network-level analysis of IoT device traffic. Our results suggest that vendors do not currently follow security best practices, and that software update standards, while available, are not being deployed.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol Independent Approach,"Internet-of-Things (IoT) devices are nowadays massively integrated in daily life: homes, factories, or public places. This technology offers attractive services to improve the quality of life as well as new economic markets through the exploitation of the collected data. However, these connected objects have also become attractive targets for attackers because their current security design is often weak or flawed, as illustrated by several vulnerabilities such as Mirai, Blueborne, etc. This paper presents a novel approach for detecting intrusions in smart spaces such as smarthomes, or smartfactories, that is based on the monitoring and profiling of radio communications at the physical layer using machine learning techniques. The approach is designed to be independent of the large and heterogeneous set of wireless communication protocols typically implemented by connected objects such as WiFi, Bluetooth, Zigbee, Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main concepts of the proposed approach are presented together with an experimental case study illustrating its feasibility based on data collected during the deployment of the intrusion detection approach in a smart home under real-life conditions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Strengthening Network Intrusion Detection in IoT Environments with Self-Supervised Learning and Few Shot Learning,"The Internet of Things (IoT) has been introduced as a breakthrough technology that integrates intelligence into everyday objects, enabling high levels of connectivity between them. As the IoT networks grow and expand, they become more susceptible to cybersecurity attacks. A significant challenge in current intrusion detection systems for IoT includes handling imbalanced datasets where labeled data are scarce, particularly for new and rare types of cyber attacks. Existing literature often fails to detect such underrepresented attack classes. This paper introduces a novel intrusion detection approach designed to address these challenges. By integrating Self Supervised Learning (SSL), Few Shot Learning (FSL), and Random Forest (RF), our approach excels in learning from limited and imbalanced data and enhancing detection capabilities. The approach starts with a Deep Infomax model trained to extract key features from the dataset. These features are then fed into a prototypical network to generate discriminate embedding. Subsequently, an RF classifier is employed to detect and classify potential malware, including a range of attacks that are frequently observed in IoT networks. The proposed approach was evaluated through two different datasets, MaleVis and WSN-DS, which demonstrate its superior performance with accuracies of 98.60% and 99.56%, precisions of 98.79% and 99.56%, recalls of 98.60% and 99.56%, and F1-scores of 98.63% and 99.56%, respectively.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Protecting Actuators in Safety-Critical IoT Systems from Control Spoofing Attacks,"In this paper, we propose a framework called Contego-TEE to secure Internet-of-Things (IoT) edge devices with timing requirements from control spoofing attacks where an adversary sends malicious control signals to the actuators. We use a trusted computing base available in commodity processors (such as ARM TrustZone) and propose an invariant checking mechanism to ensure the security and safety of the physical system. A working prototype of Contego-TEE was developed using embedded Linux kernel. We demonstrate the feasibility of our approach for a robotic vehicle running on an ARM-based platform.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Efficient Wu-Manber Pattern Matching Hardware for Intrusion and Malware Detection,"Network intrusion detection systems and antivirus software are essential in detecting malicious network traffic and attacks such as denial-of-service and malwares. Each attack, worm or virus has its own distinctive signature. Signature-based intrusion detection and antivirus systems depend on pattern matching to look for possible attack signatures. Pattern matching is a very complex task, which requires a lot of time, memory and computing resources. Software-based intrusion detection is not fast enough to match high network speeds and the increasing number of attacks. In this paper, we propose special purpose hardware for Wu-Manber pattern matching algorithm. FPGAs form an excellent choice because of their massively parallel structure, reprogrammable logic and memory resources. The hardware is designed in Verilog and implemented using Xilinx ISE. For evaluation, we dope network traffic traces collected using Wireshark with 2500 signatures from the ClamAV virus definitions database. Experimental results show high speed that reaches up to 216 Mbps. In addition, we evaluate time, device usage, and power consumption.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Feature Analysis for Machine Learning-based IoT Intrusion Detection,"Internet of Things (IoT) networks have become an increasingly attractive target of cyberattacks. Powerful Machine Learning (ML) models have recently been adopted to implement network intrusion detection systems to protect IoT networks. For the successful training of such ML models, selecting the right data features is crucial, maximising the detection accuracy and computational efficiency. This paper comprehensively analyses feature sets' importance and predictive power for detecting network attacks. Three feature selection algorithms: chi-square, information gain and correlation, have been utilised to identify and rank data features. The attributes are fed into two ML classifiers: deep feed-forward and random forest, to measure their attack detection performance. The experimental evaluation considered three datasets: UNSW-NB15, CSE-CIC-IDS2018, and ToN-IoT in their proprietary flow format. In addition, the respective variants in NetFlow format were also considered, i.e., NF-UNSW-NB15, NF-CSE-CIC-IDS2018, and NF-ToN-IoT. The experimental evaluation explored the marginal benefit of adding individual features. Our results show that the accuracy initially increases rapidly with adding features but converges quickly to the maximum. This demonstrates a significant potential to reduce the computational and storage cost of intrusion detection systems while maintaining near-optimal detection accuracy. This has particular relevance in IoT systems, with typically limited computational and storage resources.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Visibility-Aware Optimal Contagion of Malware Epidemics,"Recent innovations in the design of computer viruses have led to new trade-offs for the attacker. Multiple variants of a malware may spread at different rates and have different levels of visibility to the network. In this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth. We show that in the mean-field deterministic regime, this spread-stealth trade-off is optimized by computationally simple single-threshold policies. Specifically, we show that only one variant of the malware is spread by the attacker at each time, as there exists a time up to which the attacker prioritizes maximizing the spread of the malware, and after which she prioritizes stealth.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing,"Anti-phishing aims to detect phishing content/documents in a pool of textual data. This is an important problem in cybersecurity that can help to guard users from fraudulent information. Natural language processing (NLP) offers a natural solution for this problem as it is capable of analyzing the textual content to perform intelligent recognition. In this work, we investigate state-of-the-art techniques for text categorization in NLP to address the problem of anti-phishing for emails (i.e, predicting if an email is phishing or not). These techniques are based on deep learning models that have attracted much attention from the community recently. In particular, we present a framework with hierarchical long short-term memory networks (H-LSTMs) and attention mechanisms to model the emails simultaneously at the word and the sentence level. Our expectation is to produce an effective model for anti-phishing and demonstrate the effectiveness of deep learning for problems in cybersecurity.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
ACE of Spades in the IoT Security Game: A Flexible IPsec Security Profile for Access Control,"The Authentication and Authorization for Constrained Environments (ACE) framework provides fine-grained access control in the Internet of Things, where devices are resource-constrained and with limited connectivity. The ACE framework defines separate profiles to specify how exactly entities interact and what security and communication protocols to use. This paper presents the novel ACE IPsec profile, which specifies how a client establishes a secure IPsec channel with a resource server, contextually using the ACE framework to enforce authorized access to remote resources. The profile makes it possible to establish IPsec Security Associations, either through their direct provisioning or through the standard IKEv2 protocol. We provide the first Open Source implementation of the ACE IPsec profile for the Contiki OS and test it on the resource-constrained Zolertia Firefly platform. Our experimental performance evaluation confirms that the IPsec profile and its operating modes are affordable and deployable also on constrained IoT platforms.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Predicting IoT Device Vulnerability Fix Times with Survival and Failure Time Models,"The rapid integration of Internet of Things (IoT) devices into enterprise environments presents significant security challenges. Many IoT devices are released to the market with minimal security measures, often harbouring an average of 25 vulnerabilities per device. To enhance cybersecurity measures and aid system administrators in managing IoT patches more effectively, we propose an innovative framework that predicts the time it will take for a vulnerable IoT device to receive a fix or patch. We developed a survival analysis model based on the Accelerated Failure Time (AFT) approach, implemented using the XGBoost ensemble regression model, to predict when vulnerable IoT devices will receive fixes or patches. By constructing a comprehensive IoT vulnerabilities database that combines public and private sources, we provide insights into affected devices, vulnerability detection dates, published CVEs, patch release dates, and associated Twitter activity trends. We conducted thorough experiments evaluating different combinations of features, including fundamental device and vulnerability data, National Vulnerability Database (NVD) information such as CVE, CWE, and CVSS scores, transformed textual descriptions into sentence vectors, and the frequency of Twitter trends related to CVEs. Our experiments demonstrate that the proposed model accurately predicts the time to fix for IoT vulnerabilities, with data from VulDB and NVD proving particularly effective. Incorporating Twitter trend data offered minimal additional benefit. This framework provides a practical tool for organisations to anticipate vulnerability resolutions, improve IoT patch management, and strengthen their cybersecurity posture against potential threats.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
DatChain -- Blockchain implementation in Data transfer for IoT Devices,"Currently, the IoT ecosystem is comprised of fully connected smart devices that exchange data to provide more automated, precise, and fast decisions. This idealised situation can only be accomplished if a system for data transactions is processed efficiently and security is ensured with high scalability and practicability. The integrity of data must be maintained during the exchange or transfer of data between entities. We propose to make a application called DatChain that responds to the above situation. The application stores data sensed by the Iot sensors in the backend after encrypting it and when the data is required for any purpose it can be exchanged using a suitable blockchain network that can keep up with the transfer rate even at high traffic in a secure environment.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Analysis of Phishing Attacks and Countermeasures,"One of the biggest problems with the Internet technology is the unwanted spam emails. The well disguised phishing email comes in as part of the spam and makes its entry into the inbox quite frequently nowadays. While phishing is normally considered a consumer issue, the fraudulent tactics the phishers use are now intimidating the corporate sector as well. In this paper, we analyze the various aspects of phishing attacks and draw on some possible defenses as countermeasures. We initially address the different forms of phishing attacks in theory, and then look at some examples of attacks in practice, along with their common defenses. We also highlight some recent statistical data on phishing scam to project the seriousness of the problem. Finally, some specific phishing countermeasures at both the user level and the organization level are listed, and a multi-layered anti-phishing proposal is presented to round up our studies.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Heterogeneous Domain Adaptation for IoT Intrusion Detection: A Geometric Graph Alignment Approach,"Data scarcity hinders the usability of data-dependent algorithms when tackling IoT intrusion detection (IID). To address this, we utilise the data rich network intrusion detection (NID) domain to facilitate more accurate intrusion detection for IID domains. In this paper, a Geometric Graph Alignment (GGA) approach is leveraged to mask the geometric heterogeneities between domains for better intrusion knowledge transfer. Specifically, each intrusion domain is formulated as a graph where vertices and edges represent intrusion categories and category-wise interrelationships, respectively. The overall shape is preserved via a confused discriminator incapable to identify adjacency matrices between different intrusion domain graphs. A rotation avoidance mechanism and a centre point matching mechanism is used to avoid graph misalignment due to rotation and symmetry, respectively. Besides, category-wise semantic knowledge is transferred to act as vertex-level alignment. To exploit the target data, a pseudo-label election mechanism that jointly considers network prediction, geometric property and neighbourhood information is used to produce fine-grained pseudo-label assignment. Upon aligning the intrusion graphs geometrically from different granularities, the transferred intrusion knowledge can boost IID performance. Comprehensive experiments on several intrusion datasets demonstrate state-of-the-art performance of the GGA approach and validate the usefulness of GGA constituting components.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Deep Reinforcement Learning for Intrusion Detection in IoT: A Survey,"The rise of new complex attacks scenarios in Internet of things (IoT) environments necessitate more advanced and intelligent cyber defense techniques such as various Intrusion Detection Systems (IDSs) which are responsible for detecting and mitigating malicious activities in IoT networks without human intervention. To address this issue, deep reinforcement learning (DRL) has been proposed in recent years, to automatically tackle intrusions/attacks. In this paper, a comprehensive survey of DRL-based IDS on IoT is presented. Furthermore, in this survey, the state-of-the-art DRL-based IDS methods have been classified into five categories including wireless sensor network (WSN), deep Q-network (DQN), healthcare, hybrid, and other techniques. In addition, the most crucial performance metrics, namely accuracy, recall, precision, false negative rate (FNR), false positive rate (FPR), and F-measure, are detailed, in order to evaluate the performance of each proposed method. The paper provides a summary of datasets utilized in the studies as well.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Enhancing Cybersecurity in IoT Networks: A Deep Learning Approach to Anomaly Detection,"With the proliferation of the Internet and smart devices, IoT technology has seen significant advancements and has become an integral component of smart homes, urban security, smart logistics, and other sectors. IoT facilitates real-time monitoring of critical production indicators, enabling businesses to detect potential quality issues, anticipate equipment malfunctions, and refine processes, thereby minimizing losses and reducing costs. Furthermore, IoT enhances real-time asset tracking, optimizing asset utilization and management. However, the expansion of IoT has also led to a rise in cybercrimes, with devices increasingly serving as vectors for malicious attacks. As the number of IoT devices grows, there is an urgent need for robust network security measures to counter these escalating threats. This paper introduces a deep learning model incorporating LSTM and attention mechanisms, a pivotal strategy in combating cybercrime in IoT networks. Our experiments, conducted on datasets including IoT-23, BoT-IoT, IoT network intrusion, MQTT, and MQTTset, demonstrate that our proposed method outperforms existing baselines.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
E-GraphSAGE: A Graph Neural Network based Intrusion Detection System for IoT,"This paper presents a new Network Intrusion Detection System (NIDS) based on Graph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep neural networks, which can leverage the inherent structure of graph-based data. Training and evaluation data for NIDSs are typically represented as flow records, which can naturally be represented in a graph format. In this paper, we propose E-GraphSAGE, a GNN approach that allows capturing both the edge features of a graph as well as the topological information for network intrusion detection in IoT networks. To the best of our knowledge, our proposal is the first successful, practical, and extensively evaluated approach of applying GNNs on the problem of network intrusion detection for IoT using flow-based data. Our extensive experimental evaluation on four recent NIDS benchmark datasets shows that our approach outperforms the state-of-the-art in terms of key classification metrics, which demonstrates the potential of GNNs in network intrusion detection, and provides motivation for further research.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Human-centred home network security,"This chapter draws from across the foregoing chapters discussing many core HDI approaches and disciplinary perspectives to consider the specific application of HDI in home network security. While much work has considered the challenges of securing in home IoT devices and their communications, especially for those with limited power or computational capacity, scant attention has been paid by the research community to home network security, and its acceptability and usability, from the viewpoint of ordinary citizens. It will be clear that we need a radical transformation in our approach to designing domestic networking infrastructure to guard against widespread cyber-attacks that threaten to counter the benefits of the IoT. Our aim has to be to defend against enemies inside the walls, to protect critical functionality in the home against rogue devices and prevent the proliferation of disruptive wide-scale IoT DDOS attacks that are already occurring [1].","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Perspectives of Non-Expert Users on Cyber Security and Privacy: An Analysis of Online Discussions on Twitter,"Current research on users` perspectives of cyber security and privacy related to traditional and smart devices at home is very active, but the focus is often more on specific modern devices such as mobile and smart IoT devices in a home context. In addition, most were based on smaller-scale empirical studies such as online surveys and interviews. We endeavour to fill these research gaps by conducting a larger-scale study based on a real-world dataset of 413,985 tweets posted by non-expert users on Twitter in six months of three consecutive years (January and February in 2019, 2020 and 2021). Two machine learning-based classifiers were developed to identify the 413,985 tweets. We analysed this dataset to understand non-expert users` cyber security and privacy perspectives, including the yearly trend and the impact of the COVID-19 pandemic. We applied topic modelling, sentiment analysis and qualitative analysis of selected tweets in the dataset, leading to various interesting findings. For instance, we observed a 54% increase in non-expert users` tweets on cyber security and/or privacy related topics in 2021, compared to before the start of global COVID-19 lockdowns (January 2019 to February 2020). We also observed an increased level of help-seeking tweets during the COVID-19 pandemic. Our analysis revealed a diverse range of topics discussed by non-expert users across the three years, including VPNs, Wi-Fi, smartphones, laptops, smart home devices, financial security, and security and privacy issues involving different stakeholders. Overall negative sentiment was observed across almost all topics non-expert users discussed on Twitter in all the three years. Our results confirm the multi-faceted nature of non-expert users` perspectives on cyber security and privacy and call for more holistic, comprehensive and nuanced research on different facets of such perspectives.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Requirements and Recommendations for IoT/IIoT Models to automate Security Assurance through Threat Modelling, Security Analysis and Penetration Testing","The factories of the future require efficient interconnection of their physical machines into the cyber space to cope with the emerging need of an increased uptime of machines, higher performance rates, an improved level of productivity and a collective collaboration along the supply chain. With the rapid growth of the Internet of Things (IoT), and its application in industrial areas, the so called Industrial Internet of Things (IIoT)/Industry 4.0 emerged. However, further to the rapid growth of IoT/IIoT systems, cyber attacks are an emerging threat and simple manual security testing can often not cope with the scale of large IoT/IIoT networks. In this paper, we suggest to extract metadata from commonly used diagrams and models in a typical software development process, to automate the process of threat modelling, security analysis and penetration testing, without detailed prior security knowledge. In that context, we present requirements and recommendations for metadata in IoT/IIoT models that are needed as necessary input parameters of security assurance tools.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"The Internet of Things: New Interoperability, Management and Security Challenges","The Internet of Things (IoT) brings connectivity to about every objects found in the physical space. It extends connectivity to everyday objects. From connected fridges, cars and cities, the IoT creates opportunities in numerous domains. However, this increase in connectivity creates many prominent challenges. This paper provides a survey of some of the major issues challenging the widespread adoption of the IoT. Particularly, it focuses on the interoperability, management, security and privacy issues in the IoT. It is concluded that there is a need to develop a multifaceted technology approach to IoT security, management, and privacy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Scalable and Secure Architecture for Distributed IoT Systems,"Internet-of-things (IoT) is perpetually revolutionizing our daily life and rapidly transforming physical objects into an ubiquitous connected ecosystem. Due to their massive deployment and moderate security levels, those devices face a lot of security, management, and control challenges. Their classical centralized architecture is still cloaking vulnerabilities and anomalies that can be exploited by hackers for spying, eavesdropping, and taking control of the network. In this paper, we propose to improve the IoT architecture with additional security features using Artificial Intelligence (AI) and blockchain technology. We propose a novel architecture based on permissioned blockchain technology in order to build a scalable and decentralized end-to-end secure IoT system. Furthermore, we enhance the IoT system security with an AI-component at the gateway level to detect and classify suspected activities, malware, and cyber-attacks using machine learning techniques. Simulations and practical implementation show that the proposed architecture delivers high performance against cyber-attacks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
ARGUS: Context-Based Detection of Stealthy IoT Infiltration Attacks,"IoT application domains, device diversity and connectivity are rapidly growing. IoT devices control various functions in smart homes and buildings, smart cities, and smart factories, making these devices an attractive target for attackers. On the other hand, the large variability of different application scenarios and inherent heterogeneity of devices make it very challenging to reliably detect abnormal IoT device behaviors and distinguish these from benign behaviors. Existing approaches for detecting attacks are mostly limited to attacks directly compromising individual IoT devices, or, require predefined detection policies. They cannot detect attacks that utilize the control plane of the IoT system to trigger actions in an unintended/malicious context, e.g., opening a smart lock while the smart home residents are absent.   In this paper, we tackle this problem and propose ARGUS, the first self-learning intrusion detection system for detecting contextual attacks on IoT environments, in which the attacker maliciously invokes IoT device actions to reach its goals. ARGUS monitors the contextual setting based on the state and actions of IoT devices in the environment. An unsupervised Deep Neural Network (DNN) is used for modeling the typical contextual device behavior and detecting actions taking place in abnormal contextual settings. This unsupervised approach ensures that ARGUS is not restricted to detecting previously known attacks but is also able to detect new attacks. We evaluated ARGUS on heterogeneous real-world smart-home settings and achieve at least an F1-Score of 99.64% for each setup, with a false positive rate (FPR) of at most 0.03%.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Blockchain based Attack Detection on Machine Learning Algorithms for IoT based E-Health Applications,"The application of machine learning (ML) algorithms are massively scaling-up due to rapid digitization and emergence of new tecnologies like Internet of Things (IoT). In today's digital era, we can find ML algorithms being applied in the areas of healthcare, IoT, engineering, finance and so on. However, all these algorithms need to be trained in order to predict/solve a particular problem. There is high possibility of tampering the training datasets and produce biased results. Hence, in this article, we have proposed blockchain based solution to secure the datasets generated from IoT devices for E-Health applications. The proposed blockchain based solution uses using private cloud to tackle the aforementioned issue. For evaluation, we have developed a system that can be used by dataset owners to secure their data.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Launching Adversarial Attacks against Network Intrusion Detection Systems for IoT,"As the internet continues to be populated with new devices and emerging technologies, the attack surface grows exponentially. Technology is shifting towards a profit-driven Internet of Things market where security is an afterthought. Traditional defending approaches are no longer sufficient to detect both known and unknown attacks to high accuracy. Machine learning intrusion detection systems have proven their success in identifying unknown attacks with high precision. Nevertheless, machine learning models are also vulnerable to attacks. Adversarial examples can be used to evaluate the robustness of a designed model before it is deployed. Further, using adversarial examples is critical to creating a robust model designed for an adversarial environment. Our work evaluates both traditional machine learning and deep learning models' robustness using the Bot-IoT dataset. Our methodology included two main approaches. First, label poisoning, used to cause incorrect classification by the model. Second, the fast gradient sign method, used to evade detection measures. The experiments demonstrated that an attacker could manipulate or circumvent detection with significant probability.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security through the Eyes of AI: How Visualization is Shaping Malware Detection,"Malware, a persistent cybersecurity threat, increasingly targets interconnected digital systems such as desktop, mobile, and IoT platforms through sophisticated attack vectors. By exploiting these vulnerabilities, attackers compromise the integrity and resilience of modern digital ecosystems. To address this risk, security experts actively employ Machine Learning or Deep Learning-based strategies, integrating static, dynamic, or hybrid approaches to categorize malware instances. Despite their advantages, these methods have inherent drawbacks and malware variants persistently evolve with increased sophistication, necessitating advancements in detection strategies. Visualization-based techniques are emerging as scalable and interpretable solutions for detecting and understanding malicious behaviors across diverse platforms including desktop, mobile, IoT, and distributed systems as well as through analysis of network packet capture files. In this comprehensive survey of more than 100 high-quality research articles, we evaluate existing visualization-based approaches applied to malware detection and classification. As a first contribution, we propose a new all-encompassing framework to study the landscape of visualization-based malware detection techniques. Within this framework, we systematically analyze state-of-the-art approaches across the critical stages of the malware detection pipeline. By analyzing not only the single techniques but also how they are combined to produce the final solution, we shed light on the main challenges in visualization-based approaches and provide insights into the advancements and potential future directions in this critical field.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
SAFE: Self-Supervised Anomaly Detection Framework for Intrusion Detection,"The proliferation of IoT devices has significantly increased network vulnerabilities, creating an urgent need for effective Intrusion Detection Systems (IDS). Machine Learning-based IDS (ML-IDS) offer advanced detection capabilities but rely on labeled attack data, which limits their ability to identify unknown threats. Self-Supervised Learning (SSL) presents a promising solution by using only normal data to detect patterns and anomalies. This paper introduces SAFE, a novel framework that transforms tabular network intrusion data into an image-like format, enabling Masked Autoencoders (MAEs) to learn robust representations of network behavior. The features extracted by the MAEs are then incorporated into a lightweight novelty detector, enhancing the effectiveness of anomaly detection. Experimental results demonstrate that SAFE outperforms the state-of-the-art anomaly detection method, Scale Learning-based Deep Anomaly Detection method (SLAD), by up to 26.2% and surpasses the state-of-the-art SSL-based network intrusion detection approach, Anomal-E, by up to 23.5% in F1-score.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Joint Semantic Transfer Network for IoT Intrusion Detection,"In this paper, we propose a Joint Semantic Transfer Network (JSTN) towards effective intrusion detection for large-scale scarcely labelled IoT domain. As a multi-source heterogeneous domain adaptation (MS-HDA) method, the JSTN integrates a knowledge rich network intrusion (NI) domain and another small-scale IoT intrusion (II) domain as source domains, and preserves intrinsic semantic properties to assist target II domain intrusion detection. The JSTN jointly transfers the following three semantics to learn a domain-invariant and discriminative feature representation. The scenario semantic endows source NI and II domain with characteristics from each other to ease the knowledge transfer process via a confused domain discriminator and categorical distribution knowledge preservation. It also reduces the source-target discrepancy to make the shared feature space domain-invariant. Meanwhile, the weighted implicit semantic transfer boosts discriminability via a fine-grained knowledge preservation, which transfers the source categorical distribution to the target domain. The source-target divergence guides the importance weighting during knowledge preservation to reflect the degree of knowledge learning. Additionally, the hierarchical explicit semantic alignment performs centroid-level and representative-level alignment with the help of a geometric similarity-aware pseudo-label refiner, which exploits the value of unlabelled target II domain and explicitly aligns feature representations from a global and local perspective in a concentrated manner. Comprehensive experiments on various tasks verify the superiority of the JSTN against state-of-the-art comparing methods, on average a 10.3% of accuracy boost is achieved. The statistical soundness of each constituting component and the computational efficiency are also verified.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack,"Graph Neural Network (GNN)-based network intrusion detection systems (NIDS) are often evaluated on single datasets, limiting their ability to generalize under distribution drift. Furthermore, their adversarial robustness is typically assessed using synthetic perturbations that lack realism. This measurement gap leads to an overestimation of GNN-based NIDS resilience. To address the limitations, we propose \textbf{REAL-IoT}, a comprehensive framework for robustness evaluation of GNN-based NIDS in IoT environments. Our framework presents a methodology that creates a unified dataset from canonical datasets to assess generalization under drift. In addition, it features a novel intrusion dataset collected from a physical IoT testbed, which captures network traffic and attack scenarios under real-world settings. Furthermore, using REAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze network data and mitigate the impact of adversarial examples by filtering suspicious flows. Our evaluations using REAL-IoT reveal performance drops in GNN models compared to results from standard benchmarks, quantifying their susceptibility to drift and realistic attacks. We also demonstrate the potential of LLM-based filtering to enhance robustness. These findings emphasize the necessity of realistic threat modeling and rigorous measurement practices for developing resilient IoT intrusion detection systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security should be there by default: Investigating how journalists perceive and respond to risks from the Internet of Things,"Journalists have long been the targets of both physical and cyber-attacks from well-resourced adversaries. Internet of Things (IoT) devices are arguably a new avenue of threat towards journalists through both targeted and generalised cyber-physical exploitation. This study comprises three parts: First, we interviewed 11 journalists and surveyed 5 further journalists, to determine the extent to which journalists perceive threats through the IoT, particularly via consumer IoT devices. Second, we surveyed 34 cyber security experts to establish if and how lay-people can combat IoT threats. Third, we compared these findings to assess journalists' knowledge of threats, and whether their protective mechanisms would be effective against experts' depictions and predictions of IoT threats. Our results indicate that journalists generally are unaware of IoT-related risks and are not adequately protecting themselves; this considers cases where they possess IoT devices, or where they enter IoT-enabled environments (e.g., at work or home). Expert recommendations spanned both immediate and long-term mitigation methods, including practical actions that are technical and socio-political in nature. However, all proposed individual mitigation methods are likely to be short-term solutions, with 26 of 34 (76.5%) of cyber security experts responding that within the next five years it will not be possible for the public to opt-out of interaction with the IoT.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
JEDI: Many-to-Many End-to-End Encryption and Key Delegation for IoT,"As the Internet of Things (IoT) emerges over the next decade, developing secure communication for IoT devices is of paramount importance. Achieving end-to-end encryption for large-scale IoT systems, like smart buildings or smart cities, is challenging because multiple principals typically interact indirectly via intermediaries, meaning that the recipient of a message is not known in advance. This paper proposes JEDI (Joining Encryption and Delegation for IoT), a many-to-many end-to-end encryption protocol for IoT. JEDI encrypts and signs messages end-to-end, while conforming to the decoupled communication model typical of IoT systems. JEDI's keys support expiry and fine-grained access to data, common in IoT. Furthermore, JEDI allows principals to delegate their keys, restricted in expiry or scope, to other principals, thereby granting access to data and managing access control in a scalable, distributed way. Through careful protocol design and implementation, JEDI can run across the spectrum of IoT devices, including ultra low-power deeply embedded sensors severely constrained in CPU, memory, and energy consumption. We apply JEDI to an existing IoT messaging system and demonstrate that its overhead is modest.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine learning-based malware detection for IoT devices using control-flow data,"Embedded devices are specialised devices designed for one or only a few purposes. They are often part of a larger system, through wired or wireless connection. Those embedded devices that are connected to other computers or embedded systems through the Internet are called Internet of Things (IoT for short) devices.   With their widespread usage and their insufficient protection, these devices are increasingly becoming the target of malware attacks. Companies often cut corners to save manufacturing costs or misconfigure when producing these devices. This can be lack of software updates, ports left open or security defects by design. Although these devices may not be as powerful as a regular computer, their large number makes them suitable candidates for botnets. Other types of IoT devices can even cause health problems since there are even pacemakers connected to the Internet. This means, that without sufficient defence, even directed assaults are possible against people.   The goal of this thesis project is to provide better security for these devices with the help of machine learning algorithms and reverse engineering tools. Specifically, I study the applicability of control-flow related data of executables for malware detection. I present a malware detection method with two phases. The first phase extracts control-flow related data using static binary analysis. The second phase classifies binary executables as either malicious or benign using a neural network model. I train the model using a dataset of malicious and benign ARM applications.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
The Future Internet of Things and Security of its Control Systems,"We consider the future cyber security of industrial control systems. As best as we can see, much of this future unfolds in the context of the Internet of Things (IoT). In fact, we envision that all industrial and infrastructure environments, and cyber-physical systems in general, will take the form reminiscent of what today is referred to as the IoT. IoT is envisioned as multitude of heterogeneous devices densely interconnected and communicating with the objective of accomplishing a diverse range of objectives, often collaboratively. One can argue that in the relatively near future, the IoT construct will subsume industrial plants, infrastructures, housing and other systems that today are controlled by ICS and SCADA systems. In the IoT environments, cybersecurity will derive largely from system agility, moving-target defenses, cybermaneuvering, and other autonomous or semi-autonomous behaviors. Cyber security of IoT may also benefit from new design methods for mixed-trusted systems; and from big data analytics -- predictive and autonomous.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"From Malware Samples to Fractal Images: A New Paradigm for Classification. (Version 2.0, Previous version paper name: Have you ever seen malware?)","To date, a large number of research papers have been written on the classification of malware, its identification, classification into different families and the distinction between malware and goodware. These works have been based on captured malware samples and have attempted to analyse malware and goodware using various techniques, including techniques from the field of artificial intelligence. For example, neural networks have played a significant role in these classification methods. Some of this work also deals with analysing malware using its visualisation. These works usually convert malware samples capturing the structure of malware into image structures, which are then the object of image processing. In this paper, we propose a very unconventional and novel approach to malware visualisation based on dynamic behaviour analysis, with the idea that the images, which are visually very interesting, are then used to classify malware concerning goodware. Our approach opens an extensive topic for future discussion and provides many new directions for research in malware analysis and classification, as discussed in conclusion. The results of the presented experiments are based on a database of 6 589 997 goodware, 827 853 potentially unwanted applications and 4 174 203 malware samples provided by ESET and selected experimental data (images, generating polynomial formulas and software generating images) are available on GitHub for interested readers. Thus, this paper is not a comprehensive compact study that reports the results obtained from comparative experiments but rather attempts to show a new direction in the field of visualisation with possible applications in malware analysis.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Network Anomaly Detection for IoT Using Hyperdimensional Computing on NSL-KDD,"With the rapid growth of IoT devices, ensuring robust network security has become a critical challenge. Traditional intrusion detection systems (IDSs) often face limitations in detecting sophisticated attacks within high-dimensional and complex data environments. This paper presents a novel approach to network anomaly detection using hyperdimensional computing (HDC) techniques, specifically applied to the NSL-KDD dataset. The proposed method leverages the efficiency of HDC in processing large-scale data to identify both known and unknown attack patterns. The model achieved an accuracy of 91.55% on the KDDTrain+ subset, outperforming traditional approaches. These comparative evaluations underscore the model's superior performance, highlighting its potential in advancing anomaly detection for IoT networks and contributing to more secure and intelligent cybersecurity solutions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Fast Feature Reduction in intrusion detection datasets,"In the most intrusion detection systems (IDS), a system tries to learn characteristics of different type of attacks by analyzing packets that sent or received in network. These packets have a lot of features. But not all of them is required to be analyzed to detect that specific type of attack. Detection speed and computational cost is another vital matter here, because in these types of problems, datasets are very huge regularly. In this paper we tried to propose a very simple and fast feature selection method to eliminate features with no helpful information on them. Result faster learning in process of redundant feature omission. We compared our proposed method with three most successful similarity based feature selection algorithm including Correlation Coefficient, Least Square Regression Error and Maximal Information Compression Index. After that we used recommended features by each of these algorithms in two popular classifiers including: Bayes and KNN classifier to measure the quality of the recommendations. Experimental result shows that although the proposed method can't outperform evaluated algorithms with high differences in accuracy, but in computational cost it has huge superiority over them.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Gotham Testbed: a Reproducible IoT Testbed for Security Experiments and Dataset Generation,"The growing adoption of the Internet of Things (IoT) has brought a significant increase in attacks targeting those devices. Machine learning (ML) methods have shown promising results for intrusion detection; however, the scarcity of IoT datasets remains a limiting factor in developing ML-based security systems for IoT scenarios. Static datasets get outdated due to evolving IoT architectures and threat landscape; meanwhile, the testbeds used to generate them are rarely published. This paper presents the Gotham testbed, a reproducible and flexible security testbed extendable to accommodate new emulated devices, services or attackers. Gotham is used to build an IoT scenario composed of 100 emulated devices communicating via MQTT, CoAP and RTSP protocols, among others, in a topology composed of 30 switches and 10 routers. The scenario presents three threat actors, including the entire Mirai botnet lifecycle and additional red-teaming tools performing DoS, scanning, and attacks targeting IoT protocols. The testbed has many purposes, including a cyber range, testing security solutions, and capturing network and application data to generate datasets. We hope that researchers can leverage and adapt Gotham to include other devices, state-of-the-art attacks and topologies to share scenarios and datasets that reflect the current IoT settings and threat landscape.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Incentivized Delivery Network of IoT Software Updates Based on Trustless Proof-of-Distribution,"The prevalence of IoT devices makes them an ideal target for attackers. To reduce the risk of attacks vendors routinely deliver security updates (patches) for their devices. The delivery of security updates becomes challenging due to the issue of scalability as the number of devices may grow much quicker than vendors' distribution systems. Previous studies have suggested a permissionless and decentralized blockchain-based network in which nodes can host and deliver security updates, thus the addition of new nodes scales out the network. However, these studies do not provide an incentive for nodes to join the network, making it unlikely for nodes to freely contribute their hosting space, bandwidth, and computation resources. In this paper, we propose a novel decentralized IoT software update delivery network in which participating nodes referred to as distributors) are compensated by vendors with digital currency for delivering updates to devices. Upon the release of a new security update, a vendor will make a commitment to provide digital currency to distributors that deliver the update; the commitment will be made with the use of smart contracts, and hence will be public, binding, and irreversible. The smart contract promises compensation to any distributor that provides proof-of-distribution, which is unforgeable proof that a single update was delivered to a single device. A distributor acquires the proof-of-distribution by exchanging a security update for a device signature using the Zero-Knowledge Contingent Payment (ZKCP) trustless data exchange protocol. Eliminating the need for trust between the security update distributor and the security consumer (IoT device) by providing fair compensation, can significantly increase the number of distributors, thus facilitating rapid scale out.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT,"The rapid growth of the Internet of Things (IoT) has expanded opportunities for innovation but also increased exposure to botnet-driven cyberattacks. Conventional detection methods often struggle with scalability, privacy, and adaptability in resource-constrained IoT environments. To address these challenges, we present a lightweight and privacy-preserving botnet detection framework based on federated learning. This approach enables distributed devices to collaboratively train models without exchanging raw data, thus maintaining user privacy while preserving detection accuracy. A communication-efficient aggregation strategy is introduced to reduce overhead, ensuring suitability for constrained IoT networks. Experiments on benchmark IoT botnet datasets demonstrate that the framework achieves high detection accuracy while substantially reducing communication costs. These findings highlight federated learning as a practical path toward scalable, secure, and privacy-aware intrusion detection for IoT ecosystems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security and Machine Learning Adoption in IoT: A Preliminary Study of IoT Developer Discussions,"Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. Traditionally, we learn about the IoT ecosystem/problems by conducting surveys of IoT developers/practitioners. Another way to learn is by analyzing IoT developer discussions in popular online developer forums like Stack Overflow (SO). However, we are aware of no such studies that focused on IoT developers' security and ML-related discussions in SO. This paper offers the results of preliminary study of IoT developer discussions in SO. We find around 12% of sentences contain security discussions, while around 0.12% sentences contain ML- related discussions. We find that IoT developers discussing security issues frequently inquired about how the shared data can be stored, shared, and transferred securely across IoT devices and users. We also find that IoT developers are interested to adopt deep neural network-based ML models into their IoT devices, but they find it challenging to accommodate those into their resource-constrained IoT devices. Our findings offer implications for IoT vendors and researchers to develop and design novel techniques for improved security and ML adoption into IoT devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Difficult for Thee, But Not for Me: Measuring the Difficulty and User Experience of Remediating Persistent IoT Malware","Consumer IoT devices may suffer malware attacks, and be recruited into botnets or worse. There is evidence that generic advice to device owners to address IoT malware can be successful, but this does not account for emerging forms of persistent IoT malware. Less is known about persistent malware, which resides on persistent storage, requiring targeted manual effort to remove it. This paper presents a field study on the removal of persistent IoT malware by consumers. We partnered with an ISP to contrast remediation times of 760 customers across three malware categories: Windows malware, non-persistent IoT malware, and persistent IoT malware. We also contacted ISP customers identified as having persistent IoT malware on their network-attached storage devices, specifically QSnatch. We found that persistent IoT malware exhibits a mean infection duration many times higher than Windows or Mirai malware; QSnatch has a survival probability of 30% after 180 days, whereby most if not all other observed malware types have been removed. For interviewed device users, QSnatch infections lasted longer, so are apparently more difficult to get rid of, yet participants did not report experiencing difficulty in following notification instructions. We see two factors driving this paradoxical finding: First, most users reported having high technical competency. Also, we found evidence of planning behavior for these tasks and the need for multiple notifications. Our findings demonstrate the critical nature of interventions from outside for persistent malware, since automatic scan of an AV tool or a power cycle, like we are used to for Windows malware and Mirai infections, will not solve persistent IoT malware infections.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Position Paper: Think Globally, React Locally -- Bringing Real-time Reference-based Website Phishing Detection on macOS","Background. The recent surge in phishing attacks keeps undermining the effectiveness of the traditional anti-phishing blacklist approaches. On-device anti-phishing solutions are gaining popularity as they offer faster phishing detection locally. Aim. We aim to eliminate the delay in recognizing and recording phishing campaigns in databases via on-device solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawler's scan to finish. Additionally, utilizing operating system-specific resources and frameworks, we aim to minimize the impact on system performance and depend on local processing to protect user privacy. Method. We propose a phishing detection solution that uses a combination of computer vision and on-device machine learning models to analyze websites in real time. Our reference-based approach analyzes the visual content of webpages, identifying phishing attempts through layout analysis, credential input areas detection, and brand impersonation criteria combination. Results. Our case study shows it's feasible to perform background processing on-device continuously, for the case of the web browser requiring the resource use of 16% of a single CPU core and less than 84MB of RAM on Apple M1 while maintaining the accuracy of brand logo detection at 46.6% (comparable with baselines), and of Credential Requiring Page detection at 98.1% (improving the baseline by 3.1%), within the test dataset. Conclusions. Our results demonstrate the potential of on-device, real-time phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest, e.g., email clients and messenger windows.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An intrusion detection system in internet of things using grasshopper optimization algorithm and machine learning algorithms,"The Internet of Things (IoT) has emerged as a foundational paradigm supporting a range of applications, including healthcare, education, agriculture, smart homes, and, more recently, enterprise systems. However, significant advancements in IoT networks have been impeded by security vulnerabilities and threats that, if left unaddressed, could hinder the deployment and operation of IoT based systems. Detecting unwanted activities within the IoT is crucial, as it directly impacts confidentiality, integrity, and availability. Consequently, intrusion detection has become a fundamental research area and the focus of numerous studies. An intrusion detection system (IDS) is essential to the IoTs alarm mechanisms, enabling effective security management. This paper examines IoT security and introduces an intelligent two-layer intrusion detection system for IoT. Machine learning techniques power the system's intelligence, with a two layer structure enhancing intrusion detection. By selecting essential features, the system maintains detection accuracy while minimizing processing overhead. The proposed method for intrusion detection in IoT is implemented in two phases. In the first phase, the Grasshopper Optimization Algorithm (GOA) is applied for feature selection. In the second phase, the Support Vector Machine (SVM) algorithm is used to detect intrusions. The method was implemented in MATLAB, and the NSLKDD dataset was used for evaluation. Simulation results show that the proposed method improves accuracy compared to other approaches.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Adaptive Intrusion Detection for Evolving RPL IoT Attacks Using Incremental Learning,"The routing protocol for low-power and lossy networks (RPL) has become the de facto routing standard for resource-constrained IoT systems, but its lightweight design exposes critical vulnerabilities to a wide range of routing-layer attacks such as hello flood, decreased rank, and version number manipulation. Traditional countermeasures, including protocol-level modifications and machine learning classifiers, can achieve high accuracy against known threats, yet they fail when confronted with novel or zero-day attacks unless fully retrained, an approach that is impractical for dynamic IoT environments. In this paper, we investigate incremental learning as a practical and adaptive strategy for intrusion detection in RPL-based networks. We systematically evaluate five model families, including ensemble models and deep learning models. Our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats, all while reducing training time compared to full retraining. By combining five diverse models with attack-specific analysis, forgetting behavior, and time efficiency, this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving RPL-based IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection,"This study explores the application of quantum machine learning (QML) algorithms to enhance cybersecurity threat detection, particularly in the classification of malware and intrusion detection within high-dimensional datasets. Classical machine learning approaches encounter limitations when dealing with intricate, obfuscated malware patterns and extensive network intrusion data. To address these challenges, we implement and evaluate various QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for malware detection tasks. Our experimental analysis utilized two datasets: the Intrusion dataset, comprising 150 samples with 56 memory-based features derived from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset, containing 58,596 samples with 57 features representing benign and malicious software. Remarkably, our QML methods demonstrated superior performance compared to classical approaches, achieving accuracies of 95% for QNN and 94% for QSVM. These quantum-enhanced methods leveraged quantum superposition and entanglement principles to accurately identify complex patterns within highly obfuscated malware samples that were imperceptible to classical methods. To further advance malware analysis, we propose a novel real-time malware analysis framework that incorporates Quantum Feature Extraction using Quantum Fourier Transform, Quantum Feature Maps, and Classification using Variational Quantum Circuits. This system integrates explainable AI methods, including GradCAM++ and ScoreCAM algorithms, to provide interpretable insights into the quantum decision-making processes.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Modeling and Assessment of IoT Supply Chain Security Risks: The Role of Structural and Parametric Uncertainties,"Supply chain security threats pose new challenges to security risk modeling techniques for complex ICT systems such as the IoT. With established techniques drawn from attack trees and reliability analysis providing needed points of reference, graph-based analysis can provide a framework for considering the role of suppliers in such systems. We present such a framework here while highlighting the need for a component-centered model. Given resource limitations when applying this model to existing systems, we study various classes of uncertainties in model development, including structural uncertainties and uncertainties in the magnitude of estimated event probabilities. Using case studies, we find that structural uncertainties constitute a greater challenge to model utility and as such should receive particular attention. Best practices in the face of these uncertainties are proposed.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Sentinel: Automated Device-Type Identification for Security Enforcement in IoT,"With the rapid growth of the Internet-of-Things (IoT), concerns about the security of IoT devices have become prominent. Several vendors are producing IP-connected devices for home and small office networks that often suffer from flawed security designs and implementations. They also tend to lack mechanisms for firmware updates or patches that can help eliminate security vulnerabilities. Securing networks where the presence of such vulnerable devices is given, requires a brownfield approach: applying necessary protection measures within the network so that potentially vulnerable devices can coexist without endangering the security of other devices in the same network. In this paper, we present IOT SENTINEL, a system capable of automatically identifying the types of devices being connected to an IoT network and enabling enforcement of rules for constraining the communications of vulnerable devices so as to minimize damage resulting from their compromise. We show that IOT SENTINEL is effective in identifying device types and has minimal performance overhead.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine Learning-Enabled IoT Security: Open Issues and Challenges Under Advanced Persistent Threats,"Despite its technological benefits, Internet of Things (IoT) has cyber weaknesses due to the vulnerabilities in the wireless medium. Machine learning (ML)-based methods are widely used against cyber threats in IoT networks with promising performance. Advanced persistent threat (APT) is prominent for cybercriminals to compromise networks, and it is crucial to long-term and harmful characteristics. However, it is difficult to apply ML-based approaches to identify APT attacks to obtain a promising detection performance due to an extremely small percentage among normal traffic. There are limited surveys to fully investigate APT attacks in IoT networks due to the lack of public datasets with all types of APT attacks. It is worth to bridge the state-of-the-art in network attack detection with APT attack detection in a comprehensive review article. This survey article reviews the security challenges in IoT networks and presents the well-known attacks, APT attacks, and threat models in IoT systems. Meanwhile, signature-based, anomaly-based, and hybrid intrusion detection systems are summarized for IoT networks. The article highlights statistical insights regarding frequently applied ML-based methods against network intrusion alongside the number of attacks types detected. Finally, open issues and challenges for common network intrusion and APT attacks are presented for future research.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection,"Malware remains a big threat to cyber security, calling for machine learning based malware detection. While promising, such detectors are known to be vulnerable to evasion attacks. Ensemble learning typically facilitates countermeasures, while attackers can leverage this technique to improve attack effectiveness as well. This motivates us to investigate which kind of robustness the ensemble defense or effectiveness the ensemble attack can achieve, particularly when they combat with each other. We thus propose a new attack approach, named mixture of attacks, by rendering attackers capable of multiple generative methods and multiple manipulation sets, to perturb a malware example without ruining its malicious functionality. This naturally leads to a new instantiation of adversarial training, which is further geared to enhancing the ensemble of deep neural networks. We evaluate defenses using Android malware detectors against 26 different attacks upon two practical datasets. Experimental results show that the new adversarial training significantly enhances the robustness of deep neural networks against a wide range of attacks, ensemble methods promote the robustness when base classifiers are robust enough, and yet ensemble attacks can evade the enhanced malware detectors effectively, even notably downgrading the VirusTotal service.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security Threats and Research Challenges of IoT-A Review,"Internet of things (IoT) is the epitome of sustainable development. It has facilitated the development of smart systems, industrialization, and the state-of-the-art quality of life. IoT architecture is one of the essential baselines of understanding the widespread adoption. Security issues are very crucial for any technical infrastructure. Since IoT comprises heterogeneous devices, its security issues are diverse too. Various security attacks can be responsible for compromising confidentiality, integrity, and availability. In this paper, at first, the IoT architecture is described briefly. After that, the components of IoT are explained with perspective to various IoT based applications and services. Finally, various security issues, including recommended solutions, are elaborately described and the potential research challenges and future research directions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Internet of Things: Survey on Security and Privacy,"The Internet of Things (IoT) is intended for ubiquitous connectivity among different entities or ""things"". While its purpose is to provide effective and efficient solutions, security of the devices and network is a challenging issue. The number of devices connected along with the ad-hoc nature of the system further exacerbates the situation. Therefore, security and privacy has emerged as a significant challenge for the IoT. In this paper,we aim to provide a thorough survey related to the privacy and security challenges of the IoT. This document addresses these challenges from the perspective of technologies and architecture used. This work focuses also in IoT intrinsic vulnerabilities as well as the security challenges of various layers based on the security principles of data confidentiality, integrity and availability. This survey analyzes articles published for the IoT at the time and relates it to the security conjuncture of the field and its projection to the future.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
D-Score: An Expert-Based Method for Assessing the Detectability of IoT-Related Cyber-Attacks,"IoT devices are known to be vulnerable to various cyber-attacks, such as data exfiltration and the execution of flooding attacks as part of a DDoS attack. When it comes to detecting such attacks using network traffic analysis, it has been shown that some attack scenarios are not always equally easy to detect if they involve different IoT models. That is, when targeted at some IoT models, a given attack can be detected rather accurately, while when targeted at others the same attack may result in too many false alarms. In this research, we attempt to explain this variability of IoT attack detectability and devise a risk assessment method capable of addressing a key question: how easy is it for an anomaly-based network intrusion detection system to detect a given cyber-attack involving a specific IoT model? In the process of addressing this question we (a) investigate the predictability of IoT network traffic, (b) present a novel taxonomy for IoT attack detection which also encapsulates traffic predictability aspects, (c) propose an expert-based attack detectability estimation method which uses this taxonomy to derive a detectability score (termed `D-Score') for a given combination of IoT model and attack scenario, and (d) empirically evaluate our method while comparing it with a data-driven method.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
SyzTrust: State-aware Fuzzing on Trusted OS Designed for IoT Devices,"Trusted Execution Environments (TEEs) embedded in IoT devices provide a deployable solution to secure IoT applications at the hardware level. By design, in TEEs, the Trusted Operating System (Trusted OS) is the primary component. It enables the TEE to use security-based design techniques, such as data encryption and identity authentication. Once a Trusted OS has been exploited, the TEE can no longer ensure security. However, Trusted OSes for IoT devices have received little security analysis, which is challenging from several perspectives: (1) Trusted OSes are closed-source and have an unfavorable environment for sending test cases and collecting feedback. (2) Trusted OSes have complex data structures and require a stateful workflow, which limits existing vulnerability detection tools. To address the challenges, we present SyzTrust, the first state-aware fuzzing framework for vetting the security of resource-limited Trusted OSes. SyzTrust adopts a hardware-assisted framework to enable fuzzing Trusted OSes directly on IoT devices as well as tracking state and code coverage non-invasively. SyzTrust utilizes composite feedback to guide the fuzzer to effectively explore more states as well as to increase the code coverage. We evaluate SyzTrust on Trusted OSes from three major vendors: Samsung, Tsinglink Cloud, and Ali Cloud. These systems run on Cortex M23/33 MCUs, which provide the necessary abstraction for embedded TEEs. We discovered 70 previously unknown vulnerabilities in their Trusted OSes, receiving 10 new CVEs so far. Furthermore, compared to the baseline, SyzTrust has demonstrated significant improvements, including 66% higher code coverage, 651% higher state coverage, and 31% improved vulnerability-finding capability. We report all discovered new vulnerabilities to vendors and open source SyzTrust.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT-KEEPER: Securing IoT Communications in Edge Networks,"The increased popularity of IoT devices have made them lucrative targets for attackers. Due to insecure product development practices, these devices are often vulnerable even to very trivial attacks and can be easily compromised. Due to the sheer number and heterogeneity of IoT devices, it is not possible to secure the IoT ecosystem using traditional endpoint and network security solutions. To address the challenges and requirements of securing IoT devices in edge networks, we present IoT-Keeper, which is a novel system capable of securing the network against any malicious activity, in real time. The proposed system uses a lightweight anomaly detection technique, to secure both device-to-device and device-to-infrastructure communications, while using limited resources available on the gateway. It uses unlabeled network data to distinguish between benign and malicious traffic patterns observed in the network. A detailed evaluation, done with real world testbed, shows that IoT-Keeper detects any device generating malicious traffic with high accuracy (0.982) and low false positive rate (0.01). The results demonstrate that IoT-Keeper is lightweight, responsive and can effectively handle complex D2D interactions without requiring explicit attack signatures or sophisticated hardware.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine Learning in IoT Security: Current Solutions and Future Challenges,"The future Internet of Things (IoT) will have a deep economical, commercial and social impact on our lives. The participating nodes in IoT networks are usually resource-constrained, which makes them luring targets for cyber attacks. In this regard, extensive efforts have been made to address the security and privacy issues in IoT networks primarily through traditional cryptographic approaches. However, the unique characteristics of IoT nodes render the existing solutions insufficient to encompass the entire security spectrum of the IoT networks. This is, at least in part, because of the resource constraints, heterogeneity, massive real-time data generated by the IoT devices, and the extensively dynamic behavior of the networks. Therefore, Machine Learning (ML) and Deep Learning (DL) techniques, which are able to provide embedded intelligence in the IoT devices and networks, are leveraged to cope with different security problems. In this paper, we systematically review the security requirements, attack vectors, and the current security solutions for the IoT networks. We then shed light on the gaps in these security solutions that call for ML and DL approaches. We also discuss in detail the existing ML and DL solutions for addressing different security problems in IoT networks. At last, based on the detailed investigation of the existing solutions in the literature, we discuss the future research directions for ML- and DL-based IoT security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Lightweight and Scalable Physical Layer Attack Detection Mechanism for the Internet of Things (IoT) Using Hybrid Security Schema,"The Internet of Things, also known as the IoT, refers to the billions of devices around the world that are now connected to the Internet, collecting and sharing data. The amount of data collected through IoT sensors must be completely securely controlled. To protect the information collected by IoT sensors, a lightweight method called Discover the Flooding Attack-RPL (DFA-RPL) has been proposed. The proposed DFA-RPL method identifies intrusive nodes in several steps to exclude them from continuing routing operations. Thus, in the DFA-RPL method, it first builds a cluster and selects the most appropriate node as a cluster head in DODAG, then, due to the vulnerability of the RPL protocol to Flooding attacks, it uses an ant colony algorithm (ACO) using five steps to detect attacks. Use Flooding to prevent malicious activity on the IoT network. In other words, if it detects a node as malicious, it puts that node on the detention list and quarantines it for a certain period of time. The results obtained from the simulation show the superiority of the proposed method in terms of Packet Delivery Rate, Detection Rate, False Positive Rate, and False Negative Rate compared to IRAD and REATO methods.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Reconfigurable Security: Edge Computing-based Framework for IoT,"In various scenarios, achieving security between IoT devices is challenging since the devices may have different dedicated communication standards, resource constraints as well as various applications. In this article, we first provide requirements and existing solutions for IoT security. We then introduce a new reconfigurable security framework based on edge computing, which utilizes a near-user edge device, i.e., security agent, to simplify key management and offload the computational costs of security algorithms at IoT devices. This framework is designed to overcome the challenges including high computation costs, low flexibility in key management, and low compatibility in deploying new security algorithms in IoT, especially when adopting advanced cryptographic primitives. We also provide the design principles of the reconfigurable security framework, the exemplary security protocols for anonymous authentication and secure data access control, and the performance analysis in terms of feasibility and usability. The reconfigurable security framework paves a new way to strength IoT security by edge computing.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"IoT Network Security: Requirements, Threats, and Countermeasures","IoT devices are increasingly utilized in critical infrastructure, enterprises, and households. There are several sophisticated cyber-attacks that have been reported and many networks have proven vulnerable to both active and passive attacks by leaking private information, allowing unauthorized access, and being open to denial of service attacks.   This paper aims firstly, to assist network operators to understand the need for an IoT network security solution, and then secondly, to survey IoT network attack vectors, cyber threats, and countermeasures with a focus on improving the robustness of existing security solutions. Our first contribution highlights viewpoints on IoT security from the perspective of stakeholders such as manufacturers, service providers, consumers, and authorities. We discuss the differences between IoT and IT systems, the need for IoT security solutions, and we highlight the key components required for IoT network security system architecture. For our second contribution, we survey the types of IoT attacks by grouping them based on their impact. We discuss various attack techniques, threats, and shortfalls of existing countermeasures with an intention to enable future research into improving IoT network security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Integration of Blockchain and IoT: An Enhanced Security Perspective,"Blockchain (BC), a by-product of Bitcoin cryptocurrency, has gained immense and wide scale popularity for its applicability in various diverse domains - especially in multifaceted non-monetary systems. By adopting cryptographic techniques such as hashing and asymmetric encryption - along with distributed consensus approach, a Blockchain based distributed ledger not only becomes highly secure but also immutable and thus eliminates the need for any third-party intermediators. On the contrary, innumerable IoT (Internet of Things) devices are increasingly being added to the network. This phenomenon poses higher risk in terms of security and privacy. It is thus extremely important to address the security aspects of the growing IoT ecosystem. This paper explores the applicability of BC for ensuring enhanced security and privacy in the IoT ecosystem. Recent research articles and projects or applications were surveyed to assess the implementation of BC for IoT Security and identify associated challenges and propose solutions for BC enabled enhanced security for the IoT ecosystem.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security,"Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer enhanced bandwidth capacity for large-scale service provisioning but remain vulnerable to evolving cyber threats. Existing intrusion detection and prevention methods provide limited security as adversaries continually adapt their attack strategies. We propose a dynamic attack detection and prevention approach to address this challenge. First, blockchain-based authentication uses the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy before data transmission. Next, a bi-stage intrusion detection system is introduced: the first stage uses signature-based detection via an Improved Random Forest (IRF) algorithm. In contrast, the second stage applies feature-based anomaly detection using a Diffusion Convolution Recurrent Neural Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level Agreements (SLA), trust-aware service migration is performed using Heap-Based Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots deceive attackers and extract attack patterns, which are securely stored using the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based Intrusion Detection Systems (IDS). The proposed framework is implemented in the NS3 simulation environment and evaluated against existing methods across multiple performance metrics, including accuracy, attack detection rate, false negative rate, precision, recall, ROC curve, memory usage, CPU usage, and execution time. Experimental results demonstrate that the framework significantly outperforms existing approaches, reinforcing the security of NGWN-enabled IoT ecosystems","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks,"This paper presents a novel approach to intrusion detection by integrating traditional signature-based methods with the contextual understanding capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become increasingly sophisticated, particularly in distributed, heterogeneous, and resource-constrained environments such as those enabled by the Internet of Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems (IDSs) becomes increasingly urgent. While traditional methods remain effective for detecting known threats, they often fail to recognize new and evolving attack patterns. In contrast, GPT-2 excels at processing unstructured data and identifying complex semantic relationships, making it well-suited to uncovering subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges the robustness of signature-based techniques with the adaptability of GPT-2-driven semantic analysis. Experimental evaluations on a representative intrusion dataset demonstrate that our model enhances detection accuracy by 6.3%, reduces false positives by 9.0%, and maintains near real-time responsiveness. These results affirm the potential of language model integration to build intelligent, scalable, and resilient cybersecurity defences suited for modern connected environments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Multi-stage Attack Detection and Prediction Using Graph Neural Networks: An IoT Feasibility Study,"With the ever-increasing reliance on digital networks for various aspects of modern life, ensuring their security has become a critical challenge. Intrusion Detection Systems play a crucial role in ensuring network security, actively identifying and mitigating malicious behaviours. However, the relentless advancement of cyber-threats has rendered traditional/classical approaches insufficient in addressing the sophistication and complexity of attacks. This paper proposes a novel 3-stage intrusion detection system inspired by a simplified version of the Lockheed Martin cyber kill chain to detect advanced multi-step attacks. The proposed approach consists of three models, each responsible for detecting a group of attacks with common characteristics. The detection outcome of the first two stages is used to conduct a feasibility study on the possibility of predicting attacks in the third stage. Using the ToN IoT dataset, we achieved an average of 94% F1-Score among different stages, outperforming the benchmark approaches based on Random-forest model. Finally, we comment on the feasibility of this approach to be integrated in a real-world system and propose various possible future work.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Survey on the Applications of Blockchains in Security of IoT Systems,"The Internet of Things (IoT) has already changed our daily lives by integrating smart devices together towards delivering high quality services to its clients. These devices when integrated together form a network through which massive amount of data can be produced, transferred, and shared. A critical concern is the security and integrity of such a complex platform to ensure the sustainability and reliability of these IoT-based systems. Blockchain is an emerging technology that has demonstrated its unique features and capabilities for different problems and application domains including IoT-based systems. This survey paper reviews the adaptation of Blockchain in the context of IoT to represent how this technology is capable of addressing the integration and security problems of devices connected to IoT systems. The innovation of this survey is that we present a survey based upon the integration approaches and security issues of IoT data and discuss the role of Blockchain in connection with these issues.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security Risks Concerns of Generative AI in the IoT,"In an era where the Internet of Things (IoT) intersects increasingly with generative Artificial Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how generative AI drives innovation in IoT and we analyze the potential for data breaches when using generative AI and the misuse of generative AI technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providing insights into the future direction of these intertwined technologies.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
How Secure Is Your IoT Network?,"The proliferation of IoT devices in smart homes, hospitals, and enterprise networks is widespread and continuing to increase in a superlinear manner. With this unprecedented growth, how can one assess the security of an IoT network holistically? In this article, we explore two dimensions of security assessment, using vulnerability information of IoT devices and their underlying components ($\textit{compositional security scores}$) and SIEM logs captured from the communications and operations of such devices in a network ($\textit{dynamic activity metrics}$) to propose the notion of an $\textit{attack circuit}$. These measures are used to evaluate the security of IoT devices and the overall IoT network, demonstrating the effectiveness of attack circuits as practical tools for computing security metrics (exploitability, impact, and risk to confidentiality, integrity, and availability) of heterogeneous networks. We propose methods for generating attack circuits with input/output pairs constructed from CVEs using natural language processing (NLP) and with weights computed using standard security scoring procedures, as well as efficient optimization methods for evaluating attack circuits. Our system provides insight into possible attack paths an adversary may utilize based on their exploitability, impact, or overall risk. We have performed experiments on IoT networks to demonstrate the efficacy of the proposed techniques.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Detection and Prevention of New and Unknown Malware using Honeypots,"Security has become ubiquitous in every domain today as newly emerging malware pose an ever-increasing perilous threat to systems. Consequently, honeypots are fast emerging as an indispensible forensic tool for the analysis of malicious network traffic. Honeypots can be considered to be traps for hackers and intruders and are generally deployed complimentary to Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) in a network. They help system administrators perform a rigorous analysis of external and internal attacks on their networks. They are also used by security firms and research labs to capture the latest variants of malware. However, honeypots would serve a slightly different purpose in our proposed system. We intend to use honeypots for generating and broadcasting instant cures for new and unknown malware in a network. The cures which will be in the form of on-the-fly anti-malware signatures would spread in a fashion that is similar to the way malware spreads across networks. The most striking advantage of implementing this technology is that an effective initial control can be exercised on malware. Proposed system would be capable of providing cures for new fatal viruses which have not yet been discovered by prime security firms of the world.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Efficient Network Traffic Feature Sets for IoT Intrusion Detection,"The use of Machine Learning (ML) models in cybersecurity solutions requires high-quality data that is stripped of redundant, missing, and noisy information. By selecting the most relevant features, data integrity and model efficiency can be significantly improved. This work evaluates the feature sets provided by a combination of different feature selection methods, namely Information Gain, Chi-Squared Test, Recursive Feature Elimination, Mean Absolute Deviation, and Dispersion Ratio, in multiple IoT network datasets. The influence of the smaller feature sets on both the classification performance and the training time of ML models is compared, with the aim of increasing the computational efficiency of IoT intrusion detection. Overall, the most impactful features of each dataset were identified, and the ML models obtained higher computational efficiency while preserving a good generalization, showing little to no difference between the sets.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security,"The Internet of Things (IoT) integrates billions of smart devices that can communicate with one another with minimal human intervention. It is one of the fastest developing fields in the history of computing, with an estimated 50 billion devices by the end of 2020. On the one hand, IoT play a crucial role in enhancing several real-life smart applications that can improve life quality. On the other hand, the crosscutting nature of IoT systems and the multidisciplinary components involved in the deployment of such systems introduced new security challenges. Implementing security measures, such as encryption, authentication, access control, network security and application security, for IoT devices and their inherent vulnerabilities is ineffective. Therefore, existing security methods should be enhanced to secure the IoT system effectively. Machine learning and deep learning (ML/DL) have advanced considerably over the last few years, and machine intelligence has transitioned from laboratory curiosity to practical machinery in several important applications. Consequently, ML/DL methods are important in transforming the security of IoT systems from merely facilitating secure communication between devices to security-based intelligence systems. The goal of this work is to provide a comprehensive survey of ML /DL methods that can be used to develop enhanced security methods for IoT systems. IoT security threats that are related to inherent or newly introduced threats are presented, and various potential IoT system attack surfaces and the possible threats related to each surface are discussed. We then thoroughly review ML/DL methods for IoT security and present the opportunities, advantages and shortcomings of each method. We discuss the opportunities and challenges involved in applying ML/DL to IoT security. These opportunities and challenges can serve as potential future research directions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT Security Challenges and Mitigations: An Introduction,"The use of IoT in society is perhaps already ubiquitous, with a vast attack surface offering multiple opportunities for malicious actors. This short paper first presents an introduction to IoT and its security issues, including an overview of IoT layer models and topologies, IoT standardisation efforts and protocols. The focus then moves to IoT vulnerabilities and specific suggestions for mitigations. This work's intended audience are those relatively new to IoT though with existing network-related knowledge. It is concluded that device resource constraints and a lack of IoT standards are significant issues. Research opportunities exist to develop efficient IoT IDS and energy-saving cryptography techniques lightweight enough to reasonably deploy. The need for standardised protocols and channel-based security solutions is clear, underpinned by legislative directives to ensure high standards that prevent cost-cutting on the device manufacturing side.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Unleashing IoT Security: Assessing the Effectiveness of Best Practices in Protecting Against Threats,"The Internet of Things (IoT) market is rapidly growing and is expected to double from 2020 to 2025. The increasing use of IoT devices, particularly in smart homes, raises crucial concerns about user privacy and security as these devices often handle sensitive and critical information. Inadequate security designs and implementations by IoT vendors can lead to significant vulnerabilities.   To address these IoT device vulnerabilities, institutions, and organizations have published IoT security best practices (BPs) to guide manufacturers in ensuring the security of their products. However, there is currently no standardized approach for evaluating the effectiveness of individual BP recommendations. This leads to manufacturers investing effort in implementing less effective BPs while potentially neglecting measures with greater impact.   In this paper, we propose a methodology for evaluating the security impact of IoT BPs and ranking them based on their effectiveness in protecting against security threats. Our approach involves translating identified BPs into concrete test cases that can be applied to real-world IoT devices to assess their effectiveness in mitigating vulnerabilities. We applied this methodology to evaluate the security impact of nine commodity IoT products, discovering 18 vulnerabilities. By empirically assessing the actual impact of BPs on device security, IoT designers and implementers can prioritize their security investments more effectively, improving security outcomes and optimizing limited security budgets.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
DoT: A Federated Self-learning Anomaly Detection System for IoT,"IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present DoT, an autonomous self-learning distributed system for detecting compromised IoT devices effectively. In contrast to prior work, DoT uses a novel self-learning approach to classify devices into device types and build normal communication profiles for each of these that can subsequently be used to detect anomalous deviations in communication patterns. DoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to anomaly-detection-based intrusion detection. Consequently, DoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that DoT is highly effective (95.6% detection rate) and fast (~257 ms) at detecting devices compromised by, for instance, the infamous Mirai malware. DoT reported no false alarms when evaluated in a real-world smart home deployment setting.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing IoT Applications using Blockchain: A Survey,"The Internet of Things (IoT) has become a guiding technology behind automation and smart computing. One of the major concerns with the IoT systems is the lack of privacy and security preserving schemes for controlling access and ensuring the security of the data. A majority of security issues arise because of the centralized architecture of IoT systems. Another concern is the lack of proper authentication and access control schemes to moderate access to information generated by the IoT devices. So the question that arises is how to ensure the identity of the equipment or the communicating node. The answer to secure operations in a trustless environment brings us to the decentralized solution of Blockchain. A lot of research has been going on in the area of convergence of IoT and Blockchain, and it has resulted in some remarkable progress in addressing some of the significant issues in the IoT arena. This work reviews the challenges and threats in the IoT environment and how integration with Blockchain can resolve some of them.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Review of Critical Infrastructure Protection Approaches: Improving Security through Responsiveness to the Dynamic Modelling Landscape,"As new technologies such as the Internet of Things (IoT) are integrated into Critical National Infrastructures (CNI), new cybersecurity threats emerge that require specific security solutions. Approaches used for analysis include the modelling and simulation of critical infrastructure systems using attributes, functionalities, operations, and behaviours to support various security analysis viewpoints, recognising and appropriately managing associated security risks. With several critical infrastructure protection approaches available, the question of how to effectively model the complex behaviour of interconnected CNI elements and to configure their protection as a system-of-systems remains a challenge. Using a systematic review approach, existing critical infrastructure protection approaches (tools and techniques) are examined to determine their suitability given trends like IoT, and effective security modelling and analysis issues. It is found that empirical-based, agent-based, system dynamics-based, and network-based modelling are more commonly applied than economic-based and equation-based techniques, and empirical-based modelling is the most widely used. The energy and transportation critical infrastructure sectors reflect the most responsive sectors, and no one Critical Infrastructure Protection (CIP) approach - tool, technique, methodology or framework -- provides a fit-for-all capacity for all-round attribute modelling and simulation of security risks. Typically, deciding factors for CIP choices to adopt are often dominated by trade-offs between complexity of use and popularity of approach, as well as between specificity and generality of application in sectors.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Trust-based Approaches Towards Enhancing IoT Security: A Systematic Literature Review,"The continuous rise in the adoption of emerging technologies such as Internet of Things (IoT) by businesses has brought unprecedented opportunities for innovation and growth. However, due to the distinct characteristics of these emerging IoT technologies like real-time data processing, Self-configuration, interoperability, and scalability, they have also introduced some unique cybersecurity challenges, such as malware attacks, advanced persistent threats (APTs), DoS /DDoS (Denial of Service & Distributed Denial of Service attacks) and insider threats. As a result of these challenges, there is an increased need for improved cybersecurity approaches and efficient management solutions to ensure the privacy and security of communication within IoT networks. One proposed security approach is the utilization of trust-based systems and is the focus of this study. This research paper presents a systematic literature review on the Trust-based cybersecurity security approaches for IoT. A total of 23 articles were identified that satisfy the review criteria. We highlighted the common trust-based mitigation techniques in existence for dealing with these threats and grouped them into three major categories, namely: Observation-Based, Knowledge-Based & Cluster-Based systems. Finally, several open issues were highlighted, and future research directions presented.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture,"The ever-increasing security vulnerabilities in the Internet-of-Things (IoT) systems require improved threat detection approaches. This paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis, temporal support learning, and focused feature extraction. The proposed attention-based model benefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification accuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while maintaining high precision and recall across various scenarios. The proposed model's performance is further validated by key parameters, such as Mathews Correlation Coefficient and Cohen's kappa Correlation Coefficient. The close-to-ideal results for these parameters demonstrate the proposed model's ability to detect botnet attacks accurately and efficiently in practical settings and on unseen data. The proposed model proved to be a powerful defence mechanism for IoT networks to face emerging security challenges.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Robust Comparison of the KDDCup99 and NSL-KDD IoT Network Intrusion Detection Datasets Through Various Machine Learning Algorithms,"In recent years, as intrusion attacks on IoT networks have grown exponentially, there is an immediate need for sophisticated intrusion detection systems (IDSs). A vast majority of current IDSs are data-driven, which means that one of the most important aspects of this area of research is the quality of the data acquired from IoT network traffic. Two of the most cited intrusion detection datasets are the KDDCup99 and the NSL-KDD. The main goal of our project was to conduct a robust comparison of both datasets by evaluating the performance of various Machine Learning (ML) classifiers trained on them with a larger set of classification metrics than previous researchers. From our research, we were able to conclude that the NSL-KDD dataset is of a higher quality than the KDDCup99 dataset as the classifiers trained on it were on average 20.18% less accurate. This is because the classifiers trained on the KDDCup99 dataset exhibited a bias towards the redundancies within it, allowing them to achieve higher accuracies.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
New Era of Deeplearning-Based Malware Intrusion Detection: The Malware Detection and Prediction Based On Deep Learning,"With the development of artificial intelligence algorithms like deep learning models and the successful applications in many different fields, further similar trails of deep learning technology have been made in cyber security area. It shows the preferable performance not only in academic security research but also in industry practices when dealing with part of cyber security issues by deep learning methods compared to those conventional rules. Especially for the malware detection and classification tasks, it saves generous time cost and promotes the accuracy for a total pipeline of malware detection system. In this paper, we construct special deep neural network, ie, MalDeepNet (TB-Malnet and IB-Malnet) for malware dynamic behavior classification tasks. Then we build the family clustering algorithm based on deep learning and fulfil related testing. Except that, we also design a novel malware prediction model which could detect the malware coming in future through the Mal Generative Adversarial Network (Mal-GAN) implementation. All those algorithms present fairly considerable value in related datasets afterwards.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Rethinking IoT Security: A Protocol Based on Blockchain Smart Contracts for Secure and Automated IoT Deployments,"Proliferation of IoT devices in society demands a renewed focus on securing the use and maintenance of such systems. IoT-based systems will have a great impact on society and therefore such systems must have guaranteed resilience. We introduce cryptographic-based building blocks that strive to ensure that distributed IoT networks remain in a healthy condition throughout their lifecycle. Our presented solution utilizes deterministic and interlinked smart contracts on the Ethereum blockchain to enforce secured management and maintenance for hardened IoT devices. A key issue investigated is the protocol development for securing IoT device deployments and means for communicating securely with devices. By supporting values of openness, automation, and provenance, we can introduce novel means that reduce the threats of surveillance and theft, while also improving operator accountability and trust in IoT technology.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Lightweight Encryption for the Low Powered IoT Devices,The internet of things refers to the network of devices connected to the internet and can communicate with each other. The term things is to refer non-conventional devices that are usually not connected to the internet. The network of such devices or things is growing at an enormous rate. The security and privacy of the data flowing through these things is a major concern. The devices are low powered and the conventional encryption algorithms are not suitable to be employed on these devices. In this correspondence a survey of the contemporary lightweight encryption algorithms suitable for use in the IoT environment has been presented.,"cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Survey of the Security Challenges and Requirements for IoT Operating Systems,"The Internet of Things (IoT) is becoming an integral part of our modern lives as we converge towards a world surrounded by ubiquitous connectivity. The inherent complexity presented by the vast IoT ecosystem ends up in an insufficient understanding of individual system components and their interactions, leading to numerous security challenges. In order to create a secure IoT platform from the ground up, there is a need for a unifying operating system (OS) that can act as a cornerstone regulating the development of stable and secure solutions. In this paper, we present a classification of the security challenges stemming from the manifold aspects of IoT development. We also specify security requirements to direct the secure development of an unifying IoT OS to resolve many of those ensuing challenges. Survey of several modern IoT OSs confirm that while the developers of the OSs have taken many alternative approaches to implement security, we are far from engineering an adequately secure and unified architecture. More broadly, the study presented in this paper can help address the growing need for a secure and unified platform to base IoT development on and assure the safe, secure, and reliable operation of IoT in critical domains.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Effectiveness of Transformer Models on IoT Security Detection in StackOverflow Discussions,"The Internet of Things (IoT) is an emerging concept that directly links to the billions of physical items, or ""things"", that are connected to the Internet and are all gathering and exchanging information between devices and systems. However, IoT devices were not built with security in mind, which might lead to security vulnerabilities in a multi-device system. Traditionally, we investigated IoT issues by polling IoT developers and specialists. This technique, however, is not scalable since surveying all IoT developers is not feasible. Another way to look into IoT issues is to look at IoT developer discussions on major online development forums like Stack Overflow (SO). However, finding discussions that are relevant to IoT issues is challenging since they are frequently not categorized with IoT-related terms. In this paper, we present the ""IoT Security Dataset"", a domain-specific dataset of 7147 samples focused solely on IoT security discussions. As there are no automated tools to label these samples, we manually labeled them. We further employed multiple transformer models to automatically detect security discussions. Through rigorous investigations, we found that IoT security discussions are different and more complex than traditional security discussions. We demonstrated a considerable performance loss (up to 44%) of transformer models on cross-domain datasets when we transferred knowledge from a general-purpose dataset ""Opiner"", supporting our claim. Thus, we built a domain-specific IoT security detector with an F1-Score of 0.69. We have made the dataset public in the hope that developers would learn more about the security discussion and vendors would enhance their concerns about product security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Survey on Cross-Architectural IoT Malware Threat Hunting,"In recent years, the increase in non-Windows malware threats had turned the focus of the cybersecurity community. Research works on hunting Windows PE-based malwares are maturing, whereas the developments on Linux malware threat hunting are relatively scarce. With the advent of the Internet of Things (IoT) era, smart devices that are getting integrated into human life have become a hackers highway for their malicious activities. The IoT devices employ various Unix-based architectures that follow ELF (Executable and Linkable Format) as their standard binary file specification. This study aims at providing a comprehensive survey on the latest developments in cross-architectural IoT malware detection and classification approaches. Aided by a modern taxonomy, we discuss the feature representations, feature extraction techniques, and machine learning models employed in the surveyed works. We further provide more insights on the practical challenges involved in cross-architectural IoT malware threat hunting and discuss various avenues to instill potential future research.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing the Future: Proactive Threat Hunting for Sustainable IoT Ecosystems,"In the rapidly evolving landscape of the IoT, the security of connected devices has become a paramount concern. This paper explores the concept of proactive threat hunting as a pivotal strategy for enhancing the security and sustainability of IoT systems. Proactive threat hunting is an alternative to traditional reactive security measures that analyses IoT networks continuously and in advance to find and eliminate threats before they occure. By improving the security posture of IoT devices this approach significantly contributes to extending IoT operational lifespan and reduces environmental impact. By integrating security metrics similar to the Common Vulnerability Scoring System (CVSS) into consumer platforms, this paper argues that proactive threat hunting can elevate user awareness about the security of IoT devices. This has the potential to impact consumer choices and encourage a security-conscious mindset in both the manufacturing and user communities. Through a comprehensive analysis, this study demonstrates how proactive threat hunting can contribute to the development of a more secure, sustainable, and user-aware IoT ecosystem.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
BSAGIoT: A Bayesian Security Aspect Graph for Internet of Things (IoT),"IoT is a dynamic network of interconnected things that communicate and exchange data, where security is a significant issue. Previous studies have mainly focused on attack classifications and open issues rather than presenting a comprehensive overview on the existing threats and vulnerabilities. This knowledge helps analyzing the network in the early stages even before any attack takes place. In this paper, the researchers have proposed different security aspects and a novel Bayesian Security Aspects Dependency Graph for IoT (BSAGIoT) to illustrate their relations. The proposed BSAGIoT is a generic model applicable to any IoT network and contains aspects from five categories named data, access control, standard, network, and loss. This proposed Bayesian Security Aspect Graph (BSAG) presents an overview of the security aspects in any given IoT network. The purpose of BSAGIoT is to assist security experts in analyzing how a successful compromise and/or a failed breach could impact the overall security and privacy of the respective IoT network. In addition, root cause identification of security challenges, how they affect one another, their impact on IoT networks via topological sorting, and risk assessment could be achieved. Hence, to demonstrate the feasibility of the proposed method, experimental results with various scenarios has been presented, in which the security aspects have been quantified based on the network configurations. The results indicate the impact of the aspects on each other and how they could be utilized to mitigate and/or eliminate the security and privacy deficiencies in IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Cutting-Edge Deep Learning Method For Enhancing IoT Security,"There have been significant issues given the IoT, with heterogeneity of billions of devices and with a large amount of data. This paper proposed an innovative design of the Internet of Things (IoT) Environment Intrusion Detection System (or IDS) using Deep Learning-integrated Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. Our model, based on the CICIDS2017 dataset, achieved an accuracy of 99.52% in classifying network traffic as either benign or malicious. The real-time processing capability, scalability, and low false alarm rate in our model surpass some traditional IDS approaches and, therefore, prove successful for application in today's IoT networks. The development and the performance of the model, with possible applications that may extend to other related fields of adaptive learning techniques and cross-domain applicability, are discussed. The research involving deep learning for IoT cybersecurity offers a potent solution for significantly improving network security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
DNA Encoded Elliptic Curve Cryptography System for IoT Security,"In the field of Computer Science and Information Technology Internet of Things (IoT) is one of the emerging technologies. In IoT environment several devices are interconnected and transmit data among them. There may be some security vulnerability arise within the IoT environment. Till date, IoT has not been widely accepted due to its security flaws. Hence to keep the IoT environment most robust, we propose a stable security framework of IoT with Elliptic Curve Cryptography (ECC) using DNA Encoding. The ECC is most lightweight cryptography technique among other well known public key cryptography techniques. To increase encryption complexity, DNA encoding mechanism of DNA computing with ECC is preceded.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
SoK: How Not to Architect Your Next-Generation TEE Malware?,"Besides Intel's SGX technology, there are long-running discussions on how trusted computing technologies can be used to cloak malware. Past research showed example methods of malicious activities utilising Flicker, Trusted Platform Module, and recently integrating with enclaves. We observe two ambiguous methodologies of malware development being associated with SGX, and it is crucial to systematise their details. One methodology is to use the core SGX ecosystem to cloak malware; potentially affecting a large number of systems. The second methodology is to create a custom enclave not adhering to base assumptions of SGX, creating a demonstration code of malware behaviour with these incorrect assumptions; remaining local without any impact. We examine what malware aims to do in real-world scenarios and state-of-art techniques in malware evasion. We present multiple limitations of maintaining the SGX-assisted malware and evading it from anti-malware mechanisms. The limitations make SGX enclaves a poor choice for achieving a successful malware campaign. We systematise twelve misconceptions (myths) outlining how an overfit-malware using SGX weakens malware's existing abilities. We find the differences by comparing SGX assistance for malware with non-SGX malware (i.e., malware in the wild in our paper). We conclude that the use of hardware enclaves does not increase the preexisting attack surface, enables no new infection vector, and does not contribute any new methods to the stealthiness of malware.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Am I Infected? Lessons from Operating a Large-Scale IoT Security Diagnostic Service,"There is an expectation that users of home IoT devices will be able to secure those devices, but they may lack information about what they need to do. In February 2022, we launched a web service that scans users' IoT devices to determine how secure they are. The service aims to diagnose and remediate vulnerabilities and malware infections of IoT devices of Japanese users. This paper reports on findings from operating this service drawn from three studies: (1) the engagement of 114,747 users between February, 2022 - May, 2024; (2) a large-scale evaluation survey among service users (n=4,103), and; (3) an investigation and targeted survey (n=90) around the remediation actions of users of non-secure devices. During the operation, we notified 417 (0.36%) users that one or more of their devices were detected as vulnerable, and 171 (0.15%) users that one of their devices was infected with malware. The service found no issues for 99% of users. Still, 96% of all users evaluated the service positively, most often for it providing reassurance, being free of charge, and short diagnosis time. Of the 171 users with malware infections, 67 returned to the service later for a new check, with 59 showing improvement. Of the 417 users with vulnerable devices, 151 users revisited and re-diagnosed, where 75 showed improvement. We report on lessons learned, including a consideration of the capabilities that non-expert users will assume of a security scan.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
LNGate: Powering IoT with Next Generation Lightning Micro-payments using Threshold Cryptography,"Bitcoin has emerged as a revolutionary payment system with its decentralized ledger concept however it has significant problems such as high transaction fees and long confirmation times. Lightning Network (LN), which was introduced much later, solves most of these problems with an innovative concept called off-chain payments. With this advancement, Bitcoin has become an attractive venue to perform micro-payments which can also be adopted in many IoT applications (e.g. toll payments). Nevertheless, it is not feasible to host LN and Bitcoin on IoT devices due to the storage, memory, and processing requirements. Therefore, in this paper, we propose an efficient and secure protocol that enables an IoT device to use LN through an untrusted gateway node. The gateway hosts LN and Bitcoin nodes and can open & close LN channels, send LN payments on behalf of the IoT device. This delegation approach is powered by a (2,2)-threshold scheme that requires the IoT device and the LN gateway to jointly perform all LN operations which in turn secures both parties' funds. Specifically, we propose to thresholdize LN's Bitcoin public and private keys as well as its commitment points. With these and several other protocol level changes, IoT device is protected against revoked state broadcast, collusion, and ransom attacks. We implemented the proposed protocol by changing LN's source code and thoroughly evaluated its performance using a Raspberry Pi. Our evaluation results show that computational and communication delays associated with the protocol are negligible. To the best of our knowledge, this is the first work that implemented threshold cryptography in LN.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An Empirical Study of IoT Security Aspects at Sentence-Level in Developer Textual Discussions,"IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, ensuring the security of IoT devices is crucial. IoT devices can differ from traditional computing, thereby the design and implementation of proper security measures can be challenging in IoT devices. We observed that IoT developers discuss their security-related challenges in developer forums like Stack Overflow(SO). However, we find that IoT security discussions can also be buried inside non-security discussions in SO. In this paper, we aim to understand the challenges IoT developers face while applying security practices and techniques to IoT devices. We have two goals: (1) Develop a model that can automatically find security-related IoT discussions in SO, and (2) Study the model output to learn about IoT developer security-related challenges. First, we download 53K posts from SO that contain discussions about IoT. Second, we manually labeled 5,919 sentences from 53K posts as 1 or 0. Third, we use this benchmark to investigate a suite of deep learning transformer models. The best performing model is called SecBot. Fourth, we apply SecBot on the entire posts and find around 30K security related sentences. Fifth, we apply topic modeling to the security-related sentences. Then we label and categorize the topics. Sixth, we analyze the evolution of the topics in SO. We found that (1) SecBot is based on the retraining of the deep learning model RoBERTa. SecBot offers the best F1-Score of 0.935, (2) there are six error categories in misclassified samples by SecBot. SecBot was mostly wrong when the keywords/contexts were ambiguous (e.g., gateway can be a security gateway or a simple gateway), (3) there are 9 security topics grouped into three categories: Software, Hardware, and Network, and (4) the highest number of topics belongs to software security, followed by network security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
MADEA: A Malware Detection Architecture for IoT blending Network Monitoring and Device Attestation,"Internet-of-Things (IoT) devices are vulnerable to malware and require new mitigation techniques due to their limited resources. To that end, previous research has used periodic Remote Attestation (RA) or Traffic Analysis (TA) to detect malware in IoT devices. However, RA is expensive, and TA only raises suspicion without confirming malware presence. To solve this, we design MADEA, the first system that blends RA and TA to offer a comprehensive approach to malware detection for the IoT ecosystem. TA builds profiles of expected packet traces during benign operations of each device and then uses them to detect malware from network traffic in real-time. RA confirms the presence or absence of malware on the device. MADEA achieves 100% true positive rate. It also outperforms other approaches with 160x faster detection time. Finally, without MADEA, effective periodic RA can consume at least ~14x the amount of energy that a device needs in one hour.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Modern Analysis of Aging Machine Learning Based IoT Cybersecurity Methods,"Modern scientific advancements often contribute to the introduction and refinement of never-before-seen technologies. This can be quite the task for humans to maintain and monitor and as a result, our society has become reliant on machine learning to assist in this task. With new technology comes new methods and thus new ways to circumvent existing cyber security measures. This study examines the effectiveness of three distinct Internet of Things cyber security algorithms currently used in industry today for malware and intrusion detection: Random Forest (RF), Support-Vector Machine (SVM), and K-Nearest Neighbor (KNN). Each algorithm was trained and tested on the Aposemat IoT-23 dataset which was published in January 2020 with the earliest of captures from 2018 and latest from 2019. The RF, SVM, and KNN reached peak accuracies of 92.96%, 86.23%, and 91.48%, respectively, in intrusion detection and 92.27%, 83.52%, and 89.80% in malware detection. It was found all three algorithms are capable of being effectively utilized for the current landscape of IoT cyber security in 2021.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Performance Comparison of Intrusion Detection Systems and Application of Machine Learning to Snort System,"This study investigates the performance of two open source intrusion detection systems (IDSs) namely Snort and Suricata for accurately detecting the malicious traffic on computer networks. Snort and Suricata were installed on two different but identical computers and the performance was evaluated at 10 Gbps network speed. It was noted that Suricata could process a higher speed of network traffic than Snort with lower packet drop rate but it consumed higher computational resources. Snort had higher detection accuracy and was thus selected for further experiments. It was observed that the Snort triggered a high rate of false positive alarms. To solve this problem a Snort adaptive plug-in was developed. To select the best performing algorithm for Snort adaptive plug-in, an empirical study was carried out with different learning algorithms and Support Vector Machine (SVM) was selected. A hybrid version of SVM and Fuzzy logic produced a better detection accuracy. But the best result was achieved using an optimised SVM with firefly algorithm with FPR (false positive rate) as 8.6% and FNR (false negative rate) as 2.2%, which is a good result. The novelty of this work is the performance comparison of two IDSs at 10 Gbps and the application of hybrid and optimised machine learning algorithms to Snort.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Survey of Analysis Methods for Security and Safety verification in IoT Systems,"Internet of Things (IoT) has been rapidly growing in the past few years in all life disciplines. IoT provides automation and smart control to its users in different domains such as home automation, healthcare systems, automotive, and many more. Given the tremendous number of connected IoT devices, this growth leads to enormous automatic interactions among sizeable IoT apps in their environment, making IoT apps more intelligent and more enjoyable to their users. But some unforeseen interactions of IoT apps and any potential malicious behaviour can seriously cause insecure and unsafe consequences to its users, primarily non-experts, who lack the required knowledge regarding the potential impact of their IoT automation processes. In this paper, we study the problem of security and safety verification of IoT systems. We survey techniques that utilize program analysis to verify IoT applications' security and safety properties. The study proposes a set of categorization and classification attributes to enhance our understanding of the research landscape in this domain. Moreover, we discuss the main challenges considered in the surveyed work and potential solutions that could be adopted to ensure the security and safety of IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Critical Infrastructure Cybersecurity Challenges: IoT In Perspective,"A technology platform that is gradually bridging the gap between object visibility and remote accessibility is the Internet of Things (IoT). Rapid deployment of this application can significantly transform the health, housing, and power (distribution and generation) sectors, etc. It has considerably changed the power sector regarding operations, services optimization, power distribution, asset management and aided in engaging customers to reduce energy consumption. Despite its societal opportunities and the benefits it presents, the power generation sector is bedeviled with many security challenges on the critical infrastructure. This review discusses the security challenges posed by IoT in power generation and critical infrastructure. To achieve this, the authors present the various IoT applications, particularly on the grid infrastructure, from an empirical literature perspective. The authors concluded by discussing how the various entities in the sector can overcome these security challenges to ensure an exemplary future IoT implementation on the power critical infrastructure value chain.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoTGaze: IoT Security Enforcement via Wireless Context Analysis,"Internet of Things (IoT) has become the most promising technology for service automation, monitoring, and interconnection, etc. However, the security and privacy issues caused by IoT arouse concerns. Recent research focuses on addressing security issues by looking inside platform and apps. In this work, we creatively change the angle to consider security problems from a wireless context perspective. We propose a novel framework called IoTGaze, which can discover potential anomalies and vulnerabilities in the IoT system via wireless traffic analysis. By sniffing the encrypted wireless traffic, IoTGaze can automatically identify the sequential interaction of events between apps and devices. We discover the temporal event dependencies and generate the Wireless Context for the IoT system. Meanwhile, we extract the IoT Context, which reflects user's expectation, from IoT apps' descriptions and user interfaces. If the wireless context does not match the expected IoT context, IoTGaze reports an anomaly. Furthermore, IoTGaze can discover the vulnerabilities caused by the inter-app interaction via hidden channels, such as temperature and illuminance. We provide a proof-of-concept implementation and evaluation of our framework on the Samsung SmartThings platform. The evaluation shows that IoTGaze can effectively discover anomalies and vulnerabilities, thereby greatly enhancing the security of IoT systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoT and Man-in-the-Middle Attacks,"This paper provides an overview of the Internet of Things (IoT) and its significance. It discusses the concept of Man-in-the-Middle (MitM) attacks in detail, including their causes, potential solutions, and challenges in detecting and preventing such attacks. The paper also addresses the current issues related to IoT security and explores future methods and facilities for improving detection and prevention mechanisms against MitM.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Internet of Things Security, Device Authentication and Access Control: A Review","The Internet of Things (IoT) is one of the emerging technologies that has grabbed the attention of researchers from academia and industry. The idea behind Internet of things is the interconnection of internet enabled things or devices to each other and to humans, to achieve some common goals. In near future IoT is expected to be seamlessly integrated into our environment and human will be wholly solely dependent on this technology for comfort and easy life style. Any security compromise of the system will directly affect human life. Therefore security and privacy of this technology is foremost important issue to resolve. In this paper we present a thorough study of security problems in IoT and classify possible cyberattacks on each layer of IoT architecture. We also discuss challenges to traditional security solutions such as cryptographic solutions, authentication mechanisms and key management in IoT. Device authentication and access controls is an essential area of IoT security, which is not surveyed so far. We spent our efforts to bring the state of the art device authentication and access control techniques on a single paper.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Feature Extraction for Machine Learning-based Intrusion Detection in IoT Networks,"A large number of network security breaches in IoT networks have demonstrated the unreliability of current Network Intrusion Detection Systems (NIDSs). Consequently, network interruptions and loss of sensitive data have occurred, which led to an active research area for improving NIDS technologies. In an analysis of related works, it was observed that most researchers aim to obtain better classification results by using a set of untried combinations of Feature Reduction (FR) and Machine Learning (ML) techniques on NIDS datasets. However, these datasets are different in feature sets, attack types, and network design. Therefore, this paper aims to discover whether these techniques can be generalised across various datasets. Six ML models are utilised: a Deep Feed Forward (DFF), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Decision Tree (DT), Logistic Regression (LR), and Naive Bayes (NB). The accuracy of three Feature Extraction (FE) algorithms; Principal Component Analysis (PCA), Auto-encoder (AE), and Linear Discriminant Analysis (LDA), are evaluated using three benchmark datasets: UNSW-NB15, ToN-IoT and CSE-CIC-IDS2018. Although PCA and AE algorithms have been widely used, the determination of their optimal number of extracted dimensions has been overlooked. The results indicate that no clear FE method or ML model can achieve the best scores for all datasets. The optimal number of extracted dimensions has been identified for each dataset, and LDA degrades the performance of the ML models on two datasets. The variance is used to analyse the extracted dimensions of LDA and PCA. Finally, this paper concludes that the choice of datasets significantly alters the performance of the applied techniques. We believe that a universal (benchmark) feature set is needed to facilitate further advancement and progress of research in this field.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning,"As the number of IoT devices increases, security concerns become more prominent. The impact of threats can be minimized by deploying Network Intrusion Detection System (NIDS) by monitoring network traffic, detecting and discovering intrusions, and issuing security alerts promptly. Most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them, thus limiting the monitoring of complex IoT network attack events. Besides, anomalous traffic in real networks accounts for only a small fraction, which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult. In this paper, we propose an EG-ConMix method based on E-GraphSAGE, incorporating a data augmentation module to fix the problem of data imbalance. In addition, we incorporate contrastive learning to discern the difference between normal and malicious traffic samples, facilitating the extraction of key features. Extensive experiments on two publicly available datasets demonstrate the superior intrusion detection performance of EG-ConMix compared to state-of-the-art methods. Remarkably, it exhibits significant advantages in terms of training speed and accuracy for large-scale graphs.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Efficient Integration of Blockchain for IoT Security: The Case Study of IoT Remote Access,"The booming Internet of Things (IoT) market has drawn tremendous interest from cyber attackers. The centralized cloud-based IoT service architecture has serious limitations in terms of security, availability, and scalability, and is subject to single points of failure (SPOF). Recently, accommodating IoT services on blockchains has become a trend for better security, privacy, and reliability. However, blockchain's shortcomings of high cost, low throughput, and long latency make it unsuitable for IoT applications. In this paper, we take a retrospection of existing blockchain-based IoT solutions and propose a framework for efficient blockchain and IoT integration. Following the framework, we design a novel blockchain-assisted decentralized IoT remote accessing system, RS-IoT, which has the advantage of defending IoT devices against zero-day attacks without relying on any trusted third-party. By introducing incentives and penalties enforced by smart contracts, our work enables ""an economic approach"" to thwarting the majority of attackers who aim to achieve monetary gains. Our work presents an example of how blockchain can be used to ensure the fairness of service trading in a decentralized environment and punish misbehaviors objectively. We show the security of RS-IoT via detailed security analyses. Finally, we demonstrate its scalability, efficiency, and usability through a proof-of-concept implementation on the Ethereum testnet blockchain.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Quantifying and Managing Impacts of Concept Drifts on IoT Traffic Inference in Residential ISP Networks,"Millions of vulnerable consumer IoT devices in home networks are the enabler for cyber crimes putting user privacy and Internet security at risk. Internet service providers (ISPs) are best poised to play key roles in mitigating risks by automatically inferring active IoT devices per household and notifying users of vulnerable ones. Developing a scalable inference method that can perform robustly across thousands of home networks is a non-trivial task. This paper focuses on the challenges of developing and applying data-driven inference models when labeled data of device behaviors is limited and the distribution of data changes (concept drift) across time and space domains. Our contributions are three-fold: (1) We collect and analyze network traffic of 24 types of consumer IoT devices from 12 real homes over six weeks to highlight the challenge of temporal and spatial concept drifts in network behavior of IoT devices; (2) We analyze the performance of two inference strategies, namely ""global inference"" (a model trained on a combined set of all labeled data from training homes) and ""contextualized inference"" (several models each trained on the labeled data from a training home) in the presence of concept drifts; and (3) To manage concept drifts, we develop a method that dynamically applies the ``closest'' model (from a set) to network traffic of unseen homes during the testing phase, yielding better performance in 20% of scenarios.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems,"Intrusion detection systems (IDSs) play a critical role in protecting billions of IoT devices from malicious attacks. However, the IDSs for IoT devices face inherent challenges of IoT systems, including the heterogeneity of IoT data/devices, the high dimensionality of training data, and the imbalanced data. Moreover, the deployment of IDSs on IoT systems is challenging, and sometimes impossible, due to the limited resources such as memory/storage and computing capability of typical IoT devices. To tackle these challenges, this article proposes a novel deep neural network/architecture called Constrained Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with more separable/distinguishable and lower-dimensional representation data. Additionally, in comparison to the state-of-the-art neural networks used in IDSs, CTVAE requires less memory/storage and computing power, hence making it more suitable for IoT IDS systems. Extensive experiments with the 11 most popular IoT botnet datasets show that CTVAE can boost around 1% in terms of accuracy and Fscore in detection attack compared to the state-of-the-art machine learning and representation learning methods, whilst the running time for attack detection is lower than 2E-6 seconds and the model size is lower than 1 MB. We also further investigate various characteristics of CTVAE in the latent space and in the reconstruction representation to demonstrate its efficacy compared with current well-known methods.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Quantitative Analysis of DoS Attacks and Client Puzzles in IoT Systems,"Denial of Service (DoS) attacks constitute a major security threat to today's Internet. This challenge is especially pertinent to the Internet of Things (IoT) as devices have less computing power, memory and security mechanisms to mitigate DoS attacks. This paper presents a model that mimics the unique characteristics of a network of IoT devices, including components of the system implementing `Crypto Puzzles' - a DoS mitigation technique. We created an imitation of a DoS attack on the system, and conducted a quantitative analysis to simulate the impact such an attack may potentially exert upon the system, assessing the trade off between security and throughput in the IoT system. We model this through stochastic model checking in PRISM and provide evidence that supports this as a valuable method to compare the efficiency of different implementations of IoT systems, exemplified by a case study.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Using HTML5 to Prevent Detection of Drive-by-Download Web Malware,"The web is experiencing an explosive growth in the last years. New technologies are introduced at a very fast-pace with the aim of narrowing the gap between web-based applications and traditional desktop applications. The results are web applications that look and feel almost like desktop applications while retaining the advantages of being originated from the web. However, these advancements come at a price. The same technologies used to build responsive, pleasant and fully-featured web applications, can also be used to write web malware able to escape detection systems. In this article we present new obfuscation techniques, based on some of the features of the upcoming HTML5 standard, which can be used to deceive malware detection systems. The proposed techniques have been experimented on a reference set of obfuscated malware. Our results show that the malware rewritten using our obfuscation techniques go undetected while being analyzed by a large number of detection systems. The same detection systems were able to correctly identify the same malware in its original unobfuscated form. We also provide some hints about how the existing malware detection systems can be modified in order to cope with these new techniques.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Software-based Security Framework for Edge and Mobile IoT,"With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative. Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring. This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency. The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources. Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers. This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Lightweight Moving Target Defense Framework for Multi-purpose Malware Affecting IoT Devices,"Malware affecting Internet of Things (IoT) devices is rapidly growing due to the relevance of this paradigm in real-world scenarios. Specialized literature has also detected a trend towards multi-purpose malware able to execute different malicious actions such as remote control, data leakage, encryption, or code hiding, among others. Protecting IoT devices against this kind of malware is challenging due to their well-known vulnerabilities and limitation in terms of CPU, memory, and storage. To improve it, the moving target defense (MTD) paradigm was proposed a decade ago and has shown promising results, but there is a lack of IoT MTD solutions dealing with multi-purpose malware. Thus, this work proposes four MTD mechanisms changing IoT devices' network, data, and runtime environment to mitigate multi-purpose malware. Furthermore, it presents a lightweight and IoT-oriented MTD framework to decide what, when, and how the MTD mechanisms are deployed. Finally, the efficiency and effectiveness of the framework and MTD mechanisms are evaluated in a real-world scenario with one IoT spectrum sensor affected by multi-purpose malware.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security and Privacy in IoT Using Machine Learning and Blockchain: Threats & Countermeasures,"Security and privacy of the users have become significant concerns due to the involvement of the Internet of things (IoT) devices in numerous applications. Cyber threats are growing at an explosive pace making the existing security and privacy measures inadequate. Hence, everyone on the Internet is a product for hackers. Consequently, Machine Learning (ML) algorithms are used to produce accurate outputs from large complex databases, where the generated outputs can be used to predict and detect vulnerabilities in IoT-based systems. Furthermore, Blockchain (BC) techniques are becoming popular in modern IoT applications to solve security and privacy issues. Several studies have been conducted on either ML algorithms or BC techniques. However, these studies target either security or privacy issues using ML algorithms or BC techniques, thus posing a need for a combined survey on efforts made in recent years addressing both security and privacy issues using ML algorithms and BC techniques. In this paper, we provide a summary of research efforts made in the past few years, starting from 2008 to 2019, addressing security and privacy issues using ML algorithms and BCtechniques in the IoT domain. First, we discuss and categorize various security and privacy threats reported in the past twelve years in the IoT domain. Then, we classify the literature on security and privacy efforts based on ML algorithms and BC techniques in the IoT domain. Finally, we identify and illuminate several challenges and future research directions in using ML algorithms and BC techniques to address security and privacy issues in the IoT domain.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Holo-Block Chain: A Hybrid Approach for Secured IoT Healthcare Ecosystem,"The Internet-of-Things (IoT) is an imminent and corporal technology that enables the connectivity of smart physical devices with virtual objects contriving in distinct platforms with the help of the internet. The IoT is under massive experimentation to operate in a distributed manner, making it favourable to be utilized in the healthcare ecosystem. However, un- der the IoT healthcare ecosystem (IoT-HS), the nodes of the IoT networks are unveiled to an aberrant level of security threats. Regulating an adequate volume of sensitive and personal data, IoT-HS undergoes various security challenges for which a distributed mechanism to address such concerns plays a vital role. Although Blockchain, having a distributed ledger, is integral to solving security concerns in IoT-HSs, it undergoes major problems, including massive storage and computational requirements. Also, Holochain, which has low computational and memory requirements, lacks authentication distribution availability. Therefore, this paper proposes a hybrid Holochain and Blockchain-based privacy perseverance and security framework for IoT-HSs that combines the benefits Holochain and Blockchain provide, overcoming the computational, memory, and authentication challenges. This framework is more suited for IoT scenarios where resource needs to be optimally utilized. Comprehensive security and performance analysis is conducted to demonstrate the suitability and effectiveness of the proposed hybrid security approach for IoT-HSs in contrast to the Blockchain-only or Holochain-only based approaches.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Federated PCA on Grassmann Manifold for IoT Anomaly Detection,"With the proliferation of the Internet of Things (IoT) and the rising interconnectedness of devices, network security faces significant challenges, especially from anomalous activities. While traditional machine learning-based intrusion detection systems (ML-IDS) effectively employ supervised learning methods, they possess limitations such as the requirement for labeled data and challenges with high dimensionality. Recent unsupervised ML-IDS approaches such as AutoEncoders and Generative Adversarial Networks (GAN) offer alternative solutions but pose challenges in deployment onto resource-constrained IoT devices and in interpretability. To address these concerns, this paper proposes a novel federated unsupervised anomaly detection framework, FedPCA, that leverages Principal Component Analysis (PCA) and the Alternating Directions Method Multipliers (ADMM) to learn common representations of distributed non-i.i.d. datasets. Building on the FedPCA framework, we propose two algorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our approach enables real-time threat detection and mitigation at the device level, enhancing network resilience while ensuring privacy. Moreover, the proposed algorithms are accompanied by theoretical convergence rates even under a subsampling scheme, a novel result. Experimental results on the UNSW-NB15 and TON-IoT datasets show that our proposed methods offer performance in anomaly detection comparable to nonlinear baselines, while providing significant improvements in communication and memory efficiency, underscoring their potential for securing IoT networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Wireguard: An Efficient Solution for Securing IoT Device Connectivity,"The proliferation of vulnerable Internet-of-Things (IoT) devices has enabled large-scale cyberattacks. Solutions like Hestia and HomeSnitch have failed to comprehensively address IoT security needs. This research evaluates if Wireguard, an emerging VPN protocol, can provide efficient security tailored for resource-constrained IoT systems. We compared Wireguards performance against standard protocols OpenVPN and IPsec in a simulated IoT environment. Metrics measured included throughput, latency, and jitter during file transfers. Initial results reveal Wireguard's potential as a lightweight yet robust IoT security solution despite disadvantages for Wireguard in our experimental environment. With further testing, Wireguards simplicity and low overhead could enable widespread VPN adoption to harden IoT devices against attacks. The protocols advantages in setup time, performance, and compatibility make it promising for integration especially on weak IoT processors and networks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap,"The Internet of Things (IoT) has rapidly expanded across various sectors, with consumer IoT devices - such as smart thermostats and security cameras - experiencing growth. Although these devices improve efficiency and promise additional comfort, they also introduce new security challenges. Common and easy-to-explore vulnerabilities make IoT devices prime targets for malicious actors. Upcoming mandatory security certifications offer a promising way to mitigate these risks by enforcing best practices and providing transparency. Regulatory bodies are developing IoT security frameworks, but a universal standard for large-scale systematic security assessment is lacking. Existing manual testing approaches are expensive, limiting their efficacy in the diverse and rapidly evolving IoT domain. This paper reviews current IoT security challenges and assessment efforts, identifies gaps, and proposes a roadmap for scalable, automated security assessment, leveraging a model-based testing approach and machine learning techniques to strengthen consumer IoT security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
CND-IDS: Continual Novelty Detection for Intrusion Detection Systems,"Intrusion detection systems (IDS) play a crucial role in IoT and network security by monitoring system data and alerting to suspicious activities. Machine learning (ML) has emerged as a promising solution for IDS, offering highly accurate intrusion detection. However, ML-IDS solutions often overlook two critical aspects needed to build reliable systems: continually changing data streams and a lack of attack labels. Streaming network traffic and associated cyber attacks are continually changing, which can degrade the performance of deployed ML models. Labeling attack data, such as zero-day attacks, in real-world intrusion scenarios may not be feasible, making the use of ML solutions that do not rely on attack labels necessary. To address both these challenges, we propose CND-IDS, a continual novelty detection IDS framework which consists of (i) a learning-based feature extractor that continuously updates new feature representations of the system data, and (ii) a novelty detector that identifies new cyber attacks by leveraging principal component analysis (PCA) reconstruction. Our results on realistic intrusion datasets show that CND-IDS achieves up to 6.1x F-score improvement, and up to 6.5x improved forward transfer over the SOTA unsupervised continual learning algorithm. Our code will be released upon acceptance.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Unified View of IoT And CPS Security and Privacy,"The concepts of Internet of Things (IoT) and Cyber Physical Systems (CPS) are closely related to each other. IoT is often used to refer to small interconnected devices like those in smart home while CPS often refers to large interconnected devices like industry machines and smart cars. In this paper, we present a unified view of IoT and CPS: from the perspective of network architecture, IoT and CPS are similar given that they are based on either the OSI model or TCP/IP model. In both IoT and CPS, networking/communication modules are attached to original things so that isolated things can be integrated into cyber space. If needed, actuators can also be integrated with a thing so as to control the thing. With this unified view, we can perform risk assessment of an IoT/CPS system from six factors, hardware, networking, operating system (OS), software, data and human. To illustrate the use of such risk analysis framework, we analyze an air quality monitoring network, smart home using smart plugs and building automation system (BAS). We also discuss challenges such as cost and secure OS in IoT security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Exploring Edge TPU for Network Intrusion Detection in IoT,"This paper explores Google's Edge TPU for implementing a practical network intrusion detection system (NIDS) at the edge of IoT, based on a deep learning approach. While there are a significant number of related works that explore machine learning based NIDS for the IoT edge, they generally do not consider the issue of the required computational and energy resources. The focus of this paper is the exploration of deep learning-based NIDS at the edge of IoT, and in particular the computational and energy efficiency. In particular, the paper studies Google's Edge TPU as a hardware platform, and considers the following three key metrics: computation (inference) time, energy efficiency and the traffic classification performance. Various scaled model sizes of two major deep neural network architectures are used to investigate these three metrics. The performance of the Edge TPU-based implementation is compared with that of an energy efficient embedded CPU (ARM Cortex A53). Our experimental evaluation shows some unexpected results, such as the fact that the CPU significantly outperforms the Edge TPU for small model sizes.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Blockchain Enabled Enhanced IoT Ecosystem Security,"Blockchain (BC), the technology behind the Bitcoin cryptocurrency system, is starting to be adopted for ensuring enhanced security and privacy in the Internet of Things (IoT) ecosystem. Fervent research is currently being focused in both academia and industry in this domain. Proof of Work (PoW), a cryptographic puzzle, plays a vital role in ensuring BC security by maintaining a digital ledger of transactions, which are considered to be incorruptible. Furthermore, BC uses a changeable Public Key (PK) to record the identity of users, thus providing an extra layer of privacy. Not only in cryptocurrency has the successful adoption of the BC been implemented, but also in multifaceted non-monetary systems, such as in: distributed storage systems, proof of location and healthcare. Recent research articles and projects or applications were surveyed to assess the implementation of the BC for IoT Security and identify associated challenges and propose solutions for BC enabled enhanced security for the IoT ecosystem.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Autonomous Adaptive Security Framework for 5G-Enabled IoT,"In IoT-based critical sectors, 5G can provide more rapid connection speeds, lower latency, faster downloads, and capability to connect more devices due to the introduction of new dynamics such as softwarization and virtualization. 5G-enabled IoT networks increase systems vulnerabilities to security threats due to these dynamics. Consequently, adaptive cybersecurity solutions need to be developed for 5G-enabled IoT applications to protect them against potential cyber-attacks. This task specifies new adaptive strategies of security intelligence with associated scenarios to meet the challenges of 5G-IoT characteristics. In this task we have also developed an autonomous adaptive security framework which can protect 5G-enabaled IoT dynamically and autonomously. The framework is based on a closed feedback loop of advanced analytics to monitor, analyse, and adapt to evolving threats to 5G-enanled IoT applications.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Review on the Security Threats of Internet of Things,"Internet of Things (IoT) is being considered as the growth engine for industrial revolution 4.0. The combination of IoT, cloud computing and healthcare can contribute in ensuring well-being of people. One important challenge of IoT network is maintaining privacy and to overcome security threats. This paper provides a systematic review of the security aspects of IoT. Firstly, the application of IoT in industrial and medical service scenarios are described, and the security threats are discussed for the different layers of IoT healthcare architecture. Secondly, different types of existing malware including spyware, viruses, worms, keyloggers, and trojan horses are described in the context of IoT. Thirdly, some of the recent malware attacks such as Mirai, echobot and reaper are discussed. Next, a comparative discussion is presented on the effectiveness of different machine learning algorithms in mitigating the security threats. It is found that the k-nearest neighbor (kNN) machine learning algorithm exhibits excellent accuracy in detecting malware. This paper also reviews different tools for ransomware detection, classification and analysis. Finally, a discussion is presented on the existing security issues, open challenges and possible future scopes in ensuring IoT security.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Reliable Malware Analysis and Detection using Topology Data Analysis,"Increasingly, malwares are becoming complex and they are spreading on networks targeting different infrastructures and personal-end devices to collect, modify, and destroy victim information. Malware behaviors are polymorphic, metamorphic, persistent, able to hide to bypass detectors and adapt to new environments, and even leverage machine learning techniques to better damage targets. Thus, it makes them difficult to analyze and detect with traditional endpoint detection and response, intrusion detection and prevention systems. To defend against malwares, recent work has proposed different techniques based on signatures and machine learning. In this paper, we propose to use an algebraic topological approach called topological-based data analysis (TDA) to efficiently analyze and detect complex malware patterns. Next, we compare the different TDA techniques (i.e., persistence homology, tomato, TDA Mapper) and existing techniques (i.e., PCA, UMAP, t-SNE) using different classifiers including random forest, decision tree, xgboost, and lightgbm. We also propose some recommendations to deploy the best-identified models for malware detection at scale. Results show that TDA Mapper (combined with PCA) is better for clustering and for identifying hidden relationships between malware clusters compared to PCA. Persistent diagrams are better to identify overlapping malware clusters with low execution time compared to UMAP and t-SNE. For malware detection, malware analysts can use Random Forest and Decision Tree with t-SNE and Persistent Diagram to achieve better performance and robustness on noised data.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
MalIoT: Scalable and Real-time Malware Traffic Detection for IoT Networks,"The machine learning approach is vital in Internet of Things (IoT) malware traffic detection due to its ability to keep pace with the ever-evolving nature of malware. Machine learning algorithms can quickly and accurately analyze the vast amount of data produced by IoT devices, allowing for the real-time identification of malicious network traffic. The system can handle the exponential growth of IoT devices thanks to the usage of distributed systems like Apache Kafka and Apache Spark, and Intel's oneAPI software stack accelerates model inference speed, making it a useful tool for real-time malware traffic detection. These technologies work together to create a system that can give scalable performance and high accuracy, making it a crucial tool for defending against cyber threats in smart communities and medical institutions.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Network Intrusion Detection System in a Light Bulb,"Internet of Things (IoT) devices are progressively being utilised in a variety of edge applications to monitor and control home and industry infrastructure. Due to the limited compute and energy resources, active security protections are usually minimal in many IoT devices. This has created a critical security challenge that has attracted researchers' attention in the field of network security. Despite a large number of proposed Network Intrusion Detection Systems (NIDSs), there is limited research into practical IoT implementations, and to the best of our knowledge, no edge-based NIDS has been demonstrated to operate on common low-power chipsets found in the majority of IoT devices, such as the ESP8266. This research aims to address this gap by pushing the boundaries on low-power Machine Learning (ML) based NIDSs. We propose and develop an efficient and low-power ML-based NIDS, and demonstrate its applicability for IoT edge applications by running it on a typical smart light bulb. We also evaluate our system against other proposed edge-based NIDSs and show that our model has a higher detection performance, and is significantly faster and smaller, and therefore more applicable to a wider range of IoT edge devices.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Risk and Threat Mitigation Techniques in Internet of Things (IoT) Environments: A Survey,"Security in the Internet of Things (IoT) remains a predominant area of concern. This survey updates the state of the art covered in previous surveys and focuses on defending against threats rather than on the threats alone. This area is less extensively covered by other surveys and warrants particular attention. A life-cycle approach is adopted, articulated to form a ""defence in depth"" strategy against malicious actors compromising an IoT network laterally within it and from it. This study highlights the challenges of each mitigation step, emphasises novel perspectives, and reconnects the discussed mitigation steps to the ground principles they seek to implement.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Soteria: Automated IoT Safety and Security Analysis,"Broadly defined as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital systems have changed the way we live, play and work. Yet existing IoT platforms cannot evaluate whether an IoT app or environment is safe, secure, and operates correctly. In this paper, we present Soteria, a static analysis system for validating whether an IoT app or IoT environment (collection of apps working in concert) adheres to identified safety, security, and functional properties. Soteria operates in three phases; (a) translation of platform-specific IoT source code into an intermediate representation (IR), (b) extracting a state model from the IR, (c) applying model checking to verify desired properties. We evaluate Soteria on 65 SmartThings market apps through 35 properties and find nine (14%) individual apps violate ten (29%) properties. Further, our study of combined app environments uncovered eleven property violations not exhibited in the isolated apps. Lastly, we demonstrate Soteria on MalIoT, a novel open-source test suite containing 17 apps with 20 unique violations.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Optimal Control of Malware Propagation in IoT Networks,"The rapid proliferation of Internet of Things (IoT) devices in recent years has resulted in a significant surge in the number of cyber-attacks targeting these devices. Recent data indicates that the number of such attacks has increased by over 100 percent, highlighting the urgent need for robust cybersecurity measures to mitigate these threats. In addition, a cyber-attack will begin to spread malware across the network once it has successfully compromised an IoT network. However, to mitigate this attack, a new patch must be applied immediately. In reality, the time required to prepare and apply the new patch can vary significantly depending on the nature of the cyber-attack. In this paper, we address the issue of how to mitigate cyber-attacks before the new patch is applied by formulating an optimal control strategy that reduces the impact of malware propagation and minimise the number of infected devices across IoT networks in the smart home. A novel node-based epidemiological model susceptible, infected high, infected low, recover first, and recover complete(SI_HI_LR_FR_C) is established with immediate response state for the restricted environment. After that, the impact of malware on IoT devices using both high and low infected rates will be analyzed. Finally, to illustrate the main results, several numerical analyses are carried out in addition to simulate the real-world scenario of IoT networks in the smart home, we built a dataset to be used in the experiments.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
On the Security of Permissioned Blockchain Solutions for IoT Applications,"The blockchain has found numerous applications in many areas with the expectation to significantly enhance their security. The Internet of things (IoT) constitutes a prominent application domain of blockchain, with a number of architectures having been proposed for improving not only security but also properties like transparency and auditability. However, many blockchain solutions suffer from inherent constraints associated with the consensus protocol used. These constraints are mostly inherited by the permissionless setting, e.g. computational power in proof-of-work, and become serious obstacles in a resource-constrained IoT environment. Moreover, consensus protocols with low throughput or high latency are not suitable for IoT networks where massive volumes of data are generated. Thus, in this paper we focus on permissioned blockchain platforms and investigate the consensus protocols used, aiming at evaluating their performance and fault tolerance as the main selection criteria for (in principle highly insecure) IoT ecosystem. The results of the paper provide new insights on the essential differences of various consensus protocols and their capacity to meet IoT needs.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Efficient and Secure Cross-Domain Data-Sharing for Resource-Constrained Internet of Things,"The growing complexity of Internet of Things (IoT) environments, particularly in cross-domain data sharing, presents significant security challenges. Existing data-sharing schemes often rely on computationally expensive cryptographic operations and centralized key management, limiting their effectiveness for resource-constrained devices. To address these issues, we propose an efficient, secure blockchain-based data-sharing scheme. First, our scheme adopts a distributed key generation method, which avoids single point of failure. This method also allows independent pseudonym generation and key updates, enhancing authentication flexibility while reducing computational overhead. Additionally, the scheme provides a complete data-sharing process, covering data uploading, storage, and sharing, while ensuring data traceability, integrity, and privacy. Security analysis shows that the proposed scheme is theoretically secure and resistant to various attacks, while performance evaluations demonstrate lower computational and communication overhead compared to existing solutions, making it both secure and efficient for IoT applications.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Social Media Monitoring for IoT Cyber-Threats,"The rapid development of IoT applications and their use in various fields of everyday life has resulted in an escalated number of different possible cyber-threats, and has consequently raised the need of securing IoT devices. Collecting Cyber-Threat Intelligence (e.g., zero-day vulnerabilities or trending exploits) from various online sources and utilizing it to proactively secure IoT systems or prepare mitigation scenarios has proven to be a promising direction. In this work, we focus on social media monitoring and investigate real-time Cyber-Threat Intelligence detection from the Twitter stream. Initially, we compare and extensively evaluate six different machine-learning based classification alternatives trained with vulnerability descriptions and tested with real-world data from the Twitter stream to identify the best-fitting solution. Subsequently, based on our findings, we propose a novel social media monitoring system tailored to the IoT domain; the system allows users to identify recent/trending vulnerabilities and exploits on IoT devices. Finally, to aid research on the field and support the reproducibility of our results we publicly release all annotated datasets created during this process.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Thinking Out of the Blocks: Holochain for Distributed Security in IoT Healthcare,"The Internet-of-Things (IoT) is an emerging and cognitive technology which connects a massive number of smart physical devices with virtual objects operating in diverse platforms through the internet. IoT is increasingly being implemented in distributed settings, making footprints in almost every sector of our life. Unfortunately, for healthcare systems, the entities connected to the IoT networks are exposed to an unprecedented level of security threats. Relying on a huge volume of sensitive and personal data, IoT healthcare systems are facing unique challenges in protecting data security and privacy. Although blockchain has posed to be the solution in this scenario thanks to its inherent distributed ledger technology (DLT), it suffers from major setbacks of increasing storage and computation requirements with the network size. This paper proposes a holochain-based security and privacy-preserving framework for IoT healthcare systems that overcomes these challenges and is particularly suited for resource constrained IoT scenarios. The performance and thorough security analyses demonstrate that a holochain-based IoT healthcare system is significantly better compared to blockchain and other existing systems.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IIDS: Design of Intelligent Intrusion Detection System for Internet-of-Things Applications,"With rapid technological growth, security attacks are drastically increasing. In many crucial Internet-of-Things (IoT) applications such as healthcare and defense, the early detection of security attacks plays a significant role in protecting huge resources. An intrusion detection system is used to address this problem. The signature-based approaches fail to detect zero-day attacks. So anomaly-based detection particularly AI tools, are becoming popular. In addition, the imbalanced dataset leads to biased results. In Machine Learning (ML) models, F1 score is an important metric to measure the accuracy of class-level correct predictions. The model may fail to detect the target samples if the F1 is considerably low. It will lead to unrecoverable consequences in sensitive applications such as healthcare and defense. So, any improvement in the F1 score has significant impact on the resource protection. In this paper, we present a framework for ML-based intrusion detection system for an imbalanced dataset. In this study, the most recent dataset, namely CICIoT2023 is considered. The random forest (RF) algorithm is used in the proposed framework. The proposed approach improves 3.72%, 3.75% and 4.69% in precision, recall and F1 score, respectively, with the existing method. Additionally, for unsaturated classes (i.e., classes with F1 score < 0.99), F1 score improved significantly by 7.9%. As a result, the proposed approach is more suitable for IoT security applications for efficient detection of intrusion and is useful in further studies.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Secure Directional Modulation to Enhance Physical Layer Security in IoT Networks,"In this work, an adaptive and robust null-space projection (AR-NSP) scheme is proposed for secure transmission with artificial noise (AN)-aided directional modulation (DM) in wireless networks. The proposed scheme is carried out in three steps. Firstly, the directions of arrival (DOAs) of the signals from the desired user and eavesdropper are estimated by the Root Multiple Signal Classificaiton (Root-MUSIC) algorithm and the related signal-to-noise ratios (SNRs) are estimated based on the ratio of the corresponding eigenvalue to the minimum eigenvalue of the covariance matrix of the received signals. In the second step, the value intervals of DOA estimation errors are predicted based on the DOA and SNR estimations. Finally, a robust NSP beamforming DM system is designed according to the afore-obtained estimations and predictions. Our examination shows that the proposed scheme can significantly outperform the conventional non-adaptive robust scheme and non-robust NSP scheme in terms of achieving a much lower bit error rate (BER) at the desired user and a much higher secrecy rate (SR). In addition, the BER and SR performance gains achieved by the proposed scheme relative to other schemes increase with the value range of DOA estimation error.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Threat analysis of IoT networks Using Artificial Neural Network Intrusion Detection System,"The Internet of things (IoT) is still in its infancy and has attracted much interest in many industrial sectors including medical fields, logistics tracking, smart cities and automobiles. However as a paradigm, it is susceptible to a range of significant intrusion threats. This paper presents a threat analysis of the IoT and uses an Artificial Neural Network (ANN) to combat these threats. A multi-level perceptron, a type of supervised ANN, is trained using internet packet traces, then is assessed on its ability to thwart Distributed Denial of Service (DDoS/DoS) attacks. This paper focuses on the classification of normal and threat patterns on an IoT Network. The ANN procedure is validated against a simulated IoT network. The experimental results demonstrate 99.4% accuracy and can successfully detect various DDoS/DoS attacks.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
TAPInspector: Safety and Liveness Verification of Concurrent Trigger-Action IoT Systems,"Trigger-action programming (TAP) is a popular end-user programming framework that can simplify the Internet of Things (IoT) automation with simple trigger-action rules. However, it also introduces new security and safety threats. A lot of advanced techniques have been proposed to address this problem. Rigorously reasoning about the security of a TAP-based IoT system requires a well-defined model and verification method both against rule semantics and physical-world features, e.g., concurrency, rule latency, extended action, tardy attributes, and connection-based rule interactions, which has been missing until now. By analyzing these features, we find 9 new types of rule interaction vulnerabilities and validate them on two commercial IoT platforms. We then present TAPInspector, a novel system to detect these interaction vulnerabilities in concurrent TAP-based IoT systems. It automatically extracts TAP rules from IoT apps, translates them into a hybrid model by model slicing and state compression, and performs semantic analysis and model checking with various safety and liveness properties. Our experiments corroborate that TAPInspector is practical: it identifies 533 violations related to rule interaction from 1108 real-world market IoT apps and is at least 60000 times faster than the baseline without optimization.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Machine Learning Based Solutions for Security of Internet of Things (IoT): A Survey,"Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Security Weaknesses in IoT Management Platforms,"A diverse set of Internet of Things (IoT) devices are becoming an integrated part of daily lives, and playing an increasingly vital role in various industry, enterprise and agricultural settings. The current IoT ecosystem relies on several IoT management platforms to manage and operate a large number of IoT devices, their data, and their connectivity. Considering their key role, these platforms must be properly secured against cyber attacks. In this work, we first explore the core operations/features of leading platforms to design a framework to perform a systematic security evaluation of these platforms. Subsequently, we use our framework to analyze a representative set of 52 IoT management platforms, including 42 web-hosted and 10 locally-deployable platforms. We discover a number of high severity unauthorized access vulnerabilities in 9/52 evaluated IoT management platforms, which could be abused to perform attacks such as remote IoT SIM deactivation, IoT SIM overcharging and IoT device data forgery. More seriously, we also uncover instances of broken authentication in 13/52 platforms, including complete account takeover on 8/52 platforms along with remote code execution on 2/52 platforms. In effect, 17/52 platforms were affected by vulnerabilities that could lead to platform-wide attacks. Overall, vulnerabilities were uncovered in 33 platforms, out of which 28 platforms responded to our responsible disclosure. We were also assigned 11 CVEs and awarded bounty for our findings.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Tarallo: Evading Behavioral Malware Detectors in the Problem Space,"Machine learning algorithms can effectively classify malware through dynamic behavior but are susceptible to adversarial attacks. Existing attacks, however, often fail to find an effective solution in both the feature and problem spaces. This issue arises from not addressing the intrinsic nondeterministic nature of malware, namely executing the same sample multiple times may yield significantly different behaviors. Hence, the perturbations computed for a specific behavior may be ineffective for others observed in subsequent executions. In this paper, we show how an attacker can augment their chance of success by leveraging a new and more efficient feature space algorithm for sequential data, which we have named PS-FGSM, and by adopting two problem space strategies specially tailored to address nondeterminism in the problem space. We implement our novel algorithm and attack strategies in Tarallo, an end-to-end adversarial framework that significantly outperforms previous works in both white and black-box scenarios. Our preliminary analysis in a sandboxed environment and against two RNN-based malware detectors, shows that Tarallo achieves a success rate up to 99% on both feature and problem space attacks while significantly minimizing the number of modifications required for misclassification.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
IoTChain: A Three-Tier Blockchain-based IoT Security Architecture,"There has been increasing interest in the potential of blockchain in enhancing the security of devices and systems, such as Internet of Things (IoT). In this paper, we present a blockchain-based IoT security architecture, IoTchain. The three-tier architecture comprises an authentication layer, a blockchain layer and an application layer, and is designed to achieve identity authentication, access control, privacy protection, lightweight feature, regional node fault tolerance, denial-of-service resilience, and storage integrity. We also evaluate the performance of IoTchain to demonstrate its utility in an IoT deployment.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
An Overview of Wireless IoT Protocol Security in the Smart Home Domain,"While the application of IoT in smart technologies becomes more and more proliferated, the pandemonium of its protocols becomes increasingly confusing. More seriously, severe security deficiencies of these protocols become evident, as time-to- market is a key factor, which satisfaction comes at the price of a less thorough security design and testing. This applies especially to the smart home domain, where the consumer-driven market demands quick and cheap solutions. This paper presents an overview of IoT application domains and discusses the most important wireless IoT protocols for smart home, which are KNX-RF, EnOcean, Zigbee, Z-Wave and Thread. Finally, it describes the security features of said protocols and compares them with each other, giving advice on whose protocols are more suitable for a secure smart home.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
ZIRCON: Zero-watermarking-based approach for data integrity and secure provenance in IoT networks,"The Internet of Things (IoT) is integrating the Internet and smart devices in almost every domain such as home automation, e-healthcare systems, vehicular networks, industrial control and military applications. In these sectors, sensory data, which is collected from multiple sources and managed through intermediate processing by multiple nodes, is used for decision-making processes. Ensuring data integrity and keeping track of data provenance is a core requirement in such a highly dynamic context, since data provenance is an important tool for the assurance of data trustworthiness. Dealing with such requirements is challenging due to the limited computational and energy resources in IoT networks. This requires addressing several challenges such as processing overhead, secure provenance, bandwidth consumption and storage efficiency. In this paper, we propose ZIRCON, a novel zero-watermarking approach to establish end-to-end data trustworthiness in an IoT network. In ZIRCON, provenance information is stored in a tamper-proof centralized network database through watermarks, generated at source node before transmission. We provide an extensive security analysis showing the resilience of our scheme against passive and active attacks. We also compare our scheme with existing works based on performance metrics such as computational time, energy utilization and cost analysis. The results show that ZIRCON is robust against several attacks, lightweight, storage efficient, and better in energy utilization and bandwidth consumption, compared to prior art.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
The Dichotomy of Cloud and IoT: Cloud-Assisted IoT From a Security Perspective,"In recent years, the existence of a significant cross-impact between Cloud computing and Internet of Things (IoT) has lead to a dichotomy that gives raise to Cloud-Assisted IoT (CAIoT) and IoT-Based Cloud (IoTBC). Although it is pertinent to study both technologies, this paper focuses on CAIoT, and especially its security issues, which are inherited from both Cloud computing and IoT. This study starts with reviewing existing relevant surveys, noting their shortcomings, which motivate a comprehensive survey in this area. We proceed to highlight existing approaches towards the design of Secure CAIoT (SCAIoT) along with related security challenges and controls. We develop a layered architecture for SCAIoT. Furthermore, we take a look at what the future may hold for SCAIoT with a focus on the role of Artificial Intelligence(AI).","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Secure Decentralized IoT Service Platform using Consortium Blockchain,"Blockchain technology has gained increasing popularity in the research of Internet of Things (IoT) systems in the past decade. As a distributed and immutable ledger secured by strong cryptography algorithms, the blockchain brings a new perspective to secure IoT systems. Many studies have been devoted to integrating blockchain into IoT device management, access control, data integrity, security, and privacy. In comparison, the blockchain-facilitated IoT communication is much less studied. Nonetheless, we see the potential of blockchain in decentralizing and securing IoT communications. This paper proposes an innovative IoT service platform powered by consortium blockchain technology. The presented solution abstracts machine-to-machine (M2M) and human-to-machine (H2M) communications into services provided by IoT devices. Then, it materializes data exchange of the IoT network through smart contracts and blockchain transactions. Additionally, we introduce the auxiliary storage layer to the proposed platform to address various data storage requirements. Our proof-of-concept implementation is tested against various workloads and connection sizes under different block configurations to evaluate the platform's transaction throughput, latency, and hardware utilization. The experiment results demonstrate that our solution can maintain high performance under most testing scenarios and provide valuable insights on optimizing the blockchain configuration to achieve the best performance.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Secure Lightweight Authentication for Multi User IoT Environment,"The Internet of Things (IoT) is giving a boost to a plethora of new opportunities for the robust and sustainable deployment of cyber physical systems. The cornerstone of any IoT system is the sensing devices. These sensing devices have considerable resource constraints, including insufficient battery capacity, CPU capability, and physical security. Because of such resource constraints, designing lightweight cryptographic protocols is an opportunity. Remote User Authentication ensures that two parties establish a secure and durable session key. This study presents a lightweight and safe authentication strategy for the user-gateway (U GW) IoT network model. The proposed system is designed leveraging Elliptic Curve Cryptography (ECC). We undertake a formal security analysis with both the Automated Validation of Internet Security Protocols (AVISPA) and Burrows Abadi Needham (BAN) logic tools and an information security assessment with the Delev Yao channel. We use publish subscribe based Message Queuing Telemetry Transport (MQTT) protocol for communication. Additionally, the performance analysis and comparison of security features show that the proposed scheme is resilient to well known cryptographic threats.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
RoboMal: Malware Detection for Robot Network Systems,"Robot systems are increasingly integrating into numerous avenues of modern life. From cleaning houses to providing guidance and emotional support, robots now work directly with humans. Due to their far-reaching applications and progressively complex architecture, they are being targeted by adversarial attacks such as sensor-actuator attacks, data spoofing, malware, and network intrusion. Therefore, security for robotic systems has become crucial. In this paper, we address the underserved area of malware detection in robotic software. Since robots work in close proximity to humans, often with direct interactions, malware could have life-threatening impacts. Hence, we propose the RoboMal framework of static malware detection on binary executables to detect malware before it gets a chance to execute. Additionally, we address the great paucity of data in this space by providing the RoboMal dataset comprising controller executables of a small-scale autonomous car. The performance of the framework is compared against widely used supervised learning models: GRU, CNN, and ANN. Notably, the LSTM-based RoboMal model outperforms the other models with an accuracy of 85% and precision of 87% in 10-fold cross-validation, hence proving the effectiveness of the proposed framework.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Monet: A User-oriented Behavior-based Malware Variants Detection System for Android,"Android, the most popular mobile OS, has around 78% of the mobile market share. Due to its popularity, it attracts many malware attacks. In fact, people have discovered around one million new malware samples per quarter, and it was reported that over 98% of these new malware samples are in fact ""derivatives"" (or variants) from existing malware families. In this paper, we first show that runtime behaviors of malware's core functionalities are in fact similar within a malware family. Hence, we propose a framework to combine ""runtime behavior"" with ""static structures"" to detect malware variants. We present the design and implementation of MONET, which has a client and a backend server module. The client module is a lightweight, in-device app for behavior monitoring and signature generation, and we realize this using two novel interception techniques. The backend server is responsible for large scale malware detection. We collect 3723 malware samples and top 500 benign apps to carry out extensive experiments of detecting malware variants and defending against malware transformation. Our experiments show that MONET can achieve around 99% accuracy in detecting malware variants. Furthermore, it can defend against 10 different obfuscation and transformation techniques, while only incurs around 7% performance overhead and about 3% battery overhead. More importantly, MONET will automatically alert users with intrusion details so to prevent further malicious behaviors.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
NETRA: Enhancing IoT Security using NFV-based Edge Traffic Analysis,"This is the era of smart devices or things which are fueling the growth of Internet of Things (IoT). It is impacting every sphere around us, making our life dependent on this technological feat. It is of high concern that these smart things are being targeted by cyber criminals taking advantage of heterogeneity, minuscule security features and vulnerabilities within these devices. Conventional centralized IT security measures have limitations in terms of scalability and cost. Therefore, these smart devices are required to be monitored closer to their location ideally at the edge of IoT networks. In this paper, we explore how some security features can be implemented at the network edge to secure these smart devices. We explain the importance of Network Function Virtualization (NFV) in order to deploy security functions at the network edge. To achieve this goal, we introduce NETRA - a novel lightweight Docker-based architecture for virtualizing network functions to provide IoT security. Also, we highlight the advantages of the proposed architecture over the standardized NFV architecture in terms of storage, memory usage, latency, throughput, load average, scalability and explain why the standardized architecture is not suitable for IoT. We study the performance of proposed NFV based edge analysis for IoT security and show that attacks can be detected with more than 95% accuracy in less than a second.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Securing RPL using Network Coding: The Chained Secure Mode (CSM),"As the de facto routing protocol for many Internet of Things (IoT) networks nowadays, and to assure the confidentiality and integrity of its control messages, the Routing Protocol for Low Power and Lossy Networks (RPL) incorporates three modes of security: the Unsecured Mode (UM), Preinstalled Secure Mode (PSM), and the Authenticated Secure Mode (ASM). While the PSM and ASM are intended to protect against external routing attacks and some replay attacks (through an optional replay protection mechanism), recent research showed that RPL in PSM is still vulnerable to many routing attacks, both internal and external. In this paper, we propose a novel secure mode for RPL, the Chained Secure Mode (CSM), based on the concept of intraflow Network Coding (NC). The CSM is designed to enhance RPL resilience and mitigation capability against replay attacks while allowing the integration with external security measures such as Intrusion Detection Systems (IDSs). The security and performance of the proposed CSM were evaluated and compared against RPL in UM and PSM (with and without the optional replay protection) under several routing attacks: the Neighbor attack (NA), Wormhole (WH), and CloneID attack (CA), using average packet delivery rate (PDR), End-to-End (E2E) latency, and power consumption as metrics. It showed that CSM has better performance and more enhanced security than both the UM and PSM with the replay protection, while mitigating both the NA and WH attacks and significantly reducing the effect of the CA in the investigated scenarios.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
BotNet Intrusion Detection System in Internet of Things with Developed Deep Learning,"The rapid growth of technology has led to the creation of computing networks. The applications of the Internet of Things are becoming more and more visible with the expansion and development of sensors and the use of a series of equipment to connect to the Internet. Of course, the growth of any network will also provide some challenges. The main challenge of IoT like any other network is its security. In the field of security, there are issues such as attack detection, authentication, encryption and the so on. One of the most important attack is cyber-attacks that disrupt the network usage. One of the most important attacks on the IoT is BotNet attack. The most important challenges of this topic include very high computational complexity, lack of comparison with previous methods, lack of scalability, high execution time, lack of review of the proposed approach in terms of accuracy to detect and classify attacks and intrusions. Using intrusion detection systems for the IoT is an important step in identifying and detecting various attacks. Therefore, an algorithm that can solve these challenges has provided a near-optimal method. Using training-based models and algorithms such as Deep Dearning-Reinforcement Learning and XGBoost learning in combination (DRL-XGBoost) models can be an interesting approach to overcoming previous weaknesses. The data of this research is Bot-IoT-2018.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Blockchain Security by Design Framework for Trust and Adoption in IoT Environment,"With the recent advances of IoT (Internet of Things) new and more robust security frameworks are needed to detect and mitigate new forms of cyber-attacks, which exploit complex and heterogeneity IoT networks, as well as, the existence of many vulnerabilities in IoT devices. With the rise of blockchain technologies service providers pay considerable attention to better understand and adopt blockchain technologies in order to have better secure and trusted systems for own organisations and their customers. The present paper introduces a high level guide for the senior officials and decision makers in the organisations and technology managers for blockchain security framework by design principle for trust and adoption in IoT environments. The paper discusses Cyber-Trust project blockchain technology development as a representative case study for offered security framework. Security and privacy by design approach is introduced as an important consideration in setting up the framework.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
"Microservices in IoT Security: Current Solutions, Research Challenges, and Future Directions","In recent years, the Internet of Things (IoT) technology has led to the emergence of multiple smart applications in different vital sectors including healthcare, education, agriculture, energy management, etc. IoT aims to interconnect several intelligent devices over the Internet such as sensors, monitoring systems, and smart appliances to control, store, exchange, and analyze collected data. The main issue in IoT environments is that they can present potential vulnerabilities to be illegally accessed by malicious users, which threatens the safety and privacy of gathered data. To face this problem, several recent works have been conducted using microservices-based architecture to minimize the security threats and attacks related to IoT data. By employing microservices, these works offer extensible, reusable, and reconfigurable security features. In this paper, we aim to provide a survey about microservices-based approaches for securing IoT applications. This survey will help practitioners understand ongoing challenges and explore new and promising research opportunities in the IoT security field. To the best of our knowledge, this paper constitutes the first survey that investigates the use of microservices technology for securing IoT applications.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Systematic Mapping Study on SDN Controllers for Enhancing Security in IoT Networks,"Context: The increase in Internet of Things (IoT) devices gives rise to an increase in deceptive manipulations by malicious actors. These actors should be prevented from targeting the IoT networks. Cybersecurity threats have evolved and become dynamically sophisticated, such that they could exploit any vulnerability found in IoT networks. However, with the introduction of the Software Defined Network (SDN) in the IoT networks as the central monitoring unit, IoT networks are less vulnerable and less prone to threats. %Although, the SDN itself is vulnerable to several threats.   Objective: To present a comprehensive and unbiased overview of the state-of-the-art on IoT networks security enhancement using SDN controllers.   Method: We review the current body of knowledge on enhancing the security of IoT networks using SDN with a Systematic Mapping Study (SMS) following the established guidelines.   Results: The SMS result comprises 33 primary studies analyzed against four major research questions. The SMS highlights current research trends and identifies gaps in the SDN-IoT network security.   Conclusion: We conclude that the SDN controller architecture commonly used for securing IoT networks is the centralized controller architecture. However, this architecture is not without its limitations. Additionally, the predominant technique utilized for risk mitigation is machine learning.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
PWG-IDS: An Intrusion Detection Model for Solving Class Imbalance in IIoT Networks Using Generative Adversarial Networks,"With the continuous development of industrial IoT (IIoT) technology, network security is becoming more and more important. And intrusion detection is an important part of its security. However, since the amount of attack traffic is very small compared to normal traffic, this imbalance makes intrusion detection in it very difficult. To address this imbalance, an intrusion detection system called pretraining Wasserstein generative adversarial network intrusion detection system (PWG-IDS) is proposed in this paper. This system is divided into two main modules: 1) In this module, we introduce the pretraining mechanism in the Wasserstein generative adversarial network with gradient penalty (WGAN-GP) for the first time, firstly using the normal network traffic to train the WGAN-GP, and then inputting the imbalance data into the pre-trained WGAN-GP to retrain and generate the final required data. 2) Intrusion detection module: We use LightGBM as the classification algorithm to detect attack traffic in IIoT networks. The experimental results show that our proposed PWG-IDS outperforms other models, with F1-scores of 99% and 89% on the 2 datasets, respectively. And the pretraining mechanism we proposed can also be widely used in other GANs, providing a new way of thinking for the training of GANs.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Understanding Security Requirements and Challenges in Internet of Things (IoTs): A Review,"Internet of Things (IoT) is realized by the idea of free flow of information amongst various low power embedded devices that use Internet to communicate with one another. It is predicted that the IoT will be widely deployed and it will find applicability in various domains of life. Demands of IoT have lately attracted huge attention and organizations are excited about the business value of the data that will be generated by the IoT paradigm. On the other hand, IoT have various security and privacy concerns for the end users that limit its proliferation. In this paper we have identified, categorized and discussed various security challenges and state of the art efforts to resolve these challenges.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Secure IoT Communication with Smart Contracts in a Blockchain Infrastructure,"The Internet of Things (IoT) is undergoing rapid growth in the IT industry, but, it continues to be associated with several security and privacy concerns as a result of its massive scale, decentralised topology, and resource-constrained devices. Blockchain (BC), a distributed ledger technology used in cryptocurrency has attracted significant attention in the realm of IoT security and privacy. However, adopting BC to IoT is not straightforward in most cases, due to overheads and delays caused by BC operations. In this paper, we apply a BC technology known as Hyperledgder Fabric, to an IoT network. This technology introduces an execute-order technique for transactions that separates the transaction execution from consensus, resulting in increased efficiency. We demonstrate that our proposed IoT-BC architecture is sufficiently secure with regard to fundamental security goals i.e., confidentiality, integrity, and availability. Finally, the simulation results are highlighted that shows the performance overheads associated with our approach are as minimal as those associated with the Hyperledger Fabric framework and negligible in terms of security and privacy.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Towards Secure Management of Edge-Cloud IoT Microservices using Policy as Code,"IoT application providers increasingly use MicroService Architecture (MSA) to develop applications that convert IoT data into valuable information. The independently deployable and scalable nature of microservices enables dynamic utilization of edge and cloud resources provided by various service providers, thus improving performance. However, IoT data security should be ensured during multi-domain data processing and transmission among distributed and dynamically composed microservices. The ability to implement granular security controls at the microservices level has the potential to solve this. To this end, edge-cloud environments require intricate and scalable security frameworks that operate across multi-domain environments to enforce various security policies during the management of microservices (i.e., initial placement, scaling, migration, and dynamic composition), considering the sensitivity of the IoT data. To address the lack of such a framework, we propose an architectural framework that uses Policy-as-Code to ensure secure microservice management within multi-domain edge-cloud environments. The proposed framework contains a ""control plane"" to intelligently and dynamically utilise and configure cloud-native (i.e., container orchestrators and service mesh) technologies to enforce security policies. We implement a prototype of the proposed framework using open-source cloud-native technologies such as Docker, Kubernetes, Istio, and Open Policy Agent to validate the framework. Evaluations verify our proposed framework's ability to enforce security policies for distributed microservices management, thus harvesting the MSA characteristics to ensure IoT application security needs.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Is Your Kettle Smarter Than a Hacker? A Scalable Tool for Assessing Replay Attack Vulnerabilities on Consumer IoT Devices,"Consumer Internet of Things (IoT) devices often leverage the local network to communicate with the corresponding companion app or other devices. This has benefits in terms of efficiency since it offloads the cloud. ENISA and NIST security guidelines underscore the importance of enabling default local communication for safety and reliability. Indeed, an IoT device should continue to function in case the cloud connection is not available. While the security of cloud-device connections is typically strengthened through the usage of standard protocols, local connectivity security is frequently overlooked. Neglecting the security of local communication opens doors to various threats, including replay attacks. In this paper, we investigate this class of attacks by designing a systematic methodology for automatically testing IoT devices vulnerability to replay attacks. Specifically, we propose a tool, named REPLIOT, able to test whether a replay attack is successful or not, without prior knowledge of the target devices. We perform thousands of automated experiments using popular commercial devices spanning various vendors and categories. Notably, our study reveals that among these devices, 51% of them do not support local connectivity, thus they are not compliant with the reliability and safety requirements of the ENISA/NIST guidelines. We find that 75% of the remaining devices are vulnerable to replay attacks with REPLIOT having a detection accuracy of 0.98-1. Finally, we investigate the possible causes of this vulnerability, discussing possible mitigation strategies.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Hybrid Algorithm to Enhance Wireless Sensor Networks security on the IoT,"The Internet of Things (IoT) is a futuristic technology that promises to connect tons of devices via the internet. As more individuals connect to the internet, it is believed that communication will generate mountains of data. IoT is currently leveraging Wireless Sensor Networks (WSNs) to collect, monitor, and transmit data and sensitive data across wireless networks using sensor nodes. WSNs encounter a variety of threats posed by attackers, including unauthorized access and data security. Especially in the context of the Internet of Things, where small embedded devices with limited computational capabilities, such as sensor nodes, are expected to connect to a larger network. As a result, WSNs are vulnerable to a variety of attacks. Furthermore, implementing security is time-consuming and selective, as traditional security algorithms degrade network performance due to their computational complexity and inherent delays. This paper describes an encryption algorithm that combines the Secure IoT (SIT) algorithm with the Security Protocols for Sensor Networks (SPINS) security protocol to create the Lightweight Security Algorithm (LSA), which addresses data security concerns while reducing power consumption in WSNs without sacrificing performance.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
Challenges for Security Assessment of Enterprises in the IoT Era,"For years, attack graphs have been an important tool for security assessment of enterprise networks, but IoT devices, a new player in the IT world, might threat the reliability of this tool. In this paper, we review the challenges that must be addressed when using attack graphs to model and analyze enterprise networks that include IoT devices. In addition, we propose novel ideas and countermeasures aimed at addressing these challenges.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
SECCS: SECure Context Saving for IoT Devices,"Energy consumption of IoT devices is a very important issue. For this reason, many techniques have been developed to allow IoT nodes to be aware of the amount of available energy. When energy is missing, the device halts and saves its state. One of those techniques is context saving, relying on the use of Non-Volatile Memories (NVM) to store and restore the state of the device. However, this information, as far as IoT devices deal with security, might be the target of attacks, including tampering and theft of confidential data. In this paper, we propose a SECure Context Saving (SECCS) approach that provides a context saving procedure and a hardware module easy to implement inside a System on Chip (SoC). This approach provides both confidentiality and integrity to all the CPU content saved into the target NVM.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
A Step Towards Checking Security in IoT,"The Internet of Things (IoT) is smartifying our everyday life. Our starting point is IoT-LySa, a calculus for describing IoT systems, and its static analysis, which will be presented at Coordination 2016. We extend the mentioned proposal in order to begin an investigation about security issues, in particular for the static verification of secrecy and some other security properties.","cat:cs.CR AND (""intrusion detection"" OR malware OR ""IoT security"" OR phishing)",0
