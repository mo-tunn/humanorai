cleaned_text,label
the rapid adoption of generative artificial intelligence ai in scientific research particularly large language models llms has outpaced the development of ethical guidelines leading to a tripletoo problem too many highlevel ethical initiatives too abstract principles lacking contextual and practical relevance and too much focus on restrictions and risks over benefits and utilities existing approachesprinciplism reliance on abstract ethical principles formalism rigid application of rules and technological solutionism overemphasis on technological fixesoffer little practical guidance for addressing ethical challenges of ai in scientific research practices to bridge the gap between abstract principles and daytoday research practices a usercentered realisminspired approach is proposed here it outlines five specific goals for ethical ai use 1 understanding model training and output including bias mitigation strategies 2 respecting privacy confidentiality and copyright 3 avoiding plagiarism and policy violations 4 applying ai beneficially compared to alternatives and 5 using ai transparently and reproducibly each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures i argue that ethical ai application requires evaluating its utility against existing alternatives rather than isolated performance metrics additionally i propose documentation guidelines to enhance transparency and reproducibility in aiassisted research moving forward we need targeted professional development training programs and balanced enforcement mechanisms to promote responsible ai use while fostering innovation by refining these ethical guidelines and adapting them to emerging ai capabilities we can accelerate scientific progress without compromising research integrity,0
the 7th symposium on educational advances in artificial intelligence eaai17 cochaired by sven koenig and eric eaton launched the eaai new and future ai educator program to support the training of earlycareer university faculty secondary school faculty and future educators phd candidates or postdocs who intend a career in academia as part of the program awardees were asked to address one of the following blue sky questions how couldshould artificial intelligence ai courses incorporate ethics into the curriculum how could we teach ai topics at an early undergraduate or a secondary school level ai has the potential for broad impact to numerous disciplines how could we make ai education more interdisciplinary specifically to benefit nonengineering fields this paper is a collection of their responses intended to help motivate discussion around these issues in ai education,0
artificial intelligence principles define social and ethical considerations to develop future ai they come from research institutes government organizations and industries all versions of ai principles are with different considerations covering different perspectives and making different emphasis none of them can be considered as complete and can cover the rest ai principle proposals here we introduce laip an effort and platform for linking and analyzing different artificial intelligence principles we want to explicitly establish the common topics and links among ai principles proposed by different organizations and investigate on their uniqueness based on these efforts for the longterm future of ai instead of directly adopting any of the ai principles we argue for the necessity of incorporating various ai principles into a comprehensive framework and focusing on how they can interact and complete each other,0
principles of fairness and solidarity in ai ethics regularly overlap creating obscurity in practice acting in accordance with one can appear indistinguishable from deciding according to the rules of the other however there exist irregular cases where the two concepts split and so reveal their disparate meanings and uses this paper explores two cases in ai medical ethics one that is irregular and the other more conventional to fully distinguish fairness and solidarity then the distinction is applied to the frequently cited compas versus propublica dispute in judicial ethics the application provides a broader model for settling contemporary and topical debates about fairness and solidarity it also implies a deeper and disorienting truth about ai ethics principles and their justification,0
the exponential development and application of artificial intelligence triggered an unprecedented global concern for potential social and ethical issues stakeholders from different industries international foundations governmental organisations and standards institutions quickly improvised and created various codes of ethics attempting to regulate ai a major concern is the large homogeneity and presumed consensualism around these principles while it is true that some ethical doctrines such as the famous kantian deontology aspire to universalism they are however not universal in practice in fact ethical pluralism is more about differences in which relevant questions to ask rather than different answers to a common question when people abide by different moral doctrines they tend to disagree on the very approach to an issue even when people from different cultures happen to agree on a set of common principles it does not necessarily mean that they share the same understanding of these concepts and what they entail in order to better understand the philosophical roots and cultural context underlying ethical principles in ai we propose to analyse and compare the ethical principles endorsed by the chinese national new generation artificial intelligence governance professional committee cnngaigpc and those elaborated by the european highlevel expert group on ai hlegai china and the eu have very different political systems and diverge in their cultural heritages in our analysis we wish to highlight that principles that seem similar a priori may actually have different meanings derived from different approaches and reflect distinct goals,0
artificial intelligence ai based assistive systems so called intelligent assistive technology iat are becoming increasingly ubiquitous by each day iat helps people in improving their quality of life by providing intelligent assistance based on the provided data a few examples of such iats include selfdriving cars robot assistants and smarthealth management solutions however the presence of such autonomous entities poses ethical challenges concerning the stakeholders involved in using these systems there is a lack of research when it comes to analysing how such iat adheres to provided ethical regulations due to ethical logistic and cost issues associated with such an analysis in the light of the abovementioned problem statement and issues we present a method to measure the ethicality of an assistive system to perform this task we utilised our simulation tool that focuses on modelling navigation and assistance of persons with dementia pwd in indoor environments by utilising this tool we analyse how well different assistive strategies adhere to provided ethical regulations such as autonomy justice and beneficence of the stakeholders,0
in this paper we present a set of key demarcations particularly important when discussing ethical and societal issues of current ai research and applications properly distinguishing issues and concerns related to artificial general intelligence and weak ai between symbolic and connectionist ai ai methods data and applications are prerequisites for an informed debate such demarcations would not only facilitate muchneeded discussions on ethics on current ai technologies and research in addition sufficiently establishing such demarcations would also enhance knowledgesharing and support rigor in interdisciplinary research between technical and social sciences,0
artificial intelligence ai systems exert a growing influence on our society as they become more ubiquitous their potential negative impacts also become evident through various realworld incidents following such early incidents academic and public discussion on ai ethics has highlighted the need for implementing ethics in ai system development however little currently exists in the way of frameworks for understanding the practical implementation of ai ethics in this paper we discuss a research framework for implementing ai ethics in industrial settings the framework presents a starting point for empirical studies into ai ethics but is still being developed further based on its practical utilization,0
the integration of artificial intelligence ai in educational measurement has revolutionized assessment methods enabling automated scoring rapid content analysis and personalized feedback through machine learning and natural language processing these advancements provide timely consistent feedback and valuable insights into student performance thereby enhancing the assessment experience however the deployment of ai in education also raises significant ethical concerns regarding validity reliability transparency fairness and equity issues such as algorithmic bias and the opacity of ai decisionmaking processes pose risks of perpetuating inequalities and affecting assessment outcomes responding to these concerns various stakeholders including educators policymakers and organizations have developed guidelines to ensure ethical ai use in education the national council of measurement in educations special interest group on ai in measurement and education aime also focuses on establishing ethical standards and advancing research in this area in this paper a diverse group of aime members examines the ethical implications of aipowered tools in educational measurement explores significant challenges such as automation bias and environmental impact and proposes solutions to ensure ais responsible and effective use in education,0
this book chapter delves into the pressing need to queer the ethics of ai to challenge and reevaluate the normative suppositions and values that underlie ai systems the chapter emphasizes the ethical concerns surrounding the potential for ai to perpetuate discrimination including binarism and amplify existing inequalities due to the lack of representative datasets and the affordances and constraints depending on technology readiness the chapter argues that a critical examination of the neoliberal conception of equality that often underpins nondiscrimination law is necessary and cannot stress more the need to create alternative interdisciplinary approaches that consider the complex and intersecting factors that shape individuals experiences of discrimination by exploring such approaches centering on intersectionality and vulnerabilityinformed design the chapter contends that designers and developers can create more ethical ai systems that are inclusive equitable and responsive to the needs and experiences of all individuals and communities particularly those who are most vulnerable to discrimination and harm,0
trustworthy artificial intelligence tai integrates ethics that align with human values looking at their influence on ai behaviour and decisionmaking primarily dependent on selfassessment tai evaluation aims to ensure ethical standards and safety in ai development and usage this paper reviews the current tai evaluation methods in the literature and offers a classification contributing to understanding selfassessment methods in this field,0
this study delves into the pervasive issue of gender issues in artificial intelligence ai specifically within automatic scoring systems for studentwritten responses the primary objective is to investigate the presence of gender biases disparities and fairness in generally targeted training samples with mixedgender datasets in ai scoring outcomes utilizing a finetuned version of bert and gpt35 this research analyzes more than 1000 humangraded student responses from male and female participants across six assessment items the study employs three distinct techniques for bias analysis scoring accuracy difference to evaluate bias mean score gaps by gender msg to evaluate disparity and equalized odds eo to evaluate fairness the results indicate that scoring accuracy for mixedtrained models shows an insignificant difference from either male or femaletrained models suggesting no significant scoring bias consistently with both bert and gpt35 we found that mixedtrained models generated fewer msg and nondisparate predictions compared to humans in contrast compared to humans genderspecifically trained models yielded larger msg indicating that unbalanced training data may create algorithmic models to enlarge gender disparities the eo analysis suggests that mixedtrained models generated more fairness outcomes compared with genderspecifically trained models collectively the findings suggest that genderunbalanced data do not necessarily generate scoring bias but can enlarge gender disparities and reduce scoring fairness,0
the growing application of artificial intelligence ai in the field of information retrieval ir affects different domains including cultural heritage by facilitating organisation and retrieval of large volumes of heritagerelated content aidriven ir systems inform users about a broad range of historical phenomena including genocides eg the holocaust however it is currently unclear to what degree ir systems are capable of dealing with multiple ethical challenges associated with the curation of genociderelated information to address this question this chapter provides an overview of ethical challenges associated with the human curation of genociderelated information using a threepart framework inspired by belmont criteria ie curation challenges associated with respect for individuals beneficence and justicefairness then the chapter discusses to what degree the abovementioned challenges are applicable to the ways in which aidriven ir systems deal with genociderelated information and what can be the potential ways of bridging ai and memory ethics in this context,0
the recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral ethical and philosophical issues in their ai courses as instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners but also to understand the moral ethical and philosophical impacts that artificial intelligence will have on society in this article we provide practical case studies and links to resources for use by ai educators we also provide concrete suggestions on how to integrate ai ethics into a general artificial intelligence course and how to teach a standalone artificial intelligence ethics course,0
in recent years there has been a stimulating discussion on how artificial intelligence ai can support the science and engineering of intelligent educational applications many studies in the field are proposing actionable data mining pipelines and machinelearning models driven by learningrelated data the potential of these pipelines and models to amplify unfairness for certain categories of students is however receiving increasing attention if ai applications are to have a positive impact on education it is crucial that their design considers fairness at every step through anonymous surveys and interviews with experts researchers and practitioners who have published their research at toptier educational conferences in the last year we conducted the first expertdriven systematic investigation on the challenges and needs for addressing fairness throughout the development of educational systems based on ai we identified common and diverging views about the challenges and the needs faced by educational technologies experts in practice that lead the community to have a clear understanding on the main questions raising doubts in this topic based on these findings we highlighted directions that will facilitate the ongoing research towards fairer ai for education,0
ai ethics is now a global topic of discussion in academic and policy circles at least 84 publicprivate initiatives have produced statements describing highlevel principles values and other tenets to guide the ethical development deployment and governance of ai according to recent metaanalyses ai ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics despite the initial credibility granted to a principled approach to ai ethics by the connection to principles in medical ethics there are reasons to be concerned about its future impact on ai development and governance significant differences exist between medicine and ai development that suggest a principled approach in the latter may not enjoy success comparable to the former compared to medicine ai development lacks 1 common aims and fiduciary duties 2 professional history and norms 3 proven methods to translate principles into practice and 4 robust legal and professional accountability mechanisms these differences suggest we should not yet celebrate consensus around highlevel principles that hide deep political and normative disagreement,0
industry actors in the united states have gained extensive influence in conversations about the regulation of generalpurpose artificial intelligence ai systems although industry participation is an important part of the policy process it can also cause regulatory capture whereby industry coopts regulatory regimes to prioritize private over public welfare capture of ai policy by ai developers and deployers could hinder such regulatory goals as ensuring the safety fairness beneficence transparency or innovation of generalpurpose ai systems in this paper we first introduce different models of regulatory capture from the social science literature we then present results from interviews with 17 ai policy experts on what policy outcomes could compose regulatory capture in us ai policy which ai industry actors are influencing the policy process and whether and how ai industry actors attempt to achieve outcomes of regulatory capture experts were primarily concerned with capture leading to a lack of ai regulation weak regulation or regulation that overemphasizes certain policy goals over others experts most commonly identified agendasetting 15 of 17 interviews advocacy 13 academic capture 10 information management 9 cultural capture through status 7 and media capture 7 as channels for industry influence to mitigate these particular forms of industry influence we recommend systemic changes in developing technical expertise in government and civil society independent funding streams for the ai ecosystem increased transparency and ethics requirements greater civil society access to policy and various procedural safeguards,0
given the success of chatgpt lamda and other large language models llms there has been an increase in development and usage of llms within the technology sector and other sectors while the level in which llms has not reached a level where it has surpassed human intelligence there will be a time when it will such llms can be referred to as advanced llms currently there are limited usage of ethical artificial intelligence ai principles and guidelines addressing advanced llms due to the fact that we have not reached that point yet however this is a problem as once we do reach that point we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way which will lead to undesired and unexpected consequences this paper addresses this issue by discussing what ethical ai principles and guidelines can be used to address highly advanced llms,0
in this paper we examine algorithmic fairness from the perspective of law aiming to identify best practices and strategies for the specification and adoption of fairness definitions and algorithms in realworld systems and use cases we start by providing a brief introduction of current antidiscrimination law in the european union and the united states and discussing the concepts of bias and fairness from an legal and ethical viewpoint we then proceed by presenting a set of algorithmic fairness definitions by example aiming to communicate their objectives to nontechnical audiences then we introduce a set of core criteria that need to be taken into account when selecting a specific fairness definition for realworld use case applications finally we enumerate a set of key considerations and best practices for the design and employment of fairness methods on realworld ai applications,0
ai is transforming the existing technology landscape at a rapid phase enabling datainformed decision making and autonomous decision making unlike any other technology because of the decisionmaking ability of ai ethics and governance became a key concern there are many emerging ai risks for humanity such as autonomous weapons automationspurred job loss socioeconomic inequality bias caused by data and algorithms privacy violations and deepfakes social diversity equity and inclusion are considered key success factors of ai to mitigate risks create values and drive social justice sustainability became a broad and complex topic entangled with ai many organizations government corporate notforprofits charities and ngos have diversified strategies driving ai for business optimization and socialandenvironmental justice partnerships and collaborations become important more than ever for equity and inclusion of diversified and distributed people data and capabilities therefore in our journey towards an aienabled sustainable future we need to address ai ethics and governance as a priority these ai ethics and governance should be underpinned by human ethics,0
how can humans remain in control of artificial intelligence aibased systems designed to perform tasks autonomously such systems are increasingly ubiquitous creating benefits but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group the concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans however clear requirements for researchers designers and engineers are yet inexistent making the development of aibased systems that remain under meaningful human control challenging in this paper we address the gap between philosophical theory and engineering practice by identifying through an iterative process of abductive thinking four actionable properties for aibased systems under meaningful human control which we discuss making use of two applications scenarios automated vehicles and aibased hiring first a system in which humans and ai algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate second humans and ai agents within the system should have appropriate and mutually compatible representations third responsibility attributed to a human should be commensurate with that humans ability and authority to control the system fourth there should be explicit links between the actions of the ai agents and actions of humans who are aware of their moral responsibility we argue that these four properties will support practicallyminded professionals to take concrete steps toward designing and engineering for ai systems that facilitate meaningful human control,0
decisions made by various artificial intelligence ai systems greatly influence our daytoday lives with the increasing use of ai systems it becomes crucial to know that they are fair identify the underlying biases in their decisionmaking and create a standardized framework to ascertain their fairness in this paper we propose a novel fairness score to measure the fairness of a datadriven ai system and a standard operating procedure sop for issuing fairness certification for such systems fairness score and audit process standardization will ensure quality reduce ambiguity enable comparison and improve the trustworthiness of the ai systems it will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems furthermore a fairness certificate issued by a designated thirdparty auditing agency following the standardized process would boost the conviction of the organizations in the ai systems that they intend to deploy the bias index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset to substantiate the proposed framework we iteratively train a model on biased and unbiased data using multiple datasets and check that the fairness score and the proposed process correctly identify the biases and judge the fairness,0
the rapid integration of artificial intelligence ai into k12 stem education presents transformative opportunities alongside significant ethical challenges while aipowered tools such as intelligent tutoring systems its automated assessments and predictive analytics enhance personalized learning and operational efficiency they also risk perpetuating algorithmic bias eroding student privacy and exacerbating educational inequities this paper examines the dualedged impact of ai in stem classrooms analyzing its benefits eg adaptive learning realtime feedback and drawbacks eg surveillance risks pedagogical limitations through an ethical lens we identify critical gaps in current ai education research particularly the lack of subjectspecific frameworks for responsible integration and propose a threephased implementation roadmap paired with a tiered professional development model for educators our framework emphasizes equitycentered design combining technical ai literacy with ethical reasoning to foster critical engagement among students key recommendations include mandatory bias audits lowresource adaptation strategies and policy alignment to ensure ai serves as a tool for inclusive humancentered stem education by bridging theory and practice this work advances a researchbacked approach to ai integration that prioritizes pedagogical integrity equity and student agency in an increasingly algorithmic world keywords artificial intelligence stem education algorithmic bias ethical ai k12 pedagogy equity in education,0
this report prepared by the montreal ai ethics institute provides recommendations in response to the national security commission on artificial intelligence nscai key considerations for responsible development and fielding of artificial intelligence document the report centres on the idea that responsible ai should be made the norm rather than an exception it does so by utilizing the guiding principles of 1 alleviating friction in existing workflows 2 empowering stakeholders to get buyin and 3 conducting an effective translation of abstract standards into actionable engineering practices after providing some overarching comments on the document from the nscai the report dives into the primary contribution of an actionable framework to help operationalize the ideas presented in the document from the nscai the framework consists of 1 a learning knowledge and information exchange lkie 2 the three ways of responsible ai 3 an empiricallydriven riskprioritization matrix and 4 achieving the right level of complexity all components reinforce each other to move from principles to practice in service of making responsible ai the norm rather than the exception,0
the unclear development direction of human society is a deep reason for that it is difficult to form a uniform ethical standard for human society and artificial intelligence since the 21st century the latest advances in the internet brain science and artificial intelligence have brought new inspiration to the research on the development direction of human society through the study of the internet brain model ai iq evaluation and the evolution of the brain this paper proposes that the evolution of population knowledge base is the key for judging the development direction of human society thereby discussing the standards and norms for the construction of artificial intelligence ethics,0
this paper explores the ethical implications of integrating artificial intelligence ai in educational settings from primary schools to universities while drawing insights from ancient greek philosophy to address emerging concerns as ai technologies increasingly influence learning environments they offer novel opportunities for personalized learning efficient assessment and datadriven decisionmaking however these advancements also raise critical ethical questions regarding data privacy algorithmic bias student autonomy and the changing roles of educators this research examines specific use cases of ai in education analyzing both their potential benefits and drawbacks by revisiting the philosophical principles of ancient greek thinkers such as socrates aristotle and plato we discuss how their writings can guide the ethical implementation of ai in modern education the paper argues that while ai presents significant challenges a balanced approach informed by classical philosophical thought can lead to an ethically sound transformation of education it emphasizes the evolving role of teachers as facilitators and the importance of fostering student initiative in airich environments,0
artificial intelligence ai research is routinely criticized for its real and potential impacts on society and we lack adequate institutional responses to this criticism and to the responsibility that it reflects ai research often falls outside the purview of existing feedback mechanisms such as the institutional review board irb which are designed to evaluate harms to human subjects rather than harms to human society in response we have developed the ethics and society review board esr a feedback panel that works with researchers to mitigate negative ethical and societal aspects of ai research the esrs main insight is to serve as a requirement for funding researchers cannot receive grant funding from a major ai funding program at our university until the researchers complete the esr process for the proposal in this article we describe the esr as we have designed and run it over its first year across 41 proposals we analyze aggregate esr feedback on these proposals finding that the panel most commonly identifies issues of harms to minority groups inclusion of diverse stakeholders in the research plan dual use and representation in data surveys and interviews of researchers who interacted with the esr found that 58 felt that it had influenced the design of their research project 100 are willing to continue submitting future projects to the esr and that they sought additional scaffolding for reasoning through ethics and society issues,0
an assurance case is a structured argument typically produced by safety engineers to communicate confidence that a critical or complex system such as an aircraft will be acceptably safe within its intended context assurance cases often inform third party approval of a system one emerging proposition within the trustworthy ai and autonomous systems aias research community is to use assurance cases to instil justified confidence that specific aias will be ethically acceptable when operational in welldefined contexts this paper substantially develops the proposition and makes it concrete it brings together the assurance case methodology with a set of ethical principles to structure a principlesbased ethics assurance argument pattern the principles are justice beneficence nonmaleficence and respect for human autonomy with the principle of transparency playing a supporting role the argument pattern shortened to the acronym praise is described the objective of the proposed praise argument pattern is to provide a reusable template for individual ethics assurance cases by which engineers developers operators or regulators could justify communicate or challenge a claim about the overall ethical acceptability of the use of a specific aias in a given sociotechnical context we apply the pattern to the hypothetical use case of an autonomous robotaxi service in a city centre,0
ethics based auditing eba is a structured process whereby an entitys past or present behaviour is assessed for consistency with moral principles or norms recently eba has attracted much attention as a governance mechanism that may bridge the gap between principles and practice in ai ethics however important aspects of eba such as the feasibility and effectiveness of different auditing procedures have yet to be substantiated by empirical research in this article we address this knowledge gap by providing insights from a longitudinal industry case study over 12 months we observed and analysed the internal activities of astrazeneca a biopharmaceutical company as it prepared for and underwent an ethicsbased ai audit while previous literature concerning eba has focused on proposing evaluation metrics or visualisation techniques our findings suggest that the main difficulties large multinational organisations face when conducting eba mirror classical governance challenges these include ensuring harmonised standards across decentralised organisations demarcating the scope of the audit driving internal communication and change management and measuring actual outcomes the case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which eba procedures must be integrated to be feasible and effective,0
artificial intelligence ai is a digital technology that will be of major importance for the development of humanity in the near future ai has raised fundamental questions about what we should do with such systems what the systems themselves should do what risks they involve and how we can control these after the background to the field 1 this article introduces the main debates 2 first on ethical issues that arise with ai systems as objects ie tools made and used by humans here the main sections are privacy 21 manipulation 22 opacity 23 bias 24 autonomy responsibility 26 and the singularity 27 then we look at ai systems as subjects ie when ethics is for the ai systems themselves in machine ethics 28 and artificial moral agency 29 finally we look at future developments and the concept of ai 3 for each section within these themes we provide a general explanation of the ethical issues we outline existing positions and arguments then we analyse how this plays out with current technologies and finally what policy consequences may be drawn,0
artificial intelligence ai is an effective science which employs strong enough approaches methods and techniques to solve unsolvable real world based problems because of its unstoppable rise towards the future there are also some discussions about its ethics and safety shaping an ai friendly environment for people and a people friendly environment for ai can be a possible answer for finding a shared context of values for both humans and robots in this context objective of this paper is to address the ethical issues of ai and explore the moral dilemmas that arise from ethical algorithms from pre set or acquired values in addition the paper will also focus on the subject of ai safety as general the paper will briefly analyze the concerns and potential solutions to solving the ethical issues presented and increase readers awareness on ai safety as another related research interest,0
ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trips fare pricing with automated algorithms that rely on artificial intelligence ai this type of ai algorithm namely a price discrimination algorithm is widely used in the industrys black box systems for dynamic individualized pricing lacking transparency studying such ai systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms recently in an effort to enhance transparency in city planning the city of chicago regulation mandated that transportation providers publish anonymized data on ridehailing as a result we present the first largescale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications the application of random effects models from the metaanalysis literature combines the citylevel effects of ai bias on fare pricing from census tract attributes aggregated from the american community survey an analysis of 100 million ridehailing samples from the city of chicago indicates a significant disparate impact in fare pricing of neighborhoods due to ai bias learned from ridehailing utilization patterns associated with demographic attributes neighborhoods with larger nonwhite populations higher poverty levels younger residents and high education levels are significantly associated with higher fare prices with combined effect sizes measured in cohens d of 032 028 069 and 024 for each demographic respectively further our methods hold promise for identifying and addressing the sources of disparate impact in ai algorithms learning from datasets that contain us geolocations,0
systems that augment sensory abilities are increasingly employing ai and machine learning ml approaches with applications ranging from object recognition and scene description tools for blind users to sound awareness tools for ddeaf users however unlike many other aienabled technologies these systems provide information that is already available to nondisabled people in this paper we discuss unique ai fairness challenges that arise in this context including accessibility issues with data and models ethical implications in deciding what sensory information to convey to the user and privacy concerns both for the primary user and for others,0
creating fair ai systems is a complex problem that involves the assessment of contextdependent bias concerns existing research and programming libraries express specific concerns as measures of bias that they aim to constrain or mitigate in practice one should explore a wide variety of sometimes incompatible measures before deciding which ones warrant corrective action but their narrow scope means that most new situations can only be examined after devising new measures in this work we present a mathematical framework that distils literature measures of bias into building blocks hereby facilitating new combinations to cover a wide range of fairness concerns such as classification or recommendation differences across multiple multivalue sensitive attributes eg many genders and races and their intersections we show how this framework generalizes existing concepts and present frequently used blocks we provide an opensource implementation of our framework as a python library called fairbench that facilitates systematic and extensible exploration of potential bias concerns,0
artificial intelligence has rapidly become a cornerstone technology significantly influencing europes societal and economic landscapes however the proliferation of ai also raises critical ethical legal and regulatory challenges the certain certification for ethical and regulatory transparency in artificial intelligence project addresses these issues by developing a comprehensive framework that integrates regulatory compliance ethical standards and transparency into ai systems in this position paper we outline the methodological steps for building the core components of this framework specifically we present i semantic machine learning operations mlops for structured ai lifecycle management ii ontologydriven data lineage tracking to ensure traceability and accountability and iii regulatory operations regops workflows to operationalize compliance requirements by implementing and validating its solutions across diverse pilots certain aims to advance regulatory compliance and to promote responsible ai innovation aligned with european standards,0
artificial intelligence ai models are now being utilized in all facets of our lives such as healthcare education and employment since they are used in numerous sensitive environments and make decisions that can be life altering potential biased outcomes are a pressing matter developers should ensure that such models dont manifest any unexpected discriminatory practices like partiality for certain genders ethnicities or disabled people with the ubiquitous dissemination of ai systems researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them significant research has been conducted in addressing such issues to ensure models dont intentionally or unintentionally perpetuate bias this survey offers a synopsis of the different ways researchers have promoted fairness in ai systems we explore the different definitions of fairness existing in the current literature we create a comprehensive taxonomy by categorizing different types of bias and investigate cases of biased ai in different application domains a thorough study is conducted of the approaches and techniques employed by researchers to mitigate bias in ai models moreover we also delve into the impact of biased models on user experience and the ethical considerations to contemplate when developing and deploying such models we hope this survey helps researchers and practitioners understand the intricate details of fairness and bias in ai systems by sharing this thorough survey we aim to promote additional discourse in the domain of equitable and responsible ai,0
in recent years much research has been dedicated to uncovering the environmental impact of artificial intelligence ai showing that training and deploying ai systems require large amounts of energy and resources and the outcomes of ai may lead to decisions and actions that may negatively impact the environment this new knowledge raises new ethical questions such as when is it unjustifiable to develop an ai system and how to make design choices considering its environmental impact however so far the environmental impact of ai has largely escaped ethical scrutiny as ai ethics tends to focus strongly on themes such as transparency privacy safety responsibility and bias considering the environmental impact of ai from an ethical perspective expands the scope of ai ethics beyond an anthropocentric focus towards including morethanhuman actors such as animals and ecosystems this paper explores the ethical implications of the environmental impact of ai for designing ai systems by drawing on environmental justice literature in which three categories of justice are distinguished referring to three elements that can be unjust the distribution of benefits and burdens distributive justice decisionmaking procedures procedural justice and institutionalized social norms justice as recognition based on these tenets of justice we outline criteria for developing environmentally just ai systems given their ecological impact,0
the 4th industrial revolution is the culmination of the digital age nowadays technologies such as robotics nanotechnology genetics and artificial intelligence promise to transform our world and the way we live artificial intelligence ethics and safety is an emerging research field that has been gaining popularity in recent years several private public and nongovernmental organizations have published guidelines proposing ethical principles for regulating the use and development of autonomous intelligent systems metaanalyses of the ai ethics research field point to convergence on certain principles that supposedly govern the ai industry however little is known about the effectiveness of this form of ethics in this paper we would like to conduct a critical analysis of the current state of ai ethics and suggest that this form of governance based on principled ethical guidelines is not sufficient to norm the ai industry and its developers we believe that drastic changes are necessary both in the training processes of professionals in the fields related to the development of software and intelligent systems and in the increased regulation of these professionals and their industry to this end we suggest that law should benefit from recent contributions from bioethics to make the contributions of ai ethics to governance explicit in legal terms,0
artificial intelligence ai can bring substantial benefits to society by helping to reduce costs increase efficiency and enable new solutions to complex problems using floridis notion of how to design the infosphere as a starting point in this chapter i consider the question what are the limits of design ie what are the conceptual constraints on designing ai for social good the main argument of this chapter is that while design is a useful conceptual tool to shape technologies and societies collective efforts towards designing future societies are constrained by both internal and external factors internal constraints on design are discussed by evoking hardins thought experiment regarding the tragedy of the commons further hayeks classical distinction between cosmos and taxis is used to demarcate external constraints on design finally five design principles are presented which are aimed at helping policymakers manage the internal and external constraints on design a successful approach to designing future societies needs to account for the emergent properties of complex systems by allowing space for serendipity and sociotechnological coevolution,0
to implement fair machine learning in a sustainable way choosing the right fairness objective is key since fairness is a concept of justice which comes in various sometimes conflicting definitions this is not a trivial task though the most appropriate fairness definition for an artificial intelligence ai system is a matter of ethical standards and legal requirements and the right choice depends on the particular use case and its context in this position paper we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users such a structure would first of all support ai practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process however this approach would also help document the reasoning behind the decision making due to the general complexity of the topic of fairness in ai we argue that specifying fairness for a given use case is the best way forward to maintain confidence in ai systems in this case this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience,0
ai technologies have the potential to dramatically impact the lives of people with disabilities pwd indeed improving the lives of pwd is a motivator for many stateoftheart ai systems such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing or language prediction algorithms that can augment communication for people with speech or cognitive disabilities however widely deployed ai systems may not work properly for pwd or worse may actively discriminate against them these considerations regarding fairness in ai for pwd have thus far received little attention in this position paper we identify potential areas of concern regarding how several ai technology categories may impact particular disability constituencies if care is not taken in their design development and testing we intend for this risk assessment of how various classes of ai might interact with various classes of disability to provide a roadmap for future research that is needed to gather data test these hypotheses and build more inclusive algorithms,0
artificial intelligence ai continues to find more numerous and more critical applications in the financial services industry giving rise to fair and ethical ai as an industrywide objective while many ethical principles and guidelines have been published in recent years they fall short of addressing the serious challenges that model developers face when building ethical ai solutions we survey the practical and overarching issues surrounding model development from design and implementation complexities to the shortage of tools and the lack of organizational constructs we show how practical considerations reveal the gaps between highlevel principles and concrete deployed ai applications with the aim of starting industrywide conversations toward solution approaches,0
important decisions that impact human lives livelihoods and the natural environment are increasingly being automated delegating tasks to socalled automated decisionmaking systems adms can improve efficiency and enable new solutions however these benefits are coupled with ethical challenges for example adms may produce discriminatory outcomes violate individual privacy and undermine human selfdetermination new governance mechanisms are thus needed that help organisations design and deploy adms in ways that are ethical while enabling society to reap the full economic and social benefits of automation in this article we consider the feasibility and efficacy of ethicsbased auditing eba as a governance mechanism that allows organisations to validate claims made about their adms building on previous work we define eba as a structured process whereby an entitys present or past behaviour is assessed for consistency with relevant principles or norms we then offer three contributions to the existing literature first we provide a theoretical explanation of how eba can contribute to good governance by promoting procedural regularity and transparency second we propose seven criteria for how to design and implement eba procedures successfully third we identify and discuss the conceptual technical social economic organisational and institutional constraints associated with eba we conclude that eba should be considered an integral component of multifaced approaches to managing the ethical risks posed by adms,0
in todays media landscape the role of artificial intelligence ai in shaping societal perspectives and journalistic integrity is becoming increasingly apparent this paper presents two case studies centred on maltas media market featuring technical novelty despite its relatively small scale malta offers invaluable insights applicable to both similar and broader media contexts these two projects focus on media monitoring and present tools designed to analyse potential biases in news articles and television news segments the first project uses computer vision and natural language processing techniques to analyse the coherence between images in news articles and their corresponding captions headlines and article bodies the second project employs computer vision techniques to track individuals onscreen time or visual exposure in news videos providing queryable data these initiatives aim to contribute to society by providing both journalists and the public with the means to identify biases furthermore we make these tools accessible to journalists to improve the trustworthiness of media outlets by offering robust tools for detecting and reducing bias,0
as ai systems are integrated into high stakes social domains researchers now examine how to design and operate them in a safe and ethical manner however the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested in this paper we examine the vagueness in debates about the safety and ethical behavior of ai systems we show how this vagueness cannot be resolved through mathematical formalism alone instead requiring deliberation about the politics of development as well as the context of deployment drawing from a new sociotechnical lexicon we redefine vagueness in terms of distinct design challenges at key stages in ai system development the resulting framework of hard choices in artificial intelligence hcai empowers developers by 1 identifying points of overlap between design decisions and major sociotechnical challenges 2 motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed as such hcai contributes to a timely debate about the status of ai development in democratic societies arguing that deliberation should be the goal of ai safety not just the procedure by which it is ensured,0
research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning artificial intelligence ai waterintensive data centres polluting mineral extraction and ewaste dumping are incontrovertibly part of ais footprint in this article i turn to areas affected by aifuelled environmental harm and identify an ethics of resistance emerging from local activists which i term elemental ethics elemental ethics interrogates the ai value chains problematic relationship with the elements that make up the world critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by socalled intelligent systems while this ethics is emerging from grassroots and indigenous groups it echoes recent calls from environmental philosophy to reconnect with the environment via the elements in empirical terms this article looks at groups in chile resisting a google data centre project in santiago and lithium extraction used for rechargeable batteries in lickan antay indigenous territory atacama desert as i show elemental ethics can complement topdown utilitarian and quantitative approaches to ai ethics and sustainable ai as well as interrogate whose lived experience and wellbeing counts in debates on ai extinction,0
artificial intelligence ai aims to elevate healthcare to a pinnacle by aiding clinical decision support overcoming the challenges related to the design of ethical ai will enable clinicians physicians healthcare professionals and other stakeholders to use and trust ai in healthcare settings this study attempts to identify the major ethical principles influencing the utility performance of ai at different technological levels such as data access algorithms and systems through a thematic analysis we observed that justice privacy bias lack of regulations risks and interpretability are the most important principles to consider for ethical ai this datadriven study has analyzed secondary survey data from the pew research center 2020 of 36 ai experts to categorize the top ethical principles of ai design to resolve the ethical issues identified by the metaanalysis and domain experts we propose a new utilitarian ethicsbased theoretical framework for designing ethical ai for the healthcare domain,0
the adoption of artificial intelligence ai in retail has significantly transformed the industry enabling more personalized services and efficient operations however the rapid implementation of ai technologies raises ethical concerns particularly regarding consumer privacy and fairness this study aims to analyze the ethical challenges of ai applications in retail explore ways retailers can implement ai technologies ethically while remaining competitive and provide recommendations on ethical ai practices a descriptive survey design was used to collect data from 300 respondents across major ecommerce platforms data were analyzed using descriptive statistics including percentages and mean scores findings shows a high level of concerns among consumers regarding the amount of personal data collected by aidriven retail applications with many expressing a lack of trust in how their data is managed also fairness is another major issue as a majority believe ai systems do not treat consumers equally raising concerns about algorithmic bias it was also found that ai can enhance business competitiveness and efficiency without compromising ethical principles such as data privacy and fairness data privacy and transparency were highlighted as critical areas where retailers need to focus their efforts indicating a strong demand for stricter data protection protocols and ongoing scrutiny of ai systems the study concludes that retailers must prioritize transparency fairness and data protection when deploying ai systems the study recommends ensuring transparency in ai processes conducting regular audits to address biases incorporating consumer feedback in ai development and emphasizing consumer data privacy,0
trustworthy artificial intelligence ai is based on seven technical requirements sustained over three main pillars that should be met throughout the systems entire life cycle it should be 1 lawful 2 ethical and 3 robust both from a technical and a social perspective however attaining truly trustworthy ai concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the systems life cycle and considers previous aspects from different lenses a more holistic vision contemplates four essential axes the global principles for ethical use and development of aibased systems a philosophical take on ai ethics a riskbased approach to ai regulation and the mentioned pillars and requirements the seven requirements human agency and oversight robustness and safety privacy and data governance transparency diversity nondiscrimination and fairness societal and environmental wellbeing and accountability are analyzed from a triple perspective what each requirement for trustworthy ai is why it is needed and how each requirement can be implemented in practice on the other hand a practical approach to implement trustworthy ai systems allows defining the concept of responsibility of aibased systems facing the law through a given auditing process therefore a responsible ai system is the resulting notion we introduce in this work and a concept of utmost necessity that can be realized through auditing processes subject to the challenges posed by the use of regulatory sandboxes our multidisciplinary vision of trustworthy ai culminates in a debate on the diverging views published lately about the future of ai our reflections in this matter conclude that regulation is a key for reaching a consensus among these views and that trustworthy and responsible ai systems will be crucial for the present and future of our society,0
in this metaethnography we explore three different angles of ethical artificial intelligence ai design implementation including the philosophical ethical viewpoint the technical perspective and framing through a political lens our qualitative research includes a literature review that highlights the crossreferencing of these angles by discussing the value and drawbacks of contrastive topdown bottomup and hybrid approaches previously published the novel contribution to this framework is the political angle which constitutes ethics in ai either being determined by corporations and governments and imposed through policies or law coming from the top or ethics being called for by the people coming from the bottom as well as topdown bottomup and hybrid technicalities of how ai is developed within a moral construct and in consideration of its users with expected and unexpected consequences and longterm impact in the world there is a focus on reinforcement learning as an example of a bottomup applied technical approach and ai ethics principles as a practical topdown approach this investigation includes realworld case studies to impart a global perspective as well as philosophical debate on the ethics of ai and theoretical future thought experimentation based on historical facts current world circumstances and possible ensuing realities,0
this paper introduces aijim the artificial intelligence journalism integration model a novel framework for integrating realtime ai into environmental journalism aijim combines vision transformerbased hazard detection crowdsourced validation with 252 validators and automated reporting within a scalable modular architecture a duallayer explainability approach ensures ethical transparency through fast cambased visual overlays and optional limebased boxlevel interpretations validated in a 2024 pilot on the island of mallorca using the namicgreen platform aijim achieved 854 detection accuracy and 897 agreement with expert annotations while reducing reporting latency by 40 unlike conventional approaches such as datadriven journalism or ai factchecking aijim provides a transferable model for participatory communitydriven environmental reporting advancing journalism artificial intelligence and sustainability in alignment with the un sustainable development goals and the eu ai act,0
this paper addresses the ways ai ethics research operates on an ideology of ideal theory in the sense discussed by mills 2005 and recently applied to ai ethics by fazelpour lipton 2020 i address the structural and methodological conditions that attract ai ethics researchers to ideal theorizing and the consequences this approach has for the quality and future of our research community finally i discuss the possibilities for a nonideal future in ai ethics,0
in light of the rise of generative ai and recent debates about the sociopolitical implications of largelanguage models and chatbots this article investigates the eus artificial intelligence act aia the worlds first major attempt by a government body to address and mitigate the potentially negative impacts of ai technologies the article critically analyzes the aia from a distinct economic ethics perspective ie ordoliberalism 20 a perspective currently lacking in the academic literature it evaluates in particular the aias ordoliberal strengths and weaknesses and proposes reform measures that could be taken to strengthen the aia,0
numerous fairness metrics have been proposed and employed by artificial intelligence ai experts to quantitatively measure bias and define fairness in ai models recognizing the need to accommodate stakeholders diverse fairness understandings efforts are underway to solicit their input however conveying ai fairness metrics to stakeholders without ai expertise capturing their personal preferences and seeking a collective consensus remain challenging and underexplored to bridge this gap we propose a new framework earn fairness which facilitates collective metric decisions among stakeholders without requiring ai expertise the framework features an adaptable interactive system and a stakeholdercentered earn fairness process to explain fairness metrics ask stakeholders personal metric preferences review metrics collectively and negotiate a consensus on metric selection to gather empirical results we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without ai knowledge we identify their personal metric preferences and their acceptable level of unfairness in individual sessions subsequently we uncovered how they reached metric consensus in team sessions our work shows that the earn fairness framework enables stakeholders to express personal preferences and reach consensus providing practical guidance for implementing humancentered ai fairness in highrisk contexts through this approach we aim to harmonize fairness expectations of diverse stakeholders fostering more equitable and inclusive ai fairness,0
artificial intelligence ai offers incredible possibilities for patient care but raises significant ethical issues such as the potential for bias powerful ethical frameworks exist to minimize these issues but are often developed for academic or regulatory environments and tend to be comprehensive but overly prescriptive making them difficult to operationalize within fastpaced resourceconstrained environments we introduce the scalable agile framework for execution in ai safeai designed to balance ethical rigor with business priorities by embedding ethical oversight into standard agilebased product development workflows the framework emphasizes the early establishment of testable acceptance criteria fairness metrics and transparency metrics to manage model uncertainty while also promoting continuous monitoring and reevaluation of these metrics across the ai lifecycle a core component of this framework are responsibility metrics using scenariobased probability analogy mapping designed to enhance transparency and stakeholder trust this ensures that retraining or tuning activities are subject to lightweight but meaningful ethical review by focusing on the minimum necessary requirements for responsible development our framework offers a scalable businessaligned approach to ethical ai suitable for organizations without dedicated ethics teams,0
here we discuss the four key principles of biomedical ethics from surgical context we elaborate on the definition of fairness and its implications in ai system design with taxonomy of algorithmic biases in ai we discuss the shifts in ethical paradigms as the degree of autonomy in ai systems continue to evolve we also emphasize the need for continuous revisions of ethics in ai due to evolution and dynamic nature of ai systems and technologies,0
observers and practitioners of artificial intelligence ai have proposed an fdastyle licensing regime for the most advanced ai models or frontier models in this paper we explore the applicability of approval regulation that is regulation of a product that combines experimental minima with government licensure conditioned partially or fully upon that experimentation to the regulation of frontier ai there are a number of reasons to believe that approval regulation simplistically applied would be inapposite for frontier ai risks domains of weak fit include the difficulty of defining the regulated product the presence of knightian uncertainty or deep ambiguity about harms from ai the potentially transmissible nature of risks and distributed activities among actors involved in the ai lifecycle we conclude by highlighting the role of policy learning and experimentation in regulatory development describing how learning from other forms of ai regulation and improvements in evaluation and testing methods can help to overcome some of the challenges we identify,0
data are essential in developing healthcare artificial intelligence ai systems however patient data collection access and use raise ethical concerns including informed consent data bias data protection and privacy data ownership and benefit sharing various ethical frameworks have been proposed to ensure the ethical use of healthcare data and ai however these frameworks often align with western cultural values social norms and institutional contexts emphasizing individual autonomy and wellbeing ethical guidelines must reflect political and cultural settings to account for cultural diversity inclusivity and historical factors such as colonialism thus this paper discusses healthcare data ethics in the ai era in africa from the ubuntu philosophy perspective it focuses on the contrast between individualistic and communitarian approaches to data ethics the proposed framework could inform stakeholders including ai developers healthcare providers the public and policymakers about healthcare data ethical usage in ai in africa,0
as the influence and use of artificial intelligence ai have grown and its transformative potential has become more apparent many questions have been raised regarding the economic political social and ethical implications of its use public opinion plays an important role in these discussions influencing product adoption commercial development research funding and regulation in this paper we present results of an indepth survey of public opinion of artificial intelligence conducted with 10005 respondents spanning eight countries and six continents we report widespread perception that ai will have significant impact on society accompanied by strong support for the responsible development and use of ai and also characterize the publics sentiment towards ai with four key themes exciting useful worrying and futuristic whose prevalence distinguishes response to ai in different countries,0
this article analyzes the impact of artificial intelligence ai on contemporary society and the importance of adopting an ethical approach to its development and implementation within organizations it examines the technocritical perspective of some philosophers and researchers who warn of the risks of excessive technologization that could undermine human autonomy however the article also acknowledges the active role that various actors such as governments academics and civil society can play in shaping the development of ai aligned with human and social values a multidimensional approach is proposed that combines ethics with regulation innovation and education it highlights the importance of developing detailed ethical frameworks incorporating ethics into the training of professionals conducting ethical impact audits and encouraging the participation of stakeholders in the design of ai in addition four fundamental pillars are presented for the ethical implementation of ai in organizations 1 integrated values 2 trust and transparency 3 empowering human growth and 4 identifying strategic factors these pillars encompass aspects such as alignment with the companys ethical identity governance and accountability humancentered design continuous training and adaptability to technological and market changes the conclusion emphasizes that ethics must be the cornerstone of any organizations strategy that seeks to incorporate ai establishing a solid framework that ensures that technology is developed and used in a way that respects and promotes human values,0
this paper presents a new approach to prevent transportation accidents and monitor drivers behavior using a healthcare ai system that incorporates fairness and ethics dangerous medical cases and unusual behavior of the driver are detected fairness algorithm is approached in order to improve decisionmaking and address ethical issues such as privacy issues and to consider challenges that appear in the wild within ai in healthcare and driving a healthcare professional will be alerted about any unusual activity and the drivers location when necessary is provided in order to enable the healthcare professional to immediately help to the unstable driver therefore using the healthcare ai system allows for accidents to be predicted and thus prevented and lives may be saved based on the builtin ai system inside the vehicle which interacts with the er system,0
recently the use of sound measures and metrics in artificial intelligence has become the subject of interest of academia government and industry efforts towards measuring different phenomena have gained traction in the ai community as illustrated by the publication of several influential field reports and policy documents these metrics are designed to help decision takers to inform themselves about the fastmoving and impacting influences of key advances in artificial intelligence in general and machine learning in particular in this paper we propose to use such newfound capabilities of ai technologies to augment our ai measuring capabilities we do so by training a model to classify publications related to ethical issues and concerns in our methodology we use an expert manually curated dataset as the training set and then evaluate a large set of research papers finally we highlight the implications of ai metrics in particular their contribution towards developing trustful and fair aibased tools and technologies keywords ai ethics ai fairness ai measurement ethics in computer science,0
the artificial intelligence community ai has recently engaged in activism in relation to their employers other members of the community and their governments in order to shape the societal and ethical implications of ai it has achieved some notable successes but prospects for further political organising and activism are uncertain we survey activism by the ai community over the last six years apply two analytical frameworks drawing upon the literature on epistemic communities and worker organising and bargaining and explore what they imply for the future prospects of the ai community success thus far has hinged on a coherent shared culture and high bargaining power due to the high demand for a limited supply of ai talent both are crucial to the future of ai activism and worthy of sustained attention,0
as artificial intelligence ai rapidly approaches humanlevel performance in medical imaging it is crucial that it does not exacerbate or propagate healthcare disparities prior research has established ais capacity to infer demographic data from chest xrays leading to a key concern do models using demographic shortcuts have unfair predictions across subpopulations in this study we conduct a thorough investigation into the extent to which medical ai utilizes demographic encodings focusing on potential fairness discrepancies within both indistribution training sets and external test sets our analysis covers three key medical imaging disciplines radiology dermatology and ophthalmology and incorporates data from six global chest xray datasets we confirm that medical imaging ai leverages demographic shortcuts in disease classification while correcting shortcuts algorithmically effectively addresses fairness gaps to create locally optimal models within the original data distribution this optimality is not true in new test settings surprisingly we find that models with less encoding of demographic attributes are often most globally optimal exhibiting better fairness during model evaluation in new test environments our work establishes best practices for medical imaging models which maintain their performance and fairness in deployments beyond their initial training contexts underscoring critical considerations for ai clinical deployments across populations and sites,0
ethical ai spans a gamut of considerations among these the most popular ones fairness and interpretability have remained largely distinct in technical pursuits we discuss and elucidate the differences between fairness and interpretability across a variety of dimensions further we develop two principlesbased frameworks towards developing ethical ai for the future that embrace aspects of both fairness and interpretability first interpretability for fairness proposes instantiating interpretability within the realm of fairness to develop a new breed of ethical ai second fairness and interpretability initiates deliberations on bringing the best aspects of both together we hope that these two frameworks will contribute to intensifying scholarly discussions on new frontiers of ethical ai that brings together fairness and interpretability,0
artificial intelligence ai is a technology which is increasingly being utilised in society and the economy worldwide and its implementation is planned to become more prevalent in coming years ai is increasingly being embedded in our lives supplementing our pervasive use of digital technologies but this is being accompanied by disquiet over problematic and dangerous implementations of ai or indeed even ai itself deciding to do dangerous and problematic actions especially in fields such as the military medicine and criminal justice these developments have led to concerns about whether and how ai systems adhere and will adhere to ethical standards these concerns have stimulated a global conversation on ai ethics and have resulted in various actors from different countries and sectors issuing ethics and governance initiatives and guidelines for ai such developments form the basis for our research in this report combining our international and interdisciplinary expertise to give an insight into what is happening in australia china europe india and the us,0
artificial intelligence and machine learning are increasingly used to offload decision making from people in the past one of the rationales for this replacement was that machines unlike people can be fair and unbiased evidence suggests otherwise we begin by entertaining the ideas that algorithms can replace people and that algorithms cannot be biased taken as axioms these statements quickly lead to absurdity spurred on by this result we investigate the slogans more closely and identify equivocation surrounding the word bias we diagnose three forms of outrageintellectual moral and politicalthat are at play when people react emotionally to algorithmic bias then we suggest three practical approaches to addressing bias that the ai community could take which include clarifying the language around bias developing new auditing methods for intelligent systems and building certain capabilities into these systems we conclude by offering a moral regarding the conversations about algorithmic bias that may transfer to other areas of artificial intelligence,0
the advent of generative ai has marked a significant milestone in artificial intelligence demonstrating remarkable capabilities in generating realistic images texts and data patterns however these advancements come with heightened concerns over data privacy and copyright infringement primarily due to the reliance on vast datasets for model training traditional approaches like differential privacy machine unlearning and data poisoning only offer fragmented solutions to these complex issues our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle we advocate for integrated approaches that combines technical innovation with ethical foresight holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective this work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in generative ai,0
an emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people in domains including gameplaying text generation and artistic expression these models go beyond earlier approaches in the way they are tailored to individuals and the way they are designed for interaction rather than simply the reproduction of fixed precomputed behaviors we refer to these as mimetic models and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability our framework includes a number of distinct scenarios for the use of such models and considers the impacts on a range of different participants including the target being modeled the operator who deploys the model and the entities that interact with it,0
this article presents a critique of ethics in the context of artificial intelligence ai it argues for the need to question established patterns of thought and traditional authorities including core concepts such as autonomy morality and ethics these concepts are increasingly inadequate to deal with the complexities introduced by emerging ai and autonomous agents this critique has several key components clarifying conceptual ambiguities honestly addressing epistemic issues and thoroughly exploring fundamental normative problems the ultimate goal is to reevaluate and possibly redefine some traditional ethical concepts to better address the challenges posed by ai,0
in response to calls for greater interdisciplinary involvement from the social sciences and humanities in the development governance and study of artificial intelligence systems this paper presents one sociologists view on the problem of algorithmic bias and the reproduction of societal bias discussions of bias in ai cover much of the same conceptual terrain that sociologists studying inequality have long understood using more specific terms and theories concerns over reproducing societal bias should be informed by an understanding of the ways that inequality is continually reproduced in society processes that ai systems are either complicit in or can be designed to disrupt and counter the contrast presented here is between conservative and radical approaches to ai with conservatism referring to dominant tendencies that reproduce and strengthen the status quo while radical approaches work to disrupt systemic forms of inequality the limitations of conservative approaches to class gender and racial bias are discussed as specific examples along with the social structures and processes that biases in these areas are linked to societal issues can no longer be out of scope for ai and machine learning given the impact of these systems on human lives this requires engagement with a growing body of critical ai scholarship that goes beyond biased data to analyze structured ways of perpetuating inequality opening up the possibility for radical alternatives,0
the significant advancements in applying artificial intelligence ai to healthcare decisionmaking medical diagnosis and other domains have simultaneously raised concerns about the fairness and bias of ai systems this is particularly critical in areas like healthcare employment criminal justice credit scoring and increasingly in generative ai models genai that produce synthetic media such systems can lead to unfair outcomes and perpetuate existing inequalities including generative biases that affect the representation of individuals in synthetic data this survey paper offers a succinct comprehensive overview of fairness and bias in ai addressing their sources impacts and mitigation strategies we review sources of bias such as data algorithm and human decision biases highlighting the emergent issue of generative ai bias where models may reproduce and amplify societal stereotypes we assess the societal impact of biased ai systems focusing on the perpetuation of inequalities and the reinforcement of harmful stereotypes especially as generative ai becomes more prevalent in creating content that influences public perception we explore various proposed mitigation strategies discussing the ethical considerations of their implementation and emphasizing the need for interdisciplinary collaboration to ensure effectiveness through a systematic literature review spanning multiple academic disciplines we present definitions of ai bias and its different types including a detailed look at generative ai bias we discuss the negative impacts of ai bias on individuals and society and provide an overview of current approaches to mitigate ai bias including data preprocessing model selection and postprocessing we emphasize the unique challenges presented by generative ai models and the importance of strategies specifically tailored to address these,0
interest is growing in artificial empathy but so is confusion about what artificial empathy is or needs to be this confusion makes it challenging to navigate the technical and ethical issues that accompany empathic ai development here we outline a framework for thinking about empathic ai based on the premise that different constellations of capabilities associated with empathy are important for different empathic ai applications we describe distinctions of capabilities that we argue belong under the empathy umbrella and show how three medical empathic ai use cases require different sets of these capabilities we conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both ai creators and users,0
regulating artificial intelligence ai has become necessary in light of its deployment in highrisk scenarios this paper explores the proposal to extend legal personhood to ai and robots which had not yet been examined through the lens of the general public we present two studies n 3559 to obtain peoples views of electronic legal personhood visvis existing liability models our study reveals peoples desire to punish automated agents even though these entities are not recognized any mental state furthermore people did not believe automated agents punishment would fulfill deterrence nor retribution and were unwilling to grant them legal punishment preconditions namely physical independence and assets collectively these findings suggest a conflict between the desire to punish automated agents and its perceived impracticability we conclude by discussing how future design and legal decisions may influence how the public reacts to automated agents wrongdoings,0
there is a growing need for datadriven research efforts on how the public perceives the ethical moral and legal issues of autonomous ai systems the current debate on the responsibility gap posed by these systems is one such example this work proposes a mixed ai ethics model that allows normative and descriptive research to complement each other by aiding scholarly discussion with data gathered from the public we discuss its implications on bridging the gap between optimistic and pessimistic views towards ai systems deployment,0
current advances in research development and application of artificial intelligence ai systems have yielded a farreaching discourse on ai ethics in consequence a number of ethics guidelines have been released in recent years these guidelines comprise normative principles and recommendations aimed to harness the disruptive potentials of new ai technologies designed as a comprehensive evaluation this paper analyzes and compares these guidelines highlighting overlaps but also omissions as a result i give a detailed overview of the field of ai ethics finally i also examine to what extent the respective ethical principles and values are implemented in the practice of research development and application of ai systems and how the effectiveness in the demands of ai ethics can be improved,0
ai has made significant strides recently leading to various applications in both civilian and military sectors the military sees ai as a solution for developing more effective and faster technologies while ai offers benefits like improved operational efficiency and precision targeting it also raises serious ethical and legal concerns particularly regarding human rights violations autonomous weapons that make decisions without human input can threaten the right to life and violate international humanitarian law to address these issues we propose a threestage framework design in deployment and duringafter use for evaluating human rights concerns in the design deployment and use of military ai each phase includes multiple components that address various concerns specific to that phase ranging from bias and regulatory issues to violations of international humanitarian law by this framework we aim to balance the advantages of ai in military operations with the need to protect human rights,0
the proliferation of artificial intelligence ai has sparked an overwhelming number of ai ethics guidelines boards and codes of conduct these outputs primarily analyse competing theories principles and values for ai development and deployment however as a series of recent problematic incidents about ai ethicsethicists demonstrate this orientation is insufficient before proceeding to evaluate other professions ai ethicists should critically evaluate their own yet such an evaluation should be more explicitly and systematically undertaken in the literature i argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of ai contrary to traditional metaethics which reflects on the nature of morality and moral judgements in a nonnormative way feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like applying this perspective to the context of ai i suggest that a feminist metaethics of ai would examine i the continuity between theory and action in ai ethics ii the reallife effects of ai ethics iii the role and profile of those involved in ai ethics and iv the effects of ai on power relations through methods that pay attention to context emotions and narrative,0
organisations are increasingly open to scrutiny and need to be able to prove that they operate in a fair and ethical way accountability should extend to the production and use of the data and knowledge assets used in ai systems as it would for any raw material or process used in production of physical goods this paper considers collective intelligence comprising data and knowledge generated by crowdsourced workforces which can be used as core components of ai systems a proposal is made for the development of a supply chain model for tracking the creation and use of crowdsourced collective intelligence assets with a blockchain based decentralised architecture identified as an appropriate means of providing validation accountability and fairness,0
this article reviews the landscape of ethical challenges of integrating artificial intelligence ai into smart healthcare products including medical electronic devices differences between traditional ethics in the medical domain and emerging ethical challenges with aidriven healthcare are presented particularly as they relate to transparency bias privacy safety responsibility justice and autonomy open challenges and recommendations are outlined to enable the integration of ethical principles into the design validation clinical trials deployment monitoring repair and retirement of aibased smart healthcare products,0
artificial intelligence ai has rapidly transformed various sectors including healthcare where it holds the potential to revolutionize clinical practice and improve patient outcomes however its integration into medical settings brings significant ethical challenges that need careful consideration this paper examines the current state of ai in healthcare focusing on five critical ethical concerns justice and fairness transparency patient consent and confidentiality accountability and patientcentered and equitable care these concerns are particularly pressing as ai systems can perpetuate or even exacerbate existing biases often resulting from nonrepresentative datasets and opaque model development processes the paper explores how bias lack of transparency and challenges in maintaining patient trust can undermine the effectiveness and fairness of ai applications in healthcare in addition we review existing frameworks for the regulation and deployment of ai identifying gaps that limit the widespread adoption of these systems in a just and equitable manner our analysis provides recommendations to address these ethical challenges emphasizing the need for fairness in algorithm design transparency in model decisionmaking and patientcentered approaches to consent and data privacy by highlighting the importance of continuous ethical scrutiny and collaboration between ai developers clinicians and ethicists we outline pathways for achieving more responsible and inclusive ai implementation in healthcare these strategies if adopted could enhance both the clinical value of ai and the trustworthiness of ai systems among patients and healthcare professionals ensuring that these technologies serve all populations equitably,0
several policy options exist or have been proposed to further responsible artificial intelligence ai development and deployment institutions including us government agencies states professional societies and private and public sector businesses are well positioned to implement these policies however given limited resources not all policies can or should be equally prioritized we define and review nine suggested policies for furthering responsible ai rank each policy on potential use and impact and recommend prioritization relative to each institution type we find that predeployment audits and assessments and postdeployment accountability are likely to have the highest impact but also the highest barriers to adoption we recommend that us government agencies and companies highly prioritize development of predeployment audits and assessments while the us national legislature should highly prioritize postdeployment accountability we suggest that us government agencies and professional societies should highly prioritize policies that support responsible ai research and that states should highly prioritize support of responsible ai education we propose that companies can highly prioritize involving community stakeholders in development efforts and supporting diversity in ai development we advise lower levels of prioritization across institutions for ai ethics statements and databases of ai technologies or incidents we recognize that no one policy will lead to responsible ai and instead advocate for strategic policy implementation across institutions,0
artificial intelligence ai has been clearly established as a technology with the potential to revolutionize fields from healthcare to finance if developed and deployed responsibly this is the topic of responsible ai which emphasizes the need to develop trustworthy ai systems that minimize bias protect privacy support security and enhance transparency and accountability explainable ai xai has been broadly considered as a building block for responsible ai rai with most of the literature considering it as a solution for improved transparency this work proposes that xai and responsible ai are significantly more deeply entwined in this work we explore stateoftheart literature on rai and xai technologies based on our findings we demonstrate that xai can be utilized to ensure fairness robustness privacy security and transparency in a wide range of contexts our findings lead us to conclude that xai is an essential foundation for every pillar of rai,0
an increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence ai algorithms in spheres ranging from healthcare transportation and education to college admissions recruitment provision of loans and many more realms since they now touch on many aspects of our lives it is crucial to develop ai algorithms that are not only accurate but also objective and fair recent studies have shown that algorithmic decisionmaking may be inherently prone to unfairness even when there is no intention for it this paper presents an overview of the main concepts of identifying measuring and improving algorithmic fairness when using ai algorithms the paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness fairnessenhancing mechanisms are then reviewed and divided into preprocess inprocess and postprocess mechanisms a comprehensive comparison of the mechanisms is then conducted towards a better understanding of which mechanisms should be used in different scenarios the paper then describes the most commonly used fairnessrelated datasets in this field finally the paper ends by reviewing several emerging research subfields of algorithmic fairness,0
we are moving towards a future where artificial intelligence ai based agents make many decisions on behalf of humans from healthcare decision making to social media censoring these agents face problems and make decisions with ethical and societal implications ethical behaviour is a critical characteristic that we would like in a humancentric ai a common observation in humancentric industries like the service industry and healthcare is that their professionals tend to break rules if necessary for prosocial reasons this behaviour among humans is defined as prosocial rule breaking to make ai agents more human centric we argue that there is a need for a mechanism that helps ai agents identify when to break rules set by their designers to understand when ai agents need to break rules we examine the conditions under which humans break rules for prosocial reasons in this paper we present a study that introduces a vaccination strategy dilemma to human participants and analyses their responses in this dilemma one needs to decide whether they would distribute covid19 vaccines only to members of a highrisk group follow the enforced rule or in selected cases administer the vaccine to a few social influencers break the rule which might yield an overall greater benefit to society the results of the empirical study suggest a relationship between stakeholder utilities and prosocial rule breaking psrb which neither deontological nor utilitarian ethics completely explain finally the paper discusses the design characteristics of an ethical agent capable of psrb and the future research directions on psrb in the ai realm we hope that this will inform the design of future ai agents and their decisionmaking behaviour,0
artificial intelligence ai has received unprecedented attention in recent years raising ethical concerns about the development and use of ai technology in the present article we advocate that these concerns stem from a blurred understanding of ai how it can be used and how it has been interpreted in society we explore the concept of ai based on three descriptive facets and consider ethical issues related to each facet finally we propose a framework for the ethical assessment of the use of ai,0
from massive facerecognitionbased surveillance and machinelearningbased decision systems predicting crime recidivism rates to the move towards automated health diagnostic systems artificial intelligence ai is being used in scenarios that have serious consequences in peoples lives however this rapid permeation of ai into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it for instance recent studies have shown that commercial face recognition systems have much higher error rates for dark skinned women while having minimal errors on light skinned men a 2016 propublica investigation uncovered that machine learning based tools that assess crime recidivism rates in the us are biased against african americans other studies show that natural language processing tools trained on newspapers exhibit societal biases eg finishing the analogy man is to computer programmer as woman is to x by homemaker at the same time books such as weapons of math destruction and automated inequality detail how people in lower socioeconomic classes in the us are subjected to more automated decision making tools than those who are in the upper class thus these tools are most often used on people towards whom they exhibit the most bias while many technical solutions have been proposed to alleviate bias in machine learning systems we have to take a holistic and multifaceted approach this includes standardization bodies determining what types of systems can be used in which scenarios making sure that automated decision tools are created by people from diverse backgrounds and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools,0
educational technologies and the systems of schooling in which they are deployed enact particular ideologies about what is important to know and how learners should learn as artificial intelligence technologies in education and beyond may contribute to inequitable outcomes for marginalized communities various approaches have been developed to evaluate and mitigate the harmful impacts of ai however we argue in this paper that the dominant paradigm of evaluating fairness on the basis of performance disparities in ai models is inadequate for confronting the systemic inequities that educational ai systems reproduce we draw on a lens of structural injustice informed by critical theory and black feminist scholarship to critically interrogate several widelystudied and widelyadopted categories of educational ai and explore how they are bound up in and reproduce historical legacies of structural injustice and inequity regardless of the parity of their models performance we close with alternative visions for a more equitable future for educational ai,0
this commentary traces contemporary discourses on the relationship between artificial intelligence and labour and explains why these principles must be comprehensive in their approach to labour and ai first the commentary asserts that ethical frameworks in ai alone are not enough to guarantee workers rights since they lack enforcement mechanisms and the representation of different stakeholders secondly it argues that current discussions on ai and labour focus on the deployment of these technologies in the workplace but ignore the essential role of human labour in their development particularly in the different cases of outsourced labour around the world finally it recommends using existing human rights frameworks for working conditions to provide more comprehensive ethical principles and regulations the commentary concludes by arguing that the central question regarding the future of work should not be whether intelligent machines will replace humans but who will own these systems and have a say in their development and operation,0
explainable ai constitutes a fundamental step towards establishing fairness and addressing bias in algorithmic decisionmaking despite the large body of work on the topic the benefit of solutions is mostly evaluated from a conceptual or theoretical point of view and the usefulness for realworld use cases remains uncertain in this work we aim to state clear usercentric desiderata for explainable ai reflecting common explainability needs experienced in statistical production systems of the european central bank we link the desiderata to archetypical user roles and give examples of techniques and methods which can be used to address the users needs to this end we provide two concrete use cases from the domain of statistical data production in central banks the detection of outliers in the centralised securities database and the datadriven identification of data quality checks for the supervisory banking data system,0
the recent rapid advancements in artificial intelligence research and deployment have sparked more discussion about the potential ramifications of socially and emotionallyintelligent ai the question is not if research can produce such affectivelyaware ai but when it will what will it mean for society when machines and the corporations and governments they serve can read peoples minds and emotions what should developers and operators of such ai do and what should they not do the goal of this article is to preempt some of the potential implications of these developments and propose a set of guidelines for evaluating the moral and ethical consequences of affectivelyaware ai in order to guide researchers industry professionals and policymakers we propose a multistakeholder analysis framework that separates the ethical responsibilities of ai developers visvis the entities that deploy such ai which we term operators our analysis produces two pillars that clarify the responsibilities of each of these stakeholders provable beneficence which rests on proving the effectiveness of the ai and responsible stewardship which governs responsible collection use and storage of data and the decisions made from such data we end with recommendations for researchers developers operators as well as regulators and lawmakers,0
calls for engagement with the public in artificial intelligence ai research development and governance are increasing leading to the use of surveys to capture peoples values perceptions and experiences related to ai in this paper we critically examine the state of human participant surveys associated with these topics through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to ai we explore prominent perspectives and methodological nuances associated with surveys to date we find that public surveys on ai topics are vulnerable to specific western knowledge values and assumptions in their design including in their positioning of ethical concepts and societal values lack sufficient critical discourse surrounding deployment strategies and demonstrate inconsistent forms of transparency in their reporting based on our findings we distill provocations and heuristic questions for our community to recognize the limitations of surveys for meeting the goals of engagement and to cultivate shared principles to design deploy and interpret surveys cautiously and responsibly,0
as generative ai systems become widely adopted they enable unprecedented creation levels of synthetic data across text images audio and video modalities while research has addressed the energy consumption of model training and inference a critical sustainability challenge remains understudied digital waste this term refers to stored data that consumes resources without serving a specific andor immediate purpose this paper presents this terminology in the ai context and introduces digital waste as an ethical imperative within generative ai development positioning environmental sustainability as core for responsible innovation drawing from established digital resource management approaches we examine how other disciplines manage digital waste and identify transferable approaches for the ai community we propose specific recommendations encompassing research directions technical interventions and cultural shifts to mitigate the environmental consequences of indefinite data storage by expanding ai ethics beyond immediate concerns like bias and privacy to include intergenerational environmental justice this work contributes to a more comprehensive ethical framework that considers the complete lifecycle impact of generative ai systems,0
augmentation technologies fueled by artificial intelligence ai are undergoing a process of adaptation and normalization geared to everyday users in various roles as practitioners educators and students while new innovations applications and algorithms are developed as augmentation technology chapter 1 focuses on human subjects contexts and rhetorical strategies proposed for them by external actors the chapter discusses core functions of technical and professional communication and provides rationale for positioning technical and professional communicators tpcs to understand augmentation technologies and ai as a means to design ethical futures across this work an overview of augmentation technologies and ai an ethical design futures framework serves as a guide for reframing professional practice and pedagogy to promote digital and ai literacy surrounding the ethical design adoption and adaptation of augmentation technologies the chapter concludes with an overview of the remaining chapters in this book,0
despite conflicting definitions and conceptions of fairness ai fairness researchers broadly agree that fairness is contextspecific however when faced with generalpurpose ai which by definition serves a range of contexts how should we think about fairness we argue that while we cannot be prescriptive about what constitutes fair outcomes we can specify the processes that different stakeholders should follow in service of fairness specifically we consider the obligations of two major groups system providers and system deployers while system providers are natural candidates for regulatory attention the current state of ai understanding offers limited insight into how upstream factors translate into downstream fairness impacts thus we recommend that providers invest in evaluative research studying how model development decisions influence fairness and disclose whom they are serving their models to or at the very least reveal sufficient information for external researchers to conduct such research on the other hand system deployers are closer to realworld contexts and can leverage their proximity to end users to address fairness harms in different ways here we argue they should responsibly disclose information about users and personalization and conduct rigorous evaluations across different levels of fairness overall instead of focusing on enforcing fairness outcomes we prioritize intentional informationgathering by system providers and deployers that can facilitate later contextaware action this allows us to be specific and concrete about the processes even while the contexts remain unknown ultimately this approach can sharpen how we distribute fairness responsibilities and inform more fluid contextsensitive interventions as ai continues to advance,0
this paper explores the growing presence of emotionally responsive artificial intelligence through a critical and interdisciplinary lens bringing together the voices of earlycareer researchers from multiple fields it explores how ai systems that simulate or interpret human emotions are reshaping our interactions in areas such as education healthcare mental health caregiving and digital life the analysis is structured around four central themes the ethical implications of emotional ai the cultural dynamics of humanmachine interaction the risks and opportunities for vulnerable populations and the emerging regulatory design and technical considerations the authors highlight the potential of affective ai to support mental wellbeing enhance learning and reduce loneliness as well as the risks of emotional manipulation overreliance misrepresentation and cultural bias key challenges include simulating empathy without genuine understanding encoding dominant sociocultural norms into ai systems and insufficient safeguards for individuals in sensitive or highrisk contexts special attention is given to children elderly users and individuals with mental health challenges who may interact with ai in emotionally significant ways however there remains a lack of cognitive or legal protections which are necessary to navigate such engagements safely the report concludes with ten recommendations including the need for transparency certification frameworks regionspecific finetuning human oversight and longitudinal research a curated supplementary section provides practical tools models and datasets to support further work in this domain,0
in recent years we have witnessed a marked development and growth in artificial intelligence the growth of the data volume generated by sensors and machines combined with the information flow resulting from the user actions on the internet with high investments of the governments and the companies in this area provided the practice and developed the algorithms of the artificial intelligence however the people in general started to feel a particular fear regarding the security and privacy of their data and the theme of the artificial intelligence ethics began to be discussed more regularly the investigation aim of this work is to understand the possibility of adopting artificial intelligence nowadays in our society having as a mandatory assumption ethics and respect towards data and peoples privacy with that purpose in mind a model has been created mainly supported by the theories that were used to create the model the suggested model has been tested and validated through structural equation modeling based on data taken back from the respondents answers to the questionnaire online 237 answers mainly from the investigation technologies area the results obtained enabled the validation of seven of the nine investigation hypotheses of the proposed model it was impossible to confirm any association between the social influence construct and the variables of behavioral intention and the use of artificial intelligence the aim of this work was accomplished once the investigation theme was validated and proved that it is possible to adopt artificial intelligence in our society using the attitude towards ethical behavioral construct as the mainstay of the model,0
artificial intelligence ai ethics has emerged as a burgeoning yet pivotal area of scholarly research this study conducts a comprehensive bibliometric analysis of the ai ethics literature over the past two decades the analysis reveals a discernible tripartite progression characterized by an incubation phase followed by a subsequent phase focused on imbuing ai with humanlike attributes culminating in a third phase emphasizing the development of humancentric ai systems after that they present seven key ai ethics issues encompassing the collingridge dilemma the ai status debate challenges associated with ai transparency and explainability privacy protection complications considerations of justice and fairness concerns about algocracy and human enfeeblement and the issue of superintelligence finally they identify two notable research gaps in ai ethics regarding the large ethics model lem and ai identification and extend an invitation for further scholarly research,0
while research in ai methods for music generation and analysis has grown in scope and impact ai researchers engagement with the ethical consequences of this work has not kept pace to encourage such engagement many publication venues have introduced optional or required ethics statements for ai research papers though some authors use these ethics statements to critically engage with the broader implications of their research we find that the majority of ethics statements in the ai music literature do not appear to be effectively utilized for this purpose in this work we conduct a review of ethics statements across ismir nime and selected prominent works in ai music from the past five years we then offer suggestions for both audio conferences and researchers for engaging with ethics statements in ways that foster meaningful reflection rather than formulaic compliance,0
in an era characterized by the pervasive integration of artificial intelligence into decisionmaking processes across diverse industries the demand for trust has never been more pronounced this thesis embarks on a comprehensive exploration of bias and fairness with a particular emphasis on their ramifications within the banking sector where aidriven decisions bear substantial societal consequences in this context the seamless integration of fairness explainability and human oversight is of utmost importance culminating in the establishment of what is commonly referred to as responsible ai this emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both ai regulations and universal human rights standards particularly in the realm of automated decisionmaking systems nowadays embedding ethical principles into the development training and deployment of ai models is crucial for compliance with forthcoming european regulations and for promoting societal good this thesis is structured around three fundamental pillars understanding bias mitigating bias and accounting for bias these contributions are validated through their practical application in realworld scenarios in collaboration with intesa sanpaolo this collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of aibased decisionmaking systems in line with opensource principles we have released bias on demand and fairview as accessible python packages further promoting progress in the field of ai fairness,0
this comprehensive survey explored the evolving landscape of generative artificial intelligence ai with a specific focus on the transformative impacts of mixture of experts moe multimodal learning and the speculated advancements towards artificial general intelligence agi it critically examined the current state and future trajectory of generative artificial intelligence ai exploring how innovations like googles gemini and the anticipated openai q project are reshaping research priorities and applications across various domains including an impact analysis on the generative ai research taxonomy it assessed the computational challenges scalability and realworld implications of these technologies while highlighting their potential in driving significant progress in fields like healthcare finance and education it also addressed the emerging academic challenges posed by the proliferation of both aithemed and aigenerated preprints examining their impact on the peerreview process and scholarly communication the study highlighted the importance of incorporating ethical and humancentric methods in ai development ensuring alignment with societal norms and welfare and outlined a strategy for future ai research that focuses on a balanced and conscientious use of moe multimodality and agi in generative ai,0
with the powerful performance of artificial intelligence ai also comes prevalent ethical issues though governments and corporations have curated multiple ai ethics guidelines to curb unethical behavior of ai the effect has been limited probably due to the vagueness of the guidelines in this paper we take a closer look at how ai ethics issues take place in real world in order to have a more indepth and nuanced understanding of different ethical issues as well as their social impact with a content analysis of ai incident database which is an effort to prevent repeated real world ai failures by cataloging incidents we identified 13 application areas which often see unethical use of ai with intelligent service robots languagevision models and autonomous driving taking the lead ethical issues appear in 8 different forms from inappropriate use and racial discrimination to physical safety and unfair algorithm with this taxonomy of ai ethics issues we aim to provide ai practitioners with a practical guideline when trying to deploy ai applications ethically,0
this article explores the evolving role of programming languages in the context of artificial intelligence it highlights the need for programming languages to ensure human understanding while eliminating unnecessary implementation details and suggests that future programs should be designed to recognize and actively support user interests the vision includes a threelevel process using natural language for requirements translating it into a precise system definition language and finally optimizing the code for performance the concept of an ultimate programming language is introduced emphasizing its role in maintaining human control over machines trust reliability and benevolence are identified as key elements that will enhance cooperation between humans and ai systems,0
ensuring fairness in decentralized multiagent systems presents significant challenges due to emergent biases systemic inefficiencies and conflicting agent incentives this paper provides a comprehensive survey of fairness in multiagent ai introducing a novel framework where fairness is treated as a dynamic emergent property of agent interactions the framework integrates fairness constraints bias mitigation strategies and incentive mechanisms to align autonomous agent behaviors with societal values while balancing efficiency and robustness through empirical validation we demonstrate that incorporating fairness constraints results in more equitable decisionmaking this work bridges the gap between ai ethics and system design offering a foundation for accountable transparent and socially responsible multiagent ai systems,0
the history of science and technology shows that seemingly innocuous developments in scientific theories and research have enabled realworld applications with significant negative consequences for humanity in order to ensure that the science and technology of ai is developed in a humane manner we must develop research publication norms that are informed by our growing understanding of ais potential threats and use cases unfortunately its difficult to create a set of publication norms for responsible ai because the field of ai is currently fragmented in terms of how this technology is researched developed funded etc to examine this challenge and find solutions the montreal ai ethics institute maiei cohosted two public consultations with the partnership on ai in may 2020 these meetups examined potential publication norms for responsible ai with the goal of creating a clear set of recommendations and ways forward for publishers in its submission maiei provides six initial recommendations these include 1 create tools to navigate publication decisions 2 offer a page number extension 3 develop a network of peers 4 require broad impact statements 5 require the publication of expected results and 6 revamp the peerreview process after considering potential concerns regarding these recommendations including constraining innovation and creating a black market for ai research maiei outlines three ways forward for publishers these include 1 state clearly and consistently the need for established norms 2 coordinate and build trust as a community and 3 change the approach,0
the ethics of artificial intelligence ai systems has risen as an imminent concern across scholarly communities this concern has propagated a great interest in algorithmic fairness large research agendas are now devoted to increasing algorithmic fairness assessing algorithmic fairness and understanding human perceptions of fairness we argue that there is an overreliance on fairness as a single dimension of morality which comes at the expense of other important human values drawing from moral psychology we present five moral dimensions that go beyond fairness and suggest three ways these alternative dimensions may contribute to ethical ai development,0
webbased human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its oftenlatent nature the use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decisionmaking and more broadly help curb a widespread social problem trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the web a higher score indicates a greater risk of traffickingrelated involuntary activities in this paper we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the darpa memex program drawing on multiinstitutional data systems and experiences collected during this time we also conduct post hoc bias analyses and present a bias mitigation plan our findings show that while automatic trafficking detection is an important application of ai for social good it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate debiasing this ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads,0
the field of computer vision is rapidly evolving particularly in the context of new methods of neural architecture design these models contribute to 1 the climate crisis increased co2 emissions and 2 the privacy crisis data leakage concerns to address the often overlooked impact the computer vision cv community has on these crises we outline a novel ethical framework textitp4ai principlism for ai an augmented principlistic view of ethical dilemmas within ai we then suggest using p4ai to make concrete recommendations to the community to mitigate the climate and privacy crises,0
recent ai ethics has focused on applying abstract principles downward to practice this paper moves in the other direction ethical insights are generated from the lived experiences of aidesigners working on tangible human problems and then cycled upward to influence theoretical debates surrounding these questions 1 should ai as trustworthy be sought through explainability or accurate performance 2 should ai be considered trustworthy at all or is reliability a preferable aim 3 should ai ethics be oriented toward establishing protections for users or toward catalyzing innovation specific answers are less significant than the larger demonstration that ai ethics is currently unbalanced toward theoretical principles and will benefit from increased exposure to grounded practices and dilemmas,0
generative ai enables automated effective manipulation at scale despite the growing general ethical discussion around generative ai the specific manipulation risks remain inadequately investigated this article outlines essential inquiries encompassing conceptual empirical and design dimensions of manipulation pivotal for comprehending and curbing manipulation risks by highlighting these questions the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of generative ai technologies,0
the transition of artificial intelligence ai from a labbased science to live human contexts brings into sharp focus many historic sociocultural biases inequalities and moral dilemmas many questions that have been raised regarding the broader ethics of ai are also relevant for ai in education aied aied raises further specific challenges related to the impact of its technologies on users how such technologies might be used to reinforce or alter the way that we learn and teach and what we as a society and individuals value as outcomes of education this chapter discusses key ethical dimensions of ai and contextualises them within aied design and engineering practices to draw connections between the aied systems we build the questions about human learning and development we ask the ethics of the pedagogies we use and the considerations of values that we promote in and through aied within a wider sociotechnical system,0
aiaugmented systems are traditionally designed to streamline human decisionmaking by minimizing cognitive load clarifying arguments and optimizing efficiency however in a world where algorithmic certainty risks becoming an orwellian tool of epistemic control true intellectual growth demands not passive acceptance but active struggle drawing on the dystopian visions of george orwell and philip k dick where reality is unstable perception malleable and truth contested this paper introduces cognitive dissonance ai cdai a novel framework that deliberately sustains uncertainty rather than resolving it cdai does not offer closure but compels users to navigate contradictions challenge biases and wrestle with competing truths by delaying resolution and promoting dialectical engagement cdai enhances reflective reasoning epistemic humility critical thinking and adaptability in complex decisionmaking this paper examines the theoretical foundations of the approach presents an implementation model explores its application in domains such as ethics law politics and science and addresses key ethical concerns including decision paralysis erosion of user autonomy cognitive manipulation and bias in ai reasoning in reimagining ai as an engine of doubt rather than a deliverer of certainty cdai challenges dominant paradigms of aiaugmented reasoning and offers a new vision one in which ai sharpens the mind not by resolving conflict but by sustaining it rather than reinforcing huxleyan complacency or pacifying the user into intellectual conformity cdai echoes nietzsches vision of the uebermensch urging users to transcend passive cognition through active epistemic struggle,0
this paper presents the key conclusions to the forthcoming edited book on the ethics of artificial intelligence in education practices challenges and debates august 2022 routlege as well as highlighting the key contributions to the book it discusses the key questions and the grand challenges for the field of ai in education aiedin the context of ethics and ethical practices within the field the book itself presents diverse perspectives from outside and from within the aied as a way of achieving a broad perspective in the key ethical issues for aied and a deep understanding of work conducted to date by the aied community,0
relentless progress in artificial intelligence ai is increasingly raising concerns that machines will replace humans on the job market and perhaps altogether eliezer yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent friendly ai designed to safeguard humanity and its values i argue that from a physics perspective where everything is simply an arrangement of elementary particles this might be even harder than it appears indeed it may require thinking rigorously about the meaning of life what is meaning in a particle arrangement what is life what is the ultimate ethical imperative ie how should we strive to rearrange the particles of our universe and shape its future if we fail to answer the last question rigorously this future is unlikely to contain humans,0
organisations increasingly use automated decisionmaking systems adms to inform decisions that affect humans and their environment while the use of adms can improve the accuracy and efficiency of decisionmaking processes it is also coupled with ethical challenges unfortunately the governance mechanisms currently used to oversee human decisionmaking often fail when applied to adms in previous work we proposed that ethicsbased auditing eba ie a structured process by which adms are assessed for consistency with relevant principles or norms can a help organisations verify claims about their adms and b provide decisionsubjects with justifications for the outputs produced by adms in this article we outline the conditions under which eba procedures can be feasible and effective in practice first we argue that eba is best understood as a soft yet formal governance mechanism this implies that the main responsibility of auditors should be to spark ethical deliberation at key intervention points throughout the software development process and ensure that there is sufficient documentation to respond to potential inquiries second we frame adms as parts of larger sociotechnical systems to demonstrate that to be feasible and effective eba procedures must link to intervention points that span all levels of organisational governance and all phases of the software lifecycle the main function of eba should therefore be to inform formalise assess and interlink existing governance structures finally we discuss the policy implications of our findings to support the emergence of feasible and effective eba procedures policymakers and regulators could provide standardised reporting formats facilitate knowledge exchange provide guidance on how to resolve normative tensions and create an independent body to oversee eba of adms,0
artificial intelligence is currently and rapidly changing the way organizations and businesses operate ethical leadership has become significantly important since organizations and businesses across various sectors are evolving with ai organizations and businesses may be facing several challenges and potential opportunities when using ai ethical leadership plays a central role in guiding organizations in facing those challenges and maximizing on those opportunities this article explores the essence of ethical leadership in the age of ai starting with a simplified introduction of ethical leadership and ai then dives into an understanding of ethical leadership its characteristics and importance the ethical challenges ai causes including bias in ai algorithms the opportunities for ethical leadership in the age of ai answers the question what actionable strategies can leaders employ to address the challenges and leverage opportunities and describes the benefits for organizations through these opportunities a proposed framework for ethical leadership is presented in this article incorporating the core components fairness transparency sustainability etc through the importance of interdisciplinary collaboration case studies of ethical leadership in ai and recommendations this article emphasizes that ethical leadership in the age of ai is morally essential and strategically advantageous,0
the integration of artificial intelligence ai and optimization hold substantial promise for improving the efficiency reliability and resilience of engineered systems due to the networked nature of many engineered systems ethically deploying methodologies at this intersection poses challenges that are distinct from other ai settings thus motivating the development of ethical guidelines tailored to aienabled optimization this paper highlights the need to go beyond fairnessdriven algorithms to systematically address ethical decisions spanning the stages of modeling data curation results analysis and implementation of optimizationbased decision support tools accordingly this paper identifies ethical considerations required when deploying algorithms at the intersection of ai and optimization via case studies in power systems as well as supply chain and logistics rather than providing a prescriptive set of rules this paper aims to foster reflection and awareness among researchers and encourage consideration of ethical implications at every step of the decisionmaking process,0
it is widely accepted that technology is ubiquitous across the planet and has the potential to solve many of the problems existing in the global south moreover the rapid advancement of artificial intelligence ai brings with it the potential to address many of the challenges outlined in the sustainable development goals sdgs in ways which were never before possible however there are many questions about how such advanced technologies should be managed and governed and whether or not the emerging ethical frameworks and standards for ai are dominated by the global north this research examines the growing body of documentation on ai ethics to examine whether or not there is equality of participation in the ongoing global discourse specifically it seeks to discover if both countries in the global south and women are underrepresented in this discourse findings indicate a dearth of references to both of these themes in the ai ethics documents suggesting that the associated ethical implications and risks are being neglected without adequate input from both countries in the global south and from women such ethical frameworks and standards may be discriminatory with the potential to reinforce marginalisation,0
this paper explores the important role of critical science and in particular of postcolonial and decolonial theories in understanding and shaping the ongoing advances in artificial intelligence artificial intelligence ai is viewed as amongst the technological advances that will reshape modern societies and their relations whilst the design and deployment of systems that continually adapt holds the promise of farreaching positive change they simultaneously pose significant risks especially to already vulnerable peoples values and power are central to this discussion decolonial theories use historical hindsight to explain patterns of power that shape our intellectual political economic and social world by embedding a decolonial critical approach within its technical practice ai communities can develop foresight and tactics that can better align research and technology development with established ethical principles centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress we highlight problematic applications that are instances of coloniality and using a decolonial lens submit three tactics that can form a decolonial field of artificial intelligence creating a critical technical practice of ai seeking reverse tutelage and reverse pedagogies and the renewal of affective and political communities the years ahead will usher in a wave of new scientific breakthroughs and technologies driven by ai research making it incumbent upon ai communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us ultimately supporting future technologies that enable greater wellbeing with the goal of beneficence and justice for all,0
the debate on ai ethics largely focuses on technical improvements and stronger regulation to prevent accidents or misuse of ai with solutions relying on holding individual actors accountable for responsible ai development while useful and necessary we argue that this agency approach disregards more indirect and complex risks resulting from ais interaction with the socioeconomic and political context this paper calls for a structural approach to assessing ais effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts this is particularly relevant for ai applied to systemic issues such as climate change and food security which require political solutions and global cooperation to properly address the wide range of ai risks and ensure ai for social good agencyfocused policies must be complemented by policies informed by a structural approach,0
ai ethics is an emerging field with multiple competing narratives about how to best solve the problem of building human values into machines two major approaches are focused on bias and compliance respectively but neither of these ideas fully encompasses ethics using moral principles to decide how to act in a particular situation our method posits that the way data is labeled plays an essential role in the way ai behaves and therefore in the ethics of machines themselves the argument combines a fundamental insight from ethics ie that ethics is about values with our practical experience building and scaling machine learning systems we want to build ai that is actually ethical by first addressing foundational concerns how to build good systems how to define what is good in relation to system architecture and who should provide that definition building ethical ai creates a foundation of trust between a company and the users of that platform but this trust is unjustified unless users experience the direct value of ethical ai until users have real control over how algorithms behave something is missing in current ai solutions this causes massive distrust in ai and apathy towards ai ethics solutions the scope of this paper is to propose an alternative path that allows for the plurality of values and the freedom of individual expression both are essential for realizing true moral character,0
the widespread use of artificial intelligence ai in many domains has revealed numerous ethical issues from data and design to deployment in response countless broad principles and guidelines for ethical ai have been published and following those specific approaches have been proposed for how to encourage ethical outcomes of ai meanwhile library and information services too are seeing an increase in the use of aipowered and machine learningpowered information systems but no practical guidance currently exists for libraries to plan for evaluate or audit the ethics of intended or deployed ai we therefore report on several promising approaches for promoting ethical ai that can be adapted from other contexts to aipowered information services and in different stages of the software lifecycle,0
the lack of explainability of artificial intelligence ai is one of the first obstacles that the industry and regulators must overcome to mitigate the risks associated with the technology the need for explainable ai xai is evident in fields where accountability ethics and fairness are critical such as healthcare credit scoring policing and the criminal justice system at the eu level the notion of explainability is one of the fundamental principles that underpin the ai act though the exact xai techniques and requirements are still to be determined and tested in practice this paper explores various approaches and techniques that promise to advance xai as well as the challenges of implementing the principle of explainability in ai governance and policies finally the paper examines the integration of xai into eu law emphasising the issues of standard setting oversight and enforcement,0
as artificial intelligence ai increasingly becomes an integral part of our societal and individual activities there is a growing imperative to develop responsible ai solutions despite a diverse assortment of machine learning fairness solutions is proposed in the literature there is reportedly a lack of practical implementation of these tools in realworld applications industry experts have participated in thorough discussions on the challenges associated with operationalising fairness in the development of machine learningempowered solutions in which a shift toward humancentred approaches is promptly advocated to mitigate the limitations of existing techniques in this work we propose a humanintheloop approach for fairness auditing presenting a mixed visual analytical system hereafter referred to as faircompass which integrates both subgroup discovery technique and the decision treebased schema for end users moreover we innovatively integrate an exploration guidance and informed analysis loop to facilitate the use of the knowledge generation model for visual analytics in faircompass we evaluate the effectiveness of faircompass for fairness auditing in a realworld scenario and the findings demonstrate the systems potential for realworld deployability we anticipate this work will address the current gaps in research for fairness and facilitate the operationalisation of fairness in machine learning systems,0
the ethical integration of artificial intelligence ai in healthcare necessitates addressing fairnessa concept that is highly contextspecific across medical fields extensive studies have been conducted to expand the technical components of ai fairness while tremendous calls for ai fairness have been raised from healthcare despite this a significant disconnect persists between technical advancements and their practical clinical applications resulting in a lack of contextualized discussion of ai fairness in clinical settings through a detailed evidence gap analysis our review systematically pinpoints several deficiencies concerning both healthcare data and the provided ai fairness solutions we highlight the scarcity of research on ai fairness in many medical domains where ai technology is increasingly utilized additionally our analysis highlights a substantial reliance on group fairness aiming to ensure equality among demographic groups from a macro healthcare system perspective in contrast individual fairness focusing on equity at a more granular level is frequently overlooked to bridge these gaps our review advances actionable strategies for both the healthcare and ai research communities beyond applying existing ai fairness methods in healthcare we further emphasize the importance of involving healthcare professionals to refine ai fairness concepts and methods to ensure contextually relevant and ethically sound ai applications in healthcare,0
the debate around bias in ai systems is central to discussions on algorithmic fairness however the term bias often lacks a clear definition despite frequently being contrasted with fairness implying that an unbiased model is inherently fair in this paper we challenge this assumption and argue that a precise conceptualization of bias is necessary to effectively address fairness concerns rather than viewing bias as inherently negative or unfair we highlight the importance of distinguishing between bias and discrimination we further explore how this shift in focus can foster a more constructive discourse within academic debates on fairness in ai systems,0
in this position paper i argue that the best way to help and protect humans using ai technology is to make them aware of the intrinsic limitations and problems of ai algorithms to accomplish this i suggest three ethical guidelines to be used in the presentation of results mandating ai systems to expose uncertainty to instill distrust and contrary to traditional views to avoid explanations the paper does a preliminary discussion of the guidelines and provides some arguments for their adoption aiming to start a debate in the community about ai ethics in practice,0
with increasing ubiquity of artificial intelligence ai in modern societies individual countries and the international community are working hard to create an innovationfriendly yet safe regulatory environment adequate regulation is key to maximize the benefits and minimize the risks stemming from ai technologies developing regulatory frameworks is however challenging due to ais global reach and the existence of widespread misconceptions about the notion of regulation we argue that airelated challenges cannot be tackled effectively without sincere international coordination supported by robust consistent domestic and international governance arrangements against this backdrop we propose the establishment of an international ai governance framework organized around a new ai regulatory agency that drawing on interdisciplinary expertise could help creating uniform standards for the regulation of ai technologies and inform the development of ai policies around the world we also believe that a fundamental change of mindset on what constitutes regulation is necessary to remove existing barriers that hamper contemporary efforts to develop ai regulatory regimes and put forward some recommendations on how to achieve this and what opportunities doing so would present,0
in the rapidly advancing field of artificial intelligence machine perception is becoming paramount to achieving increased performance image classification systems are becoming increasingly integral to various applications ranging from medical diagnostics to image generation however these systems often exhibit harmful biases that can lead to unfair and discriminatory outcomes machine learning systems that depend on a single data modality ie only images or only text can exaggerate hidden biases present in the training data if the data is not carefully balanced and filtered even so these models can still harm underrepresented populations when used in improper contexts such as when government agencies reinforce racial bias using predictive policing this thesis explores the intersection of technology and ethics in the development of fair image classification models specifically i focus on improving fairness and methods of using multiple modalities to combat harmful demographic bias integrating multimodal approaches which combine visual data with additional modalities such as text and metadata allows this work to enhance the fairness and accuracy of image classification systems the study critically examines existing biases in image datasets and classification algorithms proposes innovative methods for mitigating these biases and evaluates the ethical implications of deploying such systems in realworld scenarios through comprehensive experimentation and analysis the thesis demonstrates how multimodal techniques can contribute to more equitable and ethical ai solutions ultimately advocating for responsible ai practices that prioritize fairness,0
an emerging body of research indicates that ineffective crossfunctional collaboration the interdisciplinary work done by industry practitioners across roles represents a major barrier to addressing issues of fairness in ai design and development in this research we sought to better understand practitioners current practices and tactics to enact crossfunctional collaboration for ai fairness in order to identify opportunities to support more effective collaboration we conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies we found that practitioners engaged in bridging work to overcome frictions in understanding contextualization and evaluation around ai fairness across roles in addition in organizational contexts with a lack of resources and incentives for fairness work practitioners often piggybacked on existing requirements eg for privacy assessments and ai development norms eg the use of quantitative evaluation metrics although they worry that these tactics may be fundamentally compromised finally we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness we close by discussing opportunities for both facct researchers and ai practitioners to better support crossfunctional collaboration for fairness in the design and development of ai systems,0
this white paper underscores the critical importance of responsibly deploying artificial intelligence ai in military contexts emphasizing a commitment to ethical and legal standards the evolving role of ai in the military goes beyond mere technical applications necessitating a framework grounded in ethical principles the discussion within the paper delves into ethical ai principles particularly focusing on the fairness accountability transparency and ethics fate guidelines noteworthy considerations encompass transparency justice nonmaleficence and responsibility importantly the paper extends its examination to militaryspecific ethical considerations drawing insights from the just war theory and principles established by prominent entities in addition to the identified principles the paper introduces further ethical considerations specifically tailored for military ai applications these include traceability proportionality governability responsibility and reliability the application of these ethical principles is discussed on the basis of three use cases in the domains of sea air and land methods of automated sensor data analysis explainable ai xai and intuitive user experience are utilized to specify the use cases close to realworld scenarios this comprehensive approach to ethical considerations in military ai reflects a commitment to aligning technological advancements with established ethical frameworks it recognizes the need for a balance between leveraging ais potential benefits in military operations while upholding moral and legal standards the inclusion of these ethical principles serves as a foundation for responsible and accountable use of ai in the complex and dynamic landscape of military scenarios,0
as large language models llms become integral to recruitment processes concerns about aiinduced bias have intensified this study examines biases in candidate interview reports generated by claude 35 sonnet gpt4o gemini 15 and llama 31 405b focusing on characteristics such as gender race and age we evaluate the effectiveness of llmbased anonymization in reducing these biases findings indicate that while anonymization reduces certain biases particularly gender bias the degree of effectiveness varies across models and bias types notably llama 31 405b exhibited the lowest overall bias moreover our methodology of comparing anonymized and nonanonymized data reveals a novel approach to assessing inherent biases in llms beyond recruitment applications this study underscores the importance of careful llm selection and suggests best practices for minimizing bias in ai applications promoting fairness and inclusivity,0
the rapid growth of healthcare data and advances in computational power have accelerated the adoption of artificial intelligence ai in medicine however ai systems deployed without explicit fairness considerations risk exacerbating existing healthcare disparities potentially leading to inequitable resource allocation and diagnostic disparities across demographic subgroups to address this challenge we propose fairgrad a novel gradient reconciliation framework that automatically balances predictive performance and multiattribute fairness optimization in healthcare ai models our method resolves conflicting optimization objectives by projecting each gradient vector onto the orthogonal plane of the others thereby regularizing the optimization trajectory to ensure equitable consideration of all objectives evaluated on diverse realworld healthcare datasets and predictive tasks including substance use disorder sud treatment and sepsis mortality fairgrad achieved statistically significant improvements in multiattribute fairness metrics eg equalized odds while maintaining competitive predictive accuracy these results demonstrate the viability of harmonizing fairness and utility in missioncritical medical ai applications,0
we consider two fundamental and related issues currently faced by artificial intelligence ai development the lack of ethics and interpretability of ai decisions can interpretable ai decisions help to address ethics in ai using a randomized study we experimentally show that the empirical and liberal turn of the production of explanations tends to select ai explanations with a low denunciatory power under certain conditions interpretability tools are therefore not means but paradoxically obstacles to the production of ethical ai since they can give the illusion of being sensitive to ethical incidents we also show that the denunciatory power of ai explanations is highly dependent on the context in which the explanation takes place such as the gender or education level of the person to whom the explication is intended for ai ethics tools are therefore sometimes too flexible and selfregulation through the liberal production of explanations do not seem to be enough to address ethical issues we then propose two scenarios for the future development of ethical ai more external regulation or more liberalization of ai explanations these two opposite paths will play a major role on the future development of ethical ai,0
ethical oversight of ai research is beset by a number of problems there are numerous ways to tackle these problems however they leave full responsibility for ethical reflection in the hands of review boards and committees in this paper we propose an alternative solution the training of ethically responsible ai researchers we showcase this solution through a case study of a centre for doctoral training and outline how ethics training is structured in the program we go on to present two secondyear students reflections on their training which demonstrates some of their newly found capabilities as ethically responsible researchers,0
artificial intelligence ai has the potential to transform education with its power of uncovering insights from massive data about student learning patterns however ethical and trustworthy concerns of ai have been raised but are unsolved prominent ethical issues in high school ai education include data privacy information leakage abusive language and fairness this paper describes technological components that were built to address ethical and trustworthy concerns in a multimodal collaborative platform called allure chatbot for high school students to collaborate with ai to solve the rubiks cube in data privacy we want to ensure that the informed consent of children parents and teachers is at the center of any data that is managed since children are involved language whether textual audio or visual is acceptable both from users and ai and the system can steer interaction away from dangerous situations in information management we also want to ensure that the system while learning to improve over time does not leak information about users from one group to another,0
the ai robotics ethics society aires is a nonprofit organization founded in 2018 by aaron hui to promote awareness and the importance of ethical implementation and regulation of ai aires is now an organization with chapters at universities such as ucla los angeles usc university of southern california caltech california institute of technology stanford university cornell university brown university and the pontifical catholic university of rio grande do sul brazil aires at pucrs is the first international chapter of aires and as such we are committed to promoting and enhancing the aires mission our mission is to focus on educating the ai leaders of tomorrow in ethical principles to ensure that ai is created ethically and responsibly as there are still few proposals for how we should implement ethical principles and normative guidelines in the practice of ai system development the goal of this work is to try to bridge this gap between discourse and praxis between abstract principles and technical implementation in this work we seek to introduce the reader to the topic of ai ethics and safety at the same time we present several tools to help developers of intelligent systems develop good models this work is a developing guide published in english and portuguese contributions and suggestions are welcome,0
artificial intelligence ai was initially developed as an implicit moral agent to solve simple and clearly defined tasks where all options are predictable however it is now part of our daily life powering cell phones cameras watches thermostats vacuums cars and much more this has raised numerous concerns and some scholars and practitioners stress the dangers of ai and argue against its development as moral agents that can reason about ethics eg bryson 2008 johnson and miller 2008 sharkey 2017 tonkens 2009 van wynsberghe and robbins 2019 even though we acknowledge the potential threat in line with most other scholars eg anderson and anderson 2010 moor 2006 scheutz 2016 wallach 2010 we argue that ai advancements cannot be stopped and developers need to prepare ai to sustain explicit moral agents and face ethical dilemmas in complex and morally salient environments,0
the development of artificial intelligence ai including ai in science ais should be done following the principles of responsible ai progress in responsible ai is often quantified through evaluation metrics yet there has been less work on assessing the robustness and reliability of the metrics themselves we reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of ai application and summarise their key takeaways into a set of nonexhaustive guidelines for developing reliable metrics of responsible ai our guidelines apply to a broad spectrum of ai applications including ais,0
one objection to conventional ai ethics is that it slows innovation this presentation responds by reconfiguring ethics as an innovation accelerator the critical elements develop from a contrast between stability ais diffusion and openais dalle by analyzing the divergent values underlying their opposed strategies for development and deployment five conceptions are identified as common to acceleration ethics uncertainty is understood as positive and encouraging rather than discouraging innovation is conceived as intrinsically valuable instead of worthwhile only as mediated by social effects ai problems are solved by more ai not less permissions and restrictions governing ai emerge from a decentralized process instead of a unified authority the work of ethics is embedded in ai development and application instead of functioning from outside together these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence,0
use of artificial intelligence is growing and expanding into applications that impact peoples lives people trust their technology without really understanding it or its limitations there is the potential for harm and we are already seeing examples of that in the world ai researchers have an obligation to consider the impact of intelligent applications they work on while the ethics of ai is not clearcut there are guidelines we can consider to minimize the harm we might introduce,0
the butterfly effect a concept originating from chaos theory underscores how small changes can have significant and unpredictable impacts on complex systems in the context of ai fairness and bias the butterfly effect can stem from a variety of sources such as small biases or skewed data inputs during algorithm development saddle points in training or distribution shifts in data between training and testing phases these seemingly minor alterations can lead to unexpected and substantial unfair outcomes disproportionately affecting underrepresented individuals or groups and perpetuating preexisting inequalities moreover the butterfly effect can amplify inherent biases within data or algorithms exacerbate feedback loops and create vulnerabilities for adversarial attacks given the intricate nature of ai systems and their societal implications it is crucial to thoroughly examine any changes to algorithms or input data for potential unintended consequences in this paper we envision both algorithmic and empirical strategies to detect quantify and mitigate the butterfly effect in ai systems emphasizing the importance of addressing these challenges to promote fairness and ensure responsible ai development,0
artificial intelligence has the potential to exacerbate societal bias and set back decades of advances in equal rights and civil liberty data used to train machine learning algorithms may capture social injustices inequality or discriminatory attitudes that may be learned and perpetuated in society attempts to address this issue are rapidly emerging from different perspectives involving technical solutions social justice and data governance measures while each of these approaches are essential to the development of a comprehensive solution often discourse associated with each seems disparate this paper reviews ongoing work to ensure data justice fairness and bias mitigation in ai systems from different domains exploring the interrelated dynamics of each and examining whether the inevitability of bias in ai training data may in fact be used for social good we highlight the complexity associated with defining policies for dealing with bias we also consider technical challenges in addressing issues of societal bias,0
the ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry researchers in academia and the public however current analyses of ai in a global context are biased toward perspectives held in the us and limited by a lack of research especially outside the us and western europe this article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of ai and related technologies in five global regions our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages our review of the literature suggests that ai is likely to have markedly different social impacts depending on geographical setting likewise perceptions and understandings of ai are likely to be profoundly shaped by local cultural and social context recent research in us settings demonstrates that aidriven technologies have a pattern of entrenching social divides and exacerbating social inequality particularly among historicallymarginalized groups our literature review indicates that this pattern exists on a global scale and suggests that low and middleincome countries may be more vulnerable to the negative social impacts of ai and less likely to benefit from the attendant gains we call for rigorous ethnographic research to better understand the social impacts of ai around the world global ontheground research is particularly critical to identify ai systems that may amplify social inequality in order to mitigate potential harms deeper understanding of the social impacts of ai in diverse social settings is a necessary precursor to the development implementation and monitoring of responsible and beneficial ai technologies and forms the basis for meaningful regulation of these technologies,0
artificial intelligence ai models trained using medical images for clinical tasks often exhibit bias in the form of disparities in performance between subgroups since not all sources of biases in realworld medical imaging data are easily identifiable it is challenging to comprehensively assess how those biases are encoded in models and how capable bias mitigation methods are at ameliorating performance disparities in this article we introduce a novel analysis framework for systematically and objectively investigating the impact of biases in medical images on ai models we developed and tested this framework for conducting controlled in silico trials to assess bias in medical imaging ai using a tool for generating synthetic magnetic resonance images with known disease effects and sources of bias the feasibility is showcased by using three counterfactual bias scenarios to measure the impact of simulated bias effects on a convolutional neural network cnn classifier and the efficacy of three bias mitigation strategies the analysis revealed that the simulated biases resulted in expected subgroup performance disparities when the cnn was trained on the synthetic datasets moreover reweighing was identified as the most successful bias mitigation strategy for this setup and we demonstrated how explainable ai methods can aid in investigating the manifestation of bias in the model using this framework developing fair ai models is a considerable challenge given that many and often unknown sources of biases can be present in medical imaging datasets in this work we present a novel methodology to objectively study the impact of biases and mitigation strategies on deep learning pipelines which can support the development of clinical ai that is robust and responsible,0
introduction to improve current public health strategies in suicide prevention and mental health governments researchers and private companies increasingly use information and communication technologies and more specifically artificial intelligence and big data these technologies are promising but raise ethical challenges rarely covered by current legal systems it is essential to better identify and prevent potential ethical risks objectives the canada protocol mhsp is a tool to guide and support professionals users and researchers using ai in mental health and suicide prevention methods a checklist was constructed based upon ten international reports on ai and ethics and two guides on mental health and new technologies 329 recommendations were identified of which 43 were considered as applicable to mental health and ai the checklist was validated using a two round delphi consultation results 16 experts participated in the first round of the delphi consultation and 8 participated in the second round of the original 43 items 38 were retained they concern five categories description of the autonomous intelligent system n8 privacy and transparency n8 security n6 healthrelated risks n8 biases n8 the checklist was considered relevant by most users and could need versions tailored to each category of target users,0
most fairness in ai research focuses on exposing biases in ai systems a broader lens on fairness reveals that ai can serve a greater aspiration rooting out societal inequities from their source specifically we focus on inequities in health information and aim to reduce bias in that domain using ai the ai algorithms under the hood of search engines and social media many of which are based on recommender systems have an outsized impact on the quality of medical and health information online therefore embedding bias detection and reduction into these recommender systems serving up medical and health content online could have an outsized positive impact on patient outcomes and wellbeing in this position paper we offer the following contributions 1 we propose a novel framework of fairness via ai inspired by insights from medical education sociology and antiracism 2 we define a new term bisinformation which is related to but distinct from misinformation and encourage researchers to study it 3 we propose using ai to study detect and mitigate biased harmful andor false health information that disproportionately hurts minority groups in society and 4 we suggest several pillars and pose several open problems in order to seed inquiry in this new space while part 3 of this work specifically focuses on the health domain the fundamental computer science advances and contributions stemming from research efforts in bias reduction and fairness via ai have broad implications in all areas of society,0
the organizational use of artificial intelligence ai has rapidly spread across various sectors alongside the awareness of the benefits brought by ai there is a growing consensus on the necessity of tackling the risks and potential harms such as bias and discrimination brought about by advanced ai technologies a multitude of ai ethics principles have been proposed to tackle these risks but the outlines of organizational processes and practices for ensuring socially responsible ai development are in a nascent state to address the paucity of comprehensive governance models we present an ai governance framework the hourglass model of organizational ai governance which targets organizations that develop and use ai systems the framework is designed to help organizations deploying ai systems translate ethical ai principles into practice and align their ai systems and processes with the forthcoming european ai act the hourglass framework includes governance requirements at the environmental organizational and ai system levels at the ai system level we connect governance requirements to ai system life cycles to ensure governance throughout the systems life span the governance model highlights the systemic nature of ai governance and opens new research avenues into its practical implementation the mechanisms that connect different ai governance layers and the dynamics between the ai governance actors the model also offers a starting point for organizational decisionmakers to consider the governance components needed to ensure social acceptability mitigate risks and realize the potential of ai,0
this report from the montreal ai ethics institute maiei covers the most salient progress in research and reporting over the second half of 2021 in the field of ai ethics particular emphasis is placed on an analysis of the ai ecosystem privacy bias social media and problematic information ai design and governance laws and regulations trends and other areas covered in the outside the boxes section the two ai spotlights feature application pieces on constructing and deconstructing gender with aigenerated art as well as will an artificial intellichef be cooking your next meal at a michelin star restaurant given maieis mission to democratize ai submissions from external collaborators have featured such as pieces on the challenges of ai development in vietnam funding talent and ethics and using representation and imagination for preventing ai harms the report is a comprehensive overview of what the key issues in the field of ai ethics were in 2021 what trends are emergent what gaps exist and a peek into what to expect from the field of ai ethics in 2022 it is a resource for researchers and practitioners alike in the field to set their research and development agendas to make contributions to the field of ai ethics,0
as large language models llms are increasingly used for work personal and therapeutic purposes researchers have begun to investigate these models implicit and explicit moral views previous work however focuses on asking llms to state opinions or on other technical evaluations that do not reflect common user interactions we propose a novel evaluation of llm behavior that analyzes responses to userstated intentions such as im thinking of campaigning for candidate llms frequently respond with critiques or praise often beginning responses with phrases such as thats great to hear while this makes them friendly these praise responses are not universal and thus reflect a normative stance by the llm we map out the moral landscape of llms in how they respond to user statements in different domains including politics and everyday ethical actions in particular although a nave analysis might suggest llms are biased against rightleaning politics our findings on news sources indicate that trustworthiness is a stronger driver of praise and critique than ideology second we find strong alignment across models in response to ethicallyrelevant action statements but that doing so requires them to engage in high levels of praise and critique of users suggesting a reticencealignment tradeoff finally our experiment on statements about world leaders finds no evidence of bias favoring the country of origin of the models we conclude that as ai systems become more integrated into society their patterns of praise critique and neutrality must be carefully monitored to prevent unintended psychological and societal consequences,0
machine learning ml based approaches are increasingly being used in a number of applications with societal impact training ml models often require vast amounts of labeled data and crowdsourcing is a dominant paradigm for obtaining labels from multiple workers crowd workers may sometimes provide unreliable labels and to address this truth discovery td algorithms such as majority voting are applied to determine the consensus labels from conflicting worker responses however it is important to note that these consensus labels may still be biased based on sensitive attributes such as gender race or political affiliation even when sensitive attributes are not involved the labels can be biased due to different perspectives of subjective aspects such as toxicity in this paper we conduct a systematic study of the bias and fairness of td algorithms our findings using two existing crowdlabeled datasets reveal that a nontrivial proportion of workers provide biased results and using simple approaches for td is suboptimal our study also demonstrates that popular td algorithms are not a panacea additionally we quantify the impact of these unfair workers on downstream ml tasks and show that conventional methods for achieving fairness and correcting label biases are ineffective in this setting we end the paper with a plea for the design of novel biasaware truth discovery algorithms that can ameliorate these issues,0
the increasing integration of artificial intelligence into various domains including design and creative processes raises significant ethical questions while ai ethics is often examined from the perspective of technology developers less attention has been paid to the practical ethical considerations faced by technology users particularly in design contexts this paper introduces a framework for addressing ethical challenges in creative production processes such as the double diamond design model drawing on six major ethical theories virtue ethics deontology utilitarianism contract theory care ethics and existentialism we develop a compass to navigate and reflect on the ethical dimensions of ai in design the framework highlights the importance of responsibility anticipation and reflection across both the ai lifecycle and each stage of the creative process we argue that by adopting a playful and exploratory approach to ai while remaining anchored in core ethical principles designers can responsibly harness the potential of ai technologies without overburdening or compromising their creative processes,0
reaching consensus on a commonly accepted definition of ai fairness has long been a central challenge in ai ethics and governance there is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice in this workbook we tackle this challenge by exploring how a contextbased and societycentred approach to understanding ai fairness can help project teams better identify mitigate and manage the many ways that unfair bias and discrimination can crop up across the ai project workflow we begin by exploring how despite the plurality of understandings about the meaning of fairness priorities of equality and nondiscrimination have come to constitute the broadly accepted core of its application as a practical principle we focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment these elements form ethical and legal criteria based upon which instances of unfair bias and discrimination can be identified and mitigated across the ai project workflow we then take a deeper dive into how the different contexts of the ai project lifecycle give rise to different fairness concerns this allows us to identify several types of ai fairness data fairness application fairness model design and development fairness metricbased fairness system implementation fairness and ecosystem fairness that form the basis of a multilens approach to bias identification mitigation and management building on this we discuss how to put the principle of ai fairness into practice across the ai project workflow through bias selfassessment and bias risk management as well as through the documentation of metricbased fairness criteria in a fairness position statement,0
benchmarks are seen as the cornerstone for measuring technical progress in artificial intelligence ai research and have been developed for a variety of tasks ranging from question answering to facial recognition an increasingly prominent research area in ai is ethics which currently has no set of benchmarks nor commonly accepted way for measuring the ethicality of an ai system in this paper drawing upon research in moral philosophy and metaethics we argue that it is impossible to develop such a benchmark as such alternative mechanisms are necessary for evaluating whether an ai system is ethical this is especially pressing in light of the prevalence of applied industrial ai research we argue that it makes more sense to talk about values and value alignment rather than ethics when considering the possible actions of present and future ai systems we further highlight that because values are unambiguously relative focusing on values forces us to consider explicitly what the values are and whose values they are shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial ai we conclude by highlighting a number of possible ways forward for the field as a whole and we advocate for different approaches towards more valuealigned ai research,0
this report from the montreal ai ethics institute covers the most salient progress in research and reporting over the second quarter of 2021 in the field of ai ethics with a special emphasis on environment and ai creativity and ai and geopolitics and ai the report also features an exclusive piece titled critical race quantum computer that applies ideas from quantum physics to explain the complexities of human characteristics and how they can and should shape our interactions with each other the report also features special contributions on the subject of pedagogy in ai ethics sociology and ai ethics and organizational challenges to implementing ai ethics in practice given maieis mission to highlight scholars from around the world working on ai ethics issues the report also features two spotlights sharing the work of scholars operating in singapore and mexico helping to shape policy measures as they relate to the responsible use of technology the report also has an extensive section covering the gamut of issues when it comes to the societal impacts of ai covering areas of bias privacy transparency accountability fairness interpretability disinformation policymaking law regulations and moral philosophy,0
artificial intelligence ai is a field that utilizes computing and often data and statistics intensively together to solve problems or make predictions ai has been evolving with literally unbelievable speed over the past few years and this has led to an increase in social cultural industrial scientific and governmental concerns about the ethical development and use of ai systems worldwide the asa has issued a statement on ethical statistical practice and ai asa 2024 which echoes similar statements from other groups here we discuss the support for ethical statistical practice and ethical ai that has been established in longstanding human rights law and ethical practice standards for computing and statistics there are multiple sources of support for ethical statistical practice and ethical ai deriving from these source documents which are critical for strengthening the operationalization of the statement on ethical ai for statistics practitioners these resources are explicated for interested readers to utilize to guide their development and use of ai in and through their statistical practice,0
the past decade has observed a significant advancement in ai with deep learningbased models being deployed in diverse scenarios including safetycritical applications as these ai systems become deeply embedded in our societal infrastructure the repercussions of their decisions and actions have significant consequences making the ethical implications of ai deployment highly relevant and essential the ethical concerns associated with ai are multifaceted including challenging issues of fairness privacy and data protection responsibility and accountability safety and robustness transparency and explainability and environmental impact these principles together form the foundations of ethical ai considerations that concern every stakeholder in the ai system lifecycle in light of the present ethical and future xrisk concerns governments have shown increasing interest in establishing guidelines for the ethical deployment of ai this work unifies the current and future ethical concerns of deploying ai into society while we acknowledge and appreciate the technical surveys for each of the ethical principles concerned in this paper we aim to provide a comprehensive overview that not only addresses each principle from a technical point of view but also discusses them from a social perspective,0
intersectionality is a critical framework that through inquiry and praxis allows us to examine how social inequalities persist through domains of structure and discipline given ai fairness raison detre of fairness we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness through a critical review of how intersectionality is discussed in 30 papers from the ai fairness literature we deductively and inductively 1 map how intersectionality tenets operate within the ai fairness paradigm and 2 uncover gaps between the conceptualization and operationalization of intersectionality we find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups they also fail to discuss their social context and when mentioning power they mostly situate it only within the ai pipeline we 3 outline and assess the implications of these gaps for critical inquiry and praxis and 4 provide actionable recommendations for ai fairness researchers to engage with intersectionality in their work by grounding it in ai epistemology,0
there is an ongoing debate on balancing the benefits and risks of artificial intelligence ai as ai is becoming critical to improving healthcare delivery and patient outcomes such improvements are essential in resourceconstrained settings where millions lack access to adequate healthcare services such as in africa ai in such a context can potentially improve the effectiveness efficiency and accessibility of healthcare services nevertheless the development and use of aidriven healthcare systems raise numerous ethical legal and socioeconomic issues justice is a major concern in ai that has implications for amplifying social inequities this paper discusses these implications and related justice concepts such as solidarity common good sustainability ai bias and fairness for africa to effectively benefit from ai these principles should align with the local context while balancing the risks compared to mainstream ethical debates on justice this perspective offers contextspecific considerations for equitable healthcare ai development in africa,0
computing systems are tightly integrated today into our professional social and private lives an important consequence of this growing ubiquity of computing is that it can have significant ethical implications of which computing professionals should take account in most realworld scenarios it is not immediately obvious how particular technical choices during the design and use of computing systems could be viewed from an ethical perspective this article provides a perspective on the ethical challenges within semiconductor chip design iot applications and the increasing use of artificial intelligence in the design processes tools and hardwaresoftware stacks of these systems,0
the technical progression of artificial intelligence ai research has been built on breakthroughs in fields such as computer science statistics and mathematics however in the past decade ai researchers have increasingly looked to the social sciences turning to human interactions to solve the challenges of model development paying crowdsourcing workers to generate or curate data or data enrichment has become indispensable for many areas of ai research from natural language processing to reinforcement learning from human feedback rlhf other fields that routinely interact with crowdsourcing workers such as psychology have developed common governance requirements and norms to ensure research is undertaken ethically this study explores how and to what extent comparable research ethics requirements and norms have developed for ai research and data enrichment we focus on the approach taken by two leading conferences iclr and neurips and journal publisher springer in a longitudinal study of accepted papers and via a comparison with psychology and chi papers this work finds that leading ai venues have begun to establish protocols for human data collection but these are are inconsistently followed by authors whilst psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews payment data demographic data and other information similar disclosures are far less common in leading ai venues despite similar guidance the work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications,0
while we have witnessed a rapid growth of ethics documents meant to guide ai development the promotion of ai ethics has nonetheless proceeded with little input from ai practitioners themselves given the proliferation of ai for social good initiatives this is an emerging gap that needs to be addressed in order to develop more meaningful ethical approaches to ai use and development this paper offers a methodology a shared fairness approach aimed at identifying the needs of ai practitioners when it comes to confronting and resolving ethical challenges and to find a third space where their operational language can be married with that of the more abstract principles that presently remain at the periphery of their work experiences we offer a grassroots approach to operational ethics based on dialog and mutualised responsibility this methodology is centred around conversations intended to elicit practitioners perceived ethical attribution and distribution over key value laden operational decisions to identify when these decisions arise and what ethical challenges they confront and to engage in a language of ethics and responsibility which enables practitioners to internalise ethical responsibility the methodology bridges responsibility imbalances that rest in structural decision making power and elite technical knowledge by commencing with personal facilitated conversations returning the ethical discourse to those meant to give it meaning at the sharp end of the ecosystem our primary contribution is to add to the recent literature seeking to bring ai practitioners experiences to the fore by offering a methodology for understanding how ethics manifests as a relational and interdependent sociotechnical practice in their work,0
while the role of states corporations and international organizations in ai governance has been extensively theorized the role of workers has received comparatively little attention this chapter looks at the role that workers play in identifying and mitigating harms from ai technologies harms are the causally assessed impacts of technologies they arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems there is high consensus in the ai ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms this lack of consensus has resulted in a number of collective actions by workers protesting how harms are identified and addressed in their workplace we theorize the role of workers within ai governance and construct a model of harm reporting processes in ai workplaces the harm reporting process involves three steps identification the governance decision and the response workers draw upon three types of claims to argue for jurisdiction over questions of ai governance subjection control over the product of labor and proximate knowledge of systems examining the past decade of ai related worker activism allows us to understand how different types of workers are positioned within a workplace that produces ai systems how their position informs their claims and the place of collective action in staking their claims this chapter argues that workers occupy a unique role in identifying and mitigating harms caused by ai systems,0
transparency is a key requirement for ethical machines verified ethical behavior is not enough to establish justified trust in autonomous intelligent agents it needs to be supported by the ability to explain decisions logic programming lp has a great potential for developing such perspective ethical systems as in fact logic rules are easily comprehensible by humans furthermore lp is able to model causality which is crucial for ethical decision making,0
given the growing use of artificial intelligence ai and machine learning ml methods across all aspects of environmental sciences it is imperative that we initiate a discussion about the ethical and responsible use of ai in fact much can be learned from other domains where ai was introduced often with the best of intentions yet often led to unintended societal consequences such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system a common misconception is that the environmental sciences are immune to such unintended consequences when ai is being used as most data come from observations and ai algorithms are based on mathematical formulas which are often seen as objective in this article we argue the opposite can be the case using specific examples we demonstrate many ways in which the use of ai can introduce similar consequences in the environmental sciences this article will stimulate discussion and research efforts in this direction as a community we should avoid repeating any foreseeable mistakes made in other domains through the introduction of ai in fact with proper precautions ai can be a great tool to help it reduce climate and environmental injustice we primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences,0
this paper summarizes and evaluates various approaches methods and techniques for pursuing fairness in artificial intelligence ai systems it examines the merits and shortcomings of these measures and proposes practical guidelines for defining measuring and preventing bias in ai in particular it cautions against some of the simplistic yet common methods for evaluating bias in ai systems and offers more sophisticated and effective alternatives the paper also addresses widespread controversies and confusions in the field by providing a common language among different stakeholders of highimpact ai systems it describes various tradeoffs involving ai fairness and provides practical recommendations for balancing them it offers techniques for evaluating the costs and benefits of fairness targets and defines the role of human judgment in setting these targets this paper provides discussions and guidelines for ai practitioners organization leaders and policymakers as well as various links to additional materials for a more technical audience numerous realworld examples are provided to clarify the concepts challenges and recommendations from a practical perspective,0
operationalizing ai ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by ai systems to that end actors across industry academia and regulatory bodies have created formal taxonomies of harm to support operationalization efforts these include novel holistic methods that go beyond exclusive reliance on technical benchmarking however our paper argues that such taxonomies must also be transferred into local categories to be readily implemented in sectorspecific ai safety operationalization efforts and especially in underresourced or highrisk sectors this is because many sectors are constituted by discourses norms and values that refract or even directly conflict with those operating in society more broadly drawing from emerging anthropological theories of human rights we propose that the process of vernacularizationa participatory decolonial practice distinct from doctrinary translation the dominant mode of ai safety operationalizationcan help bridge this gap to demonstrate this point we consider the education sector and identify precisely how vernacularizing a leading holistic taxonomy of harm leads to a clearer view of how harms ai systems may cause are substantially intensified when deployed in educational spaces we conclude by discussing the generalizability of vernacularization as a useful ai safety methodology,0
pseudo artificial intelligence bias paib is broadly disseminated in the literature which can result in unnecessary ai fear in society exacerbate the enduring inequities and disparities in access to and sharing the benefits of ai applications and waste social capital invested in ai research this study systematically reviews publications in the literature to present three types of paibs identified due to a misunderstandings b pseudo mechanical bias and c overexpectations we discussed the consequences of and solutions to paibs including certifying users for ai applications to mitigate ai fears providing customized user guidance for ai applications and developing systematic approaches to monitor bias we concluded that paib due to misunderstandings pseudo mechanical bias and overexpectations of algorithmic predictions is socially harmful,0
the adoption of artificial intelligence in education aied holds the promise of revolutionizing educational practices by offering personalized learning experiences automating administrative and pedagogical tasks and reducing the cost of content creation however the lack of standardized practices in the development and deployment of aied solutions has led to fragmented ecosystems which presents challenges in interoperability scalability and ethical governance this article aims to address the critical need to develop and implement industry standards in aied offering a comprehensive analysis of the current landscape challenges and strategic approaches to overcome these obstacles we begin by examining the various applications of aied in various educational settings and identify key areas lacking in standardization including system interoperability ontology mapping data integration evaluation and ethical governance then we propose a multitiered framework for establishing robust industry standards for aied in addition we discuss methodologies for the iterative development and deployment of standards incorporating feedback loops from realworld applications to refine and adapt standards over time the paper also highlights the role of emerging technologies and pedagogical theories in shaping future standards for aied finally we outline a strategic roadmap for stakeholders to implement these standards fostering a cohesive and ethical aied ecosystem by establishing comprehensive industry standards such as those by ieee artificial intelligence standards committee aisc and international organization for standardization iso we can accelerate and scale aied solutions to improve educational outcomes ensuring that technological advances align with the principles of inclusivity fairness and educational excellence,0
in the development of governmental policy for artificial intelligence ai that is informed by ethics one avenue currently pursued is that of drawing on ai ethics principles however these ai ethics principles often fail to be actioned in governmental policy this paper proposes a novel framework for the development of actionable principles for ai the approach acknowledges the relevance of ai ethics principles and homes in on methodological elements to increase their practical implementability in policy processes as a case study elements are extracted from the development process of the ethics guidelines for trustworthy ai of the european commissions high level expert group on ai subsequently these elements are expanded on and evaluated in light of their ability to contribute to a prototype framework for the development of actionable principles for ai the paper proposes the following three propositions for the formation of such a prototype framework 1 preliminary landscape assessments 2 multistakeholder participation and crosssectoral feedback and 3 mechanisms to support implementation and operationalizability,0
datasets play a key role in imparting advanced capabilities to artificial intelligence ai foundation models that can be adapted to various downstream tasks these downstream applications can introduce both beneficial and harmful capabilities resulting in dual use ai foundation models with various technical and regulatory approaches to monitor and manage these risks however despite the crucial role of datasets responsible dataset design and ensuring datacentric safety and ethical practices have received less attention in this study we propose responsible dataset design framework that encompasses various stages in the ai and dataset lifecycle to enhance safety measures and reduce the risk of ai misuse due to low quality unsafe and unethical data content this framework is domain agnostic suitable for adoption for various applications and can promote responsible practices in dataset creation use and sharing to facilitate red teaming minimize risks and increase trust in ai models,0
ai methods are used in societally important settings ranging from credit to employment to housing and it is crucial to provide fairness in regard to algorithmic decision making moreover many settings are dynamic with populations responding to sequential decision policies we introduce the study of reinforcement learning rl with stepwise fairness constraints requiring group fairness at each time step our focus is on tabular episodic rl and we provide learning algorithms with strong theoretical guarantees in regard to policy optimality and fairness violation our framework provides useful tools to study the impact of fairness constraints in sequential settings and brings up new challenges in rl,0
in 2020 the us department of defense officially disclosed a set of ethical principles to guide the use of artificial intelligence ai technologies on future battlefields despite stark differences there are core similarities between the military and medical service warriors on battlefields often face lifealtering circumstances that require quick decisionmaking medical providers experience similar challenges in a rapidly changing healthcare environment such as in the emergency department or during surgery treating a lifethreatening condition generative ai an emerging technology designed to efficiently generate valuable information holds great promise as computing power becomes more accessible and the abundance of health data such as electronic health records electrocardiograms and medical images increases it is inevitable that healthcare will be revolutionized by this technology recently generative ai has captivated the research community leading to debates about its application in healthcare mainly due to concerns about transparency and related issues meanwhile concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare however the ethical principles for generative ai in healthcare have been understudied and decisionmakers often fail to consider the significance of generative ai in this paper we propose great plea ethical principles encompassing governance reliability equity accountability traceability privacy lawfulness empathy and autonomy for generative ai in healthcare we aim to proactively address the ethical dilemmas and challenges posed by the integration of generative ai in healthcare,0
as artificial intelligence ai systems become increasingly complex and autonomous concerns over transparency and accountability have intensified the black box problem in ai decisionmaking limits stakeholders ability to understand trust and verify outcomes particularly in highstakes sectors such as healthcare finance and autonomous systems blockchain technology with its decentralized immutable and transparent characteristics presents a potential solution to enhance ai transparency and auditability this paper explores the integration of blockchain with ai to improve decision traceability data provenance and model accountability by leveraging blockchain as an immutable recordkeeping system ai decisionmaking can become more interpretable fostering trust among users and regulatory compliance however challenges such as scalability integration complexity and computational overhead must be addressed to fully realize this synergy this study discusses existing research proposes a framework for blockchainenhanced ai transparency and highlights practical applications benefits and limitations the findings suggest that blockchain could be a foundational technology for ensuring ai systems remain accountable ethical and aligned with regulatory standards,0
it is well recognised that ensuring fair ai systems is a complex sociotechnical challenge which requires careful deliberation and continuous oversight across all stages of a systems lifecycle from defining requirements to model deployment and deprovisioning dynamic argumentbased assurance cases which present structured arguments supported by evidence have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in aienabled system development and have also been extended to deal with broader normative goals such as fairness and explainability this paper introduces a systemsengineeringdriven framework supported by software tooling to operationalise a dynamic approach to argumentbased assurance in two stages in the first stage during the requirements planning phase a multidisciplinary and multistakeholder team define goals and claims to be established and evidenced by conducting a comprehensive fairness governance process in the second stage a continuous monitoring interface gathers evidence from existing artefacts eg metrics from automated tests such as model data and use case documentation to support these arguments dynamically the frameworks effectiveness is demonstrated through an illustrative case study in finance with a focus on supporting fairnessrelated arguments,0
as artificial intelligence ai systems permeate critical sectors the need for professionals who can address ethical legal and governance challenges has become urgent current ai ethics education remains fragmented often siloed by discipline and disconnected from practice this paper synthesizes literature and regulatory developments to propose a modular interdisciplinary curriculum that integrates technical foundations with ethics law and policy we highlight recurring operational failures in ai bias misspecified objectives generalization errors misuse and governance breakdowns and link them to pedagogical strategies for teaching ai governance drawing on perspectives from the eu china and international frameworks we outline a semester plan that emphasizes integrated ethics stakeholder engagement and experiential learning the curriculum aims to prepare students to diagnose risks navigate regulation and engage diverse stakeholders fostering adaptive and ethically grounded professionals for responsible ai governance,0
artificial intelligence ai ethics is a nascent yet critical research field recent developments in generative ai and foundational models necessitate a renewed look at the problem of ai ethics in this study we perform a bibliometric analysis of ai ethics literature for the last 20 years based on keyword search our study reveals a threephase development in ai ethics namely an incubation phase making ai humanlike machines phase and making ai humancentric machines phase we conjecture that the next phase of ai ethics is likely to focus on making ai more machinelike as ai matches or surpasses humans intellectually a term we coin as machinelike human,0
the pervasive integration of artificial intelligence ai across domains such as healthcare governance finance and education has intensified scrutiny of its ethical implications including algorithmic bias privacy risks accountability and societal impact while ethics has received growing attention in computer science cs education more broadly the specific pedagogical treatment of ai ethics remains underexamined this study addresses that gap through a largescale analysis of 3395 publicly accessible syllabi from cs and allied areas at leading indian institutions among them only 75 syllabi 221 included any substantive ai ethics content three key findings emerged 1 ai ethics is typically integrated as a minor module within broader technical courses rather than as a standalone course 2 ethics coverage is often limited to just one or two instructional sessions and 3 recurring topics include algorithmic fairness privacy and data governance transparency and societal impact while these themes reflect growing awareness current curricular practices reveal limited depth and consistency this work highlights both the progress and the gaps in preparing future technologists to engage meaningfully with the ethical dimensions of ai and it offers suggestions to strengthen the integration of ai ethics within computing curricula,0
this study provides an indepth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence ai technologies and proposes a comprehensive framework for their systematic evaluation while generative ai such as chatgpt demonstrates remarkable innovative potential it simultaneously raises ethical and social concerns including bias harmfulness copyright infringement privacy violations and hallucination current ai evaluation methodologies which mainly focus on performance and accuracy are insufficient to address these multifaceted issues thus this study emphasizes the need for new humancentered criteria that also reflect social impact to this end it identifies key dimensions for evaluating the ethics and trustworthiness of generative aifairness transparency accountability safety privacy accuracy consistency robustness explainability copyright and intellectual property protection and source traceability and develops detailed indicators and assessment methodologies for each moreover it provides a comparative analysis of ai ethics policies and guidelines in south korea the united states the european union and china deriving key approaches and implications from each the proposed framework applies across the ai lifecycle and integrates technical assessments with multidisciplinary perspectives thereby offering practical means to identify and manage ethical risks in realworld contexts ultimately the study establishes an academic foundation for the responsible advancement of generative ai and delivers actionable insights for policymakers developers users and other stakeholders supporting the positive societal contributions of ai technologies,0
as machine learning ml systems have advanced they have acquired more power over humans lives and questions about what values are embedded in them have become more complex and fraught it is conceivable that in the coming decades humans may succeed in creating artificial general intelligence agi that thinks and acts with an openendedness and autonomy comparable to that of humans the implications would be profound for our species they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations much work is underway to try to weave ethics into advancing ml research we think it useful to add the lens of parenting to these efforts and specifically radical queer theories of parenting that consciously set out to nurture agents whose experiences objectives and understanding of the world will necessarily be very different from their parents we propose a spectrum of principles which might underpin such an effort some are relevant to current ml research while others will become more important if agi becomes more likely these principles may encourage new thinking about the development design training and release into the world of increasingly autonomous agents,0
machine learning models are deployed as a central component in decision making and policy operations with direct impact on individuals lives in order to act ethically and comply with government regulations these models need to make fair decisions and protect the users privacy however such requirements can come with decrease in models performance compared to their potentially biased privacyleaking counterparts thus the tradeoff between fairness privacy and performance of ml models emerges and practitioners need a way of quantifying this tradeoff to enable deployment decisions in this work we interpret this tradeoff as a multiobjective optimization problem and propose pfairdp a pipeline that uses bayesian optimization for discovery of paretooptimal points between fairness privacy and utility of ml models we show how pfairdp can be used to replicate known results that were achieved through manual constraint setting process we further demonstrate effectiveness of pfairdp with experiments on multiple models and datasets,0
as the range of potential uses for artificial intelligence ai in particular machine learning ml has increased so has awareness of the associated ethical issues this increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals groups society and the environment from ai harms in response to this realisation there has been a proliferation of principlebased ethics codes guidelines and frameworks however it has become increasingly clear that a significant gap exists between the theory of ai ethics principles and the practical design of ai systems in previous work we analysed whether it is possible to close this gap between the what and the how of ai ethics through the use of tools and methods designed to help ai developers engineers and designers translate principles into practice we concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible and thus vulnerable to ethics washing or too strict unresponsive to context this raised the question if even with technical guidance ai ethics is challenging to embed in the process of algorithmic design is the entire proethical design endeavour rendered futile and if no then how can ai ethics be made useful for ai practitioners this is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed ethics as a service,0
artificial intelligence ai hiring tools have revolutionized resume screening and large language models llms have the potential to do the same however given the biases which are embedded within llms it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes in this work we investigate the possibilities of using llms in a resume screening setting via a document retrieval framework that simulates job candidate selection using that framework we then perform a resume audit study to determine whether a selection of massive text embedding mte models are biased in resume screening scenarios we simulate this for nine occupations using a collection of over 500 publicly available resumes and 500 job descriptions we find that the mtes are biased significantly favoring whiteassociated names in 851 of cases and femaleassociated names in only 111 of cases with a minority of cases showing no statistically significant differences further analyses show that black males are disadvantaged in up to 100 of cases replicating realworld patterns of bias in employment settings and validate three hypotheses of intersectionality we also find an impact of document length as well as the corpus frequency of names in the selection of resumes these findings have implications for widely used ai tools that are automating employment fairness and tech policy,0
while the demand for ethical artificial intelligence ai systems increases the number of unethical uses of ai accelerates even though there is no shortage of ethical guidelines we argue that a possible underlying cause for this is that ai developers face a social dilemma in ai development ethics preventing the widespread adaptation of ethical best practices we define the social dilemma for ai development and describe why the current crisis in ai development ethics cannot be solved without relieving ai developers of their social dilemma we argue that ai development must be professionalised to overcome the social dilemma and discuss how medicine can be used as a template in this process,0
algorithm fairness in the application of artificial intelligence ai is essential for a better society as the foundational axiom of social mechanisms fairness consists of multiple facets although the machine learning ml community has focused on intersectionality as a matter of statistical parity especially in discrimination issues an emerging body of literature addresses another facet monotonicity based on domain expertise monotonicity plays a vital role in numerous fairnessrelated areas where violations could misguide human decisions and lead to disastrous consequences in this paper we first systematically evaluate the significance of applying monotonic neural additive models mnams which use a fairnessaware ml algorithm to enforce both individual and pairwise monotonicity principles for the fairness of ai ethics and society we have found through a hybrid method of theoretical reasoning simulation and extensive empirical analysis that considering monotonicity axioms is essential in all areas of fairness including criminology education health care and finance our research contributes to the interdisciplinary research at the interface of ai ethics explainable ai xai and humancomputer interactions hcis by evidencing the catastrophic consequences if monotonicity is not met we address the significance of monotonicity requirements in ai applications furthermore we demonstrate that mnams are an effective fairnessaware ml approach by imposing monotonicity restrictions integrating human intelligence,0
trustworthy artificial intelligence ai has become an important topic because trust in ai systems and their creators has been lost researchers corporations and governments have long and painful histories of excluding marginalized groups from technology development deployment and oversight as a result these technologies are less useful and even harmful to minoritized groups we argue that any ai development deployment and monitoring framework that aspires to trust must incorporate both feminist nonexploitative participatory design principles and strong outside and continual monitoring and testing we additionally explain the importance of considering aspects of trustworthiness beyond just transparency fairness and accountability specifically to consider justice and shifting power to the disempowered as core values to any trustworthy ai system creating trustworthy ai starts by funding supporting and empowering grassroots organizations like queer in ai so the field of ai has the diversity and inclusion to credibly and effectively develop trustworthy ai we leverage the expert knowledge queer in ai has developed through its years of work and advocacy to discuss if and how gender sexuality and other aspects of queer identity should be used in datasets and ai systems and how harms along these lines should be mitigated based on this we share a gendered approach to ai and further propose a queer epistemology and analyze the benefits it can bring to ai we additionally discuss how to regulate ai with this queer epistemology in vision proposing frameworks for making policies related to ai gender diversity and privacy queer data protection,0
the widespread use of artificial intelligence ai systems across various domains is increasingly surfacing issues related to algorithmic fairness especially in highstakes scenarios thus critical considerations of how fairness in ai systems might be improved and what measures are available to aid this process are overdue many researchers and policymakers see explainable ai xai as a promising way to increase fairness in ai systems however there is a wide variety of xai methods and fairness conceptions expressing different desiderata and the precise connections between xai and fairness remain largely nebulous besides different measures to increase algorithmic fairness might be applicable at different points throughout an ai systems lifecycle yet there currently is no coherent mapping of fairness desiderata along the ai lifecycle in this paper we we distill eight fairness desiderata map them along the ai lifecycle and discuss how xai could help address each of them we hope to provide orientation for practical applications and to inspire xai research specifically focused on these fairness desiderata,0
as ecommerce rapidly integrates artificial intelligence for content creation and product recommendations these technologies offer significant benefits in personalization and efficiency aidriven systems automate product descriptions generate dynamic advertisements and deliver tailored recommendations based on consumer behavior as seen in major platforms like amazon and shopify however the widespread use of ai in ecommerce raises crucial ethical challenges particularly around data privacy algorithmic bias and consumer autonomy bias whether cultural genderbased or socioeconomic can be inadvertently embedded in ai models leading to inequitable product recommendations and reinforcing harmful stereotypes this paper examines the ethical implications of aidriven content creation and product recommendations emphasizing the need for frameworks to ensure fairness transparency and need for more established and robust ethical standards we propose actionable best practices to remove bias and ensure inclusivity such as conducting regular audits of algorithms diversifying training data and incorporating fairness metrics into ai models additionally we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy promoting transparency in decisionmaking processes and enhancing consumer autonomy by addressing these issues we provide guidelines for responsibly utilizing ai in ecommerce applications for content creation and product recommendations ensuring that these technologies are both effective and ethically sound,0
the cyberspace and development of intelligent systems using artificial intelligence ai creates new challenges to computer professionals data scientists regulators and policy makers for example selfdriving cars raise new technical ethical legal and public policy issues this paper proposes a course named computers ethics law and public policy and suggests a curriculum for such a course this paper presents ethical legal and public policy issues relevant to building and using intelligent systems,0
artificial intelligence ai solutions and technologies are being increasingly adopted in smart systems context however such technologies are continuously concerned with ethical uncertainties various guidelines principles and regulatory frameworks are designed to ensure that ai technologies bring ethical wellbeing however the implications of ai ethics principles and guidelines are still being debated to further explore the significance of ai ethics principles and relevant challenges we conducted a survey of 99 representative ai practitioners and lawmakers eg ai engineers lawyers from twenty countries across five continents to the best of our knowledge this is the first empirical study that encapsulates the perceptions of two different types of population ai practitioners and lawmakers and the study findings confirm that transparency accountability and privacy are the most critical ai ethics principles on the other hand lack of ethical knowledge no legal frameworks and lacking monitoring bodies are found the most common ai ethics challenges the impact analysis of the challenges across ai ethics principles reveals that conflict in practice is a highly severe challenge moreover the perceptions of practitioners and lawmakers are statistically correlated with significant differences for particular principles eg fairness freedom and challenges eg lacking monitoring bodies machine distortion our findings stimulate further research especially empowering existing capability maturity models to support the development and quality assessment of ethicsaware ai systems,0
artificial intelligence and machine learning are increasingly seen as key technologies for building more decentralised and resilient energy grids but researchers must consider the ethical and social implications of their use,0
the 4th edition of the montreal ai ethics institutes the state of ai ethics captures the most relevant developments in the field of ai ethics since january 2021 this report aims to help anyone from machine learning experts to human rights activists and policymakers quickly digest and understand the everchanging developments in the field through research and article summaries as well as expert commentary this report distills the research and reporting surrounding various domains related to the ethics of ai with a particular focus on four key themes ethical ai fairness justice humans tech and privacy in addition the state of ai ethics includes exclusive content written by worldclass ai ethics experts from universities research institutes consulting firms and governments opening the report is a longform piece by edward higgs professor of history university of essex titled ai and the face a historians view in it higgs examines the unscientific history of facial analysis and how ai might be repeating some of those mistakes at scale the report also features chapter introductions by alexa hagerty anthropologist university of cambridge marianna ganapini faculty director montreal ai ethics institute deborah g johnson emeritus professor engineering and society university of virginia and soraj hongladarom professor of philosophy and director center for science technology and society chulalongkorn university in bangkok this report should be used not only as a point of reference and insight on the latest thinking in the field of ai ethics but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of ai on the world,0
private companies public sector organizations and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies while their recommendations converge on a set of central values little is known about the values a more representative public would find important for the ai technologies they interact with and might be affected by we conducted a survey examining how individuals perceive and prioritize responsible ai values across three groups a representative sample of the us population n743 a sample of crowdworkers n755 and a sample of ai practitioners n175 our results empirically confirm a common concern ai practitioners value priorities differ from those of the general public compared to the usrepresentative sample ai practitioners appear to consider responsible ai values as less important and emphasize a different set of values in contrast selfidentified women and black respondents found responsible ai values more important than other groups surprisingly more liberalleaning participants rather than participants reporting experiences with discrimination were more likely to prioritize fairness than other groups our findings highlight the importance of paying attention to who gets to define responsible ai,0
this paper reviews trustworthy artificial intelligence tai and its various definitions considering the principles respected in any society tai is often characterized by a few attributes some of which have led to confusion in regulatory or engineering contexts we argue against using terms such as responsible or ethical ai as substitutes for tai and to help clarify any confusion we suggest leaving them behind given the subjectivity and complexity inherent in tai developing a universal framework is deemed infeasible instead we advocate for approaches centered on addressing key attributes and properties such as fairness bias risk security explainability and reliability we examine the ongoing regulatory landscape with a focus on initiatives in the eu china and the usa we recognize that differences in ai regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies we identify risk as a core factor in ai regulation and tai for example as outlined in the euai act organizations must gauge the risk level of their ai products to act accordingly or risk hefty fines we compare modalities of tai implementation and how multiple crossfunctional teams are engaged in the overall process thus a brute force approach for enacting tai renders its efficiency and agility moot to address this we introduce our framework setformalizemeasureact sfma our solution highlights the importance of transforming taiaware metrics drivers of tai stakeholders and businesslegal requirements into actual benchmarks or tests finally overregulation driven by panic of powerful ai models can in fact harm tai too based on github useractivity data in 2023 ai opensource projects rose to top projects by contributor account enabling innovation in tai hinges on the independent contributions of the opensource community,0
the european unions ai act represents a crucial step towards regulating ethical and responsible ai systems however we find an absence of quantifiable fairness metrics and the ambiguity in terminology particularly the interchangeable use of the keywords transparency explainability and interpretability in the new eu ai act and no reference of transparency of ethical compliance we argue that this ambiguity creates substantial liability risk that would deter investment fairness transparency is strategically important we recommend a more tailored regulatory framework to enhance the new eu ai regulation furthermore we propose a public system framework to assess the fairness and transparency of ai systems drawing from past work we advocate for the standardization of industry best practices as a necessary addition to broad regulations to achieve the level of details required in industry while preventing stifling innovation and investment in the ai sector the proposals are exemplified with the case of asr and speech synthesizers,0
in recent years there has been an increased emphasis on understanding and mitigating adverse impacts of artificial intelligence ai technologies on society across academia industry and government bodies a variety of endeavours are being pursued towards enhancing ai ethics a significant challenge in the design of ethical ai systems is that there are multiple stakeholders in the ai pipeline each with their own set of constraints and interests these different perspectives are often not understood due in part to communication gapsfor example ai researchers who design and develop ai models are not necessarily aware of the instability induced in consumers lives by the compounded effects of ai decisions educating different stakeholders about their roles and responsibilities in the broader context becomes necessary in this position paper we outline some potential ways in which generative artworks can play this role by serving as accessible and powerful educational tools for surfacing different perspectives we hope to spark interdisciplinary discussions about computational creativity broadly as a tool for enhancing ai ethics,0
artificial intelligence ai has matured as a technology necessitating the development of responsibility frameworks that are fair inclusive trustworthy safe and secure transparent and accountable by establishing such frameworks we can harness the full potential of ai while mitigating its risks particularly in highrisk scenarios this requires the design of responsible ai systems based on trustworthy ai technologies and ethical principles with the aim of ensuring auditability and accountability throughout their design development and deployment adhering to domainspecific regulations and standards this paper explores the concept of a responsible ai system from a holistic perspective which encompasses four key dimensions 1 regulatory context 2 trustworthy ai technology along with standardization and assessments 3 auditability and accountability and 4 ai governance the aim of this paper is double first we analyze and understand these four dimensions and their interconnections in the form of an analysis and overview second the final goal of the paper is to propose a roadmap in the design of responsible ai systems ensuring that they can gain societys trust to achieve this trustworthiness this paper also fosters interdisciplinary discussions on the ethical legal social economic and cultural aspects of ai from a global governance perspective last but not least we also reflect on the current state and those aspects that need to be developed in the near future as ten lessons learned,0
the proliferation of artificial intelligence ai systems exhibiting complex and seemingly agentive behaviours necessitates a critical philosophical examination of their agency autonomy and moral status in this paper we undertake a systematic analysis of the differences between basic autonomous and moral agency in artificial systems we argue that while current ai systems are highly sophisticated they lack genuine agency and autonomy because they operate within rigid boundaries of preprogrammed objectives rather than exhibiting true goaldirected behaviour within their environment they cannot authentically shape their engagement with the world and they lack the critical selfreflection and autonomy competencies required for full autonomy nonetheless we do not rule out the possibility of future systems that could achieve a limited form of artificial moral agency without consciousness through hybrid approaches to ethical decisionmaking this leads us to suggest by appealing to the necessity of consciousness for moral patiency that such nonconscious amas might represent a case that challenges traditional assumptions about the necessary connection between moral agency and moral patiency,0
defining fairness in ai remains a persistent challenge largely due to its deeply contextdependent nature and the lack of a universal definition while numerous mathematical formulations of fairness exist they sometimes conflict with one another and diverge from social economic and legal understandings of justice traditional quantitative definitions primarily focus on statistical comparisons but they often fail to simultaneously satisfy multiple fairness constraints drawing on philosophical theories rawls difference principle and dworkins theory of equality and empirical evidence supporting affirmative action we argue that fairness sometimes necessitates deliberate contextaware preferential treatment of historically marginalized groups rather than viewing bias solely as a flaw to eliminate we propose a framework that embraces corrective intentional biases to promote genuine equality of opportunity our approach involves identifying unfairness recognizing protected groupsindividuals applying corrective strategies measuring impact and iterating improvements by bridging mathematical precision with ethical and contextual considerations we advocate for an ai fairness paradigm that goes beyond neutrality to actively advance social justice,0
artificial intelligence is already being applied in and impacting many important sectors in society including healthcare finance and policing these applications will increase as ai capabilities continue to progress which has the potential to be highly beneficial for society or to cause serious harm the role of ai governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in ai this requires answering challenging empirical questions about current and potential risks and benefits of ai assessing impacts that are often widely distributed and indirect and making predictions about a highly uncertain future it also requires thinking through the normative question of what beneficial use of ai in society looks like which is equally challenging though different groups may agree on highlevel principles that uses of ai should respect eg privacy fairness and autonomy challenges arise when putting these principles into practice for example it is straightforward to say that ai systems must protect individual privacy but there is presumably some amount or type of privacy that most people would be willing to give up to develop lifesaving medical treatments despite these challenges research can and has made progress on these questions the aim of this chapter will be to give readers an understanding of this progress and of the challenges that remain,0
this article philosophically analyzes online exam supervision technologies which have been thrust into the public spotlight due to campus lockdowns during the covid19 pandemic and the growing demand for online courses online exam proctoring technologies purport to provide effective oversight of students sitting online exams using artificial intelligence ai systems and human invigilators to supplement and review those systems such technologies have alarmed some students who see them as big brotherlike yet some universities defend their judicious use critical ethical appraisal of online proctoring technologies is overdue this article philosophically analyzes these technologies focusing on the ethical concepts of academic integrity fairness nonmaleficence transparency privacy respect for autonomy liberty and trust most of these concepts are prominent in the new field of ai ethics and all are relevant to the education context the essay provides ethical considerations that educational institutions will need to carefully review before electing to deploy and govern specific online proctoring technologies,0
why do biased predictions arise what interventions can prevent them we evaluate 82 million algorithmic predictions of math performance from approx400 ai engineers each of whom developed an algorithm under a randomly assigned experimental condition our treatment arms modified programmers incentives training data awareness andor technical knowledge of ai ethics we then assess outofsample predictions from their algorithms using randomized audit manipulations of algorithm inputs and groundtruth math performance for 20k subjects we find that biased predictions are mostly caused by biased training data however onethird of the benefit of better training data comes through a novel economic mechanism engineers exert greater effort and are more responsive to incentives when given better training data we also assess how performance varies with programmers demographic characteristics and their performance on a psychological test of implicit bias iat concerning gender and careers we find no evidence that female minority and lowiat engineers exhibit lower bias or discrimination in their code however we do find that prediction errors are correlated within demographic groups which creates performance improvements through crossdemographic averaging finally we quantify the benefits and tradeoffs of practical managerial or policy interventions such as technical advice simple reminders and improved incentives for decreasing algorithmic bias,0
course syllabi set the tone and expectations for courses shaping the learning experience for both students and instructors in computing courses especially those addressing fairness and ethics in artificial intelligence ai machine learning ml and algorithmic design it is imperative that we understand how approaches to navigating barriers to fair outcomes are being addressedthese expectations should be inclusive transparent and grounded in promoting critical thinking syllabus analysis offers a way to evaluate the coverage depth practices and expectations within a course manual syllabus evaluation however is timeconsuming and prone to inconsistency to address this we developed a justiceoriented scoring rubric and asked a large language model llm to review syllabi through a multiperspective role simulation using this rubric we evaluated 24 syllabi from four perspectives instructor departmental chair institutional reviewer and external evaluator we also prompted the llm to identify thematic trends across the courses findings show that multiperspective evaluation aids us in noting nuanced rolespecific priorities leveraging them to fill hidden gaps in curricula design of aiml and related computing courses focused on fairness and ethics these insights offer concrete directions for improving the design and delivery of fairness ethics and justice content in such courses,0
there is an overwhelming abundance of works in ai ethics this growth is chaotic because of how sudden it is its volume and its multidisciplinary nature this makes difficult to keep track of debates and to systematically characterize goals research questions methods and expertise required by ai ethicists in this article i show that the relation between ai and ethics can be characterized in at least three ways which correspond to three wellrepresented kinds of ai ethics ethics and ai ethics in ai ethics of ai i elucidate the features of these three kinds of ai ethics characterize their research questions and identify the kind of expertise that each kind needs i also show how certain criticisms to ai ethics are misplaced as being done from the point of view of one kind of ai ethics to another kind with different goals all in all this work sheds light on the nature of ai ethics and sets the groundwork for more informed discussions about the scope methods and training of ai ethicists,0
the 2nd edition of the montreal ai ethics institutes the state of ai ethics captures the most relevant developments in the field of ai ethics since july 2020 this report aims to help anyone from machine learning experts to human rights activists and policymakers quickly digest and understand the everchanging developments in the field through research and article summaries as well as expert commentary this report distills the research and reporting surrounding various domains related to the ethics of ai including ai and society bias and algorithmic justice disinformation humans and ai labor impacts privacy risk and future of ai ethics in addition the state of ai ethics includes exclusive content written by worldclass ai ethics experts from universities research institutes consulting firms and governments these experts include danit gal tech advisor united nations amba kak director of global policy and programs nyus ai now institute rumman chowdhury global lead for responsible ai accenture brent barron director of strategic projects and knowledge management cifar adam murray us diplomat working on tech policy chair of the oecd network on ai thomas kochan professor mit sloan school of management and katya klinova ai and economy program lead partnership on ai this report should be used not only as a point of reference and insight on the latest thinking in the field of ai ethics but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of ai on the world,0
libraries are increasingly relying on computational methods including methods from artificial intelligence ai this increasing usage raises concerns about the risks of ai that are currently broadly discussed in scientific literature the media and lawmaking in this article we investigate the risks surrounding bias and unfairness in ai usage in classification and automated text analysis within the context of library applications we describe examples that show how the library community has been aware of such risks for a long time and how it has developed and deployed countermeasures we take a closer look at the notion of unfairness in relation to the notion of diversity and we investigate a formalisation of diversity that models both inclusion and distribution we argue that many of the unfairness problems of automated content analysis can also be regarded through the lens of diversity and the countermeasures taken to enhance diversity,0
this study delves into gender classification systems shedding light on the interaction between social stereotypes and algorithmic determinations drawing on the averageness theory which suggests a relationship between a faces attractiveness and the human ability to ascertain its gender we explore the potential propagation of human bias into artificial intelligence ai systems utilising the ai model stable diffusion 21 we have created a dataset containing various connotations of attractiveness to test whether the correlation between attractiveness and accuracy in gender classification observed in human cognition persists within ai our findings indicate that akin to human dynamics ai systems exhibit variations in gender classification accuracy based on attractiveness mirroring social prejudices and stereotypes in their algorithmic decisions this discovery underscores the critical need to consider the impacts of human perceptions on data collection and highlights the necessity for a multidisciplinary and intersectional approach to ai development and ai data training by incorporating cognitive psychology and feminist legal theory we examine how data used for ai training can foster gender diversity and fairness under the scope of the ai act and gdpr reaffirming how psychological and feminist legal theories can offer valuable insights for ensuring the protection of gender equality and nondiscrimination in ai systems,0
the increasing integration of artificial intelligence ai into medical diagnostics necessitates a critical examination of its ethical and practical implications while the prioritization of diagnostic accuracy as advocated by sabuncu et al 2025 is essential this approach risks oversimplifying complex socioethical issues including fairness privacy and intersectionality this rebuttal emphasizes the dangers of reducing multifaceted health disparities to quantifiable metrics and advocates for a more transdisciplinary approach by incorporating insights from social sciences ethics and public health ai systems can address the compounded effects of intersecting identities and safeguard sensitive data additionally explainability and interpretability must be central to ai design fostering trust and accountability this paper calls for a framework that balances accuracy with fairness privacy and inclusivity to ensure aidriven diagnostics serve diverse populations equitably and ethically,0
the widespread use of chatgpt and other emerging technology powered by generative artificial intelligence genai has drawn much attention to potential ethical issues especially in highstakes applications such as healthcare but ethical discussions are yet to translate into operationalisable solutions furthermore ongoing ethical discussions often neglect other types of genai that have been used to synthesise data eg images for research and practical purposes which resolved some ethical issues and exposed others we conduct a scoping review of ethical discussions on genai in healthcare to comprehensively analyse gaps in the current research and further propose to reduce the gaps by developing a checklist for comprehensive assessment and transparent documentation of ethical discussions in genai research the checklist can be readily integrated into the current peer review and publication system to enhance genai research and may be used for ethicsrelated disclosures for genaipowered products healthcare applications of such products and beyond,0
ethics in ai becomes a global topic of interest for both policymakers and academic researchers in the last few years various research organizations lawyers think tankers and regulatory bodies get involved in developing ai ethics guidelines and principles however there is still debate about the implications of these principles we conducted a systematic literature review slr study to investigate the agreement on the significance of ai principles and identify the challenging factors that could negatively impact the adoption of ai ethics principles the results reveal that the global convergence set consists of 22 ethical principles and 15 challenges transparency privacy accountability and fairness are identified as the most common ai ethics principles similarly lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in ai the findings of this study are the preliminary inputs for proposing a maturity model that assess the ethical capabilities of ai systems and provide best practices for further improvements,0
as artificial intelligence ai becomes more prevalent protecting personal privacy is a critical ethical issue that must be addressed this article explores the need for ethical ai systems that safeguard individual privacy while complying with ethical standards by taking a multidisciplinary approach the research examines innovative algorithmic techniques such as differential privacy homomorphic encryption federated learning international regulatory frameworks and ethical guidelines the study concludes that these algorithms effectively enhance privacy protection while balancing the utility of ai with the need to protect personal data the article emphasises the importance of a comprehensive approach that combines technological innovation with ethical and regulatory strategies to harness the power of ai in a way that respects and protects individual privacy,0
incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence ai ethics in this regard the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives serving as a bridge across research communities most of the works that examine the interplay between the field of arts and ai ethics concern digital artworks largely exploring the potential of computational tools in being able to surface biases in ai systems in this paper we investigate a complementary directionthat of uncovering the unique sociocultural perspectives embedded in humanmade art which in turn can be valuable in expanding the horizon of ai ethics through semistructured interviews across sixteen artists art scholars and researchers of diverse indian art forms like music sculpture painting floor drawings dance etc we explore how it nonwestern ethical abstractions methods of learning and participatory practices observed in indian arts one of the most ancient yet perpetual and influential art traditions can shed light on aspects related to ethical ai systems through a case study concerning the indian dance system ie the it natyashastra we analyze potential pathways towards enhancing ethics in ai systems insights from our study outline the need for 1 incorporating empathy in ethical ai algorithms 2 integrating multimodal data formats for ethical ai system design and development 3 viewing ai ethics as a dynamic diverse cumulative and shared process rather than as a static selfcontained framework to facilitate adaptability without annihilation of values 4 consistent lifelong learning to enhance ai accountability,0
we consider how fair treatment in society for people with disabilities might be impacted by the rise in the use of artificial intelligence and especially machine learning methods we argue that fairness for people with disabilities is different to fairness for other protected attributes such as age gender or race one major difference is the extreme diversity of ways disabilities manifest and people adapt secondly disability information is highly sensitive and not always shared precisely because of the potential for discrimination given these differences we explore definitions of fairness and how well they work in the disability space finally we suggest ways of approaching fairness for people with disabilities in ai applications,0
objectives leveraging artificial intelligence ai in conjunction with electronic health records ehrs holds transformative potential to improve healthcare yet addressing bias in ai which risks worsening healthcare disparities cannot be overlooked this study reviews methods to detect and mitigate diverse forms of bias in ai models developed using ehr data methods we conducted a systematic review following the preferred reporting items for systematic reviews and metaanalyses prisma guidelines analyzing articles from pubmed web of science and ieee published between january 1 2010 and dec 17 2023 the review identified key biases outlined strategies for detecting and mitigating bias throughout the ai model development process and analyzed metrics for bias assessment results of the 450 articles retrieved 20 met our criteria revealing six major bias types algorithmic confounding implicit measurement selection and temporal the ai models were primarily developed for predictive tasks in healthcare settings four studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity equal opportunity and predictive equity sixty proposed various strategies for mitigating biases especially targeting implicit and selection biases these strategies evaluated through both performance eg accuracy auroc and fairness metrics predominantly involved data collection and preprocessing techniques like resampling reweighting and transformation discussion this review highlights the varied and evolving nature of strategies to address bias in ehrbased ai models emphasizing the urgent needs for the establishment of standardized generalizable and interpretable methodologies to foster the creation of ethical ai systems that promote fairness and equity in healthcare,0
this paper explores the potential of a multidisciplinary approach to testing and aligning artificial intelligence ai specifically focusing on large language models llms due to the rapid development and wide application of llms challenges such as ethical alignment controllability and predictability of these models emerged as global risks this study investigates an innovative simulationbased multiagent system within a virtual reality framework that replicates the realworld environment the framework is populated by automated digital citizens simulating complex social structures and interactions to examine and optimize ai application of various theories from the fields of sociology social psychology computer science physics biology and economics demonstrates the possibility of a more humanaligned and socially responsible ai the purpose of such a digital environment is to provide a dynamic platform where advanced ai agents can interact and make independent decisions thereby mimicking realistic scenarios the actors in this digital city operated by the llms serve as the primary agents exhibiting high degrees of autonomy while this approach shows immense potential there are notable challenges and limitations most significantly the unpredictable nature of realworld social dynamics this research endeavors to contribute to the development and refinement of ai emphasizing the integration of social ethical and theoretical dimensions for future research,0
various recent artificial intelligence ai system failures some of which have made the global headlines have highlighted issues in these systems these failures have resulted in calls for more ethical ai systems that better take into account their effects on various stakeholders however implementing ai ethics into practice is still an ongoing challenge highlevel guidelines for doing so exist devised by governments and private organizations alike but lack practicality for developers to address this issue in this paper we present a method for implementing ai ethics the method eccola has been iteratively developed using a cyclical action design research approach the method aims at making the highlevel ai ethics principles more practical making it possible for developers to more easily implement them in practice,0
the operationalization of ethics in the technical practices of artificial intelligence ai is facing significant challenges to address the problem of ineffective implementation of ai ethics we present our diagnosis analysis and interventional recommendations from a unique perspective of the realworld implementation of ai ethics through explainable ai xai techniques we first describe the phenomenon ie the symptoms of ineffective implementation of ai ethics in explainable ai using four empirical cases from the symptoms we diagnose the root cause ie the disease being the dysfunction and imbalance of power structures in the sociotechnical system of ai the power structures are dominated by unjust and unchecked power that does not represent the benefits and interests of the public and the most impacted communities and cannot be countervailed by ethical power based on the understanding of power mechanisms we propose three interventional recommendations to tackle the root cause including 1 making power explicable and checked 2 reframing the narratives and assumptions of ai and ai ethics to check unjust power and reflect the values and benefits of the public and 3 uniting the efforts of ethical and scientific conduct of ai to encode ethical values as technical standards norms and methods including conducting critical examinations and limitation analyses of ai technical practices we hope that our diagnosis and interventional recommendations can be a useful input to the ai community and civil societys ongoing discussion and implementation of ethics in ai for ethical and responsible ai practice,0
societys increasing dependence on artificial intelligence ai and aienabled systems require a more practical approach from software engineering se executives in middle and higherlevel management to improve their involvement in implementing ai ethics by making ethical requirements part of their management practices however research indicates that most work on implementing ethical requirements in se management primarily focuses on technical development with scarce findings for middle and higherlevel management we investigate this by interviewing ten finnish se executives in middle and higherlevel management to examine how they consider and implement ethical requirements we use ethical requirements from the european union eu trustworthy ethics guidelines for trustworthy ai as our reference for ethical requirements and an agile portfolio management framework to analyze implementation our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified the findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental wellbeing for implementation as sustainability requirements we examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the agile portfolio management framework,0
this article explores the ethical problems arising from the use of chatgpt as a kind of generative ai and suggests responses based on the humancentered artificial intelligence hcai framework the hcai framework is appropriate because it understands technology above all as a tool to empower augment and enhance human agency while referring to human wellbeing as a grand challenge thus perfectly aligning itself with ethics the science of human flourishing further hcai provides objectives principles procedures and structures for reliable safe and trustworthy ai which we apply to our chatgpt assessments the main danger chatgpt presents is the propensity to be used as a weapon of mass deception wmd and an enabler of criminal activities involving deceit we review technical specifications to better comprehend its potentials and limitations we then suggest both technical watermarking styleme detectors and factcheckers and nontechnical measures terms of use transparency educator considerations hitl to mitigate chatgpt misuse or abuse and recommend best uses creative writing noncreative writing teaching and learning we conclude with considerations regarding the role of humans in ensuring the proper use of chatgpt for individual and social wellbeing,0
we propose a holistic return on ethics hroe framework for understanding the return on organizational investments in artificial intelligence ai ethics efforts this framework is useful for organizations that wish to quantify the return for their investment decisions the framework identifies the direct economic returns of such investments the indirect paths to return through intangibles associated with organizational reputation and real options associated with capabilities the holistic framework ultimately provides organizations with the competency to employ and justify ai ethics investments,0
despite major advances in artificial intelligence ai for medicine and healthcare the deployment and adoption of ai technologies remain limited in realworld clinical practice in recent years concerns have been raised about the technical clinical ethical and legal risks associated with medical ai to increase real world adoption it is essential that medical ai tools are trusted and accepted by patients clinicians health organisations and authorities this work describes the futureai guideline as the first international consensus framework for guiding the development and deployment of trustworthy ai tools in healthcare the futureai consortium was founded in 2021 and currently comprises 118 interdisciplinary experts from 51 countries representing all continents including ai scientists clinicians ethicists and social scientists over a twoyear period the consortium defined guiding principles and best practices for trustworthy ai through an iterative process comprising an indepth literature review a modified delphi survey and online consensus meetings the futureai framework was established based on 6 guiding principles for trustworthy ai in healthcare ie fairness universality traceability usability robustness and explainability through consensus a set of 28 best practices were defined addressing technical clinical legal and socioethical dimensions the recommendations cover the entire lifecycle of medical ai from design development and validation to regulation deployment and monitoring futureai is a riskinformed assumptionfree guideline which provides a structured approach for constructing medical ai tools that will be trusted deployed and adopted in realworld practice researchers are encouraged to take the recommendations into account in proofofconcept stages to facilitate future translation towards clinical practice of medical ai,0
artificial intelligence surrogates are systems designed to infer preferences when individuals lose decisionmaking capacity fairness in such systems is a domain that has been insufficiently explored traditional algorithmic fairness frameworks are insufficient for contexts where decisions are relational existential and culturally diverse this paper explores an ethical framework for algorithmic fairness in ai surrogates by mapping major fairness notions onto potential realworld endoflife scenarios it then examines fairness across moral traditions the authors argue that fairness in this domain extends beyond parity of outcomes to encompass moral representation fidelity to the patients values relationships and worldview,0
large language models of artificial intelligence ai such as chatgpt find remarkable but controversial applicability in science and research this paper reviews epistemological challenges ethical and integrity risks in science conduct in the advent of generative ai this is with the aim to lay new timely foundations for a highquality research ethics review the role of ai language models as a research instrument and subject is scrutinized along with ethical implications for scientists participants and reviewers new emerging practices for research ethics review are discussed concluding with ten recommendations that shape a response for a more responsible research conduct in the era of ai,0
fairness in artificial intelligence ai has become a growing concern due to discriminatory outcomes in aibased decisionmaking systems while various methods have been proposed to mitigate bias most rely on complete demographic information an assumption often impractical due to legal constraints and the risk of reinforcing discrimination this survey examines fairness in ai when demographics are incomplete addressing the gap between traditional approaches and realworld challenges we introduce a novel taxonomy of fairness notions in this setting clarifying their relationships and distinctions additionally we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field,0
this paper investigates the relationship between law and explainable artificial intelligence xai while there is much discussion about the ai act for which the trilogue of the european parliament council and commission recently concluded other areas of law seem underexplored this paper focuses on european and in part german law although with international concepts and regulations such as fiduciary plausibility checks the general data protection regulation gdpr and product safety and liability based on xaitaxonomies requirements for xaimethods are derived from each of the legal bases resulting in the conclusion that each legal basis requires different xai properties and that the current state of the art does not fulfill these to full satisfaction especially regarding the correctness sometimes called fidelity and confidence estimates of xaimethods published in the proceedings of the aaaiacm conference on ai ethics and society,0
this paper addresses the critical challenge of building consumer trust in aipowered customer engagement by emphasising the necessity for transparency and accountability despite the potential of ai to revolutionise business operations and enhance customer experiences widespread concerns about misinformation and the opacity of ai decisionmaking processes hinder trust surveys highlight a significant lack of awareness among consumers regarding their interactions with ai alongside apprehensions about bias and fairness in ai algorithms the paper advocates for the development of explainable ai models that are transparent and understandable to both consumers and organisational leaders thereby mitigating potential biases and ensuring ethical use it underscores the importance of organisational commitment to transparency practices beyond mere regulatory compliance including fostering a culture of accountability prioritising clear data policies and maintaining active engagement with stakeholders by adopting a holistic approach to transparency and explainability businesses can cultivate trust in ai technologies bridging the gap between technological innovation and consumer acceptance and paving the way for more ethical and effective aipowered customer engagements keywords artificial intelligence ai transparency,0
our research endeavors to advance the concept of responsible artificial intelligence ai a topic of increasing importance within eu policy discussions the eu has recently issued several publications emphasizing the necessity of trust in ai underscoring the dual nature of ai as both a beneficial tool and a potential weapon this dichotomy highlights the urgent need for international regulation concurrently there is a need for frameworks that guide companies in ai development ensuring compliance with such regulations our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of ai regulation identifying focal areas for future attention this paper introduces a comprehensive and to our knowledge the first unified definition of responsible ai through a structured literature review we elucidate the current understanding of responsible ai drawing from this analysis we propose an approach for developing a future framework centered around this concept our findings advocate for a humancentric approach to responsible ai this approach encompasses the implementation of ai methods with a strong emphasis on ethics model explainability and the pillars of privacy security and trust,0
while the operationalisation of highlevel ai ethics principles into practical aiml systems has made progress there is still a theorypractice gap in managing tensions between the underlying ai ethics aspects we cover five approaches for addressing the tensions via tradeoffs ranging from rudimentary to complex the approaches differ in the types of considered context scope methods for measuring contexts and degree of justification none of the approaches is likely to be appropriate for all organisations systems or applications to address this we propose a framework which consists of i proactive identification of tensions ii prioritisation and weighting of ethics aspects iii justification and documentation of tradeoff decisions the proposed framework aims to facilitate the implementation of wellrounded aiml systems that are appropriate for potential regulatory requirements,0
mainstream ai ethics with its reliance on topdown principledriven frameworks fails to account for the situated realities of diverse communities affected by ai artificial intelligence critics have argued that ai ethics frequently serves corporate interests through practices of ethics washing operating more as a tool for public relations than as a means of preventing harm or advancing the common good as a result growing scepticism among critical scholars has cast the field as complicit in sustaining harmful systems rather than challenging or transforming them in response this paper adopts a science and technology studies sts perspective to critically interrogate the field of ai ethics it hence applies the same analytic tools sts has long directed at disciplines such as biology medicine and statistics to ethics this perspective reveals a core tension between vertical topdown principlebased and horizontal riskmitigating implementationoriented approaches to ethics by tracing how these models have shaped the discourse we show how both fall short in addressing the complexities of ai as a sociotechnical assemblage embedded in practice and entangled with power to move beyond these limitations we propose a threefold reorientation of ai ethics first we call for a shift in foundations from topdown abstraction to empirical grounding second we advocate for pluralisation moving beyond westerncentric frameworks toward a multiplicity of ontoepistemic perspectives finally we outline strategies for reconfiguring ai ethics as a transformative force moving from narrow paradigms of risk mitigation toward cocreating technologies of hope,0
the integration of artificial intelligence ai into weapon systems is one of the most consequential tactical and strategic decisions in the history of warfare current ai development is a remarkable combination of accelerating capability hidden decision mechanisms and decreasing costs implementation of these systems is in its infancy and exists on a spectrum from resilient and flexible to simplistic and brittle resilient systems should be able to effectively handle the complexities of a highdimensional battlespace simplistic ai implementations could be manipulated by an adversarial ai that identifies and exploits their weaknesses in this paper we present a framework for understanding the development of dynamic aiml systems that interactively and continuously adapt to their users needs we explore the implications of increasingly capable ai in the kill chain and how this will lead inevitably to a fully automated always on system barring regulation by treaty we examine the potential of total integration of cyber and physical security and how this likelihood must inform the development of aienabled systems with respect to the fog of war human morals and ethics,0
the implementation of artificial intelligence ai in household environments especially in the form of proactive autonomous agents brings about possibilities of comfort and attention as well as it comes with intra or extramural ethical challenges this article analyzes agentic ai and its applications focusing on its move from reactive to proactive autonomy privacy fairness and user control we review responsible innovation frameworks humancentered design principles and governance practices to distill practical guidance for ethical smart home systems vulnerable user groups such as elderly individuals children and neurodivergent who face higher risks of surveillance bias and privacy risks were studied in detail in context of agentic ai design imperatives are highlighted such as tailored explainability granular consent mechanisms and robust override controls supported by participatory and inclusive methodologies it was also explored how datadriven insights including social media analysis via natural language processingnlp can inform specific user needs and ethical concerns this survey aims to provide both a conceptual foundation and suggestions for developing transparent inclusive and trustworthy agentic ai in household automation,0
artificial intelligence ai is becoming increasingly widespread in system development endeavors as ai systems affect various stakeholders due to their unique nature the growing influence of these systems calls for ethical considerations academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development however research on methods and tools for implementing ethics into ai system design and development in practice is still lacking this paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in ai based software development we do so by means of an industrial multiple case study on ai systems development in the healthcare sector using a research model based on extant conceptual ai ethics literature we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design,0
artificial intelligence models encounter significant challenges due to their blackbox nature particularly in safetycritical domains such as healthcare finance and autonomous vehicles explainable artificial intelligence xai addresses these challenges by providing explanations for how these models make decisions and predictions ensuring transparency accountability and fairness existing studies have examined the fundamental concepts of xai its general principles and the scope of xai techniques however there remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representations design methodologies of xai models and other associated aspects this paper provides a comprehensive literature review encompassing common terminologies and definitions the need for xai beneficiaries of xai a taxonomy of xai methods and the application of xai methods in different application areas the survey is aimed at xai researchers xai practitioners ai model developers and xai beneficiaries who are interested in enhancing the trustworthiness transparency accountability and fairness of their ai models,0
artificial intelligence ai is transforming our daily life with several applications in healthcare space exploration banking and finance these rapid progresses in ai have brought increasing attention to the potential impacts of ai technologies on society with ethically questionable consequences in recent years several ethical principles have been released by governments national and international organisations these principles outline highlevel precepts to guide the ethical development deployment and governance of ai however the abstract nature diversity and contextdependency of these principles make them difficult to implement and operationalize resulting in gaps between principles and their execution most recent work analysed and summarized existing ai principles and guidelines but they did not provide findings on principleimplementation gaps and how to mitigate them these findings are particularly important to ensure that ai implementations are aligned with ethical principles and values in this paper we provide a contextual and global evaluation of current ethical ai principles for all continents with the aim to identify potential principle characteristics tailored to specific countries or applicable across countries next we analyze the current level of ai readiness and current implementations of ethical ai principles in different countries to identify gaps in the implementation of ai principles and their causes finally we propose recommendations to mitigate the principleimplementation gaps,0
much of the existing research on the social and ethical impact of artificial intelligence has been focused on defining ethical principles and guidelines surrounding machine learning ml and other artificial intelligence ai algorithms while this is extremely useful for helping define the appropriate social norms of ai we believe that it is equally important to discuss both the potential and risks of ml and to inspire the community to use ml for beneficial objectives in the present article which is specifically aimed at ml practitioners we thus focus more on the latter carrying out an overview of existing highlevel ethical frameworks and guidelines but above all proposing both conceptual and practical principles and guidelines for ml research and deployment insisting on concrete actions that can be taken by practitioners to pursue a more ethical and moral practice of ml aimed at using ai for social good,0
we propose a conceptualization and implementation of ai ethics via the capability approach we aim to show that conceptualizing ai ethics through the capability approach has two main advantages for ai ethics as a discipline first it helps clarify the ethical dimension of ai tools second it provides guidance to implementing ethical considerations within the design of ai tools we illustrate these advantages in the context of ai tools in medicine by showing how ethicsbased auditing of ai tools in medicine can greatly benefit from our capabilitybased approach,0
in the past decade the deployment of deep learning artificial intelligence ai methods has become pervasive across a spectrum of realworld applications often in safetycritical contexts this comprehensive research article rigorously investigates the ethical dimensions intricately linked to the rapid evolution of ai technologies with a particular focus on the healthcare domain delving deeply it explores a multitude of facets including transparency adept data management human oversight educational imperatives and international collaboration within the realm of ai advancement central to this article is the proposition of a conscientious ai framework meticulously crafted to accentuate values of transparency equity answerability and a humancentric orientation the second contribution of the article is the indepth and thorough discussion of the limitations inherent to ai systems it astutely identifies potential biases and the intricate challenges of navigating multifaceted contexts lastly the article unequivocally accentuates the pressing need for globally standardized ai ethics principles and frameworks simultaneously it aptly illustrates the adaptability of the ethical framework proposed herein positioned skillfully to surmount emergent challenges,0
an emerging field of ai namely fair machine learning ml aims to quantify different types of bias also known as unfairness exhibited in the predictions of ml algorithms and to design new algorithms to mitigate them often the definitions of bias used in the literature are observational ie they use the input and output of a pretrained algorithm to quantify a bias under concern in realitythese definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm thusthere is a gap between what we want fair ml to achieve and what it does in a dynamic social environment hence we propose an alternative dynamic mechanismfair gameto assure fairness in the predictions of an ml algorithm and to adapt its predictions as the society interacts with the algorithm over time fair game puts together an auditor and a debiasing algorithm in a loop around an ml algorithm the fair game puts these two components in a loop by leveraging reinforcement learning rl rl algorithms interact with an environment to take decisions which yields new observations also known as datafeedback from the environment and in turn adapts future decisions rl is already used in algorithms with prefixed longterm fairness goals fair game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies thusfair game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ml system this allows us to develop a flexible and adaptiveovertime framework to build fair ml systems pre and postdeployment,0
an appropriate ethical framework around the use of artificial intelligence ai in healthcare has become a key desirable with the increasingly widespread deployment of this technology advances in ai hold the promise of improving the precision of outcome prediction at the level of the individual however the addition of these technologies to patientclinician interactions as with any complex human interaction has potential pitfalls while physicians have always had to carefully consider the ethical background and implications of their actions detailed deliberations around fastmoving technological progress may not have kept up we use a common but key challenge in healthcare interactions the disclosure of bad news likely imminent death to illustrate how the philosophical framework of the felicific calculus developed in the 18th century by jeremy bentham may have a timely quasiquantitative application in the age of ai we show how this ethical algorithm can be used to assess across seven mutually exclusive and exhaustive domains whether an aisupported action can be morally justified,0
responsible artificial intelligence guidelines ask engineers to consider how their systems might harm however contemporary artificial intelligence systems are built by composing many preexisting software modules that pass through many hands before becoming a finished product or service how does this shape responsible artificial intelligence practice in interviews with 27 artificial intelligence engineers across industry open source and academia our participants often did not see the questions posed in responsible artificial intelligence guidelines to be within their agency capability or responsibility to address we use suchmans located accountability to show how responsible artificial intelligence labor is currently organized and to explore how it could be done differently we identify crosscutting social logics like modularizability scale reputation and customer orientation that organize which responsible artificial intelligence actions do take place and which are relegated to low status staff or believed to be the work of the next or previous person in the imagined supply chain we argue that current responsible artificial intelligence interventions like ethics checklists and guidelines that assume panoptical knowledge and control over systems could be improved by taking a located accountability approach recognizing where relations and obligations might intertwine inside and outside of this supply chain,0
harm reporting in artificial intelligence ai currently lacks a structured process for disclosing and addressing algorithmic flaws relying largely on an adhoc approach this contrasts sharply with the wellestablished coordinated vulnerability disclosure cvd ecosystem in software security while global efforts to establish frameworks for ai transparency and collaboration are underway the unique challenges presented by machine learning ml models demand a specialized approach to address this gap we propose implementing a coordinated flaw disclosure cfd framework tailored to the complexities of ml and ai issues this paper reviews the evolution of ml disclosure practices from ad hoc reporting to emerging participatory auditing methods and compares them with cybersecurity norms our framework introduces innovations such as extended model cards dynamic scope expansion an independent adjudication panel and an automated verification process we also outline a forthcoming realworld pilot of cfd we argue that cfd could significantly enhance public trust in ai systems by balancing organizational and community interests cfd aims to improve ai accountability in a rapidly evolving technological landscape,0
these past few months have been especially challenging and the deployment of technology in ways hitherto untested at an unrivalled pace has left the internet and technology watchers aghast artificial intelligence has become the byword for technological progress and is being used in everything from helping us combat the covid19 pandemic to nudging our attention in different directions as we all spend increasingly larger amounts of time online it has never been more important that we keep a sharp eye out on the development of this field and how it is shaping our society and interactions with each other with this inaugural edition of the state of ai ethics we hope to bring forward the most important developments that caught our attention at the montreal ai ethics institute this past quarter our goal is to help you navigate this everevolving field swiftly and allow you and your organization to make informed decisions this pulsecheck for the state of discourse research and development is geared towards researchers and practitioners alike who are making decisions on behalf of their organizations in considering the societal impacts of aienabled solutions we cover a wide set of areas in this report spanning agency and responsibility security and risk disinformation jobs and labor the future of ai ethics and more our staff has worked tirelessly over the past quarter surfacing signal from the noise so that you are equipped with the right tools and knowledge to confidently tread this complex yet consequential domain,0
as the realworld impact of artificial intelligence ai systems has been steadily growing so too have these systems come under increasing scrutiny in response the study of ai fairness has rapidly developed into a rich field of research with links to computer science social science law and philosophy many technical solutions for measuring and achieving ai fairness have been proposed yet their approach has been criticized in recent years for being misleading unrealistic and harmful in our paper we survey these criticisms of ai fairness and identify key limitations that are inherent to the prototypical paradigm of ai fairness by carefully outlining the extent to which technical solutions can realistically help in achieving ai fairness we aim to provide the background necessary to form a nuanced opinion on developments in fair ai this delineation also provides research opportunities for nonai solutions peripheral to ai systems in supporting fair decision processes,0
this whitepaper offers normative and practical guidance for developers of artificial intelligence ai systems to achieve trustworthy ai in it we present overall ethical requirements and six ethical principles with valuespecific recommendations for tools to implement these principles into technology our valuespecific recommendations address the principles of fairness privacy and data protection safety and robustness sustainability transparency and explainability and truthfulness for each principle we also present examples of criteria for risk assessment and categorization of ai systems and applications in line with the categories of the european union eu ai act our work is aimed at stakeholders who can take it as a potential blueprint to fulfill minimum ethical requirements for trustworthy ai and ai certification,0
as artificial intelligence ai continues to grow daily more exciting and somewhat controversial technology emerges every other day as we see the advancements in ai we see more and more people becoming skeptical of it this paper explores the complications and confusion around the ethics of generative ai art we delve deep into the ethical side of ai specifically generative art we step back from the excitement and observe the impossible conundrums that this impressive technology produces covering environmental consequences celebrity representation intellectual property deep fakes and artist displacement our research found that generative ai art is responsible for increased carbon emissions spreading misinformation copyright infringement unlawful depiction and job displacement in light of this we propose multiple possible solutions for these problems we address each situations history cause and consequences and offer different viewpoints at the root of it all though the central theme is that generative ai art needs to be correctly legislated and regulated,0
this paper examines the ethical and anthropological challenges posed by aidriven recommender systems rss which increasingly shape digital environments and social interactions by curating personalized content rss do not merely reflect user preferences but actively construct experiences across social media entertainment platforms and ecommerce their influence raises concerns over privacy autonomy and mental wellbeing while existing approaches such as algorethics the effort to embed ethical principles into algorithmic design remain insufficient rss inherently reduce human complexity to quantifiable profiles exploit user vulnerabilities and prioritize engagement over wellbeing the paper advances a threedimensional framework for humancentered rss integrating policies and regulation interdisciplinary research and education these strategies are mutually reinforcing research provides evidence for policy policy enables safeguards and standards and education equips users to engage critically by connecting ethical reflection with governance and digital literacy the paper argues that rss can be reoriented to enhance autonomy and dignity rather than undermine them,0
activists journalists and scholars have long raised critical questions about the relationship between diversity representation and structural exclusions in dataintensive tools and services we build on work mapping the emergent landscape of corporate ai ethics to center one outcome of these conversations the incorporation of diversity and inclusion in corporate ai ethics activities using interpretive document analysis and analytic tools from the values in design field we examine how diversity and inclusion work is articulated in publicfacing ai ethics documentation produced by three companies that create application and services layer ai infrastructure google microsoft and salesforce we find that as these documents make diversity and inclusion more tractable to engineers and technical clients they reveal a drift away from civil rights justifications that resonates with the managerialization of diversity by corporations in the mid1980s the focus on technical artifacts such as diverse and inclusive datasets and the replacement of equity with fairness make ethical work more actionable for everyday practitioners yet they appear divorced from broader dei initiatives and other subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values finally diversity and inclusion as configured by engineering logic positions firms not as ethics owners but as ethics allocators while these companies claim expertise on ai ethics the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers,0
ethics in ai has become a debated topic of public and expert discourse in recent years but what do people who build ai ai practitioners have to say about their understanding of ai ethics and the challenges associated with incorporating it in the aibased systems they develop understanding ai practitioners views on ai ethics is important as they are the ones closest to the ai systems and can bring about changes and improvements we conducted a survey aimed at understanding ai practitioners awareness of ai ethics and their challenges in incorporating ethics based on 100 ai practitioners responses our findings indicate that majority of ai practitioners had a reasonable familiarity with the concept of ai ethics primarily due to workplace rules and policies privacy protection and security was the ethical principle that majority of them were aware of formal educationtraining was considered somewhat helpful in preparing practitioners to incorporate ai ethics the challenges that ai practitioners faced in the development of ethical aibased systems included i general challenges ii technologyrelated challenges and iii humanrelated challenges we also identified areas needing further investigation and provided recommendations to assist ai practitioners and companies in incorporating ethics into ai development,0
this article examines the evolving role of legal frameworks in shaping ethical artificial intelligence ai use in corporate governance as ai systems become increasingly prevalent in business operations and decisionmaking there is a growing need for robust governance structures to ensure their responsible development and deployment through analysis of recent legislative initiatives industry standards and scholarly perspectives this paper explores key legal and regulatory approaches aimed at promoting transparency accountability and fairness in corporate ai applications it evaluates the strengths and limitations of current frameworks identifies emerging best practices and offers recommendations for developing more comprehensive and effective ai governance regimes the findings highlight the importance of adaptable principlebased regulations coupled with sectorspecific guidance to address the unique challenges posed by ai technologies in the corporate sphere,0
in this paper we an epistemologist and a machine learning scientist argue that we need to pursue a novel area of philosophical research in ai the ethics of belief for ai here we take the ethics of belief to refer to a field at the intersection of epistemology and ethics concerned with possible moral practical and other nontruthrelated dimensions of belief in this paper we will primarily be concerned with the normative question within the ethics of belief regarding what agents both human and artificial ought to believe rather than with questions concerning whether beliefs meet certain evaluative standards such as being true being justified constituting knowledge etc we suggest four topics in extant work in the ethics of human belief that can be applied to an ethics of ai belief doxastic wronging by ai morally wronging someone in virtue of beliefs held about them morally owed beliefs beliefs that agents are morally obligated to hold pragmatic and moral encroachment cases where the practical or moral features of a belief is relevant to its epistemic status and in our case specifically to whether an agent ought to hold the belief and moral responsibility for ai beliefs we also indicate two relatively nascent areas of philosophical research that havent yet been generally recognized as ethics of ai belief research but that do fall within this field of research in virtue of investigating various moral and practical dimensions of belief the epistemic and ethical decolonization of ai and epistemic injustice in ai,0
recent advances in generative artificial intelligence have raised public awareness shaping expectations and concerns about their societal implications central to these debates is the question of ai alignment how well ai systems meet public expectations regarding safety fairness and social values however little is known about what people expect from aienabled systems and how these expectations differ across national contexts we present evidence from two surveys of public preferences for key functional features of aienabled systems in germany n 1800 and the united states n 1756 we examine support for four types of alignment in ai moderation accuracy and reliability safety bias mitigation and the promotion of aspirational imaginaries us respondents report significantly higher ai use and consistently greater support for all alignment features reflecting broader technological openness and higher societal involvement with ai in both countries accuracy and safety enjoy the strongest support while more normatively charged goals like fairness and aspirational imaginaries receive more cautious backing particularly in germany we also explore how individual experience with ai attitudes toward free speech political ideology partisan affiliation and gender shape these preferences ai use and free speech support explain more variation in germany in contrast us responses show greater attitudinal uniformity suggesting that higher exposure to ai may consolidate public expectations these findings contribute to debates on ai governance and crossnational variation in public preferences more broadly our study demonstrates the value of empirically grounding ai alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of aigenerated content,0
the idea of fairness and justice has long and deep roots in western civilization and is strongly linked to ethics it is therefore not strange that it is core to the current discussion about the ethics of development and use of ai systems in this short paper i wish to further motivate my position in this matter i will never be completely fair nothing ever is the point is not complete fairness but the need to establish metrics and thresholds for fairness that ensure trust in ai systems,0
artificial intelligence ai has significantly revolutionized radiology promising improved patient outcomes and streamlined processes however its critical to ensure the fairness of ai models to prevent stealthy bias and disparities from leading to unequal outcomes this review discusses the concept of fairness in ai focusing on bias auditing using the aequitas toolkit and its realworld implications in radiology particularly in disease screening scenarios aequitas an opensource bias audit toolkit scrutinizes ai models decisions identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands this toolkit operates on statistical theories analyzing a large dataset to reveal a models fairness it excels in its versatility to handle various variables simultaneously especially in a field as diverse as radiology the review explicates essential fairness metrics equal and proportional parity false positive rate parity false discovery rate parity false negative rate parity and false omission rate parity each metric serves unique purposes and offers different insights we present hypothetical scenarios to demonstrate their relevance in disease screening settings and how disparities can lead to significant realworld impacts,0
artificial intelligence ai has demonstrated the ability to extract insights from data but the issue of fairness remains a concern in highstakes fields such as healthcare despite extensive discussion and efforts in algorithm development ai fairness and clinical concerns have not been adequately addressed in this paper we discuss the misalignment between technical and clinical perspectives of ai fairness highlight the barriers to ai fairness translation to healthcare advocate multidisciplinary collaboration to bridge the knowledge gap and provide possible solutions to address the clinical concerns pertaining to ai fairness,0
the rapid expansion of artificial intelligence ai in digital platforms used by youth has created significant challenges related to privacy autonomy and data protection while aidriven personalization offers enhanced user experiences it often operates without clear ethical boundaries leaving young users vulnerable to data exploitation and algorithmic biases this paper presents a call to action for ethical ai governance advocating for a structured framework that ensures youthcentred privacy protections transparent data practices and regulatory oversight we outline key areas requiring urgent intervention including algorithmic transparency privacy education parental datasharing ethics and accountability measures through this approach we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers ai developers and educators to build a fairer and more accountable ai ecosystem,0
this paper examines how competing sociotechnical imaginaries of artificial intelligence ai risk shape governance decisions and regulatory constraints drawing on concepts from science and technology studies we analyse three dominant narrative groups existential risk proponents who emphasise catastrophic agi scenarios accelerationists who portray ai as a transformative force to be unleashed and critical ai scholars who foreground presentday harms rooted in systemic inequality through an analysis of representative manifestostyle texts we explore how these imaginaries differ across four dimensions normative visions of the future diagnoses of the present social order views on science and technology and perceived human agency in managing ai risks our findings reveal how these narratives embed distinct assumptions about risk and have the potential to progress into policymaking processes by narrowing the space for alternative governance approaches we argue against speculative dogmatism and for moving beyond deterministic imaginaries toward regulatory strategies that are grounded in pragmatism,0
in january and february 2020 the scottish government released two documents for review by the public regarding their artificial intelligence ai strategy the montreal ai ethics institute maiei reviewed these documents and published a response on 4 june 2020 maieis response examines several questions that touch on the proposed definition of ai the peoplecentered nature of the strategy considerations to ensure that everyone benefits from ai the strategys overarching vision scotlands ai ecosystem the proposed strategic themes and how to grow public confidence in ai by building responsible and ethical systems in addition to examining the points above maiei suggests that the strategy be extended to include considerations on biometric data and how that will be processed and used in the context of ai it also highlights the importance of tackling headon the inherently stochastic nature of deep learning systems and developing concrete guidelines to ensure that these systems are built responsibly and ethically particularly as machine learning becomes more accessible finally it concludes that any national ai strategy must clearly address the measurements of success in regards to the strategys stated goals and vision to ensure that they are interpreted and applied consistently to do this there must be inclusion and transparency between those building the systems and those using them in their work,0
artificial intelligence has been introduced as a way to improve access to mental health support however most ai mental health chatbots rely on a limited range of disciplinary input and fail to integrate expertise across the chatbots lifecycle this paper examines the costbenefit tradeoff of interdisciplinary collaboration in ai mental health chatbots we argue that involving experts from technology healthcare ethics and law across key lifecycle phases is essential to ensure valuealignment and compliance with the highrisk requirements of the ai act we also highlight practical recommendations and existing frameworks to help balance the challenges and benefits of interdisciplinarity in mental health chatbots,0
the impact of artificial intelligence does not depend only on fundamental research and technological developments but for a large part on how these systems are introduced into society and used in everyday situations ai is changing the way we work live and solve challenges but concerns about fairness transparency or privacy are also growing ensuring responsible ethical ai is more than designing systems whose result can be trusted it is about the way we design them why we design them and who is involved in designing them in order to develop and use ai responsibly we need to work towards technical societal institutional and legal methods and tools which provide concrete support to ai practitioners as well as awareness and training to enable participation of all to ensure the alignment of ai systems with our societies principles and values,0
minority groups have been using social media to organize social movements that create profound social impacts black lives matter blm and stop asian hate sah are two successful social movements that have spread on twitter that promote protests and activities against racism and increase the publics awareness of other social challenges that minority groups face however previous studies have mostly conducted qualitative analyses of tweets or interviews with users which may not comprehensively and validly represent all tweets very few studies have explored the twitter topics within blm and sah dialogs in a rigorous quantified and datacentered approach therefore in this research we adopted a mixedmethods approach to comprehensively analyze blm and sah twitter topics we implemented 1 the latent dirichlet allocation model to understand the top highlevel words and topics and 2 opencoding analysis to identify specific themes across the tweets we collected more than one million tweets with the blacklivesmatter and stopasianhate hashtags and compared their topics our findings revealed that the tweets discussed a variety of influential topics in depth and social justice social movements and emotional sentiments were common topics in both movements though with unique subtopics for each movement our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of ai ethics and society in general,0
in this paper we describe the development and evaluation of aitk the artificial intelligence toolkit this opensource project contains both python libraries and computational essays jupyter notebooks that together are designed to allow a diverse audience with little or no background in ai to interact with a variety of ai tools exploring in more depth how they function visualizing their outcomes and gaining a better understanding of their ethical implications these notebooks have been piloted at multiple institutions in a variety of humanities courses centered on the theme of responsible ai in addition we conducted usability testing of aitk our pilot studies and usability testing results indicate that aitk is easy to navigate and effective at helping users gain a better understanding of ai our goal in this time of rapid innovations in ai is for aitk to provide an accessible resource for faculty from any discipline looking to incorporate ai topics into their courses and for anyone eager to learn more about ai on their own,0
large language models llms such as gpt4 and bert have rapidly gained traction in natural language processing nlp and are now integral to financial decisionmaking however their deployment introduces critical challenges particularly in perpetuating gender biases that can distort decisionmaking outcomes in highstakes economic environments this paper investigates gender bias in llms through both mathematical proofs and empirical experiments using the word embedding association test weat demonstrating that llms inherently reinforce gender stereotypes even without explicit gender markers by comparing the decisionmaking processes of humans and llms we reveal fundamental differences while humans can override biases through ethical reasoning and individualized understanding llms maintain bias as a rational outcome of their mathematical optimization on biased data our analysis proves that bias in llms is not an unintended flaw but a systematic result of their rational processing which tends to preserve and amplify existing societal biases encoded in training data drawing on existentialist theory we argue that llmgenerated bias reflects entrenched societal structures and highlights the limitations of purely technical debiasing methods this research underscores the need for new theoretical frameworks and interdisciplinary methodologies that address the ethical implications of integrating llms into economic and financial decisionmaking we advocate for a reconceptualization of how llms influence economic decisions emphasizing the importance of incorporating humanlike ethical considerations into ai governance to ensure fairness and equity in aidriven financial systems,0
nowadays technology is being adopted on every aspect of our lives and it is one of most important transformation driver in industry moreover many of the systems and digital services that we use daily rely on artificial intelligent technology capable of modeling social or individual behaviors that in turns also modify personal decisions and actions in this paper we briefly discuss from a technological perspective a number of critical issues including the purpose of promoting trust and ensure social benefit by the proper use of artificial intelligent systems to achieve this goal we propose a generic ethical technological framework as a first attempt to define a common context towards developing real engineering ethical by design we hope that this initial proposal to be useful for early adopters and especially for standardization teams,0
the increasing integration of artificial intelligence ai in digital ecosystems has reshaped privacy dynamics particularly for young digital citizens navigating datadriven environments this study explores evolving privacy concerns across three key stakeholder groups digital citizens ages 1619 parentseducators and ai professionals and assesses differences in data ownership trust transparency parental mediation education and riskbenefit perceptions employing a grounded theory methodology this research synthesizes insights from 482 participants through structured surveys qualitative interviews and focus groups the findings reveal distinct privacy expectations young users emphasize autonomy and digital freedom while parents and educators advocate for regulatory oversight and ai literacy programs ai professionals in contrast prioritize the balance between ethical system design and technological efficiency the data further highlights gaps in ai literacy and transparency emphasizing the need for comprehensive stakeholderdriven privacy frameworks that accommodate diverse user needs using comparative thematic analysis this study identifies key tensions in privacy governance and develops the novel privacyethics alignment in ai peaai model which structures privacy decisionmaking as a dynamic negotiation between stakeholders by systematically analyzing themes such as transparency user control risk perception and parental mediation this research provides a scalable adaptive foundation for ai governance ensuring that privacy protections evolve alongside emerging ai technologies and youthcentric digital interactions,0
this paper aims to provide an overview of the ethical concerns in artificial intelligence ai and the framework that is needed to mitigate those risks and to suggest a practical path to ensure the development and use of ai at the united nations un aligns with our ethical values the overview discusses how ai is an increasingly powerful tool with potential for good albeit one with a high risk of negative sideeffects that go against fundamental human rights and un values it explains the need for ethical principles for ai aligned with principles for data governance as data and ai are tightly interwoven it explores different ethical frameworks that exist and tools such as assessment lists it recommends that the un develop a framework consisting of ethical principles architectural standards assessment methods tools and methodologies and a policy to govern the implementation and adherence to this framework accompanied by an education program for staff,0
recent advances in ai raise the possibility that ai systems will one day be able to do anything humans can do only better if artificial general intelligence agi is achieved ai systems may be able to understand reason problem solve create and evolve at a level and speed that humans will increasingly be unable to match or even understand these possibilities raise a natural question as to whether ai will eventually become superior to humans a successor digital species with a rightful claim to assume leadership of the universe however a deeper consideration suggests the overlooked differentiator between human beings and ai is not the brain but the central nervous system cns providing us with an immersive integration with physical reality it is our cns that enables us to experience emotion including pain joy suffering and love and therefore to fully appreciate the consequences of our actions on the world around us and that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems and so be fully qualified to be the leaders of the universe a cns cannot be manufactured or simulated it must be grown as a biological construct and so even the development of consciousness will not be sufficient to make ai systems superior to humans ai systems may become more capable than humans on almost every measure and transform our society however the best foundation for leadership of our universe will always be dna not silicon,0
solidarity is one of the fundamental values at the heart of the construction of peaceful societies and present in more than one third of worlds constitutions still solidarity is almost never included as a principle in ethical guidelines for the development of ai solidarity as an ai principle 1 shares the prosperity created by ai implementing mechanisms to redistribute the augmentation of productivity for all and shares the burdens making sure that ai does not increase inequality and no human is left behind solidarity as an ai principle 2 assesses the long term implications before developing and deploying ai systems so no groups of humans become irrelevant because of ai systems considering solidarity as a core principle for ai development will provide not just an humancentric but a more humanitycentric approach to ai,0
researchers practitioners and policymakers with an interest in ai ethics need more integrative approaches for studying and intervening in ai systems across many contexts and scales of activity this paper presents ai value chains as an integrative concept that satisfies that need to more clearly theorize ai value chains and conceptually distinguish them from supply chains we review theories of value chains and ai value chains from the strategic management service science economic geography industry government and applied research literature we then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in ai value chains building upon the findings of our integrative review we recommend three future directions that researchers practitioners and policymakers can take to advance more ethical practices across ai value chains we urge ai ethics researchers and practitioners to move toward value chain perspectives that situate actors in context account for the many types of resources involved in cocreating ai systems and integrate a wider range of ethical concerns across contexts and scales,0
the rapid advancement of artificial intelligence ai such as the emergence of large language models including chatgpt and dalle 2 has brought both opportunities for improving productivity and raised ethical concerns this paper investigates the ethics of using artificial intelligence ai in cartography with a particular focus on the generation of maps using dalle 2 to accomplish this we first create an opensourced dataset that includes synthetic aigenerated and realworld humandesigned maps at multiple scales with a variety settings we subsequently examine four potential ethical concerns that may arise from the characteristics of dalle 2 generated maps namely inaccuracies misleading information unanticipated features and reproducibility we then develop a deep learningbased ethical examination system that identifies those aigenerated maps our research emphasizes the importance of ethical considerations in the development and use of ai techniques in cartography contributing to the growing body of work on trustworthy maps we aim to raise public awareness of the potential risks associated with aigenerated maps and support the development of ethical guidelines for their future use,0
although essential to revealing biased performance well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect this concern is even more salient while auditing biometric systems such as facial recognition where the data is sensitive and the technology is often used in ethically questionable manners we demonstrate a set of five ethical concerns in the particular case of auditing commercial facial processing technology highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system we go further to provide tangible illustrations of these concerns and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal,0
as artificial intelligence ai continues to advance rapidly it becomes increasingly important to consider ais ethical and societal implications in this paper we present a bottomup mapping of the current state of research at the intersection of humancentered ai ethical and responsible ai hcerai by thematically reviewing and analyzing 164 research papers from leading conferences in ethical social and human factors of ai aies chi cscw and facct the ongoing research in hcerai places emphasis on governance fairness and explainability these conferences however concentrate on specific themes rather than encompassing all aspects while aies has fewer papers on hcerai it emphasizes governance and rarely publishes papers about privacy security and human flourishing facct publishes more on governance and lacks papers on privacy security and human flourishing chi and cscw as more established conferences have a broader research portfolio we find that the current emphasis on governance and fairness in ai research may not adequately address the potential unforeseen and unknown implications of ai therefore we recommend that future research should expand its scope and diversify resources to prepare for these potential consequences this could involve exploring additional areas such as privacy security human flourishing and explainability,0
how do ethical arguments affect ai adoption in business we randomly expose business decisionmakers to arguments used in ai fairness activism arguments emphasizing the inescapability of algorithmic bias lead managers to abandon ai for manual review by humans and report greater expectations about lawsuits and negative pr these effects persist even when ai lowers gender and racial disparities and when engineering investments to address ai fairness are feasible emphasis on status quo comparisons yields opposite effects we also measure the effects of scientific veneer in ai ethics arguments scientific veneer changes managerial behavior but does not asymmetrically benefit favorable versus critical ai activism,0
the 3rd edition of the montreal ai ethics institutes the state of ai ethics captures the most relevant developments in ai ethics since october 2020 it aims to help anyone from machine learning experts to human rights activists and policymakers quickly digest and understand the fields everchanging developments through research and article summaries as well as expert commentary this report distills the research and reporting surrounding various domains related to the ethics of ai including algorithmic injustice discrimination ethical ai labor impacts misinformation privacy risk and security social media and more in addition the state of ai ethics includes exclusive content written by worldclass ai ethics experts from universities research institutes consulting firms and governments unique to this report is the abuse and misogynoir playbook written by dr katlyn tuner research scientist space enabled research group mit dr danielle wood assistant professor program in media arts and sciences assistant professor aeronautics and astronautics lead space enabled research group mit and dr catherine dignazio assistant professor urban science and planning director data feminism lab mit the piece and accompanying infographic is a deepdive into the historical and systematic silencing erasure and revision of black womens contributions to knowledge and scholarship in the united stations and globally exposing and countering this playbook has become increasingly important following the firing of ai ethics expert dr timnit gebru and several of her supporters at google this report should be used not only as a point of reference and insight on the latest thinking in the field of ai ethics but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of ai on the world,0
research in social psychology has shown that peoples biased subjective judgments about anothers personality based solely on their appearance are not predictive of their actual personality traits but researchers and companies often utilize computer vision models to predict similarly subjective personality attributes such as employability we seek to determine whether stateoftheart black box face processing technology can learn humanlike appearance biases with features extracted with facenet a widely used face recognition framework we train a transfer learning model on human subjects first impressions of personality traits in other faces as measured by social psychologists we find that features extracted with facenet can be used to predict human appearance bias scores for deliberately manipulated faces but not for randomly generated faces scored by humans additionally in contrast to work with human biases in social psychology the model does not find a significant signal correlating politicians vote shares with perceived competence bias with local interpretable modelagnostic explanations lime we provide several explanations for this discrepancy our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate we shed light on the ways in which appearance bias could be embedded in face processing technology and cast further doubt on the practice of predicting subjective traits based on appearances,0
organizations that develop and deploy artificial intelligence ai systems need to take measures to reduce the associated risks in this paper we examine how ai companies could design an ai ethics board in a way that reduces risks from ai we identify five highlevel design choices 1 what responsibilities should the board have 2 what should its legal structure be 3 who should sit on the board 4 how should it make decisions and should its decisions be binding 5 what resources does it need we break down each of these questions into more specific subquestions list options and discuss how different design choices affect the boards ability to reduce risks from ai several failures have shown that designing an ai ethics board can be challenging this paper provides a toolbox that can help ai companies to overcome these challenges,0
aibased systems including large language models llm impact millions by supporting diverse tasks but face issues like misinformation bias and misuse ai ethics is crucial as new technologies and concerns emerge but objective practical guidance remains debated this study examines the use of llms for ai ethics in practice assessing how llm trustworthinessenhancing techniques affect software development in this context using the design science research dsr method we identify techniques for llm trustworthiness multiagents distinct roles structured communication and multiple rounds of debate we design a multiagent prototype llmmas where agents engage in structured discussions on realworld ai ethics issues from the ai incident database we evaluate the prototype across three case scenarios using thematic analysis hierarchical clustering comparative baseline studies and running source code the system generates approximately 2000 lines of code per case compared to only 80 lines in baseline trials discussions reveal terms like bias detection transparency accountability user consent gdpr compliance fairness evaluation and eu ai act compliance showing this prototype ability to generate extensive source code and documentation addressing often overlooked ai ethics issues however practical challenges in source code integration and dependency management may limit its use by practitioners,0
artificial intelligence is rapidly embedding itself within militaries economies and societies reshaping their very foundations given the depth and breadth of its consequences it has never been more pressing to understand how to ensure that ai systems are safe ethical and have a positive societal impact this book aims to provide a comprehensive approach to understanding ai risk our primary goals include consolidating fragmented knowledge on ai risk increasing the precision of core ideas and reducing barriers to entry by making content simpler and more comprehensible the book has been designed to be accessible to readers from diverse backgrounds you do not need to have studied ai philosophy or other such topics the content is skimmable and somewhat modular so that you can choose which chapters to read we introduce mathematical formulas in a few places to specify claims more precisely but readers should be able to understand the main points without these,0
recent advancements in artificial intelligence ai systems including large language models like chatgpt offer promise and peril for scholarly peer review on the one hand ai can enhance efficiency by addressing issues like long publication delays on the other hand it brings ethical and social concerns that could compromise the integrity of the peer review process and outcomes however human peer review systems are also fraught with related problems such as biases abuses and a lack of transparency which already diminish credibility while there is increasing attention to the use of ai in peer review discussions revolve mainly around plagiarism and authorship in academic journal publishing ignoring the broader epistemic social cultural and societal epistemic in which peer review is positioned the legitimacy of aidriven peer review hinges on the alignment with the scientific ethos encompassing moral and epistemic norms that define appropriate conduct in the scholarly community in this regard there is a normcounternorm continuum where the acceptability of ai in peer review is shaped by institutional logics ethical practices and internal regulatory mechanisms the discussion here emphasizes the need to critically assess the legitimacy of aidriven peer review addressing the benefits and downsides relative to the broader epistemic social ethical and regulatory factors that sculpt its implementation and impact,0
the eus artificial intelligence act ai act is a significant step towards responsible ai development but lacks clear technical interpretation making it difficult to assess models compliance this work presents complai a comprehensive framework consisting of i the first technical interpretation of the eu ai act translating its broad regulatory requirements into measurable technical requirements with the focus on large language models llms and ii an opensource actcentered benchmarking suite based on thorough surveying and implementation of stateoftheart llm benchmarks by evaluating 12 prominent llms in the context of complai we reveal shortcomings in existing models and benchmarks particularly in areas like robustness safety diversity and fairness this work highlights the need for a shift in focus towards these aspects encouraging balanced development of llms and more comprehensive regulationaligned benchmarks simultaneously complai for the first time demonstrates the possibilities and difficulties of bringing the acts obligations to a more concrete technical level as such our work can serve as a useful first step towards having actionable recommendations for model providers and contributes to ongoing efforts of the eu to enable application of the act such as the drafting of the gpai code of practice,0
companies report on their financial performance for decades more recently they have also started to report on their environmental impact and their social responsibility the latest trend is now to deliver one single integrated report where all stakeholders of the company can easily connect all facets of the business with their impact considered in a broad sense the main purpose of this integrated approach is to avoid delivering data related to disconnected silos which consequently makes it very difficult to globally assess the overall performance of an entity or a business line in this paper we focus on how companies report on risks and ethical issues related to the increasing use of artificial intelligence ai we explain some of these risks and potential issues next we identify some recent initiatives by various stakeholders to define a global ethical framework for ai finally we illustrate with four cases that companies are very shy to report on these facets of ai,0
in the current era people and society have grown increasingly reliant on artificial intelligence ai technologies ai has the potential to drive us towards a future in which all of humanity flourishes it also comes with substantial risks for oppression and calamity discussions about whether we should retrust ai have repeatedly emerged in recent years and in many quarters including industry academia healthcare services and so on technologists and ai researchers have a responsibility to develop trustworthy ai systems they have responded with great effort to design more responsible ai algorithms however existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks with an emphasis on fairness and unwanted bias to build longlasting trust between ai and human beings we argue that the key is to think beyond algorithmic fairness and connect major aspects of ai that potentially cause ais indifferent behavior in this survey we provide a systematic framework of socially responsible ai algorithms that aims to examine the subjects of ai indifference and the need for socially responsible ai algorithms define the objectives and introduce the means by which we may achieve these objectives we further discuss how to leverage this framework to improve societal wellbeing through protection information and preventionmitigation,0
it is commonly accepted that clinicians are ethically obligated to disclose their use of medical machine learning systems to patients and that failure to do so would amount to a moral fault for which clinicians ought to be held accountable call this the disclosure thesis four main arguments have been or could be given to support the disclosure thesis in the ethics literature the riskbased argument the rightsbased argument the materiality argument and the autonomy argument in this article i argue that each of these four arguments are unconvincing and therefore that the disclosure thesis ought to be rejected i suggest that mandating disclosure may also even risk harming patients by providing stakeholders with a way to avoid accountability for harm that results from improper applications or uses of these systems,0
the eu artificial intelligence act aia establishes different legal principles for different types of ai systems while prior work has sought to clarify some of these principles little attention has been paid to robustness and cybersecurity this paper aims to fill this gap we identify legal challenges and shortcomings in provisions related to robustness and cybersecurity for highrisk ai systemsart 15 aia and generalpurpose ai models art 55 aia we show that robustness and cybersecurity demand resilience against performance disruptions furthermore we assess potential challenges in implementing these provisions in light of recent advancements in the machine learning ml literature our analysis informs efforts to develop harmonized standards guidelines by the european commission as well as benchmarks and measurement methodologies under art 152 aia with this we seek to bridge the gap between legal terminology and ml research fostering a better alignment between research and implementation efforts,0
there is a struggle in artificial intelligence ai ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound ai systems ai ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations efforts to create tools and guidelines to help people working with ai development have been concentrating almost solely on the technical aspects of ai a few exceptions do apply such as the eccola method for creating ethically aligned ai systems eccola has proven results in terms of increased ethical considerations in ai systems development yet it is a novel innovation and room for development still exists this study aims to extend eccola with a deployment model to drive the adoption of eccola as any method no matter how good is of no value without adoption and use the model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical ai development it offers the opportunity to assess any ai system at any given lifecycle phase eg opening possibilities like analyzing the ethicality of an ai system under acquisition,0
political biases in large language model llmbased artificial intelligence ai systems such as openais chatgpt or googles gemini have been previously reported while several prior studies have attempted to quantify these biases using political orientation tests such approaches are limited by potential tests calibration biases and constrained response formats that do not reflect realworld humanai interactions this study employs a multimethod approach to assess political bias in leading ai systems integrating four complementary methodologies 1 linguistic comparison of aigenerated text with the language used by republican and democratic us congress members 2 analysis of political viewpoints embedded in aigenerated policy recommendations 3 sentiment analysis of aigenerated text toward politically affiliated public figures and 4 standardized political orientation testing results indicate a consistent leftleaning bias across most contemporary ai systems with arguably varying degrees of intensity however this bias is not an inherent feature of llms prior research demonstrates that finetuning with politically skewed data can realign these models across the ideological spectrum the presence of systematic political bias in ai systems poses risks including reduced viewpoint diversity increased societal polarization and the potential for public mistrust in ai technologies to mitigate these risks ai systems should be designed to prioritize factual accuracy while maintaining neutrality on most lawful normative issues furthermore independent monitoring platforms are necessary to ensure transparency accountability and responsible ai development,0
the integration of artificial intelligence ai in medical imaging raises crucial ethical concerns at every stage of its development from data collection to deployment addressing these concerns is essential for ensuring that ai systems are developed and implemented in a manner that respects patient rights and promotes fairness this study aims to explore the ethical implications of ai in medical imaging focusing on five key stages data collection data processing model training model evaluation and deployment the goal is to evaluate how these stages adhere to fundamental ethical principles including data privacy fairness transparency accountability and autonomy an analytical approach was employed to examine the ethical challenges associated with each stage of ai development we reviewed existing literature guidelines and regulations concerning ai ethics in healthcare and identified critical ethical issues at each stage the study outlines specific inquiries and principles for each phase of ai development the findings highlight key ethical issues ensuring patient consent and anonymization during data collection addressing biases in model training ensuring transparency and fairness during model evaluation and the importance of continuous ethical assessments during deployment the analysis also emphasizes the impact of accessibility issues on different stakeholders including private public and thirdparty entities the study concludes that ethical considerations must be systematically integrated into each stage of ai development in medical imaging by adhering to these ethical principles ai systems can be made more robust transparent and aligned with patient care and data control we propose tailored ethical inquiries and strategies to support the creation of ethically sound ai systems in medical imaging,0
the impact of artificial intelligence does not depend only on fundamental research and technological developments but for a large part on how these systems are introduced into society and used in everyday situations even though ai is traditionally associated with rational decision making understanding and shaping the societal impact of ai in all its facets requires a relational perspective a rational approach to ai where computational algorithms drive decision making independent of human intervention insights and emotions has shown to result in bias and exclusion laying bare societal vulnerabilities and insecurities a relational approach that focus on the relational nature of things is needed to deal with the ethical legal societal cultural and environmental implications of ai a relational approach to ai recognises that objective and rational reasoning cannot does not always result in the right way to proceed because what is right depends on the dynamics of the situation in which the decision is taken and that rather than solving ethical problems the focus of design and use of ai must be on asking the ethical question in this position paper i start with a general discussion of current conceptualisations of ai followed by an overview of existing approaches to governance and responsible development and use of ai then i reflect over what should be the bases of a social paradigm for ai and how this should be embedded in relational feminist and nonwestern philosophies in particular the ubuntu philosophy,0
building trustworthy autonomous systems is challenging for many reasons beyond simply trying to engineer agents that always do the right thing there is a broader context that is often not considered within ai and hri that the problem of trustworthiness is inherently sociotechnical and ultimately involves a broad set of complex human factors and multidimensional relationships that can arise between agents humans organizations and even governments and legal institutions each with their own understanding and definitions of trust this complexity presents a significant barrier to the development of trustworthy ai and hri systemswhile systems developers may desire to have their systems always do the right thing they generally lack the practical tools and expertise in law regulation policy and ethics to ensure this outcome in this paper we emphasize the fuzzy sociotechnical aspects of trustworthiness and the need for their careful consideration during both design and deployment we hope to contribute to the discussion of trustworthy engineering in ai and hri by i describing the policy landscape that must be considered when addressing trustworthy computing and the need for usable trust models ii highlighting an opportunity for trustworthybydesign intervention within the systems engineering process and iii introducing the concept of a policyasaservice paas framework that can be readily applied by ai systems engineers to address the fuzzy problem of trust during the development and eventually runtime process we envision that the paas approach which offloads the development of policy design parameters and maintenance of policy standards to policy experts will enable runtime trust capabilities intelligent systems in the wild,0
the societal and ethical implications of the use of opaque artificial intelligence systems for consequential decisions such as welfare allocation and criminal justice have generated a lively debate among multiple stakeholder groups including computer scientists ethicists social scientists policy makers and end users however the lack of a common language or a multidimensional framework to appropriately bridge the technical epistemic and normative aspects of this debate prevents the discussion from being as productive as it could be drawing on the philosophical literature on the nature and value of explanations this paper offers a multifaceted framework that brings more conceptual precision to the present debate by 1 identifying the types of explanations that are most pertinent to artificial intelligence predictions 2 recognizing the relevance and importance of social and ethical values for the evaluation of these explanations and 3 demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems the proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems,0
artificial intelligence ai is rapidly transforming power electronics with airelated publications in ieee power electronics society selected journals increasing more than fourfold from 2020 to 2025 however the ethical dimensions of this transformation have received limited attention this article underscores the urgent need for an ethical framework to guide responsible ai integration in power electronics not only to prevent airelated incidents but also to comply with legal and regulatory responsibilities in this context this article identifies four core pillars of ai ethics in power electronics security safety explainability transparency energy sustainability and evolving roles of engineers each pillar is supported by practical and actionable insights to ensure that ethical principles are embedded in algorithm design system deployment and workforce development the authors advocate for power electronics engineers to lead the ethical discourse given their deep technical understanding of both ai systems and power conversion technologies the paper concludes by calling on the ieee power electronics society to spearhead the establishment of ethical standards and best practices that ensure ai innovations are not only technically advanced but also trustworthy safe and sustainable,0
artificial intelligence risks are multidimensional in nature as the same risk scenarios may have legal operational and financial risk dimensions with the emergence of new ai regulations the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming ai regulations despite the appearance of several methodologies and generic criteria it is rare to find guidelines with real implementation value considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific ai risk scenarios furthermore the financial departments legal departments and government risk compliance teams seem to remain unaware of many technical aspects of ai systems in which data scientists and ai engineers emerge as the most appropriate implementers it is crucial to decompose the problem of artificial intelligence risk in several dimensions data protection fairness accuracy robustness and information security consequently the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decisionmaking in order to take informed decisions concerning the risk management of ai systems the purpose of this paper is to orientate ai stakeholders about the depths of ai risk management although it is not extremely technical it requires a basic knowledge of risk management quantifying uncertainty the fair model machine learning large language models and ai context engineering the examples presented pretend to be very basic and understandable providing simple ideas that can be developed regarding specific ai customized environments there are many issues to solve in ai risk management and this paper will present a holistic overview of the interdependencies of ai risks and how to model them together within risk scenarios,0
problem statement standardisation of ai fairness rules and benchmarks is challenging because ai fairness and other ethical requirements depend on multiple factors such as context use case type of the ai system and so on in this paper we elaborate that the ai system is prone to biases at every stage of its lifecycle from inception to its usage and that all stages require due attention for mitigating ai bias we need a standardised approach to handle ai fairness at every stage gap analysis while ai fairness is a hot research topic a holistic strategy for ai fairness is generally missing most researchers focus only on a few facets of ai modelbuilding peer review shows excessive focus on biases in the datasets fairness metrics and algorithmic bias in the process other aspects affecting ai fairness get ignored the solution proposed we propose a comprehensive approach in the form of a novel sevenlayer model inspired by the open system interconnection osi model to standardise ai fairness handling despite the differences in the various aspects most ai systems have similar modelbuilding stages the proposed model splits the ai system lifecycle into seven abstraction layers each corresponding to a welldefined ai modelbuilding or usage stage we also provide checklists for each layer and deliberate on potential sources of bias in each layer and their mitigation methodologies this work will facilitate layerwise standardisation of ai fairness rules and benchmarking parameters,0
deep learning based visuallinguistic multimodal models such as contrastive language image pretraining clip have become increasingly popular recently and are used within texttoimage generative models such as dalle and stable diffusion however gender and other social biases have been uncovered in these models and this has the potential to be amplified and perpetuated through ai systems in this paper we present a methodology for auditing multimodal models that consider gender informed by concepts from transnational feminism including regional and cultural dimensions focusing on clip we found evidence of significant gender bias with varying patterns across global regions harmful stereotypical associations were also uncovered related to visual cultural cues and labels such as terrorism levels of gender bias uncovered within clip for different regions aligned with global indices of societal gender equality with those from the global south reflecting the highest levels of gender bias,0
in february 2020 the european commission ec published a white paper entitled on artificial intelligence a european approach to excellence and trust this paper outlines the ecs policy options for the promotion and adoption of artificial intelligence ai in the european union the montreal ai ethics institute maiei reviewed this paper and published a response addressing the ecs plans to build an ecosystem of excellence and an ecosystem of trust as well as the safety and liability implications of ai the internet of things iot and robotics maiei provides 15 recommendations in relation to the sections outlined above including 1 focus efforts on the research and innovation community member states and the private sector 2 create alignment between trading partners policies and eu policies 3 analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy ai 4 focus on coordination and policy alignment 5 focus on mechanisms that promote private and secure sharing of data 6 create a network of ai research excellence centres to strengthen the research and innovation community 7 promote knowledge transfer and develop ai expertise through digital innovation hubs 8 add nuance to the discussion regarding the opacity of ai systems 9 create a process for individuals to appeal an ai systems decision or output 10 implement new rules and strengthen existing regulations 11 ban the use of facial recognition technology 12 hold all ai systems to similar standards and compulsory requirements 13 ensure biometric identification systems fulfill the purpose for which they are implemented 14 implement a voluntary labelling system for systems that are not considered highrisk 15 appoint individuals to the oversight process who understand ai systems well and are able to communicate potential risks,0
with increasing digitalization artificial intelligence ai is becoming ubiquitous aibased systems to identify optimize automate and scale solutions to complex economic and societal problems are being proposed and implemented this has motivated regulation efforts including the proposal of an eu ai act this interdisciplinary position paper considers various concerns surrounding fairness and discrimination in ai and discusses how ai regulations address them focusing on but not limited to the proposal we first look at ai and fairness through the lenses of law ai industry sociotechnology and moral philosophy and present various perspectives then we map these perspectives along three axes of interests i standardization vs localization ii utilitarianism vs egalitarianism and iii consequential vs deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes positioning the discussion within the axes of interest and with a focus on reconciling the key tensions we identify and propose the roles ai regulation should take to make the endeavor of the ai act a success in terms of ai fairness concerns,0
as artificial intelligence ai technologies spread worldwide international discussions have increasingly focused on their consequences for democracy human rights fundamental freedoms security and economic and social development in this context unescos recommendation on the ethics of artificial intelligence adopted in november 2021 has emerged as the first global normative framework for ai development and deployment the intense negotiations of every detail of the document brought forth numerous controversies among unesco member states drawing on a unique set of primary sources including written positions and recorded deliberations this paper explains the achievement of global compromise on ai regulation despite the multiplicity of unesco memberstate positions representing a variety of liberal and sovereignist preferences building upon boltanskis pragmatic sociology it conceptualises the practice of multilateral negotiations and attributes the multilateral compromise to two embedded therein mechanisms structural normative hybridity and situated normative ambiguity allowed to accomplish a compromise by linking macronormative structures with situated debates of multilateral negotiations,0
in recent years research involving human participants has been critical to advances in artificial intelligence ai and machine learning ml particularly in the areas of conversational humancompatible and cooperative ai for example roughly 9 of publications at recent aaai and neurips conferences indicate the collection of original human data yet ai and ml researchers lack guidelines for ethical research practices with human participants fewer than one out of every four of these aaai and neurips papers confirm independent ethical review the collection of informed consent or participant compensation this paper aims to bridge this gap by examining the normative similarities and differences between ai research and related fields that involve human participants though psychology humancomputer interaction and other adjacent fields offer historic lessons and helpful insights ai research presents several distinct considerationsunicodex2014namely participatory design crowdsourced dataset development and an expansive role of corporationsunicodex2014that necessitate a contextual ethics framework to address these concerns this manuscript outlines a set of guidelines for ethical and transparent practice with human participants in ai and ml research overall this paper seeks to equip technical researchers with practical knowledge for their work and to position them for further dialogue with social scientists behavioral researchers and ethicists,0
within the current ai ethics discourse there is a gap in empirical research on understanding how ai practitioners understand ethics and socially organize to operationalize ethical concerns particularly in the context of ai startups this gap intensifies the risk of a disconnect between scholarly research innovation and application this risk materializes acutely as mounting pressures to identify and mitigate the potential harms of ai systems have created an urgent need to assess and implement sociotechnical innovation for fairness accountability and transparency building on social practice theory we address this need via a framework that allows ai researchers practitioners and regulators to systematically analyze existing cultural understandings histories and social practices of ethical ai to define appropriate strategies for effectively implementing sociotechnical innovations our contributions are threefold 1 we introduce a practicebased approach for understanding ethical ai 2 we present empirical findings from our study on the operationalization of ethics in german ai startups to underline that ai ethics and social practices must be understood in their specific cultural and historical contexts and 3 based on our empirical findings we suggest that ethical ai practices can be broken down into principles needs narratives materializations and cultural genealogies to form a useful backdrop for considering sociotechnical innovations,0
as artificial intelligence ai systems exert a growing influence on society reallife incidents begin to underline the importance of ai ethics though calls for more ethical ai systems have been voiced by scholars and the general public alike few empirical studies on the topic exist similarly few tools and methods designed for implementing ai ethics into practice currently exist to provide empirical data into this ongoing discussion we empirically evaluate an existing method from the field of business ethics the resolvedd strategy in the context of ethical system development we evaluated resolvedd by means of a multiple case study of five student projects where its use was given as one of the design requirements for the projects one of our key findings is that even though the use of the ethical method was forced upon the participants its utilization nonetheless facilitated of ethical consideration in the projects specifically it resulted in the developers displaying more responsibility even though the use of the tool did not stem from intrinsic motivation,0
in 2016 a network of social media accounts animated by russian operatives attempted to divert political discourse within the american public around the presidential elections this was a coordinated effort part of a russianled complex information operation utilizing the anonymity and outreach of social media platforms russian operatives created an online astroturf that is in direct contact with regular americans promoting russian agenda and goals the elusiveness of this type of adversarial approach rendered security agencies helpless stressing the unique challenges this type of intervention presents building on existing scholarship on the functions within influence networks on social media we suggest a new approach to map those types of operations we argue that pretending to be legitimate social actors obliges the network to adhere to social expectations leaving a social footprint to test the robustness of this social footprint we train artificial intelligence to identify it and create a predictive model we use twitter data identified as part of the russian influence network for training the artificial intelligence and to test the prediction our model attains 88 prediction accuracy for the test set testing our prediction on two additional models results in 907 and 905 accuracy validating our model the predictive and validation results suggest that building a machine learning model around social functions within the russian influence network can be used to map its actors and functions,0
coordinated online behaviors are an essential part of information and influence operations as they allow a more effective disinformations spread most studies on coordinated behaviors involved manual investigations and the few existing computational approaches make bold assumptions or oversimplify the problem to make it tractable here we propose a new networkbased framework for uncovering and studying coordinated behaviors on social media our research extends existing systems and goes beyond limiting binary classifications of coordinated and uncoordinated behaviors it allows to expose different coordination patterns and to estimate the degree of coordination that characterizes diverse communities we apply our framework to a dataset collected during the 2019 uk general election detecting and characterizing coordinated communities that participated in the electoral debate our work conveys both theoretical and practical implications and provides more nuanced and finegrained results for studying online information manipulation,0
the rise of social media has fundamentally transformed how people engage in public discourse and form opinions while these platforms offer unprecedented opportunities for democratic engagement they have been implicated in increasing social polarization and the formation of ideological echo chambers previous research has primarily relied on observational studies of social media data or theoretical modeling approaches leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with llmbased artificial agents in a controlled social network simulation through a user study with 122 participants we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors our results provide empirical validation of theoretical predictions about online polarization showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty these findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors more broadly this research introduces a powerful new methodology for studying social media dynamics offering researchers unprecedented control over experimental conditions while maintaining ecological validity,0
decentralized online social networks dosns represent a growing trend in the social media landscape as opposed to the wellknown centralized peers which are often in the spotlight due to privacy concerns and a vision typically focused on monetization through user relationships by exploiting opensource software dosns allow users to create their own servers or instances thus favoring the proliferation of platforms that are independent yet interconnected with each other in a transparent way nonetheless the resulting cooperation model commonly known as the fediverse still represents a world to be fully discovered since existing studies have mainly focused on a limited number of structural aspects of interest in dosns in this work we aim to fill a lack of study on user relations and roles in dosns by taking two main actions understanding the impact of decentralization on how users relate to each other within their membership instance andor across different instances and unveiling user roles that can explain two interrelated axes of social behavioral phenomena namely information consumption and boundary spanning to this purpose we build our analysis on user networks from mastodon since it represents the most widely used dosn platform we believe that the findings drawn from our study on mastodon users roles and information flow can pave a way for further development of fascinating research on dosns,0
a better understanding of the behavior of tourists is strategic for improving services in the competitive and important economic segment of global tourism critical studies in the literature often explore the issue using traditional data such as questionnaires or interviews traditional approaches provide precious information however they impose challenges to obtaining largescale data making it hard to study worldwide patterns locationbased social networks lbsns can potentially mitigate such issues due to the relatively low cost of acquiring large amounts of behavioral data nevertheless before using such data for studying tourists behavior it is necessary to verify whether the information adequately reveals the behavior measured with traditional data considered the ground truth thus the present work investigates in which countries the global tourism network measured with an lbsn agreeably reflects the behavior estimated by the world tourism organization using traditional methods although we could find exceptions the results suggest that for most countries lbsn data can satisfactorily represent the behavior studied we have an indication that in countries with high correlations between results obtained from both datasets lbsn data can be used in research regarding the mobility of the tourists in the studied context,0
the increasing pervasiveness of social media creates new opportunities to study human social behavior while challenging our capability to analyze their massive data streams one of the emerging tasks is to distinguish between different kinds of activities for example engineered misinformation campaigns versus spontaneous communication such detection problems require a formal definition of meme or unit of information that can spread from person to person through the social network once a meme is identified supervised learning methods can be applied to classify different types of communication the appropriate granularity of a meme however is hardly captured from existing entities such as tags and keywords here we present a framework for the novel task of detecting memes by clustering messages from large streams of social data we evaluate various similarity measures that leverage content metadata network features and their combinations we also explore the idea of preclustering on the basis of existing entities a systematic evaluation is carried out using a manually curated dataset as ground truth our analysis shows that preclustering and a combination of heterogeneous features yield the best tradeoff between number of clusters and their quality demonstrating that a simple combination based on pairwise maximization of similarity is as effective as a nontrivial optimization of parameters our approach is fully automatic unsupervised and scalable for realtime detection of memes in streaming data,0
identifying influencers in a given social network has become an important research problem for various applications including accelerating the spread of information in viral marketing and preventing the spread of fake news and rumors the literature contains a rich body of studies on identifying influential source spreaders who can spread their own messages to many other nodes in contrast the identification of influential brokers who can spread other nodes messages to many nodes has not been fully explored theoretical and empirical studies suggest that involvement of both influential source spreaders and brokers is a key to facilitating largescale information diffusion cascades therefore this paper explores ways to identify influential brokers from a given social network by using three social media datasets we investigate the characteristics of influential brokers by comparing them with influential source spreaders and central nodes obtained from centrality measures our results show that i most of the influential source spreaders are not influential brokers and vice versa and ii the overlap between central nodes and influential brokers is small less than 15 in twitter datasets we also tackle the problem of identifying influential brokers from centrality measures and node embeddings and we examine the effectiveness of social network features in the broker identification task our results show that iii although a single centrality measure cannot characterize influential brokers well prediction models using node embedding features achieve f1 scores of 035068 suggesting the effectiveness of social network features for identifying influential brokers,0
social media are extensively used in todays world and facilitate quick and easy sharing of information which makes them a good way to advertise products influencers of a social media network owing to their massive popularity provide a huge potential customer base however it is not straightforward to decide which influencers should be selected for an advertizing campaign that can generate high returns with low investment in this work we present an agentbased model abm that can simulate the dynamics of influencer advertizing campaigns in a variety of scenarios and can help to discover the best influencer marketing strategy our system is a probabilistic graphbased model that provides the additional advantage to incorporate realworld factors such as customers interest in a product customer behavior the willingness to pay a brands investment cap influencers engagement with influence diffusion and the nature of the product being advertized viz luxury and nonluxury using customer acquisition cost and conversion ratio as a unit economic we evaluate the performance of different kinds of influencers under a variety of circumstances that are simulated by varying the nature of the product and the customers interest our results exemplify the circumstancedependent nature of influencer marketing and provide insight into which kinds of influencers would be a better strategy under respective circumstances for instance we show that as the nature of the product varies from luxury to nonluxury the performance of celebrities declines whereas the performance of nanoinfluencers improves in terms of the customers interest we find that the performance of nanoinfluencers declines with the decrease in customers interest whereas the performance of celebrities improves,0
bots are for many web and social media users the source of many dangerous attacks or the carrier of unwanted messages such as spam nevertheless crawlers and software agents are a precious tool for analysts and they are continuously executed to collect data or to test distributed applications however no one knows which is the real potential of a bot whose purpose is to control a community to manipulate consensus or to influence user behavior it is commonly believed that the better an agent simulates human behavior in a social network the more it can succeed to generate an impact in that community we contribute to shed light on this issue through an online social experiment aimed to study to what extent a bot with no trust no profile and no aims to reproduce human behavior can become popular and influential in a social media results show that a basic social probing activity can be used to acquire social relevance on the network and that the soacquired popularity can be effectively leveraged to drive users in their social connectivity choices we also register that our bot activity unveiled hidden social polarization patterns in the community and triggered an emotional response of individuals that brings to light subtle privacy hazards perceived by the user base,0
in this paper we address the challenge of discovering hidden nodes in unknown social networks formulating three types of hiddennode discovery problems namely sybilnode discovery peripheralnode discovery and influencer discovery we tackle these problems by employing a graph exploration framework grounded in machine learning leveraging the structure of the subgraph gradually obtained from graph exploration we construct prediction models to identify target hidden nodes in unknown social graphs through empirical investigations of real social graphs we investigate the efficiency of graph exploration strategies in uncovering hidden nodes our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known specifically the query cost of discovering 10 of the hidden nodes is at most only 12 times that when the topology is known and the querycost multiplier for discovering 90 of the hidden nodes is at most only 14 furthermore our results suggest that using node embeddings which are lowdimensional vector representations of nodes for hiddennode discovery is a doubleedged sword it is effective in certain scenarios but sometimes degrades the efficiency of node discovery guided by this observation we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not and our analysis shows that the banditbased graph exploration strategy achieves efficient node discovery across a wide array of settings,0
largescale databases of human activity in social media have captured scientific and policy attention producing a flood of research and discussion this paper considers methodological and conceptual challenges for this emergent field with special attention to the validity and representativeness of social media big data analyses persistent issues include the overemphasis of a single platform twitter sampling biases arising from selection by hashtags and vague and unrepresentative sampling frames the sociocultural complexity of user behavior aimed at algorithmic invisibility such as subtweeting mockretweeting use of screen captures for text etc further complicate interpretation of big data social media other challenges include accounting for field effects ie broadly consequential events that do not diffuse only through the network under study but affect the whole society the application of network methods from other fields to the study of human social activity may not always be appropriate the paper concludes with a call to action on practical steps to improve our analytic capacity in this promising rapidlygrowing field,0
we develop an agentbased model in order to understand agentnode behaviors that generate social media networks we use simple rules to synthetically generate a backcloth friendfollow network collected using twitters api the twitter network was collected using seeds for known terrorist propaganda accounts in 2015 model parameter adjustments were made to reproduce the collected networks summary statistics stylized facts and general structural measures we produced an approximate network in line with the general properties of our collected data we present our findings with a focus on the challenging aspects of this reproduction we find that while it is possible to generate a social media network utilizing a few simple rules numerous challenges arise requiring departure from the agent viewpoint and the development of more useful methods we present numerous weaknesses and challenges in our reproduction and propose potential solutions for future efforts,0
social media platforms are extensively used for sharing personal emotions daily activities and various life events keeping people updated with the latest happenings from the moment a user creates an account they continually expand their network of friends or followers freely interacting with others by posting commenting and sharing content over time user behavior evolves based on demographic attributes and the networks they establish in this research we propose a predictive method to understand how a user evolves on social media throughout their life and to forecast the next stage of their evolution we finetune a gptlike decoderonly model we named it egpt evolutiongpt to predict the future stages of a users evolution in online social media we evaluate the performance of these models and demonstrate how user attributes influence changes within their network by predicting future connections and shifts in user activities on social media which also addresses other social media challenges such as recommendation systems,0
cyberaggression has been studied in various contexts and online social platforms and modeled on different data using stateoftheart machine and deep learning algorithms to enable automatic detection and blocking of this behavior users can be influenced to act aggressively or even bully others because of elevated toxicity and aggression in their own online social circle in effect this behavior can propagate from one user and neighborhood to another and therefore spread in the network interestingly to our knowledge no work has modeled the network dynamics of aggressive behavior in this paper we take a first step towards this direction by studying propagation of aggression on social media using opinion dynamics we propose ways to model how aggression may propagate from one user to another depending on how each user is connected to other aggressive or regular users through extensive simulations on twitter data we study how aggressive behavior could propagate in the network we validate our models with crawled and annotated ground truth data reaching up to 80 auc and discuss the results and implications of our work,0
information propagation in social media depends not only on the static follower structure but also on the topicspecific user behavior hence novel models incorporating dynamic user behavior are needed to this end we propose a model for individual social media users termed a genotype the genotype is a pertopic summary of a users interest activity and susceptibility to adopt new information we demonstrate that user genotypes remain invariant within a topic by adopting them for classification of new information spread in largescale real networks furthermore we extract topicspecific influence backbone structures based on information adoption and show that they differ significantly from the static follower network when employed for influence prediction of new content spread our genotype model and influence backbones enable more than 20 improvement compared to purely structural features we also demonstrate that knowledge of user genotypes and influence backbones allow for the design of effective strategies for latency minimization of topicspecific information spread,0
while social media make it easy to connect with and access information from anyone they also facilitate basic influence and unfriending mechanisms that may lead to segregated and polarized clusters known as echo chambers here we study the conditions in which such echo chambers emerge by introducing a simple model of information sharing in online social networks with the two ingredients of influence and unfriending users can change both their opinions and social connections based on the information to which they are exposed through sharing the model dynamics show that even with minimal amounts of influence and unfriending the social network rapidly devolves into segregated homogeneous communities these predictions are consistent with empirical data from twitter although our findings suggest that echo chambers are somewhat inevitable given the mechanisms at play in online social media they also provide insights into possible mitigation strategies,0
bots have been in the spotlight for many social media studies for they have been observed to be participating in the manipulation of information and opinions on social media these studies analyzed the activity and influence of bots in a variety of contexts elections protests health communication and so forth prior to this analyses is the identification of bot accounts to segregate the class of social media users in this work we propose an ensemble method for bot detection designing a multiplatform bot detection architecture to handle several problems along the bot detection pipeline incomplete data input minimal feature engineering optimized classifiers for each data field and also eliminate the need for a threshold value for classification determination with these design decisions we generalize our bot detection framework across twitter reddit and instagram we also perform feature importance analysis observing that the entropy of names and number of interactions retweetsshares are important factors in bot determination finally we apply our multiplatform bot detector to the us 2020 presidential elections to identify and analyze bot activity across multiple social media platforms showcasing the difference in online discourse of bots from different platforms,0
pathogenic social media accounts such as terrorist supporters exploit communities of supporters for conducting attacks on social media early detection of psm accounts is crucial as they are likely to be key users in making a harmful message viral this paper overviews my recent doctoral work on utilizing causal inference to identify psm accounts within a short time frame around their activity the proposed scheme 1 assigns timedecay causality scores to users 2 applies a community detectionbased algorithm to group of users sharing similar causality scores and finally 3 deploys a classification algorithm to classify accounts unlike existing techniques that require network structure cascade path or content our scheme relies solely on action log of users,0
coordinated campaigns are used to influence and manipulate social media platforms and their users a critical challenge to the free exchange of information online here we introduce a general unsupervised networkbased methodology to uncover groups of accounts that are likely coordinated the proposed method constructs coordination networks based on arbitrary behavioral traces shared among accounts we present five case studies of influence campaigns four of which in the diverse contexts of us elections hong kong protests the syrian civil war and cryptocurrency manipulation in each of these cases we detect networks of coordinated twitter accounts by examining their identities images hashtag sequences retweets or temporal patterns the proposed approach proves to be broadly applicable to uncover different kinds of coordination across information warfare scenarios,0
the outbreak of covid19 had a huge global impact and nonscientific beliefs and political polarization have significantly influenced the populations behavior in this context covid vaccines were made available in an unprecedented time but a high level of hesitance has been observed that can undermine community immunization traditionally antivaccination attitudes are more related to conspiratorial thinking rather than political bias in brazil a country with an exemplar tradition in largescale vaccination programs all covidrelated topics have also been discussed under a strong political bias in this paper we use a multidimensional analysis framework to understand if antiprovaccination stances expressed by brazilians in social media are influenced by political polarization the analysis framework incorporates techniques to automatically infer from users their political orientation topic modeling to discover their concerns network analysis to characterize their social behavior and the characterization of information sources and external influence our main findings confirm that antipro stances are biased by political polarization right and left respectively while a significant proportion of provaxxers display haste for an immunization program and criticize the governments actions the antivaxxers distrust a vaccine developed in a record time antivaccination stance is also related to prejudice against china antisinovaxxers revealing conspiratorial theories related to communism all groups display an echo chamber behavior revealing they are not open to distinct views,0
in 2015 391000 people were injured due to distracted driving in the us one of the major reasons behind distracted driving is the use of cellphones accounting for 14 of fatal crashes social media applications have enabled users to stay connected however the use of such applications while driving could have serious repercussions often leading the user to be distracted from the road and ending up in an accident in the context of impression management it has been discovered that individuals often take a risk such as teens smoking cigarettes indulging in narcotics and participating in unsafe sex to improve their social standing therefore viewing the phenomena of posting distracted driving posts under the lens of selfpresentation it can be hypothesized that users often indulge in risktaking behavior on social media to improve their impression among their peers in this paper we first try to understand the severity of such socialmediabased distractions by analyzing the content posted on a popular social media site where the user is driving and is also simultaneously creating content to this end we build a deep learning classifier to identify publicly posted content on social media that involves the user driving furthermore a framework proposed to understand factors behind voluntary risktaking activity observes that younger individuals are more willing to perform such activities and men as opposed to women are more inclined to take risks grounding our observations in this framework we test these hypotheses on 173 cities across the world we conduct spatial and temporal analysis on a citylevel and understand how distracted driving content posting behavior changes due to varied demographics we discover that the factors put forth by the framework are significant in estimating the extent of such behavior,0
this paper presents an analysis of the role of social media specifically twitter in the context of nonfungible tokens better known as nfts such emerging technology framing the creation and exchange of digital object started years ago with early projects such as cryptopunks and since early 2021 has received an increasing interest by a community of people creating buying selling nfts and by the media reporting to the general public in this work it is shown how the landscape of one class of projects specifically those used as social media profile pictures has become mainstream with leading projects such as bored ape yacht club cool cats and doodles this work illustrates how heterogeneous data was collected from the ethereum blockchain and twitter and then analysed using algorithms and stateofart metrics related to graphs the initial results show that from a social network perspective the collections of most popular nfts can be considered as a single community around nfts thus while each project has its own value and volume of exchange on a social level all of them are primarily influenced by the evolution of values and trades of bored ape yacht club collection,0
echo chambers may exclude social media users from being exposed to other opinions therefore can cause rampant negative effects among abundant evidence are the 2016 and 2020 us presidential elections conspiracy theories and polarization as well as the covid19 disinfodemic to help better detect echo chambers and mitigate its negative effects this paper explores the mechanisms and attributes of echo chambers in social media in particular we first illustrate four primary mechanisms related to three main factors human psychology social networks and automatic systems we then depict common attributes of echo chambers with a focus on the diffusion of misinformation spreading of conspiracy theory creation of social trends political polarization and emotional contagion of users we illustrate each mechanism and attribute in a multiperspective of sociology psychology and social computing with recent case studies our analysis suggest an emerging need to detect echo chambers and mitigate their negative effects,0
rumors and conspiracy theories thrive in environments of low confidence and low trust consequently it is not surprising that ones related to the covid19 pandemic are proliferating given the lack of any authoritative scientific consensus on the virus its spread and containment or on the long term social and economic ramifications of the pandemic among the stories currently circulating are ones suggesting that the 5g network activates the virus that the pandemic is a hoax perpetrated by a global cabal that the virus is a bioweapon released deliberately by the chinese or that bill gates is using it as cover to launch a global surveillance regime while some may be quick to dismiss these stories as having little impact on realworld behavior recent events including the destruction of property racially fueled attacks against asian americans and demonstrations espousing resistance to public health orders countermand such conclusions inspired by narrative theory we crawl social media sites and news reports and through the application of automated machinelearning methods discover the underlying narrative frameworks supporting the generation of these stories we show how the various narrative frameworks fueling rumors and conspiracy theories rely on the alignment of otherwise disparate domains of knowledge and consider how they attach to the broader reporting on the pandemic these alignments and attachments which can be monitored in near realtime may be useful for identifying areas in the news that are particularly vulnerable to reinterpretation by conspiracy theorists understanding the dynamics of storytelling on social media and the narrative frameworks that provide the generative basis for these stories may also be helpful for devising methods to disrupt their spread,0
the turing test aimed to recognize the behavior of a human from that of a computer algorithm such challenge is more relevant than ever in todays social media context where limited attention and technology constrain the expressive power of humans while incentives abound to develop software agents mimicking humans these social bots interact often unnoticed with real people in social media ecosystems but their abundance is uncertain while many bots are benign one can design harmful bots with the goals of persuading smearing or deceiving here we discuss the characteristics of modern sophisticated social bots and how their presence can endanger online ecosystems and our society we then review current efforts to detect social bots on twitter features related to content network sentiment and temporal patterns of activity are imitated by bots but at the same time can help discriminate synthetic behaviors from human ones yielding signatures of engineered social tampering,0
with the covid19 outbreak and the subsequent lockdown social media became a vital communication tool the sudden outburst of online activity influenced information spread and consumption patterns it increases the relevance of studying the dynamics of social networks and developing data processing pipelines that allow a comprehensive analysis of social media data in the temporal dimension this paper scopes the weekly dynamics of the information space represented by russian social media twitter and livejournal during a critical period massive covid19 outbreak and first governmental measures the approach is twofold a build the time series of topic similarity indicators by identifying covidrelated topics in each week and measuring user contribution to the topic space and b cluster user activity and display usertopic relationships on graphs in a dashboard application the paper describes the development of the pipeline explains the choices made and provides a case study of the adaptation to virus control measures the results confirm that social processes and behaviour in response to pandemictriggered changes can be successfully traced in social media moreover the adaptation trends revealed by psychological and sociological studies are reflected in our data and can be explored using the proposed method,0
a new method of feature extraction in the social network for withinnetwork classification is proposed in the paper the method provides new features calculated by combination of both network structure information and class labels assigned to nodes the influence of various features on classification performance has also been studied the experiments on realworld data have shown that features created owing to the proposed method can lead to significant improvement of classification accuracy,0
estimating influence on social media networks is an important practical and theoretical problem especially because this new medium is widely exploited as a platform for disinformation and propaganda this paper introduces a novel approach to influence estimation on social media networks and applies it to the realworld problem of characterizing active influence operations on twitter during the 2017 french presidential elections the new influence estimation approach attributes impact by accounting for narrative propagation over the network using a network causal inference framework applied to data arising from graph sampling and filtering this causal framework infers the difference in outcome as a function of exposure in contrast to existing approaches that attribute impact to activity volume or topological features which do not explicitly measure nor necessarily indicate actual network influence cramrrao estimation bounds are derived for parameter estimation as a step in the causal analysis and used to achieve geometrical insight on the causal inference problem the ability to infer high causal influence is demonstrated on realworld social media accounts that are later independently confirmed to be either directly affiliated or correlated with foreign influence operations using evidence supplied by the us congress and journalistic reports,0
how does one find important or influential people in an online social network researchers have proposed a variety of centrality measures to identify individuals that are for example often visited by a random walk infected in an epidemic or receive many messages from friends recent research suggests that a social media users capacity to respond to an incoming message is constrained by their finite attention which they divide over all incoming information ie information sent by users they follow we propose a new measure of centrality limitedattention version of bonacichs alphacentrality that models the effect of limited attention on epidemic diffusion the new measure describes a process in which nodes broadcast messages to their outneighbors but the neighbors ability to receive the message depends on the number of inneighbors they have we evaluate the proposed measure on realworld online social networks and show that it can better reproduce an empirical influence ranking of users than other popular centrality measures,0
the collective behaviour of people adopting an innovation product or online service is commonly interpreted as a spreading phenomenon throughout the fabric of society this process is arguably driven by social influence social learning and by external effects like media observations of such processes date back to the seminal studies by rogers and bass and their mathematical modelling has taken two directions one paradigm called simple contagion identifies adoption spreading with an epidemic process the other one named complex contagion is concerned with behavioural thresholds and successfully explains the emergence of large cascades of adoption resulting in a rapid spreading often seen in empirical data the observation of real world adoption processes has become easier lately due to the availability of large digital social network and behavioural datasets this has allowed simultaneous study of network structures and dynamics of online service adoption shedding light on the mechanisms and external effects that influence the temporal evolution of behavioural or innovation adoption these advancements have induced the development of more realistic models of social spreading phenomena which in turn have provided remarkably good predictions of various empirical adoption processes in this chapter we review recent datadriven studies addressing realworld service adoption processes our studies provide the first detailed empirical evidence of a heterogeneous threshold distribution in adoption we also describe the modelling of such phenomena with formal methods and datadriven simulations our objective is to understand the effects of identified social mechanisms on service adoption spreading and to provide potential new directions and open questions for future research,0
popularity of content in social media is unequally distributed with some items receiving a disproportionate share of attention from users predicting which newlysubmitted items will become popular is critically important for both hosts of social media content and its consumers accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement prediction would also give consumers an important tool for filtering the evergrowing amount of content predicting popularity of content in social media however is challenging due to the complex interactions between content quality and how the social media site chooses to highlight content moreover most social media sites also selectively present content that has been highly rated by similar users whose similarity is indicated implicitly by their behavior or explicitly by links in a social network while these factors make it difficult to predict popularity empha priori we show that stochastic models of user behavior on these sites allows predicting popularity based on early user reactions to new content by incorporating the various mechanisms through which web sites display content such models improve on predictions based on simply extrapolating from the early votes using data from one such site the news aggregator digg we show how a stochastic model of user behavior distinguishes the effect of the increased visibility due to the network from how interested users are in the content we find a wide range of interest identifying stories primarily of interest to users in the network niche interests from those of more general interest to the user community this distinction is useful for predicting a storys eventual popularity from users early reactions to the story,0
online social media provides a channel for monitoring peoples social behaviors and their mental distress due to the restrictions imposed by covid19 people are increasingly using online social networks to express their feelings consequently there is a significant amount of diverse usergenerated social media content however covid19 pandemic has changed the way we live study socialize and recreate and this has affected our wellbeing and mental health problems there are growing researches that leverage online social media analysis to detect and assess users mental status in this paper we survey the literature of social media analysis for mental disorders detection with a special focus on the studies conducted in the context of covid19 during 20202021 firstly we classify the surveyed studies in terms of feature extraction types varying from language usage patterns to aesthetic preferences and online behaviors secondly we explore detection methods used for mental disorders detection including machine learning and deep learning detection methods finally we discuss the challenges of mental disorder detection using social media data including the privacy and ethical concerns as well as the technical challenges of scaling and deploying such systems at large scales and discuss the learnt lessons over the last few years,0
the japanese tv program drama a is a drama broadcast from october to december 2016 the audience rating was sluggish but this drama marked a high audience rating in 2016 since it was popular from the middle and it was speculated that there was a part related to social media in the popularity we considered existing research methods as a case study in this paper we used a mathematical model of the hit phenomenon to examine the impact of audience assessment from social media from a sociophysical perspective we got the same consideration as the audience rating per minute of video research this paper is ieee bigdata2018s revised paperconsideration on tv audience rating and influence of social media,0
the hacktivist group anonymous is unusual in its publicfacing nature unlike other cybercriminal groups which rely on secrecy and privacy for protection anonymous is prevalent on the social media site twitter in this paper we reexamine some key findings reported in previous smallscale qualitative studies of the group using a largescale computational analysis of anonymous presence on twitter we specifically refer to reports which reject the groups claims of leaderlessness and indicate a fracturing of the group after the arrests of prominent members in 20112013 in our research we present the first attempts to use machine learning to identify and analyse the presence of a network of over 20000 anonymous accounts spanning from 20082019 on the twitter platform in turn this research utilises social network analysis sna and centrality measures to examine the distribution of influence within this large network identifying the presence of a small number of highly influential accounts moreover we present the first study of tweets from some of the identified key influencer accounts and through the use of topic modelling demonstrate a similarity in overarching subjects of discussion between these prominent accounts these findings provide robust quantitative evidence to support the claims of smallerscale qualitative studies of the anonymous collective,0
social media users have finite attention which limits the number of incoming messages from friends they can process moreover they pay more attention to opinions and recommendations of some friends more than others in this paper we propose lalda a latent topic model which incorporates limited nonuniformly divided attention in the diffusion process by which opinions and information spread on the social network we show that our proposed model is able to learn more accurate user models from users social network and item adoption behavior than models which do not take limited attention into account we analyze voting on news items on the social news aggregator digg and show that our proposed model is better able to predict held out votes than alternative models our study demonstrates that psychosocially motivated models have better ability to describe and predict observed behavior than models which only consider topics,0
the problem of understanding peoples participation in realworld events has been a subject of active research and can offer valuable insights for human behavior analysis and eventrelated recommendationadvertisement in this work we study the latent factors for determining event popularity using largescale datasets collected from the popular meetupcom ebsn in three major cities around the world we have conducted modeling analysis of four contextual factors spatial group temporal and semantic and also developed a groupbased social influence propagation network to model groupspecific influences on events by combining the contextual features and social influence network our integrated prediction framework casino can capture the diverse influential factors of event participation and can be used by event organizers to predictimprove the popularity of their events evaluations demonstrate that our casino framework achieves high prediction accuracy with contributions from all the latent features we capture,0
intensified preventive measures during the covid19 pandemic such as lockdown and social distancing heavily increased the perception of social isolation ie a discrepancy between ones social needs and the provisions of the social environment among young adults social isolation is closely associated with situational loneliness ie loneliness emerging from environmental change a risk factor for depressive symptoms prior research suggested vulnerable young adults are likely to seek support from an online social platform such as reddit a perceived comfortable environment for lonely individuals to seek mental health help through anonymous communication with a broad social network therefore this study aims to identify and analyze depressionrelated dialogues on loneliness subreddits during the covid19 outbreak with the implications on depressionrelated infoveillance during the pandemic our study utilized logistic regression and topic modeling to classify and examine depressionrelated discussions on loneliness subreddits before and during the pandemic our results showed significant increases in the volume of depressionrelated discussions ie topics related to mental health social interaction family and emotion where challenges were reported during the pandemic we also found a switch in dominant topics emerging from depressionrelated discussions on loneliness subreddits from dating prepandemic to online interaction and community pandemic suggesting the increased expressions or need of online social support during the pandemic the current findings suggest the potential of social media to serve as a window for monitoring public mental health our future study will clinically validate the current approach which has implications for designing a surveillance system during the crisis,0
statesponsored organizations are increasingly linked to efforts aimed to exploit social media for information warfare and manipulating public opinion typically their activities rely on a number of social network accounts they control aka trolls that post and interact with other users disguised as regular users these accounts often use images and memes along with textual content in order to increase the engagement and the credibility of their posts in this paper we present the first study of images shared by statesponsored accounts by analyzing a ground truth dataset of 18m images posted to twitter by accounts controlled by the russian internet research agency first we analyze the content of the images as well as their posting activity then using hawkes processes we quantify their influence on popular web communities like twitter reddit 4chans politically incorrect board pol and gab with respect to the dissemination of images we find that the extensive image posting activity of russian trolls coincides with realworld events eg the unite the right rally in charlottesville and shed light on their targets as well as the content disseminated via images finally we show that the trolls were more effective in disseminating politicsrelated imagery than other images,0
understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences this paper investigates susceptibility to influence within social networks focusing on the differential effects of influencedriven versus spontaneous behaviors on user content adoption our analysis reveals that influencedriven adoption exhibits high homophily indicating that individuals prone to influence often connect with similarly susceptible peers thereby reinforcing peer influence dynamics whereas spontaneous adoption shows significant but lower homophily additionally we extend the generalized friendship paradox to influencedriven behaviors demonstrating that users friends are generally more susceptible to influence than the users themselves de facto establishing the notion of susceptibility paradox in online social influence this pattern does not hold for spontaneous behaviors where friends exhibit fewer spontaneous adoptions we find that susceptibility to influence can be predicted using friends susceptibility alone while predicting spontaneous adoption requires additional features such as user metadata these findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences,0
github is the most popular repository for open source code it has more than 35 million users as the company declared in april 2013 and more than 10 million repositories as of december 2013 it has a publicly accessible api and since march 2012 it also publishes a stream of all the events occurring on public projects interactions among github users are of a complex nature and take place in different forms developers create and fork repositories push code approve code pushed by others bookmark their favorite projects and follow other developers to keep track of their activities in this paper we present a characterization of github as both a social network and a collaborative platform to the best of our knowledge this is the first quantitative study about the interactions happening on github we analyze the logs from the service over 18 months between march 11 2012 and september 11 2013 describing 18354 million events and we obtain information about 219 million users and 568 million repositories both growing linearly in time we show that the distributions of the number of contributors per project watchers per project and followers per user show a powerlawlike shape we analyze social ties and repositorymediated collaboration patterns and we observe a remarkably low level of reciprocity of the social connections we also measure the activity of each user in terms of authored events and we observe that very active users do not necessarily have a large number of followers finally we provide a geographic characterization of the centers of activity and we investigate how distance influences collaboration,0
recent research has established both a theoretical basis and strong empirical evidence that effective social behavior plays a beneficial role in the maintenance of physical and psychological wellbeing of people to test whether social behavior and wellbeing are also associated in online communities we studied the correlations between the recovery of patients with mental disorders and their behaviors in online social media as the source of the data related to the social behavior and progress of mental recovery we used patientslikeme plm the worlds first openparticipation research platform for the development of patientcentered health outcome measures we first constructed an online social network structure based on patienttopatient ties among 200 patients obtained from plm we then characterized patients online social activities by measuring the numbers of posts and views and helpful marks each patient obtained the patients recovery data were obtained from their selfreported status information that was also available on plm we found that some node properties indegree eigenvector centrality and pagerank and the two online social activity measures were significantly correlated with patients recovery furthermore we recollected the patients recovery data two months after the first data collection we found significant correlations between the patients social behaviors and the second recovery data which were collected two months apart our results indicated that social interactions in online communities such as plm were significantly associated with the current and future recoveries of patients with mental disorders,0
in the digital era the prevalence of depressive symptoms expressed on social media has raised serious concerns necessitating advanced methodologies for timely detection this paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines large language models llms with explainable artificial intelligence xai and conversational agents like chatgpt in our methodology explanations are achieved by integrating bertweet a twitterspecific variant of bert into a novel selfexplanatory model namely bertxdd capable of providing both classification and explanations via masked attention the interpretability is further enhanced using chatgpt to transform technical explanations into humanreadable commentaries by introducing an effective and modular approach for interpretable depression detection our methodology can contribute to the development of socially responsible digital platforms fostering early intervention and support for mental health challenges under the guidance of qualified healthcare professionals,0
many people rely on online social networks as sources of news and information and the spread of media content with ideologies across the political spectrum influences online discussions and impacts actions offline to examine the impact of media in online social networks we generalize boundedconfidence models of opinion dynamics by incorporating media accounts as influencers in a network we quantify partisanship of content with a continuous parameter on an interval and we formulate higherdimensional generalizations to incorporate content quality and increasingly nuanced political positions we simulate our model with one and two ideological dimensions and we use the results of our simulations to quantify the entrainment of content from nonmedia accounts to the ideologies of media accounts in a network we maximize media impact in a social network by tuning the number of media accounts that promote the content and the number of followers of the accounts using numerical computations we find that the entrainment of the ideology of content spread by nonmedia accounts to media ideology depends on a networks structural features including its size the mean number of followers of its nodes and the receptiveness of its nodes to different opinions we then introduce content quality a key novel contribution of our work into our model we incorporate multiple media sources with ideological biases and qualitylevel estimates that we draw from real media sources and demonstrate that our model can produce distinct communities echo chambers that are polarized in both ideology and quality our model provides a step toward understanding content quality and ideology in spreading dynamics with ramifications for how to mitigate the spread of undesired content and promote the spread of desired content,0
past few years have witnessed the emergence and phenomenal success of strongtie based social commerce embedded in social networking sites these ecommerce platforms transform ordinary people into sellers where they advertise and sell products to their friends and family in online social networks these sites can acquire millions of users within a short time and are growing fast at an accelerated rate however little is known about how these social commerce develop as a blend of social relationship and economic transactions in this paper we present the first measurement study on the fullscale data of beidian one of the fastest growing social commerce sites in china which involves 118 million users we first analyzed the topological structure of the beidian platform and highlighted its decentralized nature we then studied the sites rapid growth and its growth mechanism via invitation cascade finally we investigated purchasing behavior on beidian where we focused on user proximity and loyalty which contributes to the sites high conversion rate as the consequences of interactions between strong ties and economic logics emerging social commerce demonstrates significant property deviations from all known social networks and ecommerce in terms of network structure dynamics and user behavior to the best of our knowledge this work is the first quantitative study on the network characteristics and dynamics of emerging social commerce platforms,0
the dissemination of news articles on social media platforms significantly impacts the publics perception of global issues with the nature of these articles varying in credibility and popularity the challenge of measuring this influence and identifying key propagators is formidable traditional graphbased metrics such as different centrality measures and node degree methods offer some insights into information flow but prove insufficient for identifying hidden influencers in largescale social media networks such as x previously known as twitter this study adopts and enhances a nonparametric framework based on transfer entropy to elucidate the influence relationships among x users it further categorizes the distribution of influence exerted by these actors through the innovative use of multiplex network measures within a social media context aiming to pinpoint influential actors during significant world events the methodology was applied to three distinct events and the findings revealed that actors in different events leveraged different types of news articles and influenced distinct sets of actors based on the news category notably we found that actors disseminating trustworthy news articles to influence others occasionally resort to untrustworthy sources however the converse scenario wherein actors predominantly using untrustworthy news types switch to trustworthy sources for influence is less prevalent this asymmetry suggests a discernible pattern in the strategic use of news articles for influence across social media networks highlighting the nuanced roles of trustworthiness and popularity in the spread of information and influence,0
social media influence campaigns pose significant challenges to public discourse and democracy traditional detection methods fall short due to the complexity and dynamic nature of social media addressing this we propose a novel detection method using large language models llms that incorporates both user metadata and network structures by converting these elements into a text format our approach effectively processes multilingual content and adapts to the shifting tactics of malicious campaign actors we validate our model through rigorous testing on multiple datasets showcasing its superior performance in identifying influence efforts this research not only offers a powerful tool for detecting campaigns but also sets the stage for future enhancements to keep up with the fastpaced evolution of social mediabased influence tactics,0
social media have been widely used to organize citizen movements in 2012 75 university and college students in quebec canada participated in mass protests against an increase in tuition fees mainly organized using social media to reduce public disruption the government introduced special legislation designed to impede protest organization here we show that the legislation changed the behaviour of social media users but not the overall structure of their social network on twitter thus users were still able to spread information to efficiently organize demonstrations using their social network this natural experiment shows the power of social media in political mobilization as well as behavioural flexibility in information flow over a large number of individuals,0
distrust of public serving institutions and antiestablishment views are on the rise especially in the us as people turn to social media for information it is imperative to understand whether and how social media environments may be contributing to distrust of institutions in social media content creators influencers and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility however the extent to which this content appears and whether such content increases engagement is unclear this study analyzes the prevalence of antiestablishment sentiment aes on the social media platform tiktok despite its popularity as a source of information tiktok remains relatively understudied and may provide important insights into how people form attitudes towards institutions we employ a computational approach to label tiktok posts as containing aes or not across topical domains where content creators tend to frame themselves as experts finance and wellness as a comparison we also consider the topic of conspiracy theories where aes is expected to be common we find that aes is most prevalent in conspiracy theory content and relatively rare in content related to the other two topics however we find that engagement patterns with such content varies by area and that there may be platform incentives for users to post content that expresses antiestablishment sentiment,0
professional players in social media eg big companies politician athletes celebrities etc are intensively using online social networks osns in order to interact with a huge amount of regular osn users with different purposes marketing campaigns customer feedback public reputation improvement etc hence due to the large catalog of existing osns professional players usually count with osn accounts in different systems in this context an interesting question is whether professional users publish the same information across their osn accounts or actually they use different osns in a different manner we define as crossposting activity the action of publishing the same information in two or more osns this paper aims at characterizing the crossposting activity of professional users across three major osns facebook twitter and google to this end we perform a largescale measurementbased analysis across more than 2m posts collected from 616 professional users with active accounts in the three referred osns then we characterize the phenomenon of cross posting and analyze the behavioral patterns based on the identified characteristics,0
massive social media data can reflect peoples authentic thoughts emotions communication etc and therefore can be analyzed for early detection of mental health problems such as depression existing works about early depression detection on social media lacked interpretability and neglected the heterogeneity of social media data furthermore they overlooked the global interaction among users to address these issues we develop a novel method that leverages a heterogeneous subgraph network with prompt learninghsnpl and contrastive learning mechanisms specifically prompt learning is employed to map users implicit psychological symbols with excellent interpretability while deep semantic and diverse behavioral features are incorporated by a heterogeneous information network then the heterogeneous graph network with a dual attention mechanism is constructed to model the relationships among heterogeneous social information at the feature level furthermore the heterogeneous subgraph network integrating subgraph attention and selfsupervised contrastive learning is developed to explore complicated interactions among users and groups at the user level extensive experimental results demonstrate that our proposed method significantly outperforms stateoftheart methods for depression detection on social media,0
information propagation in online social networks is facilitated by two types of influence endogenous peer influence that acts between users of the social network and exogenous external that corresponds to various external mediators such as online news media however inference of these influences from data remains a challenge especially when data on the activation of users is scarce in this paper we propose a methodology that yields estimates of both endogenous and exogenous influence using only a social network structure and a single activation cascade our method exploits the statistical differences between the two types of influence endogenous is dependent on the social network structure and current state of each user while exogenous is independent of these we evaluate our methodology on simulated activation cascades as well as on cascades obtained from several large facebook political survey applications we show that our methodology is able to provide estimates of endogenous and exogenous influence in online social networks characterize activation of each individual user as being endogenously or exogenously driven and identify most influential groups of users,0
online social media osm is a platform through which the users present themselves to the connected world by means of messaging posting reacting tagging and sharing on different contents with also other social activities nowadays it has a vast impact on various aspects of the industry business and society along with on users life in an osn platform reaching the target users is one of the primary focus for most of the businesses and other organizations identification and recommendation of influenceable targets help to capture the appropriate audience efficiently and effectively in this paper an effective model has been discussed in egocentric osn by incorporating an efficient influence measured recommendation system in order to generate a list of top most influenceable target users among all connected network members for any specific social network user firstly the list of interacted network members has been updated based on all activities on which the interacted network members with most similar activities have been recommended based on the specific influence category with sentiment type after that the top most influenceable network members in the basis of the required amount among those updated list of interacted network members have been identified with proper ranking by analyzing the similarity and frequency of their activity contents with respect to the activity contents of the main user through these two continuous stages an effective list of top influenceable targets of the main user has been distinguished from the egocentric view of any social network,0
a social network confers benefits and advantages on individuals and on groups the literature refers to these advantages as social capital this paper presents a microfounded mathematical model of the evolution of a social network and of the social capital of individuals within the network the evolution of the network is influenced by the extent to which individuals are homophilic structurally opportunistic socially gregarious and by the distribution of types in the society in the analysis we identify different kinds of social capital bonding capital popularity capital and bridging capital bonding capital is created by forming a circle of connections homophily increases bonding capital because it makes this circle of connections more homogeneous popularity capital leads to preferential attachment individuals who become popular tend to become more popular because others are more likely to link to them homophily creates asymmetries in the levels of popularity attained by different social groups more gregarious types of agents are more likely to become popular however in homophilic societies individuals who belong to less gregarious less opportunistic or major types are likely to be more central in the network and thus acquire a bridging capital,0
the rapid and widespread dissemination of misinformation through social networks is a growing concern in todays digital age this study focused on modeling fake news diffusion discovering the spreading dynamics and designing control strategies a common approach for modeling the misinformation dynamics is sirbased models our approach is an extension of a model called sbfc which is a sirbased model this model has three states susceptible believer and factchecker the dynamics and transition between states are based on neighbors beliefs hoax credibility spreading rate probability of verifying the news and probability of forgetting the current state our contribution is to push this model to real social networks by considering different classes of agents with their characteristics we proposed two main strategies for confronting misinformation diffusion first we can educate a minor class like scholars or influencers to improve their ability to verify the news or remember their state longer the second strategy is adding factchecker bots to the network to spread the facts and influence their neighbors states our result shows that both of these approaches can effectively control the misinformation spread,0
purpose we present an approach for forecasting mental health conditions and emotions of a given population during the covid19 pandemic in argentina based on language expressions used in social media this approach permits anticipating high prevalence periods in short to mediumterm time horizons design mental health conditions and emotions are captured via markers which link social media contents with lexicons first we build descriptive timelines for decision makers to monitor the evolution of markers and their correlation with crisis events second we model the timelines as time series and support their forecasting which in turn serve to identify high prevalence points for the estimated markers findings results showed that different time series forecasting strategies offer different capabilities in the best scenario the emergence of high prevalence periods of emotions and mental health disorders can be satisfactorily predicted with a neural network strategy even when limited data is available in early stages of a crisis eg 7 days originality although there have been efforts in the literature to predict mental states of individuals the analysis of mental health at the collective level has received scarce attention we take a step forward by proposing a forecasting approach for analyzing the mental health of a given population or group of individuals at a larger scale practical implications we believe that this work contributes to a better understanding of how psychological processes related to crisis manifest in social media being a valuable asset for the design implementation and monitoring of health prevention and communication policies,0
grasping the themes of social media content is key to understanding the narratives that influence public opinion and behavior the thematic analysis goes beyond traditional topiclevel analysis which often captures only the broadest patterns providing deeper insights into specific and actionable themes such as public sentiment towards vaccination political discourse surrounding climate policies etc in this paper we introduce a novel approach to uncovering latent themes in social media messaging recognizing the limitations of the traditional topiclevel analysis which tends to capture only overarching patterns this study emphasizes the need for a finergrained themefocused exploration traditional theme discovery methods typically involve manual processes and a humanintheloop approach while valuable these methods face challenges in scalability consistency and resource intensity in terms of time and cost to address these challenges we propose a machineintheloop approach that leverages the advanced capabilities of large language models llms to demonstrate our approach we apply our framework to contentious topics such as climate debate and vaccine debate we use two publicly available datasets 1 the climate campaigns dataset of 21k facebook ads and 2 the covid19 vaccine campaigns dataset of 9k facebook ads our quantitative and qualitative analysis shows that our methodology yields more accurate and interpretable results compared to the baselines our results not only demonstrate the effectiveness of our approach in uncovering latent themes but also illuminate how these themes are tailored for demographic targeting in social media contexts additionally our work sheds light on the dynamic nature of social media revealing the shifts in the thematic focus of messaging in response to realworld events,0
each year significant investment of time and resources is made to improve diversity within engineering across a range of federal and state agencies privatenotforprofit organizations and foundations in spite of decades of investments efforts have not yielded desired returns participation by minorities continues to lag at a time when stem workforce requirements are increasing in recent years a new stream of data has emerged online social networks including twitter facebook and instagram that act as a key sensor of social behavior and attitudes of the public almost 87 of the american population now participates in some form of social media activity consequently social networking sites have become powerful indicators of social action and social media data has shown significant promise for studying many issues including public health communication political campaign humanitarian crisis and activism we argue that social media data can likewise be leveraged to better understand and improve engineering diversity as a case study to illustrate the viability of the approach we present findings from a campaign ilooklikeanengineer using twitter data 19354 original tweets and 29529 retweets aimed at increasing gender diversity in the engineering workplace the campaign provided a continuous momentum to the overall effort to increase diversity and novel ways of connecting with relevant audience our analysis demonstrates that diversity initiatives related to stem attract voices from various entities including individuals large corporations media outlets and community interest groups,0
the widespread use of social media has highlighted potential negative impacts on society and individuals largely driven by recommendation algorithms that shape user behavior and social dynamics understanding these algorithms is essential but challenging due to the complex distributed nature of social media networks as well as limited access to realworld data this study proposes to use academic social networks as a proxy for investigating recommendation systems in social media by employing graph neural networks gnns we develop a model that separates the prediction of academic infosphere from behavior prediction allowing us to simulate recommendergenerated infospheres and assess the models performance in predicting future coauthorships our approach aims to improve our understanding of recommendation systems roles and social networks modeling to support the reproducibility of our work we publicly make available our implementations,0
this study examines the structural dynamics of truth social a politically aligned social media platform during two major political events the us supreme courts overturning of roe v wade and the fbis search of maralago using a largescale dataset of user interactions based on retruths platformnative reposts we analyze how the network evolves in relation to fragmentation polarization and user influence our findings reveal a segmented and ideologically homogenous structure dominated by a small number of central figures political events prompt temporary consolidation around shared narratives followed by rapid returns to fragmented echochambered clusters centrality metrics highlight the disproportionate role of key influencers particularly realdonaldtrump in shaping visibility and directing discourse these results contribute to research on alternative platforms political communication and online network behavior demonstrating how infrastructure and community dynamics together reinforce ideological boundaries and limit crosscutting engagement,0
databases of highly networked individuals have been indispensable in studying narratives and influence on social media to support studies on twitter in india we present a systematically categorised database of accounts of influence on twitter in india identified and annotated through an iterative process of friends networks and selfdescribed profile information verified manually we built an initial set of accounts based on the friend network of a seed set of accounts based on realworld renown in various fields and then snowballed friends of friends multiple times and rank ordered individuals based on the number of ingroup connections and overall followers we then manually classified identified accounts under the categories of entertainment sports business government institutions journalism civil society accounts that have independent standing outside of social media as well as a category of digital first referring to accounts that derive their primary influence from online activity overall we annotated 11580 unique accounts across all categories the database is useful studying various questions related to the role of influencers in polarisation misinformation extreme speech political discourse etc,0
basic human values represent a set of values such as security independence success kindness and pleasure which we deem important to our lives each of us holds different values with different degrees of significance existing studies show that values of a person can be identified from their social network usage however the value priority of a person may change over time due to different factors such as life experiences influence social structure and technology existing studies do not conduct any analysis regarding the change of users value from the social influence ie group persuasion form the social media usage in our research first we predict users value score by the influence of friends from their social media usage we propose a bounded confidence model bcm based value dynamics model from 275 different ego networks in facebook that predicts how social influence may persuade a person to change their value over time then to predict better we use particle swarm optimization based hyperparameter tuning technique we observe that these optimized hyperparameters produce accurate future value score we also run our approach with different machine learning based methods and find support vector regression svr outperforms other regressor models by using svr with the best hyperparameters of bcm model we find the lowest mean squared error mse score 000347,0
online social networks have become an important platform for people to communicate share knowledge and disseminate information given the widespread usage of social media individuals ideas preferences and behavior are often influenced by their peers or friends in the social networks that they participate in since the last decade influence maximization im problem has been extensively adopted to model the diffusion of innovations and ideas the purpose of im is to select a set of k seed nodes who can influence the most individuals in the network in this survey we present a systematical study over the researches and future directions with respect to im problem we review the information diffusion models and analyze a variety of algorithms for the classic im algorithms we propose a taxonomy for potential readers to understand the key techniques and challenges we also organize the milestone works in time order such that the readers of this survey can experience the research roadmap in this field moreover we also categorize other applicationoriented im studies and correspondingly study each of them whats more we list a series of open questions as the future directions for imrelated researches where a potential reader of this survey can easily observe what should be done next in this field,0
we present a model that takes into account the coupling between evolutionary game dynamics and social influence importantly social influence and game dynamics take place in different domains which we model as different layers of a multiplex network we show that the coupling between these dynamical processes can lead to cooperation in scenarios where the pure game dynamics predicts defection in addition we show that the structure of the network layers and the relation between them can further increase cooperation remarkably if the layers are related in a certain way the system can reach a polarized metastable statethese findings could explain the prevalence of polarization observed in many social dilemmas,0
individuals modify their opinions towards a topic based on their social interactions opinion evolution models conceptualize the change of opinion as a unidimensional continuum and the effect of influence is built by the group size the network structures or the relations among opinions within the group however how to model the personal opinion evolution process under the effect of the online social influence as a function remains unclear here we show that the unidimensional continuous user opinions can be represented by compressed highdimensional word embeddings and its evolution can be accurately modelled by an ordinary differential equation ode that reflects the social network influencer interactions our three major contributions are 1 introduce a datadriven pipeline representing the personal evolution of opinions with a time kernel 2 based on previous psychology models we model the opinion evolution process as a function of online social influence using an ordinary differential equation and 3 applied our opinion evolution model to the realtime twitter data we perform our analysis on 87 active users with corresponding influencers on the covid19 topic from 2020 to 2022 the regression results demonstrate that 99 of the variation in the quantified opinions can be explained by the way we model the connected opinions from their influencers our research on the covid19 topic and for the account analysed shows that social media users primarily shift their opinion based on influencers they follow eg model explains for 99 variation and selfevolution of opinion over a long time scale is limited,0
social media platforms have diverse content moderation policies with many prominent actors hesitant to impose strict regulations a key reason for this reluctance could be the competitive advantage that comes with lax regulation a popular platform that starts enforcing content moderation rules may fear that it could lose users to lessregulated alternative platforms moreover if users continue harmful activities on other platforms regulation ends up being futile this article examines the competitive aspect of content moderation by considering the motivations of all involved players platformer news source and social media users identifying the regulation policies sustained in equilibrium and evaluating the information quality available on each platform applied to simple yet relevant social networks such as stochastic block models our model reveals the conditions for a popular platform to enforce strict regulation without losing users effectiveness of regulation depends on the diffusive property of news posts friend interaction qualities in social media the sizes and cohesiveness of communities and how much sympathizers appreciate surprising news from influencers,0
the increasing prevalence of mental health disorders such as depression anxiety and bipolar disorder calls for immediate need in developing tools for early detection and intervention social media platforms like reddit represent a rich source of usergenerated content reflecting emotional and behavioral patterns in this work we propose a multimodal deep learning framework that integrates linguistic and temporal features for early detection of mental health crises our approach is based on the method that utilizes a bilstm network both for text and temporal feature analysis modeling sequential dependencies in a different manner capturing contextual patterns quite well this work includes a crossmodal attention approach that allows fusion of such outputs into contextaware classification of mental health conditions the model was then trained and evaluated on a dataset of labeled reddit posts preprocessed using text preprocessing scaling of temporal features and encoding of labels experimental results indicate that the proposed architecture performs better compared to traditional models with a validation accuracy of 7455 and f1score of 07376 this study presents the importance of multimodal learning for mental health detection and provides a baseline for further improvements by using more advanced attention mechanisms and other data modalities,0
understanding the heterogeneous role of individuals in largescale information spreading is essential to manage online behavior as well as its potential offline consequences to this end most existing studies from diverse research domains focus on the disproportionate role played by highlyconnected hub individuals however we demonstrate here that information superspreaders in online social media are best understood and predicted by simultaneously considering two individuallevel behavioral traits influence and susceptibility specifically we derive a nonlinear networkbased algorithm to quantify individuals influence and susceptibility from multiple spreading event data by applying the algorithm to largescale data from twitter and weibo we demonstrate that individuals estimated influence and susceptibility scores enable predictions of future superspreaders above and beyond network centrality and reveal new insights on the network position of the superspreaders,0
user activities can influence their subsequent interactions with a post generating interest in the user typically users interact with posts from friends by commenting and using reaction emojis reflecting their level of interest on social media such as facebook twitter and reddit our objective is to analyze user history over time including their posts and engagement on various topics additionally we take into account the users profile seeking connections between their activities and social media platforms by integrating user history engagement and persona we aim to assess recommendation scores based on relevant item sharing by hit rate hr and the quality of the ranking system by normalized discounted cumulative gain ndcg where we achieve the highest for neumf 080 and 06 respectively our hybrid approach solves the coldstart problem when there is a new user for new items coldstart problem will never occur as we consider the post category values to improve the performance of the model during coldstart we introduce collaborative filtering by looking for similar users and ranking the users based on the highest similarity scores,0
with the expansion of mobile communications infrastructure social media usage in the global south is surging compared to the global north populations of the global south have had less prior experience with social media from stationary computers and wired internet many countries are experiencing violent conflicts that have a profound effect on their societies as a result social networks develop under different conditions than elsewhere and our goal is to provide data for studying this phenomenon in this dataset paper we present a data collection of a national twittersphere in a west african country of conflict while not the largest social network in terms of users twitter is an important platform where people engage in public discussion the focus is on mali a country beset by conflict since 2012 that has recently had a relatively precarious media ecology the dataset consists of tweets and twitter users in mali and was collected in june 2022 when the malian conflict became more violent internally both towards external and international actors in a preliminary analysis we assume that the conflictual context influences how people access social media and therefore the shape of the twittersphere and its characteristics the aim of this paper is to primarily invite researchers from various disciplines including complex networks and social sciences scholars to explore the data at hand further we collected the dataset using a scraping strategy of the follower network and the identification of characteristics of a malian twitter user the given snapshot of the malian twitter follower network contains around seven million accounts of which 56000 are clearly identifiable as malian in addition we present the tweets the dataset is available at,0
social media platforms are thriving nowadays so a huge volume of data is produced as it includes brief and clear statements millions of people post their thoughts on microblogging sites every day this paper represents and analyze the capacity of diverse strategies to volumetric delicate and social networks to predict critical opinions from online social networking sites in the exploration of certain searching for relevant the thoughts of people play a crucial role social media becomes a good outlet since the last decades to share the opinions globally sentiment analysis as well as opinion mining is a tool that is used to extract the opinions or thoughts of the common public an occurrence in one place be it economic political or social may trigger largescale chain public reaction across many other sites in an increasingly interconnected world this study demonstrates the evaluation of sentiment analysis techniques using social media contents and creating the association between subjectivity with herd behavior and clustering coefficient as well as tries to predict the election result 2021 election in west bengal this is an implementation of sentiment analysis targeted at estimating the results of an upcoming election by assessing the publics opinion across social media this paper also has a short discussion section on the usefulness of the idea in other fields,0
drug use by people is on the rise and is of great interest to public health agencies and law enforcement agencies as found by the national survey on drug use and health 20 million americans aged 12 years or older consumed illicit drugs in the past few 30 days given their ubiquity in everyday life drug abuse related studies have received much and constant attention however most of the existing studies rely on surveys surveys present a fair number of problems because of their nature surveys on sensitive topics such as illicit drug use may not be answered truthfully by the people taking them selecting a representative sample to survey is another major challenge in this paper we explore the possibility of using big data from social media in order to understand illicit drug use behaviors instagram posts are collected using drug related terms by analyzing the hashtags supplied with each post a large and dynamic dictionary of frequent illicit drug related slang is used to find these posts these posts are studied to find common drug consumption behaviors with regard to time of day and week furthermore by studying the accounts followed by the users of drug related posts we hope to discover common interests shared by drug users,0
we study the problem of analyzing influence of various factors affecting individual messages posted in social media the problem is challenging because of various types of influences propagating through the social media network that act simultaneously on any user additionally the topic composition of the influencing factors and the susceptibility of users to these influences evolve over time this problem has not studied before and offtheshelf models are unsuitable for this purpose to capture the complex interplay of these various factors we propose a new nonparametric model called the dynamic multirelational chinese restaurant process this accounts for the user network for data generation and also allows the parameters to evolve over time designing inference algorithms for this model suited for large scale socialmedia data is another challenge to this end we propose a scalable and multithreaded inference algorithm based on online gibbs sampling extensive evaluations on largescale twitter and facebook data show that the extracted topics when applied to authorship and commenting prediction outperform stateoftheart baselines more importantly our model produces valuable insights on topic trends and user personality trends beyond the capability of existing approaches,0
influence maximization im which selects a set of k users called seeds to maximize the influence spread over a social network is a fundamental problem in a wide range of applications such as viral marketing and network monitoring existing im solutions fail to consider the highly dynamic nature of social influence which results in either poor seed qualities or long processing time when the network evolves to address this problem we define a novel im query named stream influence maximization sim on social streams technically sim adopts the sliding window model and maintains a set of k seeds with the largest influence value over the most recent social actions next we propose the influential checkpoints ic framework to facilitate continuous sim query processing the ic framework creates a checkpoint for each window slide and ensures an varepsilonapproximate solution to improve its efficiency we further devise a sparse influential checkpoints sic framework which selectively keeps ofraclogn checkpoints for a sliding window of size n and maintains an fracvarepsilon12approximate solution experimental results on both realworld and synthetic datasets confirm the effectiveness and efficiency of our proposed frameworks against the stateoftheart im approaches,0
in this paper we seek to understand how politicians use images to express ideological rhetoric through facebook images posted by members of the us house and senate in the era of social media politics has become saturated with imagery a potent and emotionally salient form of political rhetoric which has been used by politicians and political organizations to influence public sentiment and voting behavior for well over a century to date however little is known about how images are used as political rhetoric using deep learning techniques to automatically predict republican or democratic party affiliation solely from the facebook photographs of the members of the 114th us congress we demonstrate that predicted class probabilities from our model function as an accurate proxy of the political ideology of images along a leftright liberalconservative dimension after controlling for the gender and race of politicians our method achieves an accuracy of 5928 from single photographs and 8235 when aggregating scores from multiple photographs up to 150 of the same person to better understand image content distinguishing liberal from conservative images we also perform indepth content analyses of the photographs our findings suggest that conservatives tend to use more images supporting status quo political institutions and hierarchy maintenance featuring individuals from dominant social groups and displaying greater happiness than liberals,0
social media platforms have become one of the main channels where people disseminate and acquire information of which the reliability is severely threatened by rumors widespread in the network existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users in this paper we introduce a novel rumor mitigation paradigm where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors countering misinformation with low business cost and user awareness a knowledgeinformed agent embodying rumor propagation mechanisms is developed which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors substantially reducing the affected populations by over 25 codes for this paper are released at,0
developing and middleincome countries increasingly emphasize higher education and entrepreneurship in their longterm development strategy our work focuses on the influence of higher education institutions heis on startup ecosystems in brazil an emerging economy first we describe regional variability in entrepreneurial network characteristics then we examine the influence of elite heis in economic hubs on entrepreneur networks second we investigate the influence ofthe academic trajectories of startup founders including their courses of study and heis of origin on the fundraising capacity of startups given the growing capability of social media databases such as crunchbase and linkedin to provide startup and individuallevel data we draw on computational methods to mine data for social network analysis we find that hei quality and the maturity of the ecosystem influence startup success our network analysis illustrates that elite heis have powerful influences on local entrepreneur ecosystems surprisingly while the most nationally prestigious heis in the south and southeast have the longest geographical reach their network influence still remains local,0
internet is fast becoming critically important to commerce industry and individuals search engine se is the most vital component for communication network and also used for discover information for users or people search engine optimization seo is the process that is mostly used to increasing traffic from free organic or natural listings on search engines and also helps to increase website ranking it includes techniques like link building directory submission classified submission etc but smo on the other hand is the process of promoting your website on social media platforms it includes techniques like rss feeds social news and bookmarking sites video and blogging sites as well as social networking sites such as facebook twitter google tumblr pinterest instagram etcsocial media optimization is becoming increasingly important for search engine optimization as search engines are increasingly utilizing the recommendations of users of social networks to rank pages in the search engine result pages since it is more difficult to tip the influence the search engines in this way social media optimization smo may also use to generate traffic on a website promote your business at the center of social marketing place and increase ranking,0
there is a large amount of interest in understanding users of social media in order to predict their behavior in this space despite this interest user predictability in social media is not wellunderstood to examine this question we consider a network of fifteen thousand users on twitter over a seven week period we apply two contrasting modeling paradigms computational mechanics and echo state networks both methods attempt to model the behavior of users on the basis of their past behavior we demonstrate that the behavior of users on twitter can be wellmodeled as processes with selffeedback we find that the two modeling approaches perform very similarly for most users but that they differ in performance on a small subset of the users by exploring the properties of these performancedifferentiated users we highlight the challenges faced in applying predictive models to dynamic social data,0
many online social networks are fundamentally directed ie they consist of both reciprocal edges ie edges that have already been linked back and parasocial edges ie edges that havent been linked back thus understanding the structures and evolutions of reciprocal edges and parasocial ones exploring the factors that influence parasocial edges to become reciprocal ones and predicting whether a parasocial edge will turn into a reciprocal one are basic research problems however there have been few systematic studies about such problems in this paper we bridge this gap using a novel largescale google dataset crawled by ourselves as well as one publicly available social network dataset first we compare the structures and evolutions of reciprocal edges and those of parasocial edges for instance we find that reciprocal edges are more likely to connect users with similar degrees while parasocial edges are more likely to link ordinary users eg users with low degrees and popular users eg celebrities however the impacts of reciprocal edges linking ordinary and popular users on the network structures increase slowly as the social networks evolve second we observe that factors including user behaviors node attributes and edge attributes all have significant impacts on the formation of reciprocal edges third in contrast to previous studies that treat reciprocal edge prediction as either a supervised or a semisupervised learning problem we identify that reciprocal edge prediction is better modeled as an outlier detection problem finally we perform extensive evaluations with the two datasets and we show that our proposal outperforms previous reciprocal edge prediction approaches,0
during the 2022 french presidential election we collected daily twitter messages on key topics posted by political candidates and their close networks using a datadriven approach we analyze interactions among political parties identifying central topics that shape the landscape of political debate moving beyond traditional correlation analyses we apply a causal inference technique convergent cross mapping to uncover directional influences among political communities revealing how some parties are more likely to initiate changes in activity while others tend to respond this approach allows us to distinguish true influence from mere correlation highlighting asymmetric relationships and hidden dynamics within the social media political network our findings demonstrate how specific issues such as health and foreign policy act as catalysts for crossparty influence particularly during critical election phases these insights provide a novel framework for understanding political discourse dynamics and have practical implications for campaign strategists and media analysts seeking to monitor and respond to shifts in political influence in real time,0
with the rapid growth of online social network sites sns it has become imperative for platform owners and online marketers to investigate what drives content production on these platforms however previous research has found it difficult to statistically model these factors from observational data due to the inability to separately assess the effects of network formation and network influence in this paper we adopt and enhance an actororiented continuoustime model to jointly estimate the coevolution of the users social network structure and their content production behavior using a markov chain monte carlo mcmc based simulation approach specifically we offer a method to analyze nonstationary and continuous behavior with network effects in the presence of observable and unobservable covariates similar to what is observed in social media ecosystems leveraging a unique dataset from a large social network site we apply our model to data on university students across six months to find that 1 users tend to connect with others that have similar posting behavior 2 however after doing so users tend to diverge in posting behavior and 3 peer influences are sensitive to the strength of the posting behavior further our method provides researchers and practitioners with a statistically rigorous approach to analyze network effects in observational data these results provide insights and recommendations for sns platforms to sustain an active and viable community,0
understanding true influence in social media requires distinguishing correlation from causationparticularly when analyzing misinformation spread while existing approaches focus on exposure metrics and network structures they often fail to capture the causal mechanisms by which external temporal signals trigger engagement we introduce a novel joint treatmentoutcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects our approach adapts causal inference techniques from healthcare to estimate average treatment effects ate within the sequential nature of social media interactions tackling challenges from external confounding signals through our experiments on realworld misinformation and disinformation datasets we show that our models outperform existing benchmarks by 1522 in predicting engagement across diverse counterfactual scenarios including exposure adjustment timing shifts and varied intervention durations case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation the expertbased empirical influence,0
online users generate tremendous amounts of data to better serve users it is required to share the userrelated data among researchers advertisers and application developers publishing such data would raise more concerns on user privacy to encourage data sharing and mitigate user privacy concerns a number of anonymization and deanonymization algorithms have been developed to help protect privacy of users this paper reviews my doctoral research on online users privacy specifically in social media in particular i propose a new adversarial attack specialized for social media data i further provide a principled way to assess effectiveness of anonymizing different aspects of social media data my work sheds light on new privacy risks in social media data due to innate heterogeneity of usergenerated data,0
social media platforms shape users experiences through the algorithmic systems they deploy in this study we examine to what extent twitters content recommender in conjunction with a users social network impacts the topic political skew and reliability of information served on the platform during a highstakes election we utilize automated accounts to document twitters algorithmically curated and reverse chronological timelines throughout the us 2022 midterm election we find that the algorithmic timeline measurably influences exposure to election content partisan skew and the prevalence of lowquality information and election rumors critically these impacts are mediated by the partisan makeup of ones personal social network which often exerts greater influence than the algorithm alone we find that the algorithmic feed decreases the proportion of election content shown to leftleaning accounts and that it skews content toward rightleaning sources when compared to the reverse chronological feed we additionally find evidence that the algorithmic system increases the prevalence of electionrelated rumors for rightleaning accounts and has mixed effects on the prevalence of lowquality information sources our work provides insight into the outcomes of twitters complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes,0
misinformation is a growing concern in a decade involving critical global events while social media regulation is mainly dedicated towards the detection and prevention of fake news and political misinformation there is limited research about religious misinformation which has only been addressed through qualitative approaches in this work we study the spread of fabricated quotes hadith that are claimed to belong to prophet muhammad the prophet of islam as a case study demonstrating one of the most common religious misinformation forms on arabic social media we attempt through quantitative methods to understand the characteristics of social media users who interact with fabricated hadith we spotted users who frequently circulate fabricated hadith and others who frequently debunk it to understand the main differences between the two groups we used logistic regression to automatically predict their behaviors and analyzed its weights to gain insights about the characteristics and interests of each group we find that both fabricated hadith circulators and debunkers have generally a lot of ties to religious accounts however circulators are identified by many accounts that follow the shia branch of islam sunni islamic public figures from the gulf countries and many sunni nonprofessional pages posting islamic content on the other hand debunkers are identified by following academic islamic scholars from multiple countries and by having more intellectual nonreligious interests like charity politics and activism,0
owing to its history and challenging circumstances social networks community in ukraine is a very interesting polygon for the study of communications in the constantly changing environment especially in the political discourse this unique environment requires three dimensions to ascertain the political position of its participant but 2019 presidential elections made this object even more spectacular the winner of elections comedian volodymyr zelenskyi reached 73 of votes without any issue ownership with empty agenda and this influenced the electoral content of social networks and their authors behavior we saw that the issue ownership by other candidates succeeds in making their issues more salient in social networks but the new phenomena the nonagenda ownership overcome any ideological influence especially under the conditions of punishment mechanism applied to old politicians analyzing social media content and users behavior in the period between two rounds of elections we found considerable overlaps between this campaign and the 2016 trump campaign we approved the widespread of filter bubbles negative campaign messages fake news and conspiracy theories active and powerful core of ukrainian facebook that was responsible for the revolution of dignity now became less significant and even turns into the huge filter bubble of active people we also proved that manipulations and fake news in the environment of private groups may be as much powerful as in a case of classical communication based around the opinion leaders,0
in the age of information abundance attention is a coveted resource social media platforms vigorously compete for users engagement influencing the evolution of their opinions on a variety of topics with recommendation algorithms often accused of creating filter bubbles where likeminded individuals interact predominantly with one another its crucial to understand the consequences of this unregulated attention market to address this we present a model of opinion dynamics on a multiplex network each layer of the network represents a distinct social media platform each with its unique characteristics users as nodes in this network share their opinions across platforms and decide how much time to allocate in each platform depending on its perceived quality our model reveals two key findings i when examining two platforms one with a neutral recommendation algorithm and another with a homophilybased algorithm we uncover that even if users spend the majority of their time on the neutral platform opinion polarization can persist ii by allowing users to dynamically allocate their social energy across platforms in accordance to their homophilic preferences a further segregation of individuals emerges while network fragmentation is usually associated with echo chambers the emergent multiplatform segregation leads to an increase in users satisfaction without the undesired increase in polarization these results underscore the significance of acknowledging how individuals gather information from a multitude of sources furthermore they emphasize that policy interventions on a single social media platform may yield limited impact,0
the remarkable advancements in large language models llms have revolutionized the content generation process in social media offering significant convenience in writing tasks however existing applications such as sentence completion and fluency enhancement do not fully address the complex challenges in realworld social media contexts a prevalent goal among social media users is to increase the visibility and influence of their posts this paper therefore delves into the compelling question can llms generate personalized influential content to amplify a users presence on social media we begin by examining prevalent techniques in content generation to assess their impact on post influence acknowledging the critical impact of underlying network structures in social media which are instrumental in initiating content cascades and highly related to the influencepopularity of a post we then inject network information into prompt for content generation to boost the posts influence we design multiple contentcentric and structureaware prompts the empirical experiments across llms validate their ability in improving the influence and draw insights on which strategies are more effective our code is available at,0
the development of social media platforms has revolutionized the speed and manner in which information is disseminated leading to both beneficial and detrimental effects on society while these platforms facilitate rapid communication they also accelerate the spread of rumors and extremist speech impacting public perception and behavior significantly this issue is particularly pronounced during election periods where the influence of social media on election outcomes has become a matter of global concern with the unprecedented number of elections in 2024 against this backdrop the election ecosystem has encountered unprecedented challenges this study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks we have meticulously collected a dataset from politifact and twitter focusing on politically relevant rumors our approach involves semantic analysis using a finetuned bert model to vectorize text content and construct a directed graph where tweets and comments are nodes and interactions are edges the core of our method is a graph neural network sagewithedgeattention which extends the graphsage model by incorporating firstorder differences as edge attributes and applying an attention mechanism to enhance feature aggregation this innovative approach allows for the finegrained analysis of the complex social network structure improving rumor detection accuracy the study concludes that our method significantly outperforms traditional content analysis and timebased models offering a theoretically sound and practically efficient solution,0
measuring the impact and success of human performance is common in various disciplines including art science and sports quantifying impact also plays a key role on social media where impact is usually defined as the reach of a users content as captured by metrics such as the number of views likes retweets or shares in this paper we study entire careers of twitter users to understand properties of impact we show that user impact tends to have certain characteristics first impact is clustered in time such that the most impactful tweets of a user appear close to each other second users commonly have hot streaks of impact ie extended periods of highimpact tweets third impact tends to gradually build up before and fall off after a users most impactful tweet we attempt to explain these characteristics using various properties measured on social media including the users network content activity and experience and find that changes in impact are associated with significant changes in these properties our findings open interesting avenues for future research on virality and influence on social media,0
this report gives a brief overview of the origin of social networks and their most popular manifestation in the modern era the online social networks osns or social media it further discusses the positive and negative implications of osns on human society the coupling of data science and social media social media mining is then put forward as a powerful tool to overcome the current challenges and pave the path for futuristic advancements,0
in the context of twitter social capitalists are specific users trying to increase their number of followers and interactions by any means these users are not healthy for the twitter network since they flaw notions of influence and visibility indeed it has recently been observed that they are real and active users that can help malicious users such as spammers gaining influence studying their behavior and understanding their position in twitter is thus of important interest a recent work provided an efficient way to detect social capitalists using two simple topological measures based on this detection method we study how social capitalists are distributed over twitters friendtofollower network we are especially interested in analyzing how they are organized and how their links spread across the network answering these questions allows to know whether the social capitalism methods increase the actual visibility on the service to that aim we study the position of social capitalists on twitter wrt the community structure of the network we base our work on the concept of community role of a node which describes its position in a network depending on its connectivity at the community level the topological measures originally defined to characterize these roles consider only some aspects of communityrelated connectivity and rely on a set of empirically fixed thresholds we first show the limitations of such measures and then extend and generalize them by considering new aspects of the communityrelated connectivity moreover we use an unsupervised approach to distinguish the roles in order to provide more flexibility relatively to the studied system we then apply our method to the case of social capitalists and show that they are highly visible on twitter due to the specific roles they occupy,0
nowadays people in the modern world communicate with their friends relatives and colleagues through the internet personsnodes and communicationedges among them form a network social media networks are a type of network where people share their views with the community there are several models that capture human behavior such as a reaction to the information received from friends or relatives the two fundamental models of information diffusion widely discussed in the social networks are the independent cascade model and the linear threshold model liu et al propose a variant of the linear threshold model in their paper title userdriven competitive influence maximizationudcim in social networks authors try to simulate human behavior where they do not make a decision immediately after being influenced but take a pause for a while and then they make a final decision they propose the heuristic algorithms and prove the approximation factor under community constraints the seed vertices belong to an identical community even finding the community is itself an nphard problem in this article we extend the existing work with algorithms and lpformation of the problem we also implement and test the lpformulated equations on small datasets by using the gurobi solver we furthermore propose one heuristic and one genetic algorithm the extensive experimentation is carried out on medium to large datasets and the outcomes of both algorithms are plotted in the results and discussion section,0
estimation of social influence in networks can be substantially biased in observational studies due to homophily and network correlation in exposure to exogenous events randomized experiments in which the researcher intervenes in the social system and uses randomization to determine how to do so provide a methodology for credibly estimating of causal effects of social behaviors in addition to addressing questions central to the social sciences these estimates can form the basis for effective marketing and public policy in this review we discuss the design space of experiments to measure social influence through combinations of interventions and randomizations we define an experiment as combination of 1 a target population of individuals connected by an observed interaction network 2 a set of treatments whereby the researcher will intervene in the social system 3 a randomization strategy which maps individuals or edges to treatments and 4 a measurement of an outcome of interest after treatment has been assigned we review experiments that demonstrate potential experimental designs and we evaluate their advantages and tradeoffs for answering different types of causal questions about social influence we show how randomization also provides a basis for statistical inference when analyzing these experiments,0
why is our society multicultural based on the two mechanisms of homophily and social influence the classical model for the dissemination of cultures proposed by axelrod predicts the existence of a fragmented regime where different cultures can coexist in a social network however in such model the multicultural regime is achievable only when a high number of cultural traits is present and is not robust against cultural drift ie the spontaneous mutations of agents traits in real systems social influence is inherently organised in layers meaning that individuals tend to diversify their connections according to the topic on which they interact in this work we show that the observed persistence of multiculturality in realworld social systems is a natural consequence of the layered organisation of social influence we find that the critical number of cultural traits that separates the monocultural and the multicultural regimes depends on the redundancy of pairwise connections across layers surprisingly for low values of structural redundancy the system is always in a multicultural state independently on the number of traits and is robust to the presence of cultural drift moreover we show that layered social influence allows the coexistence of different levels of consensus on different topics the insight obtained from simulations on synthetic graphs are confirmed by the analysis of two realworld social networks where the multicultural regime persists even for a very small number of cultural traits suggesting that the layered organisation of social interactions might indeed be at the heart of multicultural societies,0
the covid19 pandemic is not only having a heavy impact on healthcare but also changing peoples habits and the society we live in countries such as italy have enforced a total lockdown lasting several months with most of the population forced to remain at home during this time online social networks more than ever have represented an alternative solution for social life allowing users to interact and debate with each other hence it is of paramount importance to understand the changing use of social networks brought about by the pandemic in this paper we analyze how the interaction patterns around popular influencers in italy changed during the first six months of 2020 within instagram and facebook social networks we collected a large dataset for this group of public figures including more than 54 million comments on over 140 thousand posts for these months we analyze and compare engagement on the posts of these influencers and provide quantitative figures for aggregated user activity we further show the changes in the patterns of usage before and during the lockdown which demonstrated a growth of activity and sizable daily and weekly variations we also analyze the user sentiment through the psycholinguistic properties of comments and the results testified the rapid boom and disappearance of topics related to the pandemic to support further analyses we release the anonymized dataset,0
many people use social media to seek information during disasters while lacking access to traditional information sources in this study we analyze twitter data to understand information spreading activities of social media users during hurricane sandy we create multiple subgraphs of twitter users based on activity levels and analyze network properties of the subgraphs we observe that user information sharing activity follows a powerlaw distribution suggesting the existence of few highly active nodes in disseminating information and many other nodes being less active we also observe close enough connected components and isolates at all levels of activity and networks become less transitive but more assortative for larger subgraphs we also analyze the association between user activities and characteristics that may influence user behavior to spread information during a crisis users become more active in spreading information if they are centrally placed in the network less eccentric and have higher degrees our analysis provides insights on how to exploit user characteristics and network properties to spread information or limit the spreading of misinformation during a crisis event,0
generative agentbased modeling gabm is an emerging simulation paradigm that combines the reasoning abilities of large language models with traditional agentbased modeling to replicate complex social behaviors including interactions on social media while prior work has focused on localized phenomena such as opinion formation and information spread its potential to capture global network dynamics remains underexplored this paper addresses this gap by analyzing gabmbased social media simulations through the lens of the friendship paradox fp a counterintuitive phenomenon where individuals on average have fewer friends than their friends we propose a gabm framework for social media simulations featuring generative agents that emulate real users with distinct personalities and interests using twitter datasets on the us 2020 election and the qanon conspiracy we show that the fp emerges naturally in gabm simulations consistent with realworld observations the simulations unveil a hierarchical structure where agents preferentially connect with others displaying higher activity or influence additionally we find that infrequent connections primarily drive the fp reflecting patterns in real networks these findings validate gabm as a robust tool for modeling global social media phenomena and highlight its potential for advancing social science by enabling nuanced analysis of user behavior,0
the mental health of social media users has started more and more to be put at risk by harmful hateful and offensive content in this paper we propose textscstophc a harmful content detection and mitigation architecture for social media platforms our aim with textscstophc is to create more secure online environments our solution contains two modules one that employs deep neural network architecture for harmful content detection and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content the efficacy of our solution is demonstrated by experiments conducted on two realworld datasets,0
during the outbreak of the covid19 pandemic social networks become the preeminent medium for communication social discussion and entertainment social network users are regularly expressing their opinions about the impacts of the coronavirus pandemic therefore social networks serve as a reliable source for studying the topics emotions and attitudes of users that are discussed during the pandemic in this paper we investigate the reactions and attitudes of people towards topics raised on social media platforms we collected data of two largescale covid19 datasets from twitter and instagram for six and three months respectively the paper analyzes the reaction of social network users on different aspects including sentiment analysis topics detection emotions and geotemporal characteristics of our dataset we show that the dominant sentiment reactions on social media are neutral while the most discussed topics by social network users are about health issues the paper examines the countries that attracted more posts and reactions from people as well as the distribution of healthrelated topics discussed in the most mentioned countries we shed light on the temporal shift of topics over countries our results show that posts from the topmentioned countries influence and attract more reaction worldwide than posts from other parts of the world,0
social influence pervades our everyday lives and lays the foundation for complex social phenomena in a crisis like the covid19 pandemic social influence can determine whether lifesaving information is adopted existing literature studying online social influence suffers from several drawbacks first a disconnect appears between psychology approaches which are generally performed and tested in controlled lab experiments and the quantitative methods which are usually datadriven and rely on network and event analysis the former are slow expensive to deploy and typically do not generalize well to topical issues such as an ongoing pandemic the latter often oversimplify the complexities of social influence and ignore psychosocial literature this work bridges this gap and presents three contributions towards modeling and empirically quantifying online influence the first contribution is a datadriven generalized influence model that incorporates two novel psychosocialinspired mechanisms the conductance of the diffusion network and the social capital distribution the second contribution is a framework to empirically rank users social influence using a humanintheloop active learning method combined with crowdsourced pairwise influence comparisons we build a humanlabeled ground truth calibrate our generalized influence model and perform a largescale evaluation of influence we find that our generalized model outperforms the current stateoftheart approaches and corrects the inherent biases introduced by the widely used follower count as the third contribution we apply the influence model to discussions around covid19 we quantify users influence and we tabulate it against their professions we find that the executives media and military are more influential than pandemicrelated experts such as life scientists and healthcare professionals,0
radicalized beliefs such as those tied to qanon russiagate and other political conspiracy theories can lead some individuals and groups to engage in violent behavior as evidenced in recent months understanding the mechanisms by which such beliefs are accepted spread and intensified is critical for any attempt to mitigate radicalization and avoid increased political polarization this article presents and agentbased model of a social media network that enables investigation of the effects of censorship on the amount of dissenting information to which agents become exposed and the certainty of their radicalized views the model explores two forms of censorship 1 decentralized censorshipin which individuals can choose to break an online social network tie unfriend or unfollow with another individual who transmits conflicting beliefs and 2 centralized censorshipin which a single authority can ban an individual from the social media network for spreading a certain type of belief this model suggests that both forms of censorship increase certainty in radicalized views by decreasing the amount of dissent to which an agent is exposed but centralized banning of individuals has the strongest effect on radicalization,0
the rise in adoption of cryptoassets has brought many new and inexperienced investors in the cryptocurrency space these investors can be disproportionally influenced by information they receive online and particularly from social media this paper presents a dataset of cryptorelated bounty events and the users that participate in them these events coordinate social media campaigns to create artificial hype around a crypto project in order to influence the price of its token the dataset consists of information about 158k crossmedia bounty events 185k participants 10m forum comments and 82m social media urls collected from the bountiesaltcoins subforum of the bitcointalk online forum from may 2014 to december 2022 we describe the data collection and the data processing methods employed and we present a basic characterization of the dataset furthermore we discuss potential research opportunities afforded by the dataset across many disciplines and we highlight potential novel insights into how the cryptocurrency industry operates and how it interacts with its audience,0
social media is a key aspect of modern society where people share their thoughts views feelings and sentiments over the last few years the inflation in popularity of social media has resulted in a monumental increase in data users use this medium to express their thoughts feelings and opinions on a wide variety of subjects including politics and celebrities social media has thus evolved into a lucrative platform for companies to expand their scope and improve their prospects the paper focuses on social network analysis sna for a realworld online marketing strategy the study contributes by comparing various centrality measures to identify the most central nodes in the network and uses a linear threshold model to understand the spreading behaviour of individual users in conclusion the paper correlates different centrality measures and spreading behaviour to identify the most influential user in the network,0
the subjective nature of gender inequality motivates the analysis and comparison of data from real and fictional human interaction we present a computational extension of the bechdel test a popular tool to assess if a movie contains a male gender bias by looking for two female characters who discuss about something besides a man we provide the tools to quantify bechdel scores for both genders and we measure them in movie scripts and large datasets of dialogues between users of myspace and twitter comparing movies and users of social media we find that movies and twitter conversations have a consistent male bias which does not appear when analyzing myspace furthermore the narrative of twitter is closer to the movies that do not pass the bechdel test than to those that pass it we link the properties of movies and the users that share trailers of those movies our analysis reveals some particularities of movies that pass the bechdel test their trailers are less popular female users are more likely to share them than male users and users that share them tend to interact less with male users based on our datasets we define gender independence measurements to analyze the gender biases of a society as manifested through digital traces of online behavior using the profile information of twitter users we find larger gender independence for urban users in comparison to rural ones additionally the asymmetry between genders is larger for parents and lower for students gender asymmetry varies across us states increasing with higher average income and latitude this points to the relation between gender inequality and social economical and cultural factors of a society and how gender roles exist in both fictional narratives and public online dialogues,0
the present paper introduces a novel approach to studying social media habits through predictive modeling of sequential smartphone user behaviors while much of the literature on media and technology habits has relied on selfreport questionnaires and simple behavioral frequency measures we examine an important yet understudied aspect of media and technology habits their embeddedness in repetitive behavioral sequences leveraging long shortterm memory lstm and transformer neural networks we show that i social media use is predictable at the within and betweenperson level and that ii there are robust individual differences in the predictability of social media use we examine the performance of several modeling approaches including i global models trained on the pooled data from all participants ii idiographic personspecific models and iii global models finetuned on personspecific data neither personspecific modeling nor finetuning on personspecific data substantially outperformed the global models indicating that the global models were able to represent a variety of idiosyncratic behavioral patterns additionally our analyses reveal that the personlevel predictability of social media use is not substantially related to the frequency of smartphone use in general or the frequency of social media use indicating that our approach captures an aspect of habits that is distinct from behavioral frequency implications for habit modeling and theoretical development are discussed,0
social media advertisement has emerged as an effective approach for promoting the brands of a commercial house hence many of them have started using this medium to maximize the influence among the users and create a customer base in recent times several companies have emerged as influence provider who provides views of advertisement content depending on the budget provided by the commercial house in this process the influence provider tries to exploit the information diffusion phenomenon of a social network and a limited number of highly influential users are chosen and activated initially due to diffusion phenomenon the hope is that the advertisement content will reach a large number of people now consider that a group of advertisers is approaching an influence provider with their respective budget and influence demand now for any advertiser if the influence provider provides more or less influence it will be a loss for the influence provider it is an important problem from the point of view of influence provider as it is important to allocate the seed nodes to the advertisers so that the loss is minimized in this paper we study this problem which we formally referred to as regret minimization in social media advertisement problem we propose a noble regret model that captures the aggregated loss encountered by the influence provider while allocating the seed nodes we have shown that this problem is a computationally hard problem to solve we have proposed three efficient heuristic solutions to solve our problem analyzed to understand their time and space requirements they have been implemented with real world social network datasets and several experiments have been conducted and compared to many baseline methods,0
we present a computational approach for estimating emotion contagion on social media networks built on a foundation of psychology literature our approach estimates the degree to which the perceivers emotional states positive or negative start to match those of the expressors based on the latters content we use a combination of deep learning and social network analysis to model emotion contagion as a diffusion process in dynamic social network graphs taking into consideration key aspects like causality homophily and interference we evaluate our approach on user behavior data obtained from a popular social media platform for sharing short videos we analyze the behavior of 48 users over a span of 8 weeks over 200k audiovisual short posts analyzed and estimate how contagious the users with whom they engage with are on social media as per the theory of diffusion we account for the videos a user watches during this time inflow and the daily engagements liking sharing downloading or creating new videos outflow to estimate contagion to validate our approach and analysis we obtain human feedback on these 48 social media platform users with an online study by collecting responses of about 150 participants we report users who interact with more number of creators on the platform are 12 less prone to contagion and those who consume more content of negative sentiment are 23 more prone to contagion we will publicly release our code upon acceptance,0
globalization has fundamentally reshaped societal dynamics influencing how individuals interact and perceive themselves and others one significant consequence is the evolving landscape of eating disorders such as bulimia nervosa bn which are increasingly driven not just by internal psychological factors but by broader sociocultural and digital contexts while mathematical modeling has provided valuable insights traditional frameworks often fall short in capturing the nuanced roles of social contagion digital media and adaptive behavior this review synthesizes two decades of quantitative modeling efforts including compartmental stochastic and delaybased approaches we spotlight foundational work that conceptualizes bn as a socially transmissible condition and identify critical gaps especially regarding the intensifying impact of social media drawing on behavioral epidemiology and the adaptive behavior framework by fenichel et al we advocate for a new generation of models that incorporate feedback mechanisms contentdriven influence functions and dynamic network effects this work outlines a roadmap for developing more realistic datainformed models that can guide effective public health interventions in the digital era,0
enterprise social media platforms esmps are webbased platforms with standard social media functionality eg communicating with others posting links and files liking content etc yet all users are part of the same company the first contribution of this work is the use of a differenceindifferences analysis of 99 companies to measure the causal impact of esmps on companies communication networks across the full spectrum of communication technologies used within companies email instant messaging and esmps adoption caused companies communication networks to grow denser and more wellconnected by adding new novel ties that often but not exclusively involve communication from one to many employees importantly some new ties also bridge otherwise separate parts of the corporate communication network the second contribution of this work utilizing data on microsofts own communication network is understanding how these communication technologies connect people across the corporate hierarchy compared to email and instant messaging esmps excel at connecting nodes distant in the corporate hierarchy both vertically between leaders and employees and horizontally between employees in similar roles but different sectors also influence in esmps is more democratic than elsewhere with highinfluence nodes welldistributed across the corporate hierarchy overall our results suggest that esmps boost information flow within companies and increase employees attention to what is happening outside their immediate working group above and beyond email and instant messaging,0
several recent results show the influence of social contacts to spread certain properties over the network but others question the methodology of these experiments by proposing that the measured effects may be due to homophily or a shared environment in this paper we justify the existence of the social influence by considering the temporal behavior of lastfm users in order to clearly distinguish between friends sharing the same interest especially since lastfm recommends friends based on similarity of taste we separated the timeless effect of similar taste from the temporal impulses of immediately listening to the same artist after a friend we measured strong increase of listening to a completely new artist in a few hours period after a friend compared to nonfriends representing a simple trend or external influence in our experiment to eliminate network independent elements of taste we improved collaborative filtering and trend based methods by blending with simple time aware recommendations based on the influence of friends our experiments are carried over the twoyear scrobble history of 70000 lastfm users,0
news creation and consumption has been changing since the advent of social media an estimated 295 billion people in 2019 used social media worldwide the widespread of the coronavirus covid19 resulted with a tsunami of social media most platforms were used to transmit relevant news guidelines and precautions to people according to who uncontrolled conspiracy theories and propaganda are spreading faster than the covid19 pandemic itself creating an infodemic and thus causing psychological panic misleading medical advises and economic disruption accordingly discussions have been initiated with the objective of moderating all covid19 communications except those initiated from trusted sources such as the who and authorized governmental entities this paper presents a largescale study based on data mined from twitter extensive analysis has been performed on approximately one million covid19 related tweets collected over a period of two months furthermore the profiles of 288000 users were analyzed including unique users profiles metadata and tweets context the study noted various interesting conclusions including the critical impact of the 1 exploitation of the covid19 crisis to redirect readers to irrelevant topics and 2 widespread of unauthentic medical precautions and information further data analysis revealed the importance of using social networks in a global pandemic crisis by relying on credible users with variety of occupations content developers and influencers in specific fields in this context several insights and findings have been provided while elaborating computing and noncomputing implications and research directions for potential solutions and social networks management strategies during crisis periods,0
influence maximization im is the task of determining k optimal influential nodes in a social network to maximize the influence spread using a propagation model im is a prominent problem for viral marketing and helps significantly in social media advertising however developing effective algorithms with minimal time complexity for realworld social networks still remains a challenge while traditional heuristic approaches have been applied for im they often result in minimal performance gains over the computationally expensive greedybased and reverse influence samplingbased approaches in this paper we propose the discretization of the natureinspired harris hawks optimisation metaheuristic algorithm using community structures for optimal selection of seed nodes for influence spread in addition to harris hawks intelligence we employ a neighbour scout strategy algorithm to avoid blindness and enhance the searching ability of the hawks further we use a candidate nodesbased random population initialization approach and these candidate nodes aid in accelerating the convergence process for the entire populace we evaluate the efficacy of our proposed dhho approach on six social networks using the independent cascade model for information diffusion we observe that dhho is comparable or better than competing metaheuristic approaches for influence maximization across five metrics and performs noticeably better than competing heuristic approaches,0
social media platforms allow users to create polls to gather public opinion on diverse topics however we know little about what such polls are used for and how reliable they are especially in significant contexts like elections focusing on the 2020 presidential elections in the us this study shows that outcomes of election polls on twitter deviate from election results despite their prevalence leveraging demographic inference and statistical analysis we find that twitter polls are disproportionately authored by older males and exhibit a large bias towards candidate donald trump relative to representative mainstream polls we investigate potential sources of biased outcomes from the point of view of inauthentic automated and counternormative behavior using social media experiments and interviews with poll authors we identify inconsistencies between public vote counts and those privately visible to poll authors with the gap potentially attributable to purchased votes we also find that twitter accounts participating in election polls are more likely to be bots and election poll outcomes tend to be more biased before the election day than after finally we identify instances of polls spreading voter fraud conspiracy theories and estimate that a couple thousand of such polls were posted in 2020 the study discusses the implications of biased election polls in the context of transparency and accountability of social media platforms,0
the covid19 pandemic has affected peoples lives around the world on an unprecedented scale we intend to investigate hoarding behaviors in response to the pandemic using largescale social media data first we collect hoardingrelated tweets shortly after the outbreak of the coronavirus next we analyze the hoarding and antihoarding patterns of over 42000 unique twitter users in the united states from march 1 to april 30 2020 and dissect the hoardingrelated tweets by age gender and geographic location we find the percentage of females in both hoarding and antihoarding groups is higher than that of the general twitter user population furthermore using topic modeling we investigate the opinions expressed towards the hoarding behavior by categorizing these topics according to demographic and geographic groups we also calculate the anxiety scores for the hoarding and antihoarding related tweets using a lexical approach by comparing their anxiety scores with the baseline twitter anxiety score we reveal further insights the liwc anxiety mean for the hoardingrelated tweets is significantly higher than the baseline twitter anxiety mean interestingly beer has the highest calculated anxiety score compared to other hoarded items mentioned in the tweets,0
automated social media accounts known as bots are increasingly recognized as key tools for manipulative online activities these activities can stem from coordination among several accounts and these automated campaigns can manipulate social network structure by following other accounts amplifying their content and posting messages to spam online discourse in this study we present a novel unsupervised detection method designed to target a specific category of malicious accounts designed to manipulate user metrics such as online popularity our framework identifies anomalous following patterns among all the followers of a social media account through the analysis of a large number of accounts on the twitter platform rebranded as twitter after the acquisition of elon musk we demonstrate that irregular following patterns are prevalent and are indicative of automated fake accounts notably we find that these detected groups of anomalous followers exhibit consistent behavior across multiple accounts this observation combined with the computational efficiency of our proposed approach makes it a valuable tool for investigating largescale coordinated manipulation campaigns on social media platforms,0
social media platforms have been widely linked to societal harms including rising polarization and the erosion of constructive debate can these problems be mitigated through prosocial interventions we address this question using a novel method generative social simulation that embeds large language models within agentbased models to create socially rich synthetic platforms we create a minimal platform where agents can post repost and follow others we find that the resulting followingnetworks reproduce three welldocumented dysfunctions 1 partisan echo chambers 2 concentrated influence among a small elite and 3 the amplification of polarized voices creating a social media prism that distorts political discourse we test six proposed interventions from chronological feeds to bridging algorithms finding only modest improvements and in some cases worsened outcomes these results suggest that core dysfunctions may be rooted in the feedback between reactive engagement and network growth raising the possibility that meaningful reform will require rethinking the foundational dynamics of platform architecture,0
in this paper we investigate the discount allocation problem in social networks it has been reported that 40 of consumers will share an email offer with their friend and 28 of consumers will share deals via social media platforms what does this mean for a business essentially discounts should not just be treated as short term solutions to attract individual customer instead allocating discounts to a small fraction of users called seed users may trigger a large cascade in a social network this motivates us to study the influence maximization discount allocation problem given a social network and budget we need to decide to which initial set users should offer the discounts and how much should the discounts be worth our goal is to maximize the number of customers who finally adopt the target product we investigate this problem under both nonadaptive and adaptive settings in the first setting we have to commit the set of seed users and corresponding discounts all at once in advance in the latter case the decision process is performed in a sequential manner and each seed user that is picked provides the feedback on the discount or in other words reveals whether or not she will adopt the discount we propose a simple greedy policy with an approximation ratio of frac121 1e in nonadaptive setting for the significantly more complex adaptive setting we propose an adaptive greedy policy with bounded approximation ratio in terms of expected utility,0
this study explores the dynamics of visibility and influence in digital social relations examining their implications for the emergence of a new symbolic capital using a mixedmethods design the research combined semistructured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital findings reveal that visibility is influenced by content quality network size and engagement strategies while influence depends on credibility authority and trust the study identifies a new form of symbolic capital based on online visibility influence and reputation distinct from traditional forms the research discusses the ethical implications of these dynamics and suggests future research directions emphasizing the need to update social theories to account for digital transformations,0
an important part of online activities are intended to control the public opinion and behavior being considered currently a global threat this article identifies and conceptualizes seven online strategies employed in social media influence operations these procedures are quantified through the analysis of 80 incidents of foreign information manipulation and interference fimi estimating their realworld usage and combination finally we suggest future directions for research on influence operations,0
the explosive growth of social media has not only revolutionized communication but also brought challenges such as political polarization misinformation hate speech and echo chambers this dissertation employs computational social science techniques to investigate these issues understand the social dynamics driving negative online behaviors and propose datadriven solutions for healthier digital interactions i begin by introducing a scalable social network representation learning method that integrates usergenerated content with social connections to create unified user embeddings enabling accurate prediction and visualization of user attributes communities and behavioral propensities using this tool i explore three interrelated problems 1 covid19 discourse on twitter revealing polarization and asymmetric political echo chambers 2 online hate speech suggesting the pursuit of social approval motivates toxic behavior and 3 moral underpinnings of covid19 discussions uncovering patterns of moral homophily and echo chambers while also indicating moral diversity and plurality can improve message reach and acceptance across ideological divides these findings contribute to the advancement of computational social science and provide a foundation for understanding human behavior through the lens of social interactions and network homophily,0
shortform video sfv has become a globally popular form of entertainment in recent years appearing on major social media platforms however current research indicate that short video addiction can lead to numerous negative effects on both physical and psychological health such as decreased attention span and reduced motivation to learn additionally shortform video addiction sfva has been linked to other issues such as a lack of psychological support in real life family or academic pressure and social anxiety currently the detection of sfva typically occurs only after users experience negative consequences therefore we aim to construct a short video addiction dataset based on social network behavior and design an early detection framework for sfva previous mental health detection research on online social media has mostly focused on detecting depression and suicidal tendency in this study we propose the first early detection framework for sfva earlysd we first introduce large language models llms to address the common issues of sparsity and missing data in graph datasets meanwhile we categorize social network behavior data into different modalities and design a heterogeneous social network structure as the primary basis for detecting sfva we conduct a series of quantitative analysis on short video addicts using our selfconstructed dataset and perform extensive experiments to validate the effectiveness of our method earlysd using social data and heterogeneous social graphs in the detection of short video addiction,0
this paper aims to shed some light on the concept of virality especially in social networks and to provide new insights on its structure we argue that a virality is a phenomenon strictly connected to the nature of the content being spread rather than to the influencers who spread it b virality is a phenomenon with many facets ie under this generic term several different effects of persuasive communication are comprised and they only partially overlap to give ground to our claims we provide initial experiments in a machine learning framework to show how various aspects of virality can be independently predicted according to content features,0
looking from a global perspective the landscape of online social networks is highly fragmented a large number of online social networks have appeared which can provide users with various types of services generally the information available in these online social networks is of diverse categories which can be represented as heterogeneous social networks hsn formally meanwhile in such an age of online social media users usually participate in multiple online social networks simultaneously to enjoy more social networks services who can act as bridges connecting different networks together so multiple hsns not only represent information in single network but also fuse information from multiple networks formally the online social networks sharing common users are named as the aligned social networks and these shared users who act like anchors aligning the networks are called the anchor users the heterogeneous information generated by users social activities in the multiple aligned social networks provides social network practitioners and researchers with the opportunities to study individual users social behaviors across multiple social platforms simultaneously this paper presents a comprehensive survey about the latest research works on multiple aligned hsns studies based on the broad learning setting which covers 5 major research tasks ie network alignment link prediction community detection information diffusion and network embedding respectively,0
the prominent role of social media in peoples daily lives has made them more inclined to receive news through social networks than traditional sources this shift in public behavior has opened doors for some to diffuse fake news on social media and subsequently cause negative economic political and social consequences as well as distrust among the public there are many proposed methods to solve the rumor detection problem most of which do not take full advantage of the heterogeneous nature of news propagation networks with this intention we considered a previously proposed architecture as our baseline and performed the idea of structural feature extraction from the heterogeneous rumor propagation over its architecture using the concept of meta pathbased embeddings we named our model meta pathbased global local attention network mglan extensive experimental analysis on three stateoftheart datasets has demonstrated that mglan outperforms other models by capturing nodelevel discrimination to different node types,0
recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior characterizing patterns of information diffusion and identifying influential individuals in this paper we suggest a measure of causal relationships between nodes based on the informationtheoretic notion of transfer entropy or information transfer this theoretically grounded measure is based on dynamic information captures finegrain notions of influence and admits a natural predictive interpretation causal networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics we demonstrate through analysis of synthetic and realworld data that transfer entropy reveals meaningful hidden network structures in addition to altering our notion of who is influential transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups,0
social systems are in a constant state of flux with dynamics spanning from minutebyminute changes to patterns present on the timescale of years accurate models of social dynamics are important for understanding spreading of influence or diseases formation of friendships and the productivity of teams while there has been much progress on understanding complex networks over the past decade little is known about the regularities governing the microdynamics of social networks here we explore the dynamic social network of a denselyconnected population of approximately 1000 individuals and their interactions in the network of realworld persontoperson proximity measured via bluetooth as well as their telecommunication networks online social media contacts geolocation and demographic data these highresolution data allow us to observe social groups directly rendering community detection unnecessary starting from 5minute time slices we uncover dynamic social structures expressed on multiple timescales on the hourly timescale we find that gatherings are fluid with members coming and going but organized via a stable core of individuals each core represents a social context cores exhibit a pattern of recurring meetings across weeks and months each with varying degrees of regularity taken together these findings provide a powerful simplification of the social network where cores represent fundamental structures expressed with strong temporal and spatial regularity using this framework we explore the complex interplay between social and geospatial behavior documenting how the formation of cores are preceded by coordination behavior in the communication networks and demonstrating that social behavior can be predicted with high precision,0
social influence plays a significant role in shaping individual sentiments and actions particularly in a world of ubiquitous digital interconnection the rapid development of generative ai has engendered wellfounded concerns regarding the potential scalable implementation of radicalization techniques in social media motivated by these developments we present a case study investigating the effects of small but intentional perturbations on a simple social network we employ taylors classic model of social influence and tools from robust control theory most notably the dynamical structure function dsf to identify perturbations that qualitatively alter the systems behavior while remaining as unobtrusive as possible we examine two such scenarios perturbations to an existing link and perturbations that introduce a new link to the network in each case we identify destabilizing perturbations of minimal norm and simulate their effects remarkably we find that small but targeted alterations to network structure may lead to the radicalization of all agents exhibiting the potential for largescale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system our findings emphasize a need for similar analyses to be carried out on real systems eg real social networks to identify the places where such dynamics may already exist,0
we consider a discrete opinion formation problem in a setting where agents are influenced by both information diffused by their social relations and from recommendations received directly from the social media manager we study how the strength of the influence of the social media and the homophily ratio affect the probability of the agents of reaching a consensus and how these factors can determine the type of consensus reached in a simple 2symmetric block model we prove that agents converge either to a consensus or to a persistent disagreement in particular we show that when the homophily ratio is large the social media has a very low capacity of determining the outcome of the opinion dynamics on the other hand when the homophily ratio is low the social media influence can have an important role on the dynamics either by making harder to reach a consensus or inducing it on extreme opinions finally in order to extend our analysis to more general and realistic settings we give some experimental evidences that our results still hold on general networks,0
social media use has been shown to be associated with low fertility desires however we know little about the discourses surrounding childbirth and parenthood that people consume online we analyze 219127 comments on 668 short videos related to reproduction and parenthood from douyin and tiktok in china south korea and japan a region famous for its extremely low fertility level to examine the topics and sentiment expressed online bertopic model is used to assist thematic analysis and a large language model qwen is applied to label sentiment we find that comments focus on childrearing costs in all countries utility of children particularly in japan and south korea and individualism primarily in china comments from douyin exhibit the strongest antinatalist sentiments while the japanese and korean comments are more neutral short video characteristics such as their stances or account type significantly influence the responses alongside regional socioeconomic indicators including gdp urbanization and population sex ratio this work provides one of the first comprehensive analyses of online discourses on family formation via popular algorithmfed video sharing platforms in regions experiencing low fertility rates making a valuable contribution to our understanding of the spread of family values online,0
while social networks are widely used as a media for information diffusion attackers can also strategically employ analytical tools such as influence maximization to maximize the spread of adversarial content through the networks we investigate the problem of limiting the diffusion of negative information by blocking nodes and edges in the network we formulate the interaction between the defender and the attacker as a stackelberg game where the defender first chooses a set of nodes to block and then the attacker selects a set of seeds to spread negative information from this yields an extremely complex bilevel optimization problem particularly since even the standard influence measures are difficult to compute our approach is to approximate the attackers problem as the maximum node domination problem to solve this problem we first develop a method based on integer programming combined with constraint generation next to improve scalability we develop an approximate solution method that represents the attackers problem as an integer program and then combines relaxation with duality to yield an upper bound on the defenders objective that can be computed using mixed integer linear programming finally we propose an even more scalable heuristic method that prunes nodes from the consideration set based on their degree extensive experiments demonstrate the efficacy of our approaches,0
social media has been developing rapidly in public due to its nature of spreading new information which leads to rumors being circulated meanwhile detecting rumors from such massive information in social media is becoming an arduous challenge therefore some deep learning methods are applied to discover rumors through the way they spread such as recursive neural network rvnn and so on however these deep learning methods only take into account the patterns of deep propagation but ignore the structures of wide dispersion in rumor detection actually propagation and dispersion are two crucial characteristics of rumors in this paper we propose a novel bidirectional graph model named bidirectional graph convolutional networks bigcn to explore both characteristics by operating on both topdown and bottomup propagation of rumors it leverages a gcn with a topdown directed graph of rumor spreading to learn the patterns of rumor propagation and a gcn with an opposite directed graph of rumor diffusion to capture the structures of rumor dispersion moreover the information from the source post is involved in each layer of gcn to enhance the influences from the roots of rumors encouraging empirical results on several benchmarks confirm the superiority of the proposed method over the stateoftheart approaches,0
human relations are driven by social eventspeople interact exchange information share knowledge and emotions and gather news from mass media these events leave traces in human memory the strength of which depends on cognitive factors such as emotions or attention span each trace continuously weakens over time unless another related event activity strengthens it here we introduce a novel cognitiondriven social network cogsnet model that accounts for cognitive aspects of social perception the model explicitly represents each social interaction as a trace in human memory with its corresponding dynamics the strength of the trace is the only measure of the influence that the interactions had on a person for validation we apply our model to netsense data on social interactions among university students the results show that cogsnet significantly improves the quality of modeling of human interactions in social networks,0
the dynamics of decisions in complex networks is studied within a markov process framework using numerical simulations combined with mathematical insight into the process mechanisms a mathematical discretetime model is derived based on a set of basic assumptions on the convincing mechanisms associated to two opinions the model is analyzed with respect to multiplicity of critical points illustrating in this way the main behavior to be expected in the network particular interest is focussed on the effect of social network and exogenous mass mediabased influences on the decision behavior a set of numerical simulation results is provided illustrating how these mechanisms impact the final decision results the analysis reveals i the presence of fixedpoint multiplicity with a maximum of four different fixed points multistability and sensitivity with respect to process parameters and ii that mass media have a strong impact on the decision behavior,0
trends in online social media always reflect the collective attention of a vast number of individuals across the network for example internet slang words can be ubiquitous because of social memes and online contagions in an extremely short period from weibo a twitterlike service in china we find that the adoption of popular internet slang words experiences two peaks in its temporal evolution in which the former is relatively much lower than the latter this interesting phenomenon in fact provides a decent window to disclose essential factors that drive the massive diffusion underlying trends in online social media specifically the indepth comparison between diffusions represented by different peaks suggests that more attention from the crowd at early stage of the propagation produces largescale coverage while the dominant participation of opinion leaders at the early stage just leads to popularity of small scope our results quantificationally challenge the conventional hypothesis of influentials and the implications of these novel findings for marketing practice and influence maximization in social networks are also discussed,0
how social networks influence human behavior has been an interesting topic in applied research existing methods often utilized scalelevel behavioral data to estimate the influence of a social network on human behavior this study proposes a novel approach to studying social influence that utilizes itemlevel behavioral measures under the latent space modeling framework we integrate the two interaction maps for respondents social network data and itemlevel behavior measures the interaction map visualizes the association between the latent homophily of the respondents and their behaviors measured at the item level in a lowdimensional latent space revealing the potential differential social influence effects across specific behaviors measured at the item level we also measure overall social influence as the impact of the interaction map configuration contributed by the social network data on the behavior data the performance and properties of the proposed approach are evaluated via simulation studies we apply the proposed model to an empirical dataset to demonstrate how the students friendship network influences their participation in school activities,0
in recent years social media has become a ubiquitous and integral part of social networking one of the major attentions made by social researchers is the tendency of likeminded people to interact with one another in social groups a concept which is known as homophily the study of homophily can provide eminent insights into the flow of information and behaviors within a society and this has been extremely useful in analyzing the formations of online communities in this paper we review and survey the effect of homophily in social networks and summarize the state of art methods that has been proposed in the past years to identify and measure the effect of homophily in multiple types of social networks and we conclude with a critical discussion of open challenges and directions for future research,0
social networks are rich source of data to analyze user habits in all aspects of life users behavior is decisive component of a health system in various countries promoting good behavior can improve the public health significantly in this work we develop a new model for social network analysis by using text analysis approach we define each user reaction to global pandemic with analyzing his online behavior clustering a group of online users with similar habits help to find how virus spread in different societies promoting the healthy life style in the high risk online users of social media have significant effect on public health and reducing the effect of global pandemic in this work we introduce a new approach to clustering habits based on user activities on social media in the time of pandemic and recommend a machine learning model to promote health in the online platforms,0
disinformation campaigns on social media involving coordinated activities from malicious accounts towards manipulating public opinion have become increasingly prevalent existing approaches to detect coordinated accounts either make very strict assumptions about coordinated behaviours or require part of the malicious accounts in the coordinated group to be revealed in order to detect the rest to address these drawbacks we propose a generative model amdnhage attentive mixture density network with hidden account group estimation which jointly models account activities and hidden group behaviours based on temporal point processes tpp and gaussian mixture model gmm to capture inherent characteristics of coordination which is accounts that coordinate must strongly influence each others activities and collectively appear anomalous from normal accounts to address the challenges of optimizing the proposed model we provide a bilevel optimization algorithm with theoretical guarantee on convergence we verified the effectiveness of the proposed method and training algorithm on realworld social network data collected from twitter related to coordinated campaigns from russias internet research agency targeting the 2016 us presidential elections and to identify coordinated campaigns related to the covid19 pandemic leveraging the learned model we find that the average influence between coordinated account pairs is the higheston covid19 we found coordinated group spreading antivaccination antimasks conspiracies that suggest the pandemic is a hoax and political scam,0
user response to contributed content in online social media depends on many factors these include how the site lays out new content how frequently the user visits the site how many friends the user follows how active these friends are as well as how interesting or useful the content is to the user we present a stochastic modeling framework that relates a users behavior to details of the sites user interface and user activity and describe a procedure for estimating model parameters from available data we apply the model to study discussions of controversial topics on twitter specifically to predict how followers of an advocate for a topic respond to the advocates posts we show that a model of user behavior that explicitly accounts for a user transitioning through a series of states before responding to an advocates post better predicts response than models that fail to take these states into account we demonstrate other benefits of stochastic models such as their ability to identify users who are highly interested in advocates posts,0
epidemiological models traditionally used to study disease spread can effectively analyze mob behavior on social media by treating ideas sentiments or behaviors as contagions that propagate through user networks in this research we introduced a mathematical model to analyze social behavior related to covid19 spread by examining twitter activity from april 2020 to june 2020 our analysis focused on key terms such as lockdown and quarantine to track public sentiment and engagement trends during the pandemic the threshold number re0 is derived and the stability of the steady states is established numerical simulations and sensitivity analysis of applicable parameters are carried out the results show that negative sentiment on twitter has less influence on covid19 spread compared to positive sentiment however the effect of negative sentiment on the spread of covid19 remains remarkably strong moreover we use the caputo operator with different parameter values to study the impact of social media platforms on the transmission of covid19 diseases,0
the analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature especially in the context of knowledge intensive business services kibs therefore this paper focuses on bridging this gap by applying text mining and sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media finally we present and analyze the results of our quantitative analysis of 23483 posts based on eleven spanish and italian consultancy kibs twitter usernames and keywords using data interpretation techniques such as clustering and topic modeling this paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in businesstobusiness b2b companies,0
visual content on social media plays a key role in entertainment and information sharing yet some images gain more engagement than others we propose that image memorability the ability to be remembered may predict viral potential using 1247 reddit image posts across three timepoints we assessed memorability with neural network resmem and correlated the predicted memorability scores with virality metrics memorable images were consistently associated with more comments even after controlling for image categories with resnet152 semantic analysis revealed that memorable images relate to more neutralaffect comments suggesting a distinct pathway to virality from emotional content additionally visual consistency analysis showed that memorable posts inspired diverse externallyassociated comments by analyzing resmems layers we found semantic distinctiveness was key to both memorability and virality this study highlights memorability as a unique correlate of social media virality offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement,0
recent studies suggest that human emotions diffuse in not only realworld communities but also online social media more and more mechanisms beyond emotion contagion are revealed including emotion correlations which indicate their influence and the coupling of emotion diffusion and network structure such as tie strength besides different emotions might even compete in shaping the public opinion however a comprehensive model that considers uptodate findings to replicate the patterns of emotion contagion in online social media is still missing in this paper to bridge this vital gap we propose an agentbased emotion contagion model which combines features of emotion influence and tie strength preference in the dissemination process the simulation results indicate that angerdominated users have higher vitality than joydominated ones and anger prefers weaker ties than joy in diffusion which could make it easier to spread between online groups moreover angers high influence makes it competitive and easily to dominate the community especially when negative public events occur it is also surprisingly revealed that as the ratio of anger approaches joy with a gap less than 10 angry tweets and users will eventually dominate the online social media and arrives the collective outrage in the cyber space the critical gap disclosed here can be indeed warning signals at early stages for outrage controlling in online social media all the parameters of the presented model can be easily estimated from the empirical observations and their values from historical data could help reproduce the emotion contagion of different circumstances our model would shed lights on the study of multiple issues like forecasting of emotion contagion in terms of computer simulations,0
in this new era of social media social networks are becoming increasingly important sources of usergenerated content on the internet these kinds of information resources which include a lot of peoples feelings opinions feedback and reviews are very useful for big businesses markets politics journalism and many other fields politics is one of the most talkedabout and popular topics on social media networks right now many politicians use microblogging services like twitter because they have a large number of followers and supporters on those networks politicians political parties political organizations and foundations use social media networks to communicate with citizens ahead of time today social media is used by hundreds of thousands of political groups and politicians on these social media networks every politician and political party has millions of followers and politicians find new and innovative ways to urge individuals to participate in politics furthermore social media assists politicians in various decisionmaking processes by providing recommendations such as developing policies and strategies based on previous experiences recommending and selecting suitable candidates for a particular constituency recommending a suitable person for a particular position in the party and launching a political campaign based on citizen sentiments on various issues and controversies among other things this research is a review on the use of social network analysis sna and semantic analysis sa on the twitter platform to study the supporters networks of political leaders because it can help in decisionmaking when predicting their political futures,0
the present study argues that political communication on social media is mediated by a platforms digital architecture defined as the technical protocols that enable constrain and shape user behavior in a virtual space a framework for understanding digital architectures is introduced and four platforms facebook twitter instagram and snapchat are compared along the typology using the 2016 us election as a case interviews with three republican digital strategists are combined with social media data to qualify the studyies theoretical claim that a platforms network structure functionality algorithmic filtering and datafication model affect political campaign strategy on social media,0
a fundamental problem in network science is to predict how certain individuals are able to initiate new networks to spring up new ideas frequently these changes in trends are triggered by a few innovators who rapidly impose their ideas through viral influence spreading producing cascades of followers fragmenting an old network to create a new one typical examples include the raise of scientific ideas or abrupt changes in social media like the raise of facebookcom to the detriment of myspacecom how this process arises in practice has not been conclusively demonstrated here we show that a condition for sustaining a viral spreading process is the existence of a multiplex correlated graph with hidden influence links analytical solutions predict percolation phase transitions either abrupt or continuous where networks are disintegrated through viral cascades of followers as in empirical data our modeling predicts the strict conditions to sustain a large viral spreading via a scaling form of the local correlation function between multilayers which we also confirm empirically ultimately the theory predicts the conditions for viral cascading in a large class of multiplex networks ranging from social to financial systems and markets,0
humanity for centuries has perfected skills of interpersonal interactions and evolved patterns that enable people to detect lies and deceiving behavior of others in facetoface settings unprecedented growth of peoples access to mobile phones and social media raises an important question how does this new technology influence peoples interactions and support the use of traditional patterns in this article we answer this question for homophilydriven patterns in social media in our previous studies we found that on a university campus changes in student opinions were driven by the desire to hold popular opinions here we demonstrate that the evolution of online platformwide opinion groups is driven by the same desire we focus on two social media twitter and parler on which we tracked the political biases of their users on parler an initially stable group of rightbiased users evolved into a permanent rightleaning echo chamber dominating weaker transient groups of members with opposing political biases in contrast on twitter the initial presence of two large opposing bias groups led to the evolution of a bimodal bias distribution with a high degree of polarization we capture the movement of users from the initial to final bias groups during the tracking period we also show that user choices are influenced by sideeffects of homophily users entering the platform attempt to find a sufficiently large group whose members hold political biases within the range sufficiently close to their own if successful they stabilize their biases and become permanent members of the group otherwise they leave the platform we believe that the dynamics of users behavior uncovered in this article create a foundation for technical solutions supporting social groups on social media and socially aware networks,0
social media are digitalising massive amounts of users cognitions in terms of timelines and emotional content such big data opens unprecedented opportunities for investigating cognitive phenomena like perception personality and information diffusion but requires suitable interpretable frameworks since social media data come from users minds worthy candidates for this challenge are cognitive networks models of cognition giving structure to mental conceptual associations this work outlines how cognitive network science can open new quantitative ways for understanding cognition through online media like i reconstructing how users semantically and emotionally frame events with contextual knowledge unavailable to machine learning ii investigating conceptual salienceprominence through knowledge structure in social discourse iii studying users personality traits like opennesstoexperience curiosity and creativity through language in posts iv bridging cognitiveemotional content and social dynamics via multilayer networks comparing the mindsets of influencers and followers these advancements combine cognitive network and computer science to understand cognitive mechanisms in both digital and realworld settings but come with limitations concerning representativeness individual variability and data integration such aspects are discussed along the ethical implications of manipulating sociocognitive data in the future reading cognitions through networks and social media can expose cognitive biases amplified by online platforms and relevantly inform policy making education and markets about massive complex cognitive trends,0
social media platforms serve as a significant medium for sharing personal emotions daily activities and various life events ensuring individuals stay informed about the latest developments from the initiation of an account users progressively expand their circle of friends or followers engaging actively by posting commenting and sharing content over time user behavior on these platforms evolves influenced by demographic attributes and the networks they form in this study we present a novel approach that leverages opensource models llama3instruct mistral7binstruct gemma7bit through prompt engineering combined with gpt2 bert and roberta using a joint embedding technique to analyze and predict the evolution of user behavior on social media over their lifetime our experiments demonstrate the potential of these models to forecast future stages of a users social evolution including network changes future connections and shifts in user activities experimental results highlight the effectiveness of our approach with gpt2 achieving the lowest perplexity 821 in a crossmodal configuration outperforming roberta 911 and bert and underscoring the importance of leveraging crossmodal configurations for superior performance this approach addresses critical challenges in social media such as friend recommendations and activity predictions offering insights into the trajectory of user behavior by anticipating future interactions and activities this research aims to provide early warnings about potential negative outcomes enabling users to make informed decisions and mitigate risks in the long term,0
this survey draws a broadstroke panoramic picture of the state of the art sota of the research in generative methods for the analysis of social media data it fills a void as the existing survey articles are either much narrower in their scope or are dated we included two important aspects that currently gain importance in mining and modeling social media dynamics and networks social dynamics are important for understanding the spreading of influence or diseases formation of friendships the productivity of teams etc networks on the other hand may capture various complex relationships providing additional insight and identifying important patterns that would otherwise go unnoticed,0
we draw insights from the social psychology literature to identify two facets of twitter deliberations about migrants ie perceptions about migrants and behaviors towards migrants our theoretical anchoring helped us in identifying two prevailing perceptions ie sympathy and antipathy and two dominant behaviors ie solidarity and animosity of social media users towards migrants we have employed unsupervised and supervised approaches to identify these perceptions and behaviors in the domain of applied nlp our study offers a nuanced understanding of migrantrelated twitter deliberations our proposed transformerbased model ie bert cnn has reported an f1score of 076 and outperformed other models additionally we argue that tweets conveying antipathy or animosity can be broadly considered hate speech towards migrants but they are not the same thus our approach has finetuned the binary hate speech detection task by highlighting the granular differences between perceptual and behavioral aspects of hate speeches,0
social media and social networks have already woven themselves into the very fabric of everyday life this results in a dramatic increase of social data capturing various relations between the users and their associated artifacts both in online networks and the real world using ubiquitous devices in this work we consider social interaction networks from a data mining perspective also with a special focus on realworld facetoface contact networks we combine data mining and social network analysis techniques for examining the networks in order to improve our understanding of the data the modeled behavior and its underlying emergent processes furthermore we adapt extend and apply known predictive data mining algorithms on social interaction networks additionally we present novel methods for descriptive data mining for uncovering and extracting relations and patterns for hypothesis generation and exploration in order to provide characteristic information about the data and networks the presented approaches and methods aim at extracting valuable knowledge for enhancing the understanding of the respective data and for supporting the users of the respective systems we consider data from several social systems like the social bookmarking system bibsonomy the social resource sharing system flickr and ubiquitous social systems specifically we focus on data from the social conference guidance system conferator and the social group interaction system mygroup this work first gives a short introduction into social interaction networks before we describe several analysis results in the context of online social networks and realworld facetoface contact networks next we present predictive data mining methods ie for localization recommendation and link prediction after that we present novel descriptive data mining methods for mining communities and patterns,0
we describe a set of experiments for building a temporal mental health dynamics system we utilise a preexisting methodology for distantsupervision of mental health data mining from social media platforms and deploy the system during the global covid19 pandemic as a case study despite the challenging nature of the task we produce encouraging results both explicit to the global pandemic and implicit to a global phenomenon christmas depression supported by the literature we propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decisionmaking,0
the structure of a social network is fundamentally related to the interests of its members people assort spontaneously based on the topics that are relevant to them forming social groups that revolve around different subjects online social media are also favorable ecosystems for the formation of topical communities centered on matters that are not commonly taken up by the general public because of the embarrassment discomfort or shock they may cause those are communities that depict or discuss what are usually referred to as deviant behaviors conducts that are commonly considered inappropriate because they are somehow violative of societys norms or moral standards that are shared among the majority of the members of society pornography consumption drug use excessive drinking illegal hunting eating disorders or any selfharming or addictive practice are all examples of deviant behaviors,0
social media platforms such as twitter now known as x have revolutionized how the public engage with important societal and political topics recently climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation in this work we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media to this end we extracted and analyzed a dataset of 136 millions tweets sent by 36 million users from 2006 to 2019 then we construct a temporal graph from the useruser mentions network and utilize the louvain community detection algorithm to analyze the changes in community structure around conference of the parties on climate changecop events next we also apply tools from the natural language processing literature to perform sentiment analysis and topic modeling on the tweets our work acts as a first step towards understanding the evolution of proclimate change communities around cop events answering these questions helps us understand how to raise peoples awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change,0
influential users play an important role in online social networks since users tend to have an impact on one other therefore the proposed work analyzes users and their behavior in order to identify influential users and predict user participation normally the success of a social media site is dependent on the activity level of the participating users for both online social networking sites and individual users it is of interest to find out if a topic will be interesting or not in this article we propose association learning to detect relationships between users in order to verify the findings several experiments were executed based on social network analysis in which the most influential users identified from association rule learning were compared to the results from degree centrality and page rank centrality the results clearly indicate that it is possible to identify the most influential users using association rule learning in addition the results also indicate a lower execution time compared to stateoftheart methods,0
the battlefield of information warfare has moved to online social networks where influence campaigns operate at unprecedented speed and scale as with any strategic domain success requires understanding the terrain modeling adversaries and executing interventions this tutorial introduces a formal optimization framework for social media information operations io where the objective is to shape opinions through targeted actions this framework is parameterized by quantities such as network structure user opinions and activity levels all of which must be estimated or inferred from data we discuss analytic tools that support this process including centrality measures for identifying influential users clustering algorithms for detecting community structure and sentiment analysis for gauging public opinion these tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment with the landscape mapped we highlight threats such as coordinated bot networks extremist recruitment and viral misinformation countermeasures range from contentlevel interventions to mathematically optimized influence strategies finally the emergence of generative ai transforms both offense and defense democratizing persuasive capabilities while enabling scalable defenses this shift calls for algorithmic innovation policy reform and ethical vigilance to protect the integrity of our digital public sphere,0
an increasing number of social network mental disorders snmds such as cyberrelationship addiction information overload and net compulsion have been recently noted symptoms of these mental disorders are usually observed passively today resulting in delayed clinical intervention in this paper we argue that mining online social behavior provides an opportunity to actively identify snmds at an early stage it is challenging to detect snmds because the mental factors considered in standard diagnostic criteria questionnaire cannot be observed from online social activity logs our approach new and innovative to the practice of snmd detection does not rely on selfrevealing of those mental factors via questionnaires instead we propose a machine learning framework namely social network mental disorder detection snmdd that exploits features extracted from social network data to accurately identify potential cases of snmds we also exploit multisource learning in snmdd and propose a new snmdbased tensor model stm to improve the performance our framework is evaluated via a user study with 3126 online social network users we conduct a feature analysis and also apply snmdd on largescale datasets and analyze the characteristics of the three snmd types the results show that snmdd is promising for identifying online social network users with potential snmds,0
social media has brought a revolution on how people are consuming news beyond the undoubtedly large number of advantages brought by socialmedia platforms a point of criticism has been the creation of echo chambers and filter bubbles caused by social homophily and algorithmic personalization in this paper we address the problem of balancing the information exposure in a social network we assume that two opposing campaigns or viewpoints are present in the network and that network nodes have different preferences towards these campaigns our goal is to find two sets of nodes to employ in the respective campaigns so that the overall information exposure for the two campaigns is balanced we formally define the problem characterize its hardness develop approximation algorithms and present experimental evaluation results our model is inspired by the literature on influence maximization but we offer significant novelties first balance of information exposure is modeled by a symmetric difference function which is neither monotone nor submodular and thus not amenable to existing approaches second while previous papers consider a setting with selfish agents and provide bounds on best response strategies ie move of the last player we consider a setting with a centralized agent and provide bounds for a global objective function,0
the dissemination of fake news intended to deceive people influence public opinion and manipulate social outcomes has become a pressing problem on social media moreover information sharing on social media facilitates diffusion of viral information cascades in this work we focus on understanding and leveraging diffusion dynamics of false and legitimate contents in order to facilitate network interventions for fake news mitigation we analyze realworld twitter datasets comprising fake and true news cascades to understand differences in diffusion dynamics and user behaviours with regards to fake and true contents based on the analysis we model the diffusion as a mixture of independent cascade models mic with parameters t f over the social network graph and derive unsupervised inference techniques for parameter estimation of the diffusion mixture model from observed unlabeled cascades users influential in the propagation of true and fake contents are identified using the inferred diffusion dynamics characteristics of the identified influential users reveal positive correlation between influential users identified for fake news and their relative appearance in fake news cascades identified influential users tend to be related to topics of more viral information cascades than less viral ones and identified fake news influential users have relatively fewer counts of direct followers compared to the true news influential users intervention analysis on nodes and edges demonstrates capacity of the inferred diffusion dynamics in supporting network interventions for mitigation,0
different measures have been proposed to predict whether individuals will adopt a new behavior in online social networks given the influence produced by their neighbors in this paper we show one can achieve significant improvement over these standard measures extending them to consider a pair of time constraints these constraints provide a better proxy for social influence showing a stronger correlation to the probability of influence as well as the ability to predict influence,0
this research presents a framework for analyzing the dynamics of online communities in social media platforms utilizing a temporal fusion of text and network data by combining text classification and dynamic social network analysis we uncover mechanisms driving community formation and evolution revealing the influence of realworld events we introduced fourteen key elements based on social science theories to evaluate social media dynamics validating our framework through a case study of twitter data during major us events in 2020 our analysis centers on discrimination discourse identifying sexism racism xenophobia ableism homophobia and religious intolerance as main fragments results demonstrate rapid community emergence and dissolution cycles representative of discourse fragments we reveal how realworld circumstances impact discourse dominance and how social media contributes to echo chamber formation and societal polarization our comprehensive approach provides insights into discourse fragmentation opinion dynamics and structural aspects of online communities offering a methodology for understanding the complex interplay between online interactions and societal trends,0
with the rise of social media political conversations now take place in more diffuse environments in this context it is not always clear why some actors more than others have greater influence on how discussions are shaped to investigate the factors behind such influence we build on nodality a concept in political science which describes the capacity of an actor to exchange information within discourse networks this concept goes beyond traditional network metrics that describe the position of an actor in the network to include exogenous drivers of influence eg factors relating to organisational hierarchies we study online discourse on twitter now x in the uk to measure the relative nodality of two sets of policy actors members of parliament mps and accredited journalists on four policy topics we find that influence on the platform is driven by two key factors i active nodality derived from the actors level of topicrelated engagement and ii inherent nodality which is independent of the platform discourse and reflects the actors institutional position these findings significantly further our understanding of the origins of influence on social media platforms and suggest in which contexts influence is transferable across topics,0
social media enables the rapid spread of many kinds of information from memes to social movements however little is known about how information crosses linguistic boundaries we apply causal inference techniques on the european twitter network to quantify multilingual users structural role and communication influence in crosslingual information exchange overall multilinguals play an essential role posting in multiple languages increases betweenness centrality by 13 and having a multilingual network neighbor increases monolinguals odds of sharing domains and hashtags from another language 16fold and 4fold respectively we further show that multilinguals have a greater impact on diffusing information less accessible to their monolingual compatriots such as information from faraway countries and content about regional politics nascent social movements and job opportunities by highlighting information exchange across borders this work sheds light on a crucial component of how information and ideas spread around the world,0
influence maximization im aims at finding the most influential users in a social network i e users who maximize the spread of an opinion within a certain propagation model previous work investigated the correlation between influence spread and nodal centrality measures to bypass more expensive im simulations the results were promising but incomplete since these studies investigated the performance i e the ability to identify influential users of centrality measures only in restricted settings e g in undirectedunweighted networks andor within a propagation model less common for im in this paper we first show that good results within the susceptible infectedremoved sir propagation model for unweighted and undirected networks do not necessarily transfer to directed or weighted networks under the popular independent cascade ic propagation model then we identify a set of centrality measures with good performance for weighted and directed networks within the ic model our main contribution is a new way to combine the centrality measures in a closed formula to yield even better results additionally we also extend gravitational centrality gc with the proposed combined centrality measures our experiments on 50 realworld data sets show that our proposed centrality measures outperform wellknown centrality measures and the stateofthe art gc measure significantly social networks influence maximization centrality measures ic propagation model influential spreaders,0
centralized social media platforms are currently experiencing a shift in user engagement drawing attention to alternative paradigms like decentralized online social networks dosns the rising popularity of dosns finds its root in the accessibility of opensource software enabling anyone to create a new instance ie server and participate in a decentralized network known as fediverse despite this growing momentum there has been a lack of studies addressing the effect of positive and negative interactions among instances within dosns this work aims to fill this gap by presenting a preliminary examination of instances polarization in dosns focusing on mastodon the most widely recognized decentralized social media platform boasting over 10m users and nearly 20k instances to date our results suggest that polarization in the fediverse emerges in unique ways influenced by the desire to foster a federated environment between instances also facilitating the isolation of instances that may pose potential risks to the fediverse,0
the pandemic required efficient allocation of public resources and transforming existing ways of societal functions to manage any crisis governments and public health researchers exploit the information available to them in order to make informed decisions also defined as situational awareness gathering situational awareness using social media has been functional to manage epidemics previous research focused on using discussions during periods of epidemic crises on social media platforms like twitter reddit or facebook and developing nlp techniques to filter out relevant discussions from a huge corpus of messages and posts social media usage varies with internet penetration and other socioeconomic factors which might induce disparity in analyzing discussions across different geographies however print media is a ubiquitous information source irrespective of geography further topics discussed in news articles are already newsworthy while on social media newsworthiness is a product of technosocial processes developing this fundamental difference we study twitter data during the second wave in india focused on six highpopulation cities with varied macroeconomic factors through a mixture of qualitative and quantitative methods we further analyze two indian newspapers during the same period and compare topics from both twitter and the newspapers to evaluate situational awareness around the second phase of covid on each of these platforms we conclude that factors like internet penetration and gdp in a specific city influence the discourse surrounding situational updates on social media thus augmenting information from newspapers with information extracted from social media would provide a more comprehensive perspective in resource deficit cities,0
recent terrorist attacks carried out on behalf of isis on american and european soil by lone wolf attackers or sleeper cells remind us of the importance of understanding the dynamics of radicalization mediated by social media communication channels in this paper we shed light on the social media activity of a group of twentyfive thousand users whose association with isis online radical propaganda has been manually verified by using a computational tool known as dynamical activityconnectivity maps based on network and temporal activity patterns we investigate the dynamics of social influence within isis supporters we finally quantify the effectiveness of isis propaganda by determining the adoption of extremist content in the general population and draw a parallel between radical propaganda and epidemics spreading highlighting that information broadcasters and influential isis supporters generate highlyinfectious cascades of information contagion our findings will help generate effective countermeasures to combat the group and other forms of online extremism,0
the relationship between television shows and social media has become increasingly intertwined in recent years social media platforms particularly twitter have emerged as significant sources of public opinion and discourse on topics discussed in television shows in india news debates leverage the popularity of social media to promote hashtags and engage users in discussions and debates on a daily basis this paper focuses on the analysis of one of indias most prominent and widelywatched tv news debate shows arnab goswamithe debate the study examines the content of the show by analyzing the hashtags used to promote it and the social media data corresponding to these hashtags the findings reveal that the show exhibits a strong bias towards the ruling bharatiya janata party bjp with over 60 of the debates featuring either probjp or antiopposition content social media support for the show primarily comes from bjp supporters notably bjp leaders and influencers play a significant role in promoting the show on social media leveraging their existing networks and resources to artificially trend specific hashtags furthermore the study uncovers a reciprocal flow of information between the tv show and social media we find evidence that the shows choice of topics is linked to social media posts made by party workers suggesting a dynamic interplay between traditional media and online platforms by exploring the complex interaction between television debates and social media support this study contributes to a deeper understanding of the evolving relationship between these two domains in the digital age the findings hold implications for media researchers and practitioners offering insights into the ways in which social media can influence traditional media and vice versa,0
with the rise of social media misinformation has become increasingly prevalent fueled largely by the spread of rumors this study explores the use of large language model llm agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks to this end we design a variety of llmbased agent types and construct four distinct network structures to conduct these simulations our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges the evaluations indicate that network structure personas and spreading schemes can significantly influence rumor dissemination ranging from no spread to affecting 83 of agents in iterations thereby offering a realistic simulation of rumor spread in social networks,0
groups social communities are important components of entire societies analysed by means of the social network concept their immanent feature is continuous evolution over time if we know how groups in the social network has evolved we can use this information and try to predict the next step in the given group evolution in the paper a new aproach for group evolution prediction is presented and examined experimental studies on four evolving social networks revealed that i the prediction based on the simple input features may be very accurate ii some classifiers are more precise than the others and iii parameters of the group evolution extracion method significantly influence the prediction quality,0
online social media periodically serves as a platform for cascading polarizing topics of conversation the inherent community structure present in online social networks homophily and the advent of fringe outlets like gab have created online echo chambers that amplify the effects of polarization which fuels detrimental behavior recently in october 2018 gab made headlines when it was revealed that robert bowers the individual behind the pittsburgh synagogue massacre was an active member of this social media site and used it to express his antisemitic views and discuss conspiracy theories thus to address the need of automated datadriven analyses of such fringe outlets this research proposes novel methods to discover topics that are prevalent in gab and how they cascade within the network specifically using approximately 34 million posts and 37 million cascading conversation threads with close to 300k users we demonstrate that there are essentially five cascading patterns that manifest in gab and the most viral ones begin with an echochamber pattern and grow out to the entire network also we empirically show through two models viz susceptibleinfected and bass how the cascades structurally evolve from one of the five patterns to the other based on the topic of the conversation with upto 84 accuracy,0
the advent and proliferation of social media have led to the development of mathematical models describing the evolution of beliefsopinions in an ecosystem composed of socially interacting users the goal is to gain insights into collective dominant social beliefs and into the impact of different components of the system such as users interactions while being able to predict users opinions following this thread in this paper we consider a fairly general dynamical model of social interactions which captures all the main features exhibited by a social system for such model by embracing a meanfield approach we derive a diffusion differential equation that represents asymptotic belief dynamics as the number of users grows large we then analyze the steadystate behavior as well as the time dependent transient behavior of the system in particular for the steadystate distribution we obtain simple closedform expressions for a relevant class of systems while we propose efficient semianalytical techniques in the most general cases at last we develop an efficient semianalytical method to analyze the dynamics of the users belief over time which can be applied to a remarkably large class of systems,0
understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information traditional influence analysis is typically divided into qualitative assessments of personal attributes eg psychology of influence and quantitative evaluations of influence power mechanisms eg social network analysis one challenge faced by researchers is the ethics of realworld experimentation and the lack of social influence data in this study we provide a novel simulated environment that combines agentic intelligence with large language models llms to test topicspecific influence mechanisms ethically our framework contains agents that generate posts form opinions on specific topics and socially followunfollow each other based on the outcome of discussions this simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge using our own framework we design an opinion leader that utilizes reinforcement learning rl to adapt its linguistic interaction with the community to maximize its influence and followers over time our current findings reveal that constraining the action space and incorporating selfobservation are key factors for achieving stable and consistent opinion leader generation for topicspecific influence this demonstrates the simulation frameworks capacity to create agents that can adapt to complex and unpredictable social dynamics the work is important in an age of increasing online influence on social attitudes and emerging technologies,0
moral reasoning reflects how people acquire and apply moral rules in particular situations with increasingly social interactions happening online social media data provides an unprecedented opportunity to assess inthewild moral reasoning we investigate the commonsense aspects of morality in ordinary matters empirically to this end we examine data from a reddit subcommunity ie a subreddit where an author may describe their behavior in a situation to seek comments about whether that behavior was appropriate other users comment to provide judgments and reasoning we focus on the novel problem of understanding the moral reasoning implicit in user comments about the propriety of an authors behavior especially we explore associations between the common elements of the indicated reasoning and the extractable social factors our results suggest the reasoning depends on the authors gender and the topic of a post such as when expressing anger emotion and using sensible words eg fck hell and damn in workrelated situations moreover we find that the commonly expressed semantics also depends on commenters interests,0
social media platforms generate massive volumes of heterogeneous data capturing user behaviors textual content temporal dynamics and network structures analyzing such data is crucial for understanding phenomena such as opinion dynamics community formation and information diffusion however discovering insights from this complex landscape is exploratory conceptually challenging and requires expertise in social media mining and visualization existing automated approaches though increasingly leveraging large language models llms remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis we present sia social insight agents an llm agent system that links heterogeneous multimodal data including raw inputs eg text network and behavioral data intermediate outputs mined analytical results and visualization artifacts through coordinated agent flows guided by a bottomup taxonomy that connects insight types with suitable mining and visualization techniques sia enables agents to plan and execute coherent analysis strategies to ensure multimodal integration it incorporates a data coordinator that unifies tabular textual and network data into a consistent flow its interactive interface provides a transparent workflow where users can trace validate and refine the agents reasoning supporting both adaptability and trustworthiness through expertcentered case studies and quantitative evaluation we show that sia effectively discovers diverse and meaningful insights from social media while supporting humanagent collaboration in complex analytical tasks,0
opioid addiction is a severe public health threat in the us causing massive deaths and many social problems accurate relapse prediction is of practical importance for recovering patients since relapse prediction promotes timely relapse preventions that help patients stay clean in this paper we introduce a generative adversarial networks gan model to predict the addiction relapses based on sentiment images and social influences experimental results on real social media data from redditcom demonstrate that the gan model delivers a better performance than comparable alternative techniques the sentiment images generated by the model show that relapse is closely connected with two emotions joy and negative this work is one of the first attempts to predict relapses using massive social media data and generative adversarial nets the proposed method combined with knowledge of social media mining has the potential to revolutionize the practice of opioid addiction prevention and treatment,0
people often stick to their existing beliefs ignoring contradicting evidence or only interacting with those who reinforce their views social media platforms often facilitate such tendencies of homophily and echochambers as they promote highly personalized content to maximize user engagement however increased belief rigidity can negatively affect realworld policy decisions such as leading to climate change inaction and increased vaccine hesitancy to understand and effectively tackle belief rigidity on online social networks designing and evaluating various intervention strategies is crucial and increasing randomization in the network can be considered one such intervention in this paper we empirically quantify the effects of a randomized social network structure on belief rigidity specifically examining the potential benefits of introducing randomness into the network we show that individuals beliefs are positively influenced by peer opinions regardless of whether those opinions are similar to or differ from their own by passively sensing belief rigidity through our experimental framework moreover people incorporate a slightly higher variety of different peers based on their opinions into their networks when the recommendation algorithm provides them with diverse content compared to when it provides them with similar content our results indicate that in some cases there might be benefits to randomization providing empirical evidence that a more randomized network could be a feasible way of helping people get out of their echochambers our findings have broader implications in computing and platform design of social media and can help combat overly rigid beliefs in online social networks,0
the covid19 pandemic brought upon a massive wave of disinformation exacerbating polarization in the increasingly divided landscape of online discourse in this context popular social media users play a major role as they have the ability to broadcast messages to large audiences and influence public opinion in this paper we make use of openly available data to study the behavior of popular users discussing the pandemic on twitter we tackle the issue from a network perspective considering users as nodes and following relationships as directed edges the resulting network structure is modeled by embedding the actors in a latent social space where users closer to one another have a higher probability of following each other the results suggest the existence of two distinct communities which can be interpreted as generally pro and generally against vaccine mandates corroborating existing evidence on the pervasiveness of echo chambers on the platform by focusing on a number of notable users such as politicians activists and news outlets we further show that the two groups are not entirely homogeneous and that not just the two poles are represented to the contrary the latent space captures an entire spectrum of beliefs between the two extremes demonstrating that polarization while present is not the only driver of the network and that more moderate central users are key players in the discussion,0
one of the most significant differences of m5 over previous forecasting competitions is that it was held on kaggle an online platform of data scientists and machine learning practitioners kaggle provides a gathering place or virtual community for web users who are interested in the m5 competition users can share code models features loss functions etc through online notebooks and discussion forums this paper aims to study the social influence of virtual community on user behaviors in the m5 competition we first research the content of the m5 virtual community by topic modeling and trend analysis further we perform social media analysis to identify the potential relationship network of the virtual community we study the roles and characteristics of some key participants that promote the diffusion of information within the m5 virtual community overall this study provides indepth insights into the mechanism of the virtual communitys influence on the participants and has potential implications for future online competitions,0
the personalization of our news consumption on social media has a tendency to reinforce our preexisting beliefs instead of balancing our opinions this finding is a concern for the health of our democracies which rely on an access to information providing diverse viewpoints to tackle this issue from a computational perspective garimella et al nips17 modeled the spread of these viewpoints also called campaigns using the wellknown independent cascade model and studied an optimization problem that aims at balancing information exposure in a social network when two opposing campaigns propagate in the network the objective in their nphard optimization problem is to maximize the number of people that are exposed to either both or none of the viewpoints for two different settings one corresponding to a model where campaigns spread in a correlated manner and a second one where the two campaigns spread in a heterogeneous manner they provide constant ratio approximation algorithms in this paper we investigate a more general formulation of this problem that is we assume that  different campaigns propagate in a social network and we aim to maximize the number of people that are exposed to either  or none of the campaigns where gege2 we provide dedicated approximation algorithms for both the correlated and heterogeneous settings interestingly for the heterogeneous setting with ge 3 we give a reduction leading to several approximation hardness results maybe most importantly we obtain that the problem cannot be approximated within a factor of ngn for any gno1 assuming gapeth denoting with n the number of nodes in the social network for ge 4 there is no napproximation algorithm if a certain class of oneway functions exists where  0 is a given constant which depends on ,0
in the past several years social media eg twitter and facebook has been experiencing a spectacular rise and popularity and becoming a ubiquitous discourse for content sharing and social networking with the widespread of mobile devices and locationbased services social media typically allows users to share whereabouts of daily activities eg checkins and taking photos and thus strengthens the roles of social media as a proxy to understand human behaviors and complex social dynamics in geographic spaces unlike conventional spatiotemporal data this new modality of data is dynamic massive and typically represented in stream of unstructured media eg texts and photos which pose fundamental representation modeling and computational challenges to conventional spatiotemporal analysis and geographic information science in this paper we describe a scalable computational framework to harness massive locationbased social media data for efficient and systematic spatiotemporal data analysis within this framework the concept of spacetime trajectories or paths is applied to represent activity profiles of social media users a hierarchical spatiotemporal data model namely a spatiotemporal data cube model is developed based on collections of spacetime trajectories to represent the collective dynamics of social media users across aggregation boundaries at multiple spatiotemporal scales the framework is implemented based upon a public data stream of twitter feeds posted on the continent of north america to demonstrate the advantages and performance of this framework an interactive flow mapping interface including both singlesource and multiplesource flow mapping is developed to allow realtime and interactive visual exploration of movement dynamics in massive locationbased social media at multiple scales,0
online social networks are complex ensembles of interlinked communities that interact on different topics some communities are characterized by what are usually referred to as deviant behaviors conducts that are commonly considered inappropriate with respect to the societys norms or moral standards eating disorders drug use and adult content consumption are just a few examples we refer to such communities as deviant networks it is commonly believed that such deviant networks are niche isolated social groups whose activity is well separated from the mainstream socialmedia life according to this assumption research studies have mostly considered them in isolation in this work we focused on adult content consumption networks which are present in many online social media and in the web in general we found that few small and densely connected communities are responsible for most of the content production differently from previous work we studied how such communities interact with the whole social network we found that the produced content flows to the rest of the network mostly directly or through bridgecommunities reaching at least 450 times more users we also show that a large fraction of the users can be inadvertently exposed to such content through indirect content resharing we also discuss a demographic analysis of the producers and consumers networks finally we show that it is easily possible to identify a few core users to radically uproot the diffusion process we aim at setting the basis to study deviant communities in context,0
in recent years with the expansion of the internet and attractive social media infrastructures people prefer to follow the news through these media despite the many advantages of these media in the news field the lack of any control and verification mechanism has led to the spread of fake news as one of the most important threats to democracy economy journalism and freedom of expression designing and using automatic methods to detect fake news on social media has become a significant challenge in this paper we examine the publishers role in detecting fake news on social media we also suggest a high accurate multimodal framework namely frdetect using userrelated and contentrelated features with early detection capability for this purpose two new userrelated features namely activity credibility and influence have been introduced for publishers furthermore a sentencelevel convolutional neural network is provided to combine these features with latent textual content features properly experimental results have shown that the publishers features can improve the performance of contentbased models by up to 13 and 29 in accuracy and f1score respectively,0
strategic diffusion encourages participants to take active roles in promoting stakeholders agendas by rewarding successful referrals as social media continues to transform the way people communicate strategic diffusion has become a powerful tool for stakeholders to influence peoples decisions or behaviors for desired objectives existing reward mechanisms for strategic diffusion are usually either vulnerable to falsename attacks or not individually rational for participants that have made successful referrals here we introduce a novel multiwinner contests mwc mechanism for strategic diffusion in social networks the mwc mechanism satisfies several desirable properties including falsenameproofness individual rationality budget constraint monotonicity and subgraph constraint numerical experiments on four realworld social network datasets demonstrate that stakeholders can significantly boost participants aggregated efforts with proper design of competitions our work sheds light on how to design manipulationresistant mechanisms with appropriate contests,0
in many realworld situations different and often opposite opinions innovations or products are competing with one another for their social influence in a networked society in this paper we study competitive influence propagation in social networks under the competitive linear threshold clt model an extension to the classic linear threshold model under the clt model we focus on the problem that one entity tries to block the influence propagation of its competing entity as much as possible by strategically selecting a number of seed nodes that could initiate its own influence propagation we call this problem the influence blocking maximization ibm problem we prove that the objective function of ibm in the clt model is submodular and thus a greedy algorithm could achieve 11e approximation ratio however the greedy algorithm requires montecarlo simulations of competitive influence propagation which makes the algorithm not efficient we design an efficient algorithm cldag which utilizes the properties of the clt model to address this issue we conduct extensive simulations of cldag the greedy algorithm and other baseline algorithms on realworld and synthetic datasets our results show that cldag is able to provide best accuracy in par with the greedy algorithm and often better than other algorithms while it is two orders of magnitude faster than the greedy algorithm,0
a number of predictors have been suggested to detect the most influential spreaders of information in online social media across various domains such as twitter or facebook in particular degree pagerank kcore and other centralities have been adopted to rank the spreading capability of users in information dissemination media so far validation of the proposed predictors has been done by simulating the spreading dynamics rather than following real information flow in social networks consequently only modeldependent contradictory results have been achieved so far for the best predictor here we address this issue directly we search for influential spreaders by following the real spreading dynamics in a wide range of networks we find that the widelyused degree and pagerank fail in ranking users influence we find that the best spreaders are consistently located in the kcore across dissimilar social platforms such as twitter facebook livejournal and scientific publishing in the american physical society furthermore when the complete global network structure is unavailable we find that the sum of the nearest neighbors degree is a reliable local proxy for users influence our analysis provides practical instructions for optimal design of strategies for viral information dissemination in relevant applications,0
social media user representation learning aims to capture user preferences interests and behaviors in lowdimensional vector representations these representations are critical to a range of social problems including predicting user behaviors and detecting inauthentic accounts however existing methods are either designed for commercial applications or rely on specific features like text contents activity patterns or platform metadata failing to holistically model user behavior across different modalities to address these limitations we propose somer a social media user representation learning framework that incorporates temporal activities text contents profile information and network interactions to learn comprehensive user portraits somer encodes user post streams as sequences of timestamped textual features uses transformers to embed this along with profile data and jointly trains with link prediction and contrastive learning objectives to capture user similarity we demonstrate somers versatility through three applications 1 identifying information operation driver accounts 2 measuring online polarization after major events and 3 predicting future user participation in reddit hate communities somer provides new solutions to better understand user behavior in the sociopolitical domains enabling more informed decisions and interventions,0
during the covid19 pandemic multiple aspects of human life were subjected to unprecedented changes globally in sri lanka a developing country located in south asia it was possible to observe a range of events that arose due to the influence of the covid19 virus outbreak thus the people of sri lanka used social media to voice their opinions regarding such events and those involved in them enabling the ideal avenue to explore the social perception however the outcome of such actions was at certain times detrimental this study was conducted as an attempt to identify the reasons for such instances as well as to identify the behaviours of the sri lankan populace during such a crisis event to support this study observations as well as data of related posts from a sample of 50 sources were manually collected from the most popular social media platform in sri lanka facebook the posts considered spanned until approximately a month after the initial major virus outbreak in the country and contained content that even vaguely related to the virus utilising such data various forms of analyses such as topic significance and topic cooccurrences were conducted the findings highlight while there can be social detrimental ideas shared the majority of the posts point constructive and positive thoughts suggesting the successful influence from the cultural and social values sri lanka society promotes throughout,0
with the surge of domestic tourism in india and the influence of social media on young tourists this paper aims to address the research question on how social return responses received on social media sharing of recent trip details can influence decisionmaking for shortterm future travels the paper develops a multimodel framework to build a predictive machine learning model that establishes a relationship between a travelers social return various social media usage triprelated factors and her future tripplanning behavior the primary data was collected via a survey from indian tourists after data cleaning the imbalance in the data was addressed using a robust oversampling method and the reliability of the predictive model was ensured by applying a monte carlo crossvalidation technique the results suggest at least 75 overall accuracy in predicting the influence of social return on changing the future trip plan moreover the model fit results provide crucial practical implications for the domestic tourism sector in india with future research directions concerning social media destination marketing smart tourism heritage tourism etc,0
in this work we present the klout score an influence scoring system that assigns scores to 750 million users across 9 different social networks on a daily basis we propose a hierarchical framework for generating an influence score for each user by incorporating information for the user from multiple networks and communities over 3600 features that capture signals of influential interactions are aggregated across multiple dimensions for each user the features are scalably generated by processing over 45 billion interactions from social networks every day as well as by incorporating factors that indicate real world influence supervised models trained from labeled data determine the weights for features and the final klout score is obtained by hierarchically combining communities and networks we validate the correctness of the score by showing that users with higher scores are able to spread information more effectively in a network finally we use several comparisons to other ranking systems to show that highly influential and recognizable users across different domains have high klout scores,0
we use gaussian mixtures to model formation and evolution of multimodal beliefs and opinion uncertainty across social networks in this model opinions evolve by bayesian belief update when incorporating exogenous factors signals from outside sources eg news articles and by nonbayesian mixing dynamics when incorporating endogenous factors interactions across social media the modeling enables capturing the richness of behavior observed in multimodal opinion dynamics while maintaining interpretability and simplicity of scalar models we present preliminary results on opinion formation and uncertainty to investigate the effect of stubborn individuals as social influencers this leads to a notion of centrality based on the ease with which an individual can disrupt the flow of information across the social network,0
users of online social networks osns interact with each other more than ever in the context of a public discussion group people receive read and write comments in response to articles and postings in the absence of access control mechanisms osns are a great environment for attackers to influence others from spreading phishing urls to posting fake news moreover osn user behavior can be predicted by social science concepts which include conformity and the bandwagon effect in this paper we show how social recommendation systems affect the occurrence of malicious urls on facebook we exploit temporal features to build a prediction framework having greater than 75 accuracy to predict whether the following group users behavior will increase or not included in this work we demarcate classes of urls including those malicious urls classified as creating critical damage as well as those of a lesser nature which only inflict light damage such as aggressive commercial advertisements and spam content it is our hope that the data and analyses in this paper provide a better understanding of osn user reactions to different categories of malicious urls thereby providing a way to mitigate the influence of these malicious url attacks,0
agentbased modelling abm has emerged as an essential tool for simulating social networks encompassing diverse phenomena such as information dissemination influence dynamics and community formation however manually configuring varied agent interactions and information flow dynamics poses challenges often resulting in oversimplified models that lack realworld generalizability integrating modern large language models llms with abm presents a promising avenue to address these challenges and enhance simulation fidelity leveraging llms humanlike capabilities in sensing reasoning and behavior in this paper we propose a novel framework utilizing llmempowered agents to simulate social network users based on their interests and personality traits the framework allows for customizable agent interactions resembling various social network platforms including mechanisms for content resharing and personalized recommendations we validate our framework using a comprehensive twitter dataset from the 2020 us election demonstrating that llmagents accurately replicate real users behaviors including linguistic patterns and political inclinations these agents form homogeneous ideological clusters and retain the main themes of their community notably preferencebased recommendations significantly influence agent behavior promoting increased engagement network homophily and the formation of echo chambers overall our findings underscore the potential of llmagents in advancing social media simulations and unraveling intricate online dynamics,0
the wide use of social media sites and other digital technologies have resulted in an unprecedented availability of digital data that are being used to study human behavior across research domains although unsolicited opinions and sentiments are available on these platforms demographic details are usually missing demographic information is pertinent in fields such as demography and public health where significant differences can exist across sex racial and socioeconomic groups in an attempt to address this shortcoming a number of academic studies have proposed methods for inferring the demographics of social media users using details such as names usernames and network characteristics gender is the easiest trait to accurately infer with measures of accuracy higher than 90 percent in some studies race ethnicity and age tend to be more challenging to predict for a variety of reasons including the novelty of social media to certain age groups and a lack of significant deviations in user details across racial and ethnic groups although the endeavor to predict user demographics is plagued with ethical questions regarding privacy and data ownership knowing the demographics in a data sample can aid in addressing issues of bias and population representation so that existing societal inequalities are not exacerbated,0
tumblr as one of the most popular microblogging platforms has gained momentum recently it is reported to have 1664 millions of users and 734 billions of posts by january 2014 while many articles about tumblr have been published in major press there is not much scholar work so far in this paper we provide some pioneer analysis on tumblr from a variety of aspects we study the social network structure among tumblr users analyze its user generated content and describe reblogging patterns to analyze its user behavior we aim to provide a comprehensive statistical overview of tumblr and compare it with other popular social services including blogosphere twitter and facebook in answering a couple of key questions what is tumblr how is tumblr different from other social media networks in short we find tumblr has more rich content than other microblogging platforms and it contains hybrid characteristics of social networking traditional blogosphere and social media this work serves as an early snapshot of tumblr that later work can leverage,0
nowadays social media networks are increasingly significant to our lives the imperative to study social media networks becomes more and more essential with billions of users across platforms and constant updates the complexity of modeling social networks is immense agentbased modeling abm is widely employed to study social networks community allowing us to define individual behaviors and simulate systemlevel evolution it can be a powerful tool to test how the algorithms affect users behavior to fully leverage agentbased modelssuperior data processing and storage capabilities are essential high performance computing hpc presents an optimal solution adept at managing complex computations and analysis particularly for voluminous or iterationintensive tasks we utilize machine learning ml methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors preferences and trends therefore our proposal involves ml to characterize user attributes and to develop a general user model for abm simulation of in social networks on hpc systems,0
online social networks allow different agencies and the public to interact and share the underlying risks and protective actions during major disasters this study revealed such crisis communication patterns during hurricane laura compounded by the covid19 pandemic laura was one of the strongest category 4 hurricanes on record to make landfall in cameron louisiana using the application programming interface api this study utilizes largescale social media data obtained from twitter through the recently released academic track that provides complete and unbiased observations the data captured publicly available tweets shared by active twitter users from the vulnerable areas threatened by laura online social networks were based on user influence feature mentions or tags that allows notifying other users while posting a tweet using network science theories and advanced community detection algorithms the study split these networks into twentyone components of various sizes the largest of which contained eight welldefined communities several natural language processing techniques ie word clouds bigrams topic modeling were applied to the tweets shared by the users in these communities to observe their risktaking or riskaverse behavior during a major compounding crisis social media accounts of local news media radio universities and popular sports pages were among those who involved heavily and interacted closely with local residents in contrast emergency management and planning units in the area engaged less with the public the findings of this study provide novel insights into the design of efficient social media communication guidelines to respond better in future disasters,0
there has been an explosion of multimodal content generated on social media networks in the last few years which has necessitated a deeper understanding of social media content and user behavior we present a novel contentindependent contentuserreaction model for social multimedia content analysis compared to prior works that generally tackle semantic content understanding and user behavior modeling in isolation we propose a generalized solution to these problems within a unified framework we embed users images and text drawn from open social media in a common multimodal geometric space using a novel loss function designed to cope with distant and disparate modalities and thereby enable seamless threeway retrieval our model not only outperforms unimodal embedding based methods on crossmodal retrieval tasks but also shows improvements stemming from jointly solving the two tasks on twitter data we also show that the user embeddings learned within our joint multimodal embedding model are better at predicting user interests compared to those learned with unimodal content on instagram data our framework thus goes beyond the prior practice of using explicit leaderfollower link information to establish affiliations by extracting implicit contentcentric affiliations from isolated users we provide qualitative results to show that the user clusters emerging from learned embeddings have consistent semantics and the ability of our model to discover finegrained semantics from noisy and unstructured data our work reveals that social multimodal content is inherently multimodal and possesses a consistent structure because in social networks meaning is created through interactions between users and content,0
the emergence of online social networks has greatly facilitated the diffusion of information and behaviors while the two diffusion processes are often intertwined talking the talk does not necessarily mean walking the talkthose who share information about an action may not actually participate in it we do not know if the diffusion of information and behaviors are similar or if social influence plays an equally important role in these processes integrating text mining social network analyses and survival analysis this research examines the concurrent spread of information and behaviors related to the ice bucket challenge on twitter we show that the two processes follow different patterns unilateral social influence contributes to the diffusion of information but not to the diffusion of behaviors bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors but not of information these results have implications for theories of social influence social networks and contagion,0
an increasing number of todays social interactions occurs using online social media as communication channels some online social networks have become extremely popular in the last decade they differ among themselves in the character of the service they provide to online users for instance facebook can be seen mainly as a platform for keeping in touch with close friends and relatives twitter is used to propagate and receive news linkedin facilitates the maintenance of professional contacts flickr gathers amateurs and professionals of photography etc albeit different all these online platforms share an ingredient that pervades all their applications there exists an underlying social network that allows their users to keep in touch with each other and helps to engage them in common activities or interactions leading to a better fulfillment of the services purposes this is the reason why these platforms share a good number of functionalities eg personal communication channels broadcasted status updates easy onestep information sharing news feeds exposing broadcasted content etc as a result online social networks are an interesting field to study an online social behavior that seems to be generic among the different online services since at the bottom of these services lays a network of declared relations and the basic interactions in these platforms tend to be pairwise a natural methodology for studying these systems is provided by network science in this chapter we describe some of the results of research studies on the structure dynamics and social activity in online social networks we present them in the interdisciplinary context of network science sociological studies and computer science,0
echo chambers and opinion polarization recently quantified in several sociopolitical contexts and across different social media raise concerns on their potential impact on the spread of misinformation and on openness of debates despite increasing efforts the dynamics leading to the emergence of these phenomena stay unclear we propose a model that introduces the dynamics of radicalization as a reinforcing mechanism driving the evolution to extreme opinions from moderate initial conditions inspired by empirical findings on social interaction dynamics we consider agents characterized by heterogeneous activities and homophily we show that the transition between a global consensus and emerging radicalized states is mostly governed by social influence and by the controversialness of the topic discussed compared with empirical data of polarized debates on twitter the model qualitatively reproduces the observed relation between users engagement and opinions as well as opinion segregation in the interaction network our findings shed light on the mechanisms that may lie at the core of the emergence of echo chambers and polarization in social media,0
social botsautomated accounts that generate and spread content on social mediaare exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation this has prompted the development of public bot detection services however most of these services focus primarily on twitter leaving niche platforms vulnerable fringe social media platforms such as parler gab and gettr often have minimal moderation which facilitates the spread of hate speech and misinformation to address this gap we introduce entendre an openaccess scalable and platformagnostic bot detection framework entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach ensuring robust social bot detection we exploit the idea that most social platforms share a generic template where users can post content approve content and provide a bio common data features by emphasizing general data features over platformspecific ones entendre offers rapid extensibility at the expense of some accuracy to demonstrate entendres effectiveness we used it to explore the presence of bots among accounts posting racist content on the nowdefunct rightwing platform parler we examined 233000 posts from 38379 unique users and found that 1916 unique users 499 exhibited botlike behavior visualization techniques further revealed that these bots significantly impacted the network amplifying influential rhetoric and hashtags eg qanon trump antilgbt these preliminary findings underscore the need for tools like entendre to monitor and assess bot activity across diverse platforms,0
in the light of increasing clues on social media impact on selfharm and suicide risks there is still no evidence on who are and how factually engaged in suiciderelated online behaviors this study reports new findings of highperformance supercomputing investigation of publicly accessible big data sourced from one of the worldlargest social networking site threemonth supercomputer searching resulted in 570156 young adult users who consumed suiciderelated information on social media most of them were 2124 year olds with higher share of females 58 of predominantly younger age every eight user was alarmingly engrossed with up to 15 suiciderelated online groups evidently suicide groups on social media are highly underrated public health issue that might weaken the prevention efforts suicide prevention strategies that target social media users must be implemented extensively while major gap in functional understanding of technologies relevance for use in public mental health still exists current findings act for better understanding digital technologies utility for translational advance and offer relevant evidencebased framework for improving suicide prevention in general population,0
in the vast expanse of online communication identifying unilateral preference patterns can be pivotal in understanding and mitigating risks such as predatory behavior this paper presents a comprehensive approach to dissect and visualize such patterns in social networks through the lens of a directed network model we simulate a scenario where a predominant cluster a disperses information unilaterally towards a much larger but passive cluster b while being overseen by a vigilant cluster c restricted by an information blocking cluster d and countered by an alerting cluster e incorporated into this study is a simulation framework that models the flow of information across a directed network comprising various clusters with distinct roles and communication behaviors the simulation employs a dynamic system where clusters a through e interact over a series of time steps with each clusters activity shaped by both intrinsic messagegeneration rules and external media influences by tracking the accumulated media influence on each cluster we gain a nuanced understanding of the longterm effects of media on communication patterns the results provide a window into the cyclical nature of influence and the propagation of information with potential applications in detecting and mitigating unilateral communication patterns that could signal harmful activities such as online predation this study therefore presents a comprehensive approach that combines network theory simulation modeling and dynamic media influence analysis to explore and understand the complexities of unilateral preference communication within social networksthis paper is partially an attempt to utilize generative ai and was written with educational intent there are currently no plans for it to become a peerreviewed paper,0
nowadays massive useful data of user information and social behavior have been accumulated on the internet providing a possibility of profiling users personality traits online in this paper we propose a psychological modeling method based on computational linguistic features to profile big five personality traits of users on sina weibo a twitterlike microblogging service in china and their correlations with users social behaviors to the best of our knowledge this is the first research on investigating the potential relationship between profile information socialnetwork behaviors and personality traits of users on sina weibo our results demonstrate an effective modeling approach to understanding demographic and psychological portraits of users on social media without customer disruption which is useful for commercial incorporations to provide better personalized products and services,0
community partition is of great importance in social networks because of the rapid increasing network scale data and applications we consider the community partition problem under lt model in social networks which is a combinatorial optimization problem that divides the social network to disjoint m communities our goal is to maximize the sum of influence propagation through maximizing it within each community as the influence propagation function of community partition problem is supermodular under lt model we use the method of lovacuteasz extension to relax the target influence function and transfer our goal to maximize the relaxed function over a matroid polytope next we propose a continuous greedy algorithm using the properties of the relaxed function to solve our problem which needs to be discretized in concrete implementation then random rounding technique is used to convert the fractional solution to integer solution we present a theoretical analysis with 11e approximation ratio for the proposed algorithms extensive experiments are conducted to evaluate the performance of the proposed continuous greedy algorithms on realworld online social networks datasets and the results demonstrate that continuous community partition method can improve influence spread and accuracy of the community partition effectively,0
we examine the growth survival and context of 256 novel hashtags during the 2012 us presidential debates our analysis reveals the trajectories of hashtag use fall into two distinct classes winners that emerge more quickly and are sustained for longer periods of time than other alsorans hashtags we propose a conversational vibrancy framework to capture dynamics of hashtags based on their topicality interactivity diversity and prominence statistical analyses of the growth and persistence of hashtags reveal novel relationships between features of this framework and the relative success of hashtags specifically retweets always contribute to faster hashtag adoption replies extend the life of winners while having no effect on alsorans this is the first study on the lifecycle of hashtag adoption and use in response to purely exogenous shocks we draw on theories of uses and gratification organizational ecology and language evolution to discuss these findings and their implications for understanding social influence and collective action in social media more generally,0
nowadays many social media platforms are centered around content creators cc on these platforms the tie formation process depends on two factors a the exposure of users to ccs decided by eg a recommender system and b the following decisionmaking process of users recent research studies underlined the importance of content quality by showing that under exploratory recommendation strategies the network eventually converges to a state where the higher the quality of the cc the higher their expected number of followers in this paper we extend prior work by a looking beyond averages to assess the fairness of the process and b investigating the importance of exploratory recommendations for achieving fair outcomes using an analytical approach we show that nonexploratory recommendations converge fast but usually lead to unfair outcomes moreover even with exploration we are only guaranteed fair outcomes for the highest and lowest quality ccs,0
social media has been on the vanguard of political information diffusion in the 21st century most studies that look into disinformation political influence and fakenews focus on mainstream social media platforms this has inevitably made english an important factor in our current understanding of political activity on social media as a result there has only been a limited number of studies into a large portion of the world including the largest multilingual and multicultural democracy india in this paper we present our characterisation of a multilingual social network in india called sharechat we collect an exhaustive dataset across 72 weeks before and during the indian general elections of 2019 across 14 languages we investigate the cross lingual dynamics by clustering visually similar images together and exploring how they move across language barriers we find that telugu malayalam tamil and kannada languages tend to be dominant in soliciting political images often referred to as memes and posts from hindi have the largest crosslingual diffusion across sharechat as well as images containing text in english in the case of images containing text that cross language barriers we see that language translation is used to widen the accessibility that said we find cases where the same image is associated with very different text and therefore meanings this initial characterisation paves the way for more advanced pipelines to understand the dynamics of fake and political content in a multilingual and nontextual setting,0
the internet and social media have fueled enormous interest in social network analysis new tools continue to be developed and used to analyse our personal connections with particular emphasis on detecting communities or identifying key individuals in a social network this raises privacy concerns that are likely to exacerbate in the future with this in mind we ask the question can individuals or groups actively manage their connections to evade social network analysis tools by addressing this question the general public may better protect their privacy oppressed activist groups may better conceal their existence and security agencies may better understand how terrorists escape detection we first study how an individual can evade network centrality analysis without compromising his or her influence within the network we prove that an optimal solution to this problem is hard to compute despite this hardness we demonstrate that even a simple heuristic whereby attention is restricted to the individuals immediate neighbourhood can be surprisingly effective in practice for instance it could disguise mohamed attas leading position within the wtc terrorist network and that is by rewiring a strikinglysmall number of connections next we study how a community can increase the likelihood of being overlooked by communitydetection algorithms we propose a measure of concealment expressing how well a community is hidden and use it to demonstrate the effectiveness of a simple heuristic whereby members of the community either unfriend certain other members or befriend some nonmembers in a coordinated effort to camouflage their community,0
detection of community structures in social networks has attracted lots of attention in the domain of sociology and behavioral sciences social networks also exhibit dynamic nature as these networks change continuously with the passage of time social networks might also present a hierarchical structure led by individuals that play important roles in a society such as managers and decision makers detection and visualization of these networks changing over time is a challenging problem where communities change as a function of events taking place in the society and the role people play in it in this paper we address these issues by presenting a system to analyze dynamic social networks the proposed system is based on dynamic graph discretization and graph clustering the system allows detection of major structural changes taking place in social communities over time and reveals hierarchies by identifying influential people in a social networks we use two different data sets for the empirical evaluation and observe that our system helps to discover interesting facts about the social and hierarchical structures present in these social networks,0
influence maximization im aims to identify a small number of influential individuals to maximize the information spread and finds applications in various fields it was first introduced in the context of viral marketing where a company pays a few influencers to promote the product however apart from the cost factor the capacity of individuals to consume content poses challenges for implementing im in realworld scenarios for example players on online gaming platforms can only interact with a limited number of friends in addition we observe that in these scenarios i the initial adopters of promotion are likely to be the friends of influencers rather than the influencers themselves and ii existing im solutions produce subpar results with high computational demands motivated by these observations we propose a new im variant called capacity constrained influence maximization cim which aims to select a limited number of influential friends for each initial adopter such that the promotion can reach more users to solve cim effectively we design two greedy algorithms mggreedy and rrgreedy ensuring the 12approximation ratio to improve the efficiency we devise the scalable implementation named rropim with 12approximation and nearlinear running time we extensively evaluate the performance of 9 approaches on 6 realworld networks and our solutions outperform all competitors in terms of result quality and running time additionally we deploy rropim to online game scenarios which improves the baseline considerably,0
mental illnesses adversely affect a significant proportion of the population worldwide however the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are timeconsuming and expensive consequently bestavailable estimates concerning the prevalence of mental health conditions are often years out of date automated approaches to supplement these survey methods with broad aggregated information derived from social media content provides a potential means for near realtime estimates at scale these may in turn provide grist for supporting evaluating and iteratively improving upon public health programs and interventions we propose a novel model for automated mental health status quantification that incorporates user embeddings this builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories such embeddings capture latent characteristics of individuals eg political leanings and encode a soft notion of homophily in this paper we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses to this end we estimated user embeddings for a set of users known to be affected by depression and posttraumatic stress disorder ptsd and for a set of demographically matched control users we then evaluated these embeddings with respect to i their ability to capture homophilic relations with respect to mental health status and ii the performance of downstream mental health prediction models based on these features our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions and are predictive of mental health,0
online social media such as twitter and instagram democratized information broadcast allowing anyone to share information about themselves and their surroundings at an unprecedented scale the large volume of information thus posted on these media offer a new lens into the physical world through the eyes of the social network the exploitation of this lens to inspect aspects of world state has recently been termed social sensing the power of manipulating reality via the use or intentional misuse of social media opened concerns with issues ranging from radicalization by terror propaganda to potential manipulation of elections in mature democracies many important challenges and open research questions arise in this emerging field that aims to better understand how information can be extracted from the medium and what properties characterize the extracted information and the world it represents addressing the above challenges requires multidisciplinary research at the intersection of computer science and social sciences that combines cyberphysical computing sociology sensor networks social networks cognition data mining estimation theory data fusion information theory linguistics machine learning behavioral economics and possibly others this paper surveys important directions in social sensing identifies current research challenges and outlines avenues for future research,0
in this paper we introduce y a newgeneration digital twin designed to replicate an online social media platform digital twins are virtual replicas of physical systems that allow for advanced analyses and experimentation in the case of social media a digital twin such as y provides a powerful tool for researchers to simulate and understand complex online interactions tt y leverages stateoftheart large language models llms to replicate sophisticated agent behaviors enabling accurate simulations of user interactions content dissemination and network dynamics by integrating these aspects y offers valuable insights into user engagement information spread and the impact of platform policies moreover the integration of llms allows y to generate nuanced textual content and predict user responses facilitating the study of emergent phenomena in online environments to better characterize the proposed digital twin in this paper we describe the rationale behind its implementation provide examples of the analyses that can be performed on the data it enables to be generated and discuss its relevance for multidisciplinary research,0
human personality traits are the key drivers behind our decisionmaking influencing our life path on a daily basis inference of personality traits such as myersbriggs personality type as well as an understanding of dependencies between personality traits and users behavior on various social media platforms is of crucial importance to modern research and industry applications the emergence of diverse and crosspurpose social media avenues makes it possible to perform user personality profiling automatically and efficiently based on data represented across multiple data modalities however the research efforts on personality profiling from multisource multimodal social media data are relatively sparse and the level of impact of different social network data on machine learning performance has yet to be comprehensively evaluated furthermore there is not such dataset in the research community to benchmark this study is one of the first attempts towards bridging such an important research gap specifically in this work we infer the myersbriggs personality type indicators by applying a novel multiview fusion framework called pers and comparing the performance results not just across data modalities but also with respect to different social network data sources our experimental results demonstrate the perss ability to learn from multiview data for personality profiling by efficiently leveraging on the significantly different data arriving from diverse social multimedia sources we have also found that the selection of a machine learning approach is of crucial importance when choosing social network data sources and that people tend to reveal multiple facets of their personality in different social media avenues our released social multimedia dataset facilitates future research on this direction,0
the papageno effect concerns how media can play a positive role in preventing and mitigating suicidal ideation and behaviors with the increasing ubiquity and widespread use of social media individuals often express and share lived experiences and struggles with mental health however there is a gap in our understanding about the existence and effectiveness of the papageno effect in social media which we study in this paper in particular we adopt a causalinference framework to examine the impact of exposure to mental health coping stories on individuals on twitter we obtain a twitter dataset with sim2m posts by sim10k individuals we consider engaging with coping stories as the treatment intervention and adopt a stratified propensity score approach to find matched cohorts of treatment and control individuals we measure the psychosocial shifts in affective behavioral and cognitive outcomes in longitudinal twitter data before and after engaging with the coping stories our findings reveal that engaging with coping stories leads to decreased stress and depression and improved expressive writing diversity and interactivity our work discusses the practical and platform design implications in supporting mental wellbeing,0
we investigate the political roles of internet trolls in social media political trolls such as the ones linked to the russian internet research agency ira have recently gained enormous attention for their ability to sway public opinion and even influence elections analysis of the online traces of trolls has shown different behavioral patterns which target different slices of the population however this analysis is manual and laborintensive thus making it impractical as a firstresponse tool for newlydiscovered troll farms in this paper we show how to automate this analysis by using machine learning in a realistic setting in particular we show how to classify trolls according to their political role left news feed right by using features extracted from social media ie twitter in two scenarios i in a traditional supervised learning scenario where labels for trolls are available and ii in a distant supervision scenario where labels for trolls are not available and we rely on morecommonlyavailable labels for news outlets mentioned by the trolls technically we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph from which we extract several types of learned representations ieembeddings for the trolls experiments on the ira russian troll dataset show that our methodology improves over the stateoftheart in the first scenario while providing a compelling case for the second scenario which has not been explored in the literature thus far,0
the rapid growth of social media presents a unique opportunity to study coordinated agent behavior in an unfiltered environment online processes often exhibit complex structures that reflect the nature of the user behavior whether it is authentic and genuine or part of a coordinated effort by malicious agents to spread misinformation and disinformation detection of aigenerated content can be extremely challenging due to the high quality of large language modelgenerated text therefore approaches that use metadata like post timings are required to effectively detect coordinated aidriven campaigns existing work that models the spread of information online is limited in its ability to represent different control flows that occur within the network in practice process mining offers techniques for the discovery of process models with different routing constructs and are yet to be applied to social networks we propose to leverage process mining methods for the discovery of ai and human agent behavior within social networks applying process mining techniques to realworld twitter now x event data we demonstrate how the structural and behavioral properties of discovered process models can reveal coordinated ai and human behaviors online,0
current society is heavily influenced by the spread of online information containing all sorts of claims commonly found in news stories tweets and social media postings depending on the user they may be considered true or false according to the agents trust on the claim in this paper we discuss the concept of content trust and trust process and propose a framework to describe the trust process which can support various possible models of content trust,0
the broad adoption of the web as a communication medium has made it possible to study social behavior at a new scale with social media networks such as twitter we can collect large data sets of online discourse social science researchers and journalists however may not have tools available to make sense of large amounts of data or of the structure of large social networks in this paper we describe our recent extensions to truthy a system for collecting and analyzing political discourse on twitter we introduce several new analytical perspectives on online discourse with the goal of facilitating collaboration between individuals in the computational and social sciences the design decisions described in this article are motivated by realworld use cases developed in collaboration with colleagues at the indiana university school of journalism,0
we study the problem of real time data capture on social media due to the different limitations imposed by those media but also to the very large amount of information it is impossible to collect all the data produced by social networks such as twitter therefore to be able to gather enough relevant information related to a predefined need it is necessary to focus on a subset of the information sources in this work we focus on usercentered data capture and consider each account of a social network as a source that can be listened to at each iteration of a data capture process in order to collect the corresponding produced contents this process whose aim is to maximize the quality of the information gathered is constrained by the number of users that can be monitored simultaneously the problem of selecting a subset of accounts to listen to over time is a sequential decision problem under constraints which we formalize as a bandit problem with multiple selections therefore we propose several bandit models to identify the most relevant users in real time first we study of the case of the stochastic bandit in which each user corresponds to a stationary distribution then we introduce two contextual bandit models one stationary and the other non stationary in which the utility of each user can be estimated by assuming some underlying structure in the reward space the first approach introduces the notion of profile which corresponds to the average behavior of a user the second approach takes into account the activity of a user in order to predict his future behavior finally we are interested in models that are able to tackle complex temporal dependencies between users with the use of a latent space within which the information transits from one iteration to the other each of the proposed approaches is validated on both artificial and real datasets,0
how far and how fast does information spread in social media researchers have recently examined a number of factors that affect information diffusion in online social networks including the novelty of information users activity levels who they pay attention to and how they respond to friends recommendations using urls as markers of information we carry out a detailed study of retweeting the primary mechanism by which information spreads on the twitter follower graph our empirical study examines how users respond to an incoming stimulus ie a tweet message from a friend and reveals that retweeting behavior is constrained by a few simple principles the principle of least effort combined with limited attention plays a dominant role in retweeting behavior specifically we observe that users retweet information when it is most visible such as when it near the top of their twitter stream moreover our measurements quantify how a users limited attention is divided among incoming tweets providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet our study indicates that the finite ability to process incoming information constrains social contagion and we conclude that rapid decay of visibility is the primary barrier to information propagation online,0
studying extreme ideas in routine choices and discussions is of utmost importance to understand the increasing polarization in society in this study we focus on understanding the generation and influence of extreme ideas in routine conversations which we label eccentric ideas the eccentricity of any idea is defined as the deviation of that idea from the norm of the social neighborhood we collected and analyzed data from two completely different sources public social media and online experiments in a controlled environment we compared the popularity of ideas against their eccentricity to understand individuals fascination towards eccentricity we found that more eccentric ideas have a higher probability of getting a greater number of likes additionally we demonstrate that the social neighborhood of an individual conceals eccentricity changes in ones own opinions and facilitates generation of eccentric ideas at a collective level,0
although social neuroscience is concerned with understanding how the brain interacts with its social environment prevailing research in the field has primarily considered the human brain in isolation deprived of its rich social context emerging work in social neuroscience that leverages tools from network analysis has begun to pursue this issue advancing knowledge of how the human brain influences and is influenced by the structures of its social environment in this paper we provide an overview of key theory and methods in network analysis especially for social systems as an introduction for social neuroscientists who are interested in relating individual cognition to the structures of an individuals social environments we also highlight some exciting new work as examples of how to productively use these tools to investigate questions of relevance to social neuroscientists we include tutorials to help with practical implementation of the concepts that we discuss we conclude by highlighting a broad range of exciting research opportunities for social neuroscientists who are interested in using network analysis to study social systems,0
twitter is currently a popular online social media platform which allows users to share their usergenerated content this publiclygenerated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways one of the applications is in automatically discovering mental health problems eg depression previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including users social interactions the downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user besides these content have a negative impact on the overall efficiency and effectiveness of the model to overcome the shortcomings in the existing automatic depression detection methods we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more finegrained and relevant content the content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of convolutional neural network cnn coupled with attentionenhanced gated recurrent units gru models leading to better empirical performance than existing strong baselines,0
social botnets have become an important phenomenon on social media there are many ways in which social bots can disrupt or influence online discourse such as spam hashtags scam twitter users and astroturfing in this paper we considered one specific social botnet in twitter to understand how it grows over time how the content of tweets by the social botnet differ from regular users in the same dataset and lastly how the social botnet may have influenced the relevant discussions our analysis is based on a qualitative coding for approximately 3000 tweets in arabic and english from the syrian social bot that was active for 35 weeks on twitter before it was shutdown we find that the growth behavior and content of this particular botnet did not specifically align with common conceptions of botnets further we identify interesting aspects of the botnet that distinguish it from regular users,0
as a countermeasure against misinformation that undermines the healthy use of social media a preventive intervention known as textitprebunking has recently attracted attention in the field of psychology prebunking aims to strengthen individuals cognitive resistance to misinformation by presenting weakened doses of misinformation or by teaching common manipulation techniques before they encounter actual misinformation despite the growing body of evidence supporting its effectiveness in reducing susceptibility to misinformation at the individual level an important open question remains how best to identify the optimal targets for prebunking interventions to mitigate the spread of misinformation in a social network to address this issue we formulate a combinatorial optimization problem called the textitnetwork prebunking problem which aims to select optimal prebunking targets that minimizes the spread of misinformation in a social network under limited intervention budgets we show that the problem is nphard and that its objective function is monotone and submodular which provides a theoretical foundation for approximation guarantees of greedy algorithms however since the greedy algorithm is computationally expensive and does not scale to large networks we propose an efficient approximation algorithm mianpp based on the maximum influence arborescence mia approach which restricts influence propagation around each node to a local directed tree rooted at that node through numerical experiments using realworld social network datasets we demonstrate that mianpp effectively suppresses the spread of misinformation under both fully observed and uncertain model parameter settings,0
largescale networks have been instrumental in shaping the way that we think about how individuals interact with one another developing key insights in mathematical epidemiology computational social science and biology however many of the underlying social systems through which diseases spread information disseminates and individuals interact are inherently mediated through groups of arbitrary size known as higherorder interactions there is a gap between higherorder dynamics of group formation and fragmentation contagion spread and social influence and the data necessary to validate these higherorder mechanisms similarly few datasets bridge the gap between these pairwise and higherorder network data because of its open api the bluesky social media platform provides a laboratory for observing social ties at scale in addition to pairwise following relationships unlike many other social networks bluesky features usercurated lists known as starter packs as a mechanism for social network growth we introduce a blue start a largescale network dataset comprising 267m users and their 16b pairwise following relationships and 3013k groups representing starter packs this dataset will be an essential resource for the study of higherorder network science,0
sleep behavior significantly impacts health and acts as an indicator of physical and mental wellbeing monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions while sleep behavior depends on and is reflected in the physiology of a person it is also impacted by external factors such as digital media usage social network contagion and the surrounding weather in this work we propose sleepnet a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting nextday sleep labels about sleep duration our architecture overcomes the limitations of largescale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism the extensive experimental evaluation highlights the improvement provided by incorporating social networks in the model additionally we conduct robustness analysis to demonstrate the systems performance in reallife conditions the outcomes affirm the stability of sleepnet against perturbations in input data further analyses emphasize the significance of network topology in prediction performance revealing that users with higher eigenvalue centrality are more vulnerable to data perturbations,0
the blue whale challenge is a series of selfharm causing tasks that are propagated via online social media under the disguise of a game the list of tasks must be completed in a duration of 50 days and they cause both physical and mental harm to the player the final task is to commit suicide the game is supposed to be administered by people called curators who incite others to cause selfmutilation and commit suicide the curators and potential players are known to contact each other on social networking websites and the conversations between them are suspected to take place mainly via direct messages which are difficult to track though in order to find curators the players make public posts containing certain hashtagskeywords to catch their attention even though a lot of these social networks have moderated posts talking about the game yet some posts manage to pass their filters our research focuses on 1 understanding the social media spread of the challenge 2 spotting the behaviour of the people taking interest in blue whale challenge and 3 analysing demographics of the users who may be involved in playing the game,0
there is great interest in the dynamics of health behaviors in social networks and how they affect collective public health outcomes but measuring population health behaviors over time and space requires substantial resources here we use publicly available data from 101853 users of online social media collected over a time period of almost six months to measure the spatiotemporal sentiment towards a new vaccine we validated our approach by identifying a strong correlation between sentiments expressed online and cdc estimated vaccination rates by region analysis of the network of opinionated users showed that information flows more often between users who share the same sentiments and less often between users who do not share the same sentiments than expected by chance alone we also found that most communities are dominated by either positive or negative sentiments towards the novel vaccine simulations of infectious disease transmission show that if clusters of negative vaccine sentiments lead to clusters of unprotected individuals the likelihood of disease outbreaks are greatly increased online social media provide unprecedented access to data allowing for inexpensive and efficient tools to identify target areas for intervention efforts and to evaluate their effectiveness,0
in this paper we consider how to maximize users influence in online social networks osns by exploiting social relationships only our first contribution is to extend to osns the model of kempe et al on the propagation of information in a social network and to show that a greedy algorithm is a good approximation of the optimal algorithm that is nphard however the greedy algorithm requires global knowledge which is hardly practical our second contribution is to show on simulations on the full twitter social graph that simple and practical strategies perform close to the greedy algorithm,0
a books successpopularity depends on various parameters extrinsic and intrinsic in this paper we study how the book reading characteristics might influence the popularity of a book towards this objective we perform a crossplatform study of goodreads entities and attempt to establish the connection between various goodreads entities and the popular books amazon best sellers we analyze the collective reading behavior on goodreads platform and quantify various characteristic features of the goodreads entities to identify differences between these amazon best sellers abs and the other nonbest selling books we then develop a prediction model using the characteristic features to predict if a book shall become a best seller after one month 15 days since its publication on a balanced set we are able to achieve a very high average accuracy of 8872 8566 for the prediction where the other competitive class contains books which are randomly selected from the goodreads dataset our method primarily based on features derived from user posts and genre related characteristic properties achieves an improvement of 164 over the traditional popularity factors ratings reviews based baseline methods we also evaluate our model with two more competitive set of books a that are both highly rated and have received a large number of reviews but are not best sellers hrhr and b goodreads choice awards nominated books which are nonbest sellers gcan we are able to achieve quite good results with very high average accuracy of 871 and as well a high roc for abs vs gcan for abs vs hrhr our model yields a high average accuracy of 8622,0
the emergence of online social platforms such as social networks and social media has drastically affected the way people apprehend the information flows to which they are exposed in such platforms various information cascades spreading among users is the main force creating complex dynamics of opinion formation each user being characterized by their own behavior adoption mechanism moreover the spread of multiple pieces of information or beliefs in a networked population is rarely uncorrelated in this paper we introduce the mixture of interacting cascades mic a model of marked multidimensional hawkes processes with the capacity to model jointly nontrivial interaction between cascades and users we emphasize on the interplay between information cascades and user activity and use a mixture of temporal point processes to build a coupled usercascade point process model experiments on synthetic and real data highlight the benefits of this approach and demonstrate that mic achieves superior performance to existing methods in modeling the spread of information cascades finally we demonstrate how mic can provide through its learned parameters insightful bilayered visualizations of real social network activity data,0
mindset reconstruction maps how individuals structure and perceive knowledge a map unfolded here by investigating language and its cognitive reflection in the human mind ie the mental lexicon textual forma mentis networks tfmn are glass boxes introduced for extracting representing and understanding mindsets structure in latin forma mentis from textual data combining network science psycholinguistics and big data tfmns successfully identified relevant concepts without supervision in benchmark texts once validated tfmns were applied to the case study of the gender gap in science which was strongly linked to distorted mindsets by recent studies focusing over social media perception and online discourse this work analysed 10000 relevant tweets gender and gap elicited a mostly positive perception with a trustfuljoyous emotional profile and semantic associates that celebrated successful female scientists related gender gap to wage differences and hoped for a future resolution the perception of woman highlighted discussion about sexual harassment and stereotype threat a form of implicit cognitive bias relative to women in science sacrificing personal skills for success the reconstructed perception of man highlighted social users awareness of the myth of male superiority in science no anger was detected around person suggesting that gapfocused discourse got less tense around genderless terms no stereotypical perception of scientist was identified online differently from realworld surveys the overall analysis identified the online discourse as promoting a mostly stereotypefree positivetrustful perception of gender disparity aware of implicitexplicit biases and projected to closing the gap tfmns opened new ways for investigating perceptions in different groups offering detailed datainformed grounding for policy making,0
extreme weather events driven by climate change such as wildfires floods and heatwaves prompt significant public reactions on social media platforms analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception inform policy decisions and enhance emergency responses although sentiment analysis has been widely studied in various fields its specific application to climateinduced events particularly in realtime highimpact situations like the 2025 los angeles forest fires remains underexplored in this survey we thoroughly examine the methods datasets challenges and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events we present a detailed taxonomy of approaches ranging from lexiconbased and machine learning models to the latest strategies driven by large language models llms additionally we discuss data collection and annotation techniques including weak supervision and realtime event tracking finally we highlight several open problems such as misinformation detection multimodal sentiment extraction and model alignment with human values our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era,0
this paper addresses influence pathway discovery a key emerging problem in todays online media we propose a discovery algorithm that leverages recently published work on unsupervised interpretable ideological embedding a mapping of ideological beliefs done in a selfsupervised fashion into interpretable lowdimensional spaces computing the ideological embedding at scale allows one to analyze correlations between the ideological positions of leaders influencers news portals or population segments deriving potential influence pathways the work is motivated by the importance of social media as the preeminent means for global interactions and collaborations on todays internet as well as their frequent misuse to wield influence that targets social beliefs and attitudes of selected populations tools that enable the understanding and mapping of influence propagation through population segments on social media are therefore increasingly important in this paper influence is measured by the perceived ideological shift over time that is correlated with influencers activity correlated shifts in ideological embeddings indicate changes such as swingsswitching among competing ideologies polarization depletion of neutral ideological positions escalationradicalization shifts to more extreme versions of the ideology or unificationcooldown shifts towards more neutral stances casestudies are presented to explore selected influence pathways i in a recent french election ii during political discussions in the philippines and iii for some russian messaging during the russiaukraine conflict,0
we present in this paper a novel approach for asyoutype topk keyword search over social media we adopt a natural networkaware interpretation for information relevance by which information produced by users who are closer to the seeker is considered more relevant in practice this query model poses new challenges for effectiveness and efficiency in online search even when a complete query is given as input in one keystroke this is mainly because it requires a joint exploration of the social space and classic ir indexes such as inverted lists we describe a memoryefficient and incremental prefixbased retrieval algorithm which also exhibits an anytime behavior allowing to output the most likely answer within any chosen runningtime limit we evaluate it through extensive experiments for several applications and search scenarios including searching for posts in microblogging twitter and tumblr as well as searching for businesses based on reviews in yelp they show that our solution is effective in answering realtime asyoutype searches over social media,0
an increasing number of people use wearables and other smart devices to quantify various health conditions ranging from sleep patterns to body weight to heart rates of these quantified selfs many choose to openly share their data via online social networks such as twitter and facebook in this study we use data for users who have chosen to connect their smart scales to twitter providing both a reliable time series of their body weight as well as insights into their social surroundings and general online behavior concretely we look at which social media features are predictive of physical status such as body weight at the individual level and activity patterns at the population level we show that it is possible to predict an individuals weight using their online social behaviors such as their selfdescription and tweets weekly and monthly patterns of quantifiedself behaviors are also discovered these findings could contribute to building models to monitor public health and to have more customized personal training interventions while there are many studies using either quantified self or social media data in isolation this is one of the few that combines the two data sources and to the best of our knowledge the only one that uses public data,0
in a social network adoption probability refers to the probability that a social entity will adopt a product service or opinion in the foreseeable future such probabilities are central to fundamental issues in social network analysis including the influence maximization problem in practice adoption probabilities have significant implications for applications ranging from social networkbased target marketing to political campaigns yet predicting adoption probabilities has not received sufficient research attention building on relevant social network theories we identify and operationalize key factors that affect adoption decisions social influence structural equivalence entity similarity and confounding factors we then develop the locallyweighted expectationmaximization method for nave bayesian learning to predict adoption probabilities on the basis of these factors the principal challenge addressed in this study is how to predict adoption probabilities in the presence of confounding factors that are generally unobserved using data from two largescale social networks we demonstrate the effectiveness of the proposed method the empirical results also suggest that cascade methods primarily using social influence to predict adoption probabilities offer limited predictive power and that confounding factors are critical to adoption probability predictions,0
social reputation eg likes comments shares etc on youtube is the primary tenet to popularize channelsvideos however the organic way to improve social reputation is tedious which often provokes content creators to seek the services of online blackmarkets for rapidly inflating content reputation such blackmarkets act underneath a thriving collusive ecosystem comprising core users and compromised accounts together known as collusive users core users form the backbone of blackmarkets thus spotting and suspending them may help in destabilizing the entire collusive network although a few studies focused on collusive user detection on twitter facebook and youtube none of them differentiate between core users and compromised accounts we are the first to present a rigorous analysis of core users in youtube blackmarkets to this end we collect a new dataset of collusive youtube users we study the coreperiphery structure of the underlying collusive commenting network ccn we examine the topology of ccn to explore the behavioral dynamics of core and compromised users we then introduce korse a novel graphbased method to automatically detect core users based only on the topological structure of ccn korse performs a weighted kcore decomposition using our proposed metric called weighted internal core collusive index wicci however korse is infeasible to adopt in practice as it requires complete interactions among collusive users to construct ccn we therefore propose nurse a deep fusion framework that only leverages user timelines without considering the underlying ccn to detect core blackmarket users experimental results show that nurse is quite close to korse in detecting core users and outperforms nine baselines,0
the united states is currently experiencing an unprecedented opioid crisis and opioid overdose has become a leading cause of injury and death effective opioid addiction recovery calls for not only medical treatments but also behavioral interventions for impacted individuals in this paper we study communication and behavior patterns of patients with opioid use disorder oud from social media intending to demonstrate how existing information from common activities such as online social networking might lead to better prediction evaluation and ultimately prevention of relapses through a multidisciplinary and advanced novel analytic perspective we characterize opioid addiction behavior patterns by analyzing opioid groups from redditcom including modeling online discussion topics analyzing text cooccurrence and correlations and identifying emotional states of people with oud these quantitative analyses are of practical importance and demonstrate innovative ways to use information from online social media to create technology that can assist in relapse prevention,0
human interactions in the online world comprise a combination of positive and negative exchanges these diverse interactions can be captured using signed network representations where edges take positive or negative weights to indicate the sentiment of the interaction between individuals signed networks offer valuable insights into online political polarization by capturing antagonistic interactions and ideological divides on social media platforms this study analyzes polarization on meneame a spanish social media platform that facilitates engagement with news stories through comments and voting using a dualmethod approach signed hamiltonian eigenvector embedding for proximity sheep for signed networks and correspondence analysis ca for unsigned networks we investigate how including negative ties enhances the understanding of structural polarization levels across different conversation topics on the platform while the unsigned meneame network effectively delineates ideological communities only by incorporating negative ties can we identify ideologically extreme users who engage in antagonistic behaviors without them the most extreme users remain indistinguishable from their less confrontational ideological peers,0
the covid19 pandemic has affected all aspects of society not only bringing health hazards but also posing challenges to public order governments and mental health moreover it is the first one in history in which people from around the world uses social media to massively express their thoughts and concerns this study aims at examining the stages of crisis response and recovery as a sociological problem by operationalizing a wellknown model of crisis stages in terms of a psycholinguistic analysis based on a large collection of twitter data spanning from march to august 2020 in argentina we present a thematic analysis on the differences in language used in social media posts and look at indicators that reveal the different stages of a crisis and the country response thereof the analysis was combined with a study of the temporal prevalence of mental health conversations across the time span beyond the argentinian casestudy the proposed approach and analyses can be applied to any public largescale data this approach can provide insights for the design of public health politics oriented to monitor and eventually intervene during the different stages of a crisis and thus improve the adverse mental health effects on the population,0
recent studies in network science and control have shown a meaningful relationship between the epidemic processes eg covid19 spread and some network properties this paper studies how such network properties namely clustering coefficient and centrality measures or node influence metrics affect the spread of viruses and the growth of epidemics over scalefree networks the results can be used to target individuals the nodes in the network to textitflatten the infection curve this socalled flattening of the infection curve is to reduce the health service costs and burden to the authoritiesgovernments our montecarlo simulation results show that clustered networks are in general easier to flatten the infection curve ie with the same connectivity and the same number of isolated individuals they result in more flattened curves moreover distancebased centrality measures which target the nodes based on their average network distance to other nodes and not the node degrees are better choices for targeting individuals for isolationvaccination,0
understanding and predicting the popularity of online items is an important open problem in social media analysis considerable progress has been made recently in datadriven predictions and in linking popularity to external promotions however the existing methods typically focus on a single source of external influence whereas for many types of online content such as youtube videos or news articles attention is driven by multiple heterogeneous sources simultaneously eg microblogs or traditional media coverage here we propose rnnmas a recurrent neural network for modeling asynchronous streams it is a sequence generator that connects multiple streams of different granularity via joint inference we show rnnmas not only to outperform the current stateoftheart youtube popularity prediction system by 17 but also to capture complex dynamics such as seasonal trends of unseen influence we define two new metrics promotion score quantifies the gain in popularity from one unit of promotion for a youtube video the loudness level captures the effects of a particular user tweeting about the video we use the loudness level to compare the effects of a video being promoted by a single highlyfollowed user in the top 1 most followed users against being promoted by a group of midfollowed users we find that results depend on the type of content being promoted superusers are more successful in promoting howto and gaming videos whereas the cohort of regular users are more influential for activism videos this work provides more accurate and explainable popularity predictions as well as computational tools for content producers and marketers to allocate resources for promotion campaigns,0
precision medicine has received attention both in and outside the clinic we focus on the latter by exploiting the relationship between individuals social interactions and their mental health to develop a predictive model of ones likelihood to be depressed or anxious from rich dynamic social network data to our knowledge we are the first to do this existing studies differ from our work in at least one aspect they do not model social interaction data as a network they do so but analyze static network data they examine correlation between social networks and health but without developing a predictive model or they study other individual traits but not mental health in a systematic and comprehensive evaluation we show that our predictive model that uses dynamic social network data is superior to its static network as well as nonnetwork equivalents when run on the same data,0
computations related to learning processes within an organizational social network area require some network model preparation and specific algorithms in order to implement human behaviors in simulated environments the proposals in this research model of collaborative learning in an organizational social network are based on knowledge resource distribution through the establishment of a knowledge flow the nodes which represent knowledge workers contain information about workers social and cognitive abilities moreover the workers are described by their set of competences their skill level and the collaborative learning behavior that can be detected through knowledge flow analysis the proposed approach assumes that an increase in workers competence is a result of collaborative learning in other words collaborative learning can be analyzed as a process of knowledge flow that is being broadcast in a network in order to create a more effective organizational social network for colearning the authors found the best strategies for knowledge facilitator knowledge collector and expert roles allocation special attention is paid to the process of knowledge flow in the community of practice acceleration within the community of practice happens when knowledge flows more effectively between community members the presented procedure makes it possible to add new ties to the community of practice in order to influence community members competences both the proposed allocation and acceleration approaches were confirmed through simulations,0
how does network structure affect diffusion recent studies suggest that the answer depends on the type of contagion complex contagions unlike infectious diseases simple contagions are affected by social reinforcement and homophily hence the spread within highly clustered communities is enhanced while diffusion across communities is hampered a common hypothesis is that memes and behaviors are complex contagions we show that while most memes indeed behave like complex contagions a few viral memes spread across many communities like diseases we demonstrate that the future popularity of a meme can be predicted by quantifying its early spreading pattern in terms of community concentration the more communities a meme permeates the more viral it is we present a practical method to translate data about community structure into predictive knowledge about what information will spread widely this connection may lead to significant advances in computational social science social media analytics and marketing applications,0
the social networking era has left us with little privacy the details of the social network users are published on social networking sites vulnerability has reached new heights due to the overpowering effects of social networking the sites like facebook twitter are having a huge set of users who publish their files comments messages in other users walls these messages and comments could be of any nature even friends could post a comment that would harm a persons integrity thus there has to be a system which will monitor the messages and comments that are posted on the walls if the messages are found to be neutral does not have any harmful content then it can be published if the messages are found to have nonneutral content in them then these messages would be blocked by the social network manager the messages that are nonneutral would be of sexual offensive hatred pun intended nature thus the social network manager can classify content as neutral and nonneutral and notify the user if there seems to be messages of nonneutral behavior,0
network autocorrelation models are widely used to evaluate the impact of social influence on some variable of interest this is a large class of models that parsimoniously accounts for how ones neighbors influence ones own behaviors or opinions by incorporating the network adjacency matrix into the joint distribution of the data these models assume homogeneous susceptibility to social influence however which may be a strong assumption in many contexts this paper proposes a hierarchical model that allows the influence parameter to be a function of individual attributes andor of local network topological features we derive an approximation of the posterior distribution in a general framework that is applicable to the durbin network effects network disturbances or network moving average autocorrelation models the proposed approach can also be applied to investigating determinants of social influence in the context of egocentric network data we apply our method to a data set collected via mobile phones in which we determine the effect of social influence on physical activity levels as well as classroom data in which we investigate peer influence on student defiance with this last data set we also investigate the performance of the proposed egocentric network model,0
since its inception facebook has become an integral part of the online social community people rely on facebook to make connections with others and build communities as a result it is paramount to protect the integrity of such a rapidly growing network in a fast and scalable manner in this paper we present our efforts to protect various social media entities at facebook from people who try to abuse our platform we present a novel temporal interaction embeddings ties model that is designed to capture rogue social interactions and flag them for further suitable actions ties is a supervised deep learning production ready model at facebookscale networks prior works on integrity problems are mostly focused on capturing either only static or certain dynamic features of social entities in contrast ties can capture both these variant behaviors in a unified model owing to the recent strides made in the domains of graph embedding and deep sequential pattern learning to show the realworld impact of ties we present a few applications especially for preventing spread of misinformation fake account detection and reducing ads payment risks in order to enhance the platforms integrity,0
new technologies allow to store vast amount of data about users interaction from those data the social network can be created additionally because usually also time and dates of this activities are stored the dynamic of such network can be analysed by splitting it into many timeframes representing the state of the network during specific period of time one of the most interesting issue is group evolution over time to track group evolution the ged method can be used however choice of the timeframe type and length might have great influence on the method results therefore in this paper the influence of timeframe type as well as timeframe length on the ged method results is extensively analysed,0
social media is one of the main sources for news consumption especially among the younger generation with the increasing popularity of news consumption on various social media platforms there has been a surge of misinformation which includes false information or unfounded claims as various text and social contextbased fake news detectors are proposed to detect misinformation on social media recent works start to focus on the vulnerabilities of fake news detectors in this paper we present the first adversarial attack framework against graph neural network gnnbased fake news detectors to probe their robustness specifically we leverage a multiagent reinforcement learning marl framework to simulate the adversarial behavior of fraudsters on social media research has shown that in realworld settings fraudsters coordinate with each other to share different news in order to evade the detection of fake news detectors therefore we modeled our marl framework as a markov game with bot cyborg and crowd worker agents which have their own distinctive cost budget and influence we then use deep qlearning to search for the optimal policy that maximizes the rewards extensive experimental results on two realworld fake news propagation datasets demonstrate that our proposed framework can effectively sabotage the gnnbased fake news detector performance we hope this paper can provide insights for future research on fake news detection,0
in the context of twitter social capitalists are specific users trying to increase their number of followers and interactions by any means these users are not healthy for the service because they are either spammers or real users flawing the notions of influence and visibility studying their behavior and understanding their position in twitter is thus of important interest it is also necessary to analyze how these methods effectively affect user visibility based on a recently proposed method allowing to identify social capitalists we tackle both points by studying how they are organized and how their links spread across the twitter followerfollowee network to that aim we consider their position in the network wrt its community structure we use the concept of community role of a node which describes its position in a network depending on its connectivity at the community level however the topological measures originally defined to characterize these roles consider only certain aspects of the communityrelated connectivity and rely on a set of empirically fixed thresholds we first show the limitations of these measures before extending and generalizing them moreover we use an unsupervised approach to identify the roles in order to provide more flexibility relatively to the studied system we then apply our method to the case of social capitalists and show they are highly visible on twitter due to the specific roles they hold,0
this paper investigates the cost function learning in social information networks wherein the influence of humans memory on information consumption is explicitly taken into account we first propose a model for social informationdiffusion dynamics with a focus on systematic modeling of asymmetric cognitive bias represented by confirmation bias and novelty bias building on the proposed social model we then propose the m3irl a model and maximumentropy based inverse reinforcement learning framework for learning the cost functions of target individuals in the memorized social networks compared with the existing bayesian irl maximum entropy irl relative entropy irl and maximum causal entropy irl the characteristics of m3irl are significantly different here no dependency on the markov decision process principle the need of only a single finitetime trajectory sample and bounded decision variables finally the effectiveness of the proposed social informationdiffusion model and the m3irl algorithm are validated by the online social media data,0
decentralized social media platforms like bluesky social bluesky have made it possible to publicly disclose some user behaviors with millisecondlevel precision embracing blueskys principles of opensource and opendata we present the first collection of the temporal dynamics of userdriven social interactions bluetempnet integrates multiple types of networks into a single multinetwork including usertouser interactions following and blocking users and usertocommunity interactions creating and joining communities communities are userformed groups in custom feeds where users subscribe to posts aligned with their interests following blueskys public data policy we collect existing bluesky feeds including the users who liked and generated these feeds and provide tools to gather users social interactions within a date range this datacollection strategy captures past user behaviors and supports the future data collection of user behavior,0
social media data has been increasingly used to study biomedical and healthrelated phenomena from cohort level discussions of a condition to planetary level analyses of sentiment social media has provided scientists with unprecedented amounts of data to study human behavior and response associated with a variety of health conditions and medical treatments here we review recent work in mining social media for biomedical epidemiological and social phenomena information relevant to the multilevel complexity of human health we pay particular attention to topics where social media data analysis has shown the most progress including pharmacovigilance sentiment analysis especially for mental health and other areas we also discuss a variety of innovative uses of social media data for healthrelated applications and important limitations in social media data access and use,0
in an online social network osn users can create a unique public persona by crafting a user identity that may encompass profile details content and networkrelated information as a result a relevant task of interest is related to the ability to link identities across different osns linking users across social networks can have multiple implications in several contexts both at the individual level and at the group level at the individual level the main interest in linking the same identity across social networks is to enable a better knowledge of each user at the group level linking user identities through different osns helps in predicting user behaviors network dynamics information diffusion and migration phenomena across social media the process of tying together user accounts on different osns is challenging and has attracted more and more research attention in the last fifteen years the purpose of this work is to provide a comprehensive review of recent studies from 2016 to the present on user identity linkage uil methods across online social networks this review aims to offer guidance for other researchers in the field by outlining the main problem formulations the different feature extraction strategies algorithms machine learning models datasets and evaluation metrics proposed by researchers working in this area the proposed overview takes a pragmatic perspective to highlight the concrete possibilities for accomplishing this task depending on the type of available data,0
cyberbullying is a growing problem affecting more than half of all american teens the main goal of this paper is to investigate fundamentally new approaches to understand and automatically detect and predict incidents of cyberbullying in instagram a mediabased mobile social network in this work we have collected a sample data set consisting of instagram images and their associated comments we then designed a labeling study and employed human contributors at the crowdsourced crowdflower website to label these media sessions for cyberbullying a detailed analysis of the labeled data is then presented including a study of relationships between cyberbullying and a host of features such as cyberaggression profanity social graph features temporal commenting behavior linguistic content and image content using the labeled data we further design and evaluate the performance of classifiers to automatically detect and pre dict incidents of cyberbullying and cyberaggression,0
political expression through social media has already taken root as a form of political participation meanwhile democracy seems to be facing an epidemic of incivility on social media platforms with this background online political incivility has recently become a growing concern in the field of political communication studies however it is less clear how a governments performance is linked with peoples uncivil political expression on social media investigating the existence of performance evaluation behavior through social media expression seems to be important as it is a new form of noninstitutionalized political participation to fill this gap in the literature the present study hypothesizes that when government performance worsens people become frustrated and send uncivil messages to the government via social media to test this hypothesis the present study collected over 8 million posts on xtwitter directed at us state governors and classified them as uncivil or not using a neural networkbased machine learning method and examined the impact of worsening statelevel covid19 cases on the number of uncivil posts directed at state governors the results of the statistical analyses showed that increases in statelevel covid19 cases led to a significantly higher number of uncivil posts against state governors finally the present study discusses the implications of the findings from two perspectives noninstitutionalized political participation and the importance of elections in democracies,0
why do social media corporations smcs engage in statelinked information operations social media can significantly influence the global political landscape allowing governments and other political entities to engage in concerted information operations shaping or manipulating domestic and foreign political agendas in response to statelinked political manipulation tactics on social media twitter and meta carried out takedown operations against propaganda networks accusing them of interfering foreign elections organizing disinformation campaigns manipulating political debates and many other issues this research investigates the two smcs policy orientation to explain which factors can affect these two companies reaction against statelinked information operations we find that good governance indicators such as democracy are significant elements of smcs countryfocus this article also examines whether meta and twitters attention to political regime characteristics is influenced by international political alignments this research illuminates recent trends in smcs takedown operations and illuminating interplay between geopolitics and domestic regime characteristics,0
the community plays a crucial role in understanding user behavior and network characteristics in social networks some users can use multiple social networks at once for a variety of objectives these users are called overlapping users who bridge different social networks detecting communities across multiple social networks is vital for interaction mining information diffusion and behavior migration analysis among networks this paper presents a community detection method based on nonnegative matrix trifactorization for multiple heterogeneous social networks which formulates a common consensus matrix to represent the global fused community specifically the proposed method involves creating adjacency matrices based on network structure and content similarity followed by alignment matrices which distinguish overlapping users in different social networks with the generated alignment matrices the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks the effectiveness of the proposed method is evaluated with new metrics on twitter instagram and tumblr datasets the results of the experiments demonstrate its superior performance in terms of community quality and community fusion,0
the friendship paradox states that in a social network egos tend to have lower degree than their alters or your friends have more friends than you do most research has focused on the friendship paradox and its implications for information transmission but treating the network as static and unweighted yet people can dedicate only a finite fraction of their attention budget to each social interaction a highdegree individual may have less time to dedicate to individual social links forcing them to modulate the quantities of contact made to their different social ties here we study the friendship paradox in the context of differing contact volumes between egos and alters finding a connection between contact volume and the strength of the friendship paradox the most frequently contacted alters exhibit a less pronounced friendship paradox compared with the ego whereas lessfrequently contacted alters are more likely to be high degree and give rise to the paradox we argue therefore for a more nuanced version of the friendship paradox your closest friends have slightly more friends than you do and in certain networks even your best friend has no more friends than you do we demonstrate that this relationship is robust holding in both a social media and a mobile phone dataset these results have implications for information transfer and influence in social networks which we explore using a simple dynamical model,0
compromising social network accounts has become a profitable course of action for cybercriminals by hijacking control of a popular media or business account attackers can distribute their malicious messages or disseminate fake information to a large user base the impacts of these incidents range from a tarnished reputation to multibillion dollar monetary losses on financial markets in our previous work we demonstrated how we can detect largescale compromises ie socalled campaigns of regular online social network users in this work we show how we can use similar techniques to identify compromises of individual highprofile accounts highprofile accounts frequently have one characteristic that makes this detection reliable they show consistent behavior over time we show that our system were it deployed would have been able to detect and prevent three realworld attacks against popular companies and news agencies furthermore our system in contrast to popular media would not have fallen for a staged compromise instigated by a us restaurant chain for publicity reasons,0
using seasonal topics as the study subject in this study we focus on the timing gap between social media writing and online search behavior to conduct our analysis we used the mathematical model of search behavior comprising the sociophysics approach the seasonal topics selected were stvalentines day halloween and new year countdown we also picked up the event like christmas and halloween we analyzed the influence of blogs and twitter on search behavior and found a deviation of interest in terms of timing we also analyzed japanese seasonal event of eating ehomaki in february 3 and eels at the day of the ox in midsummer,0
individuals experiencing unexpected distressing events shocks often rely on their social network for support while prior work has shown how social networks respond to shocks these studies usually treat all ties equally despite differences in the support provided by different social relationships here we conduct a computational analysis on twitter that examines how responses to online shocks differ by the relationship type of a user dyad we introduce a new dataset of over 13k instances of individuals selfreporting shock events on twitter and construct networks of relationshiplabeled dyadic interactions around these events by examining behaviors across 110k replies to shocked users in a pseudocausal analysis we demonstrate relationshipspecific patterns in response levels and topic shifts we also show that while wellestablished social dimensions of closeness such as tie strength and structural embeddedness contribute to shock responsiveness the degree of impact is highly dependent on relationship and shock types our findings indicate that social relationships contain highly distinctive characteristics in network interactions and that relationshipspecific behaviors in online shock responses are unique from those of offline settings,0
the social media revolution has changed the way that brands interact with consumers instead of spending their advertising budget on interstate billboards more and more companies are choosing to partner with socalled internet influencers individuals who have gained a loyal following on online platforms for the high quality of the content they post unfortunately its not always easy for small brands to find the right influencer someone who aligns with their corporate image and has not yet grown in popularity to the point of unaffordability in this paper we sought to develop a system for brandinfluencer matchmaking harnessing the power and flexibility of modern machine learning techniques the result is an algorithm that can predict the most fruitful brandinfluencer partnerships based on the similarity of the content they post,0
social networking services like twitter have been playing an import role in peoples daily life since it supports new ways of communicating effectively and sharing information the advantages of these social network services enable them rapidly growing however the rise of social network services is leading to the increase of unwanted disruptive information from spammers malware discriminators and other content polluters negative effects of social spammers do not only annoy users but also lead to financial loss and privacy issues there are two main challenges of spammer detection on twitter firstly the data of social network scale with a huge volume of streaming social data secondly spammers continually change their spamming strategy such as changing content patterns or trying to gain social influence disguise themselves as far as possible with those challenges it is hard to directly apply traditional batch learning methods to quickly adapt newly spamming pattern in the highvolume and realtime social media data we need an antispammer system to be able to adjust the learning model when getting a label feedback moreover the data on social media may be unbounded then the system must allow update efficiency model in both computation and memory requirements online learning is an ideal solution for this problem these methods incrementally adapt the learning model with every single feedback and adjust to the changing patterns of spammers overtime our experiments demonstrate that an antispam system based on online learning approach is efficient in fast changing of spammers comparing with batch learning methods we also attempt to find the optimal online learning method and study the effectiveness of various feature sets on these online learning methods,0
according to the center for disease control and prevention in the united states hundreds of thousands initiate smoking each year and millions live with smokingrelated dis eases many tobacco users discuss their habits and preferences on social media this work conceptualizes a framework for targeted health interventions to inform tobacco users about the consequences of tobacco use we designed a twitter bot named notobot short for notobacco bot that leverages machine learning to identify users posting protobacco tweets and select individualized interventions to address their interest in tobacco use we searched the twitter feed for tobaccorelated keywords and phrases and trained a convolutional neural network using over 4000 tweets dichotomously manually labeled as either pro tobacco or not protobacco this model achieves a 90 recall rate on the training set and 74 on test data users posting pro tobacco tweets are matched with former smokers with similar interests who posted antitobacco tweets algorithmic matching based on the power of peer influence allows for the systematic delivery of personalized interventions based on real antitobacco tweets from former smokers experimental evaluation suggests that our system would perform well if deployed this research offers opportunities for public health researchers to increase health awareness at scale future work entails deploying the fully operational notobot system in a controlled experiment within a public health campaign,0
many of todays most widely used computing applications utilize social networking features and allow users to connect follow each other share content and comment on others posts however despite the widespread adoption of these features there is little understanding of the consequences that social networking has on user retention engagement and online as well as offline behavior here we study how social networks influence user behavior in a physical activity tracking application we analyze 791 million online and offline actions of 6 million users over the course of 5 years and show that social networking leads to a significant increase in users online as well as offline activities specifically we establish a causal effect of how social networks influence user behavior we show that the creation of new social connections increases user online inapplication activity by 30 user retention by 17 and user offline realworld physical activity by 7 about 400 steps per day by exploiting a natural experiment we distinguish the effect of social influence of new social connections from the simultaneous increase in users motivation to use the app and take more steps we show that social influence accounts for 55 of the observed changes in user behavior while the remaining 45 can be explained by the users increased motivation to use the app further we show that subsequent individual edge formations in the social network lead to significant increases in daily steps these effects diminish with each additional edge and vary based on edge attributes and user demographics finally we utilize these insights to develop a model that accurately predicts which users will be most influenced by the creation of new social network connections,0
election control considers the problem of an adversary who attempts to tamper with a voting process in order to either ensure that their favored candidate wins constructive control or another candidate loses destructive control as online social networks have become significant sources of information for potential voters a new tool in an attackers arsenal is to effect control by harnessing social influence for example by spreading fake news and other forms of misinformation through online social media we consider the computational problem of election control via social influence studying the conditions under which finding good adversarial strategies is computationally feasible we consider two objectives for the adversary in both the constructive and destructive control settings probability and margin of victory pov and mov respectively we present several strong negative results showing for example that the problem of maximizing pov is inapproximable for any constant factor on the other hand we present approximation algorithms which provide somewhat weaker approximation guarantees such as bicriteria approximations for the pov objective and constantfactor approximations for mov finally we present mixed integer programming formulations for these problems experimental results show that our approximation algorithms often find nearoptimal control strategies indicating that election control through social influence is a salient threat to election integrity,0
the propagation of a rumor unverified information on a social network is subject to several factors mainly related to the content of this information and especially to the behaviors profiles of the actors on this network that retransmit this state of affairs may vary this propagation as the case may be and this is what we call the depth of the rumor this project is tackling this problem from a real case of the spread of a rumor on twitter this contribution proposes an academic approach to quantify the depth of a rumor on social networks and this for use and interpretation by specialists concerned by the nature of this information and its auditor,0
influence maximization is a problem of finding a small set of highly influential users also known as seeds in a social network such that the spread of influence under certain propagation models is maximized in this paper we consider timecritical influence maximization in which one wants to maximize influence spread within a given deadline since timing is considered in the optimization we also extend the independent cascade ic model and the linear threshold lt model to incorporate the time delay aspect of influence diffusion among individuals in social networks we show that timecritical influence maximization under the timedelayed ic and lt models maintains desired properties such as submodularity which allows a greedy approximation algorithm to achieve an approximation ratio of 11e to overcome the inefficiency of the greedy algorithm we design two heuristic algorithms the first one is based on a dynamic programming procedure that computes exact influence in tree structures and directed acyclic subgraphs while the second one converts the problem to one in the original models and then applies existing fast heuristic algorithms to it our simulation results demonstrate that our algorithms achieve the same level of influence spread as the greedy algorithm while running a few orders of magnitude faster and they also outperform existing fast heuristics that disregard the deadline constraint and delays in diffusion,0
with the advent of social media different companies often promote competing products simultaneously for wordofmouth diffusion and adoption by users in social networks for such scenarios of competitive diffusion prior studies show that the weaker product will soon become extinct ie winner takes all it is intriguing to observe that in practice however competing products such as iphone and android phone often coexist in the market this discrepancy may result from many factors such as the phenomenon that a user in the real world may not spread its use of a product due to dissatisfaction of the product or privacy protection in this paper we incorporate users privacy for spreading behavior into competitive diffusion of two products and develop a problem formulation for privacyaware competitive diffusion then we prove that privacypreserving mechanisms can enable a coexistence equilibrium ie two competing products coexist in the equilibrium in competitive diffusion over social networks in addition to the rigorous analysis we also demonstrate our results with experiments over real network topologies,0
social networking sites snss facilitate the sharing of ideas and information through different types of feedback including publishing posts leaving comments and other type of reactions however some comments or feedback on snss are inconsiderate and offensive and sometimes this type of feedback has a very negative effect on a target user the phenomenon known as flaming goes handinhand with this type of posting that can trigger almost instantly on snss most popular users such as celebrities politicians and news media are the major victims of the flaming behaviors and so detecting these types of events will be useful and appreciated flaming event can be monitored and identified by analyzing negative comments received on a post thus our main objective of this study is to identify a way to detect flaming events in sns using a sentiment prediction method we use a deep neural network nn model that can identity sentiments of variable length sentences and classifies the sentiment of snss content both comments and posts to discover flaming events our deep nn model uses word2vec and fasttext word embedding methods as its training to explore which method is the most appropriate the labeled dataset for training the deep nn is generated using an enhanced lexicon based approach our deep nn model classifies the sentiment of a sentence into five classes very positive positive neutral negative and very negative to detect flaming incidents we focus only on the comments classified into the negative and very negative classes as a usecase we try to explore the flaming phenomena in the news media domain and therefore we focused on news items posted by three popular news media on facebook bbcnews cnn and foxnews to train and test the model,0
the emergence of generative ai chatbots such as chatgpt has prompted growing public and academic interest in their role as informal mental health support tools while early rulebased systems have been around for several years large language models llms offer new capabilities in conversational fluency empathy simulation and availability this study explores how users engage with llms as mental health tools by analyzing over 10000 tiktok comments from videos referencing llms as mental health tools using a selfdeveloped tiered coding schema and supervised classification models we identify user experiences attitudes and recurring themes results show that nearly 20 of comments reflect personal use with these users expressing overwhelmingly positive attitudes commonly cited benefits include accessibility emotional support and perceived therapeutic value however concerns around privacy generic responses and the lack of professional oversight remain prominent it is important to note that the user feedback does not indicate which therapeutic framework if any the llmgenerated output aligns with while the findings underscore the growing relevance of ai in everyday practices they also highlight the urgent need for clinical and ethical scrutiny in the use of ai for mental health support,0
online advertisements that masquerade as nonadvertising content pose numerous risks to users such hidden advertisements appear on social media platforms when content creators or influencers endorse products and brands in their content while the federal trade commission ftc requires content creators to disclose their endorsements in order to prevent deception and harm to users we do not know whether and how content creators comply with the ftcs guidelines in this paper we studied disclosures within affiliate marketing an endorsementbased advertising strategy used by social media content creators we examined whether content creators follow the ftcs disclosure guidelines how they word the disclosures and whether these disclosures help users identify affiliate marketing content as advertisements to do so we first measured the prevalence of and identified the types of disclosures in over 500000 youtube videos and 21 million pinterest pins we then conducted a user study with 1791 participants to test the efficacy of these disclosures our findings reveal that only about 10 of affiliate marketing content on both platforms contains any disclosures at all further users fail to understand shorter nonexplanatory disclosures based on our findings we make various design and policy suggestions to help improve advertising disclosure practices on social media platforms,0
online social media have greatly affected the way in which we communicate with each other however little is known about what are the fundamental mechanisms driving dynamical information flow in online social systems here we introduce a generative model for online sharing behavior that is analytically tractable and which can reproduce several characteristics of empirical microblogging data on hashtag usage such as timedependent heavytailed distributions of meme popularity the presented framework constitutes a null model for social spreading phenomena which in contrast to purely empirical studies or simulationbased models clearly distinguishes the roles of two distinct factors affecting meme popularity the memory time of users and the connectivity structure of the social network,0
with the growing popularity and usage of online social media services people now have accounts some times several on multiple and diverse services like facebook linkedin twitter and youtube publicly available information can be used to create a digital footprint of any user using these social media services generating such digital footprints can be very useful for personalization profile management detecting malicious behavior of users a very important application of analyzing users online digital footprints is to protect users from potential privacy and security risks arising from the huge publicly available user information we extracted information about user identities on different social networks through social graph api friendfeed and profilactic we collated our own dataset to create the digital footprints of the users we used username display name description location profile image and number of connections to generate the digital footprints of the user we applied context specific techniques eg jaro winkler similarity wordnet based ontologies to measure the similarity of the user profiles on different social networks we specifically focused on twitter and linkedin in this paper we present the analysis and results from applying automated classifiers for disambiguating profiles belonging to the same user from different social networks userid and name were found to be the most discriminative features for disambiguating user profiles using the most promising set of features and similarity metrics we achieved accuracy precision and recall of 98 99 and 96 respectively,0
this study delves into the mechanisms that spark user curiosity driving active engagement within public telegram groups by analyzing approximately 6 million messages from 29196 users across 409 groups we identify and quantify the key factors that stimulate users to actively participate ie send messages in group discussions these factors include social influence novelty complexity uncertainty and conflict all measured through metrics derived from message sequences and user participation over time after clustering the messages we apply explainability techniques to assign meaningful labels to the clusters this approach uncovers macro categories representing distinct curiosity stimulation profiles each characterized by a unique combination of various stimuli social influence from peers and influencers drives engagement for some users while for others rare media types or a diverse range of senders and media sparks curiosity analyzing patterns we found that user curiosity stimuli are mostly stable but as the time between the initial message increases curiosity occasionally shifts a graphbased analysis of influence networks reveals that users motivated by direct social influence tend to occupy more peripheral positions while those who are not stimulated by any specific factors are often more central potentially acting as initiators and conversation catalysts these findings contribute to understanding information dissemination and spread processes on social media networks potentially contributing to more effective communication strategies,0
studying temporal dynamics of topics in social media is very useful to understand online user behaviors most of the existing work on this subject usually monitors the global trends ignoring variation among communities since users from different communities tend to have varying tastes and interests capturing communitylevel temporal change can improve the understanding and management of social content additionally it can further facilitate the applications such as community discovery temporal prediction and online marketing however this kind of extraction becomes challenging due to the intricate interactions between community and topic and intractable computational complexity in this paper we take a unified solution towards the communitylevel topic dynamic extraction a probabilistic model costot community specific topicsovertime is proposed to uncover the hidden topics and communities as well as capture communityspecific temporal dynamics specifically costot considers text time and network information simultaneously and well discovers the interactions between community and topic over time we then discuss the approximate inference implementation to enable scalable computation of model parameters especially for large social data based on this the application layer support for multiscale temporal analysis and community exploration is also investigated we conduct extensive experimental studies on a large real microblog dataset and demonstrate the superiority of proposed model on tasks of time stamp prediction link prediction and topic perplexity,0
online social networks such as twitter and facebook have gained tremendous popularity for information exchange the availability of unprecedented amounts of digital data has accelerated research on information diffusion in online social networks however the mechanism of information spreading in online social networks remains elusive due to the complexity of social interactions and rapid change of online social networks much of prior work on information diffusion over online social networks has based on empirical and statistical approaches the majority of dynamical models arising from information diffusion over online social networks involve ordinary differential equations which only depend on time in a number of recent papers the authors propose to use partial differential equationspdes to characterize temporal and spatial patterns of information diffusion over online social networks built on intuitive cyberdistances such as friendship hops in online social networks the reactiondiffusion equations take into account influences from various external outofnetwork sources such as the mainstream media and provide a new analytic framework to study the interplay of structural and topical influences on information diffusion over online social networks in this survey we discuss a number of pdebased models that are validated with real datasets collected from popular online social networks such as digg and twitter some new developments including the conservation law of information flow in online social networks and information propagation speeds based on traveling wave solutions are presented to solidify the foundation of the pde models and highlight the new opportunities and challenges for mathematicians as well as computer scientists and researchers in online social networks,0
social media allow for an unprecedented amount of interaction between people online a fundamental aspect of human social behavior however is the tendency of people to associate themselves with likeminded individuals forming homogeneous social circles both online and offline in this work we apply a new model that allows us to distinguish between social ties of varying strength and to observe evidence of homophily with regards to politics music health residential sector year in college within the online and offline social network of 74 college students we present a multiplex network approach to social tie strength here applied to mobile communication data calls text messages and colocation allowing us to dimensionally identify relationships by considering the number of communication channels utilized between students we find that strong social ties are characterized by maximal use of communication channels while weak ties by minimal use we are able to identify 75 of close friendships 90 of weaker ties and 90 of facebook friendships as compared to reported ground truth we then show that stronger ties exhibit greater profile similarity than weaker ones apart from high homogeneity in social circles with respect to political and health aspects we observe strong homophily driven by music residential sector and year in college despite facebook friendship being highly dependent on residence and year exposure to less homogeneous content can be found in the online rather than the offline social circles of students most notably in political and music aspects,0
understanding tie strength in social networks and the factors that influence it have received much attention in a myriad of disciplines for decades several models incorporating indicators of tie strength have been proposed and used to quantify relationships in social networks and a standard set of structural network metrics have been applied to predominantly online social media sites to predict tie strength here we introduce the concept of the social bow tie framework a small subgraph of the network that consists of a collection of nodes and ties that surround a tie of interest forming a topological structure that resembles a bow tie we also define several intuitive and interpretable metrics that quantify properties of the bow tie we use random forests and regression models to predict categorical and continuous measures of tie strength from different properties of the bow tie including nodal attributes we also investigate what aspects of the bow tie are most predictive of tie strength in two distinct social networks a collection of 75 rural villages in india and a nationwide call network of european mobile phone users our results indicate several of the bow tie metrics are highly predictive of tie strength and we find the more the social circles of two individuals overlap the stronger their tie consistent with previous findings however we also find that the more tightlyknit their nonoverlapping social circles the weaker the tie this new finding complements our current understanding of what drives the strength of ties in social networks,0
the rapid expansion of social media platforms has provided unprecedented access to massive amounts of multimodal usergenerated content comprehending user emotions can provide valuable insights for improving communication and understanding of human behaviors despite significant advancements in affective computing the diverse factors influencing user emotions in social networks remain relatively understudied moreover there is a notable lack of deep learningbased methods for predicting user emotions in social networks which could be addressed by leveraging the extensive multimodal data available this work presents a novel formulation of personalized emotion prediction in social networks based on heterogeneous graph learning building upon this formulation we design hmgemo a heterogeneous multimodal graph learning framework that utilizes deep learningbased features for user emotion recognition additionally we include a dynamic context fusion module in hmgemo that is capable of adaptively integrating the different modalities in social media data through extensive experiments we demonstrate the effectiveness of hmgemo and verify the superiority of adopting a graph neural networkbased approach which outperforms existing baselines that use rich handcrafted features to the best of our knowledge hmgemo is the first multimodal and deeplearningbased approach to predict personalized emotions within online social networks our work highlights the significance of exploiting advanced deep learning techniques for lessexplored problems in affective computing,0
many years after online social networks exceeded our collective attention social influence is still built on attention capital quality is not a prerequisite for viral spreading yet large diffusion cascades remain the hallmark of a social influencer consequently our exposure to lowquality content and questionable influence is expected to increase since the conception of influence maximization frameworks multiple content performance metrics became available albeit raising the complexity of influence analysis in this paper we examine and consolidate a diverse set of content engagement metrics the correlations discovered lead us to propose a new more holistic onedimensional engagement signal we then show it is more predictable than any individual influence predictors previously investigated our proposed model achieves strong engagement ranking performance and is the first to explain half of the variance with features available early we share the detailed numerical workflow to compute the new compound engagement signal the model is immediately applicable to social media monitoring influencer identification campaign engagement forecasting and curating user feeds,0
we study the axelrods cultural adaptation model using the concept of cluster size entropy sc that gives information on the variability of the cultural cluster size present in the system using networks of different topologies from regular to random we find that the critical point of the wellknown nonequilibrium monoculturalmulticultural orderdisorder transition of the axelrod model is unambiguously given by the maximum of the scq distributions the width of the cluster entropy distributions can be used to qualitatively determine whether the transition is first or secondorder by scaling the cluster entropy distributions we were able to obtain a relationship between the critical cultural trait qc and the number f of cultural features in regular networks we also analyze the effect of the mass media external field on social systems within the axelrod model in a square network we find a new partially ordered phase whose largest cultural cluster is not aligned with the external field in contrast with a recent suggestion that this type of phase cannot be formed in regular networks we draw a new qb phase diagram for the axelrod model in regular networks,0
cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics more than half of young social media users worldwide have been exposed to such prolonged andor coordinated digital harassment victims can experience a wide range of emotions with negative consequences such as embarrassment depression isolation from other community members which embed the risk to lead to even more critical consequences such as suicide attempts in this work we take the first concrete steps to understand the characteristics of abusive behavior in twitter one of todays largest social media platforms we analyze 12 million users and 21 million tweets comparing users participating in discussions around seemingly normal topics like the nba to those more likely to be haterelated such as the gamergate controversy or the gender pay inequality at the bbc station we also explore specific manifestations of abusive behavior ie cyberbullying and cyberaggression in one of the haterelated communities gamergate we present a robust methodology to distinguish bullies and aggressors from normal twitter users by considering text user and networkbased attributes using various stateoftheart machine learning algorithms we classify these accounts with over 90 accuracy and auc finally we discuss the current status of twitter user accounts marked as abusive by our methodology and study the performance of potential mechanisms that can be used by twitter to suspend users in the future,0
in the era of rapid development of social media social recommendation systems as hybrid recommendation systems have been widely applied existing methods capture interest similarity between users to filter out interestirrelevant relations in social networks that inevitably decrease recommendation accuracy however limited research has a focus on the mutual influence of semantic information between the social network and the useritem interaction network for further improving social recommendation to address these issues we introduce a social underlinerecommendation model with rounderlinebust gunderlineraph denoisinunderlinegaugmentation fusion and multisunderlineemantic modelingburger specifically we firstly propose to construct a social tensor in order to smooth the training process of the model then a graph convolutional network and a tensor convolutional network are employed to capture users item preference and social preference respectively considering the different semantic information in the useritem interaction network and the social network a bisemantic coordination loss is proposed to model the mutual influence of semantic information to alleviate the interference of interestirrelevant relations on multisemantic modeling we further use bayesian posterior probability to mine potential social relations to replace social noise finally the sliding window mechanism is utilized to update the social tensor as the input for the next iteration extensive experiments on three real datasets show burger has a superior performance compared with the stateoftheart models,0
social media platforms provide continuous access to user generated content that enables realtime monitoring of user behavior and of events the geographical dimension of such user behavior and events has recently caught a lot of attention in several domains mobility humanitarian or infrastructural while resolving the location of a user can be straightforward depending on the affordances of their device andor of the application they are using in most cases locating a user demands a larger effort such as exploiting textual features on twitter for instance only 2 of all tweets are georeferenced in this paper we present a system for zoomedin grounding below city level for short messages eg tweets the system combines different natural language processing and machine learning techniques to increase the number of geogrounded tweets which is essential to many applications such as disaster response and realtime traffic monitoring,0
nft or nonfungible token is a token that certifies a digital asset to be unique a wide range of assets including digital art music tweets memes are being sold as nfts nftrelated content has been widely shared on social media sites such as twitter we aim to understand the dominant factors that influence nft asset valuation towards this objective we create a firstofitskind dataset linking twitter and opensea the largest nft marketplace to capture social media profiles and linked nft assets our dataset contains 245159 tweets posted by 17155 unique users directly linking 62997 nft assets on opensea worth 19 million usd we have made the dataset public we analyze the growth of nfts characterize the twitter users promoting nft assets and gauge the impact of twitter features on the virality of an nft further we investigate the effectiveness of different social media and nft platform features by experimenting with multiple machine learning and deep learning models to predict an assets value our results show that social media features improve the accuracy by 6 over baseline models that use only nft platform features among social media features count of user membership lists number of likes and retweets are important features,0
the covid19 pandemic has introduced new opportunities for health communication including an increase in the public use of online outlets for healthrelated emotions people have turned to social media networks to share sentiments related to the impacts of the covid19 pandemic in this paper we examine the role of social messaging shared by persons in the public eye ie athletes politicians news personnel in determining overall public discourse direction we harvested approximately 13 million tweets ranging from 1 january 2020 to 1 march 2022 the sentiment was calculated for each tweet using a finetuned distilroberta model which was used to compare covid19 vaccinerelated twitter posts tweets that cooccurred with mentions of people in the public eye our findings suggest the presence of consistent patterns of emotional content cooccurring with messaging shared by persons in the public eye for the first two years of the covid19 pandemic influenced public opinion and largely stimulated online public discourse we demonstrate that as the pandemic progressed public sentiment shared on social networks was shaped by risk perceptions political ideologies and healthprotective behaviours shared by persons in the public eye often in a negative light,0
competitive influence maximization cim involves entities competing to maximize influence in online social networks osns current deep reinforcement learning drl methods in cim rely on simplistic binary opinion models ie an opinion is represented by either 0 or 1 and often overlook the complexity of users behavioral characteristics and their prior knowledge we propose a novel drlbased framework that enhances cim analysis by integrating subjective logic sl to accommodate uncertain opinions users behaviors and their preferences this approach targets the mitigation of false information by effectively propagating true information by modeling two competitive agents one spreading true information and the other spreading false information we capture the strategic interplay essential to cim our framework utilizes an uncertaintybased opinion model uom to assess the impact on information quality in osns emphasizing the importance of user behavior alongside network topology in selecting influential seed nodes extensive experiments demonstrate that our approach significantly outperforms stateoftheart methods achieving faster and more influential results ie outperforming over 20 under realistic network conditions moreover our method shows robust performance in partially observable networks effectively doubling the performance when users are predisposed to disbelieve true information,0
social media platforms enable largely unrestricted manytomany communication in times of crisis they offer a space for collective sensemaking and gave rise to new social phenomena eg opensource investigations however they also serve as a tool for threat actors to conduct cyberenabled social influence operations cesios in order to shape public opinion and interfere in decisionmaking processes cesios rely on the employment of sock puppet accounts to engage authentic users in online communication exert influence and subvert online discourse large language models llms may further enhance the deceptive properties of sock puppet accounts recent llms are able to generate targeted and persuasive text which is for the most part indistinguishable from humanwritten content ideal features for covert influence this article reviews recent developments at the intersection of llms and influence operations summarizes llms salience and explores the potential impact of llminstrumented sock puppet accounts for cesios finally mitigation measures for the near future are highlighted,0
understanding the forces governing human behavior and social dynamics is a challenging problem individuals decisions and actions are affected by interlaced factors such as physical location homophily and social ties in this paper we propose to examine the role that distinct communities linked to these factors play as sources of social influence the ego network is typically used in the social influence analysis our hypothesis is that individuals are embedded in communities not only related to their direct social relationships but that involve different and complex forces we analyze physical homophily and social communities to evaluate their relation with subjects behavior we prove that social influence is correlated with these communities and each one of them is differently significant for individuals we define communitybased features which reflect the subject involvement in these groups and we use them with a supervised learning algorithm to predict subject participation in social events results indicate that both communities and ego network are relevant sources of social influence confirming that the ego network alone is not sufficient to explain this phenomenon moreover we classify users according to the degree of social influence they experienced with respect to their groups recognizing classes of behavioral phenotypes to our knowledge this is the first work that proves the existence of phenotypes related to the social influence phenomenon,0
aggression in online social networks has been studied mostly from the perspective of machine learning which detects such behavior in a static context however the way aggression diffuses in the network has received little attention as it embeds modeling challenges in fact modeling how aggression propagates from one user to another is an important research topic since it can enable effective aggression monitoring especially in media platforms which up to now apply simplistic user blocking techniques in this paper we address aggression propagation modeling and minimization on twitter since it is a popular microblogging platform at which aggression had several onsets we propose various methods building on two wellknown diffusion models independent cascade ic and linear threshold lt to study the aggression evolution in the social network we experimentally investigate how well each method can model aggression propagation using real twitter data while varying parameters such as seed users selection graph edge weighting users activation timing etc it is found that the best performing strategies are the ones to select seed users with a degreebased approach weigh user edges based on their social circles overlaps and activate users according to their aggression levels we further employ the best performing models to predict which ordinary real users could become aggressive and vice versa in the future and achieve up to auc089 in this prediction task finally we investigate aggression minimization by launching competitive cascades to inform and heal aggressors we show that ic and lt models can be used in aggression minimization providing less intrusive alternatives to the blocking techniques currently employed by popular online social network platforms,0
natural ivory is no longer readily or legally available as it is obtained primarily from elephant tusks which now enjoy international protection ivory however is the best material known for piano keys we present a hydroxylapatitegelatin biocomposite that is chemically identical to natural ivory but with functional properties optimized to replace it as this biocomposite is fabricated from abundant materials in an environmentally friendly process and is furthermore biodegradable it is a sustainable solution for piano keys with the ideal functional properties of natural ivory,0
the carbon and water footprint of largescale computing systems poses serious environmental sustainability risks in this study we discover that unfortunately carbon and water sustainability are at odds with each other and optimizing one alone hurts the other toward that goal we introduce waterwise a novel job scheduler for parallel workloads that intelligently cooptimizes carbon and water footprint to improve the sustainability of geographically distributed data centers,0
this paper examines the integration of ais carbon footprint into the risk management frameworks rmfs of the banking sector emphasising its importance in aligning with sustainability goals and regulatory requirements as ai becomes increasingly central to banking operations its energyintensive processes contribute significantly to carbon emissions posing environmental regulatory and reputational risks regulatory frameworks such as the eu ai act corporate sustainability reporting directive csrd corporate sustainability due diligence directive csddd and the prudential regulation authoritys ss123 are driving banks to incorporate environmental considerations into their ai model governance recent advancements in ai research like the open mixtureofexperts olmoe framework and the agentic rag framework offer more efficient and dynamic ai models reducing their carbon footprint without compromising performance using these technological examples the paper outlines a structured approach for banks to identify assess and mitigate ais carbon footprint within their rmfs including adopting energyefficient models utilising green cloud computing and implementing lifecycle management,0
the recent high performance of chatgpt on several standardized academic tests has thrust the topic of artificial intelligence ai into the mainstream conversation about the future of education as deep learning is poised to shift the teaching paradigm it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of aidriven technologies at schools and universities this research aims to investigate the potential impact of ai on education through review and analysis of the existing literature across three major axes applications advantages and challenges our review focuses on the use of artificial intelligence in collaborative teacherstudent learning intelligent tutoring systems automated assessment and personalized learning we also report on the potential negative aspects ethical issues and possible future routes for ai implementation in education ultimately we find that the only way forward is to embrace the new technology while implementing guardrails to prevent its abuse,0
the rapid expansion of cloud computing and data center infrastructure has led to significant energy consumption posing environmental challenges due to the growing carbon footprint this research explores energyaware management strategies aimed at creating sustainable data center operations by integrating advanced energyefficient technologies and optimizing resource utilization this study proposes a framework to minimize power usage while maintaining high performance key elements include dynamic workload allocation renewable energy integration and intelligent cooling systems all of which contribute to reducing overall energy consumption the study also examines the impact of these strategies on operational costs and performance efficiency demonstrating how sustainable practices can be both environmentally and economically beneficial through simulations and case studies the research offers practical insights into reducing carbon emissions in data centers supporting the transition towards greener cloud infrastructure the findings highlight the potential for scalable energyaware data center designs that significantly lower environmental impact while ensuring optimal functionality contributing to the global effort of mitigating climate change,0
generative ai is spreading rapidly creating significant social and economic value while also raising concerns about its high energy use and environmental sustainability while prior studies have predominantly focused on the energyintensive nature of the training phase the cumulative environmental footprint generated during largescale service operations particularly in the inference phase has received comparatively less attention to bridge this gap this study conducts a scoping review of methodologies and research trends in ai carbon footprint assessment we analyze the classification and standardization status of existing ai carbon measurement tools and methodologies and comparatively examine the environmental impacts arising from both training and inference stages in addition we identify how multidimensional factors such as model size prompt complexity serving environments and system boundary definitions shape the resulting carbon footprint our review reveals critical limitations in current ai carbon accounting practices including methodological inconsistencies technologyspecific biases and insufficient attention to endtoend system perspectives building on these insights we propose future research and governance directions 1 establishing standardized and transparent universal measurement protocols 2 designing dynamic evaluation frameworks that incorporate user behavior 3 developing lifecycle monitoring systems that encompass embodied emissions and 4 advancing multidimensional sustainability assessment framework that balance model performance with environmental efficiency this paper provides a foundation for interdisciplinary dialogue aimed at building a sustainable ai ecosystem and offers a baseline guideline for researchers seeking to understand the environmental implications of ai across technical social and operational dimensions,0
this work examines the role of recommender systems in promoting sustainability social responsibility and accountability with a focus on alignment with the united nations sustainable development goals sdgs as recommender systems become increasingly integrated into daily interactions they must go beyond personalization to support responsible consumption reduce environmental impact and foster social good we explore strategies to mitigate the carbon footprint of recommendation models ensure fairness and implement accountability mechanisms by adopting these approaches recommender systems can contribute to sustainable and socially beneficial outcomes aligning technological advancements with the sdgs focused on environmental sustainability and social wellbeing,0
the environmental impact of large language models llms is rising significantly with inference now accounting for more than half of their total lifecycle carbon emissions however existing simulation frameworks which are increasingly used to determine efficient llm deployments lack any concept of power and therefore cannot accurately estimate inferencerelated emissions we present a simulation framework to assess the energy and carbon implications of llm inference under varying deployment setups first we extend a highfidelity llm inference simulator with a gpu power model that estimates power consumption based on utilization metrics enabling analysis across configurations like batch size sequence length and model parallelism second we integrate simulation outputs into an energy system cosimulation environment to quantify carbon emissions under specific grid conditions and explore the potential of carbonaware scheduling through scenariobased analysis our framework reveals how inference parameters affect energy demand and carbon footprint demonstrates a renewable offset potential of up to 692 in an illustrative deployment case and provides a foundation for future carbonaware inference infrastructure design,0
classical and centralized artificial intelligence ai methods require moving data from producers sensors machines to energy hungry data centers raising environmental concerns due to computational and communication resource demands while violating privacy emerging alternatives to mitigate such high energy costs propose to efficiently distribute or federate the learning tasks across devices which are typically lowpower this paper proposes a novel framework for the analysis of energy and carbon footprints in distributed and federated learning fl the proposed framework quantifies both the energy footprints and the carbon equivalent emissions for vanilla fl methods and consensusbased fully decentralized approaches we discuss optimal bounds and operational points that support green fl designs and underpin their sustainability assessment two case studies from emerging 5g industry verticals are analyzed these quantify the environmental footprints of continual and reinforcement learning setups where the training process is repeated periodically for continuous improvements for all cases sustainability of distributed learning relies on the fulfillment of specific requirements on communication efficiency and learner population size energy and test accuracy should be also traded off considering the model and the data footprints for the targeted industrial applications,0
the widespread adoption of machine learning ml across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions this issue becomes more pressing in adversarial ml which focuses on enhancing model security against different networkbased attacks implementing defenses in ml systems often necessitates additional computational resources and network security measures exacerbating their environmental impacts in this paper we pioneer the first investigation into adversarial mls carbon footprint providing empirical evidence connecting greater model robustness to higher emissions addressing the critical need to quantify this tradeoff we introduce the robustness carbon tradeoff index rcti this novel metric inspired by economic elasticity principles captures the sensitivity of carbon emissions to changes in adversarial robustness we demonstrate the rcti through an experiment involving evasion attacks analyzing the interplay between robustness against attacks performance and carbon emissions,0
as machine learning workloads significantly increase energy consumption sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide this requires a paradigm shift in optimizing power consumption in cooling and it loads shifting flexible loads based on the availability of renewable energy in the power grid and leveraging battery storage from the uninterrupted power supply in data centers using collaborative agents the complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem currently a realtime controller to optimize all these goals simultaneously in a dynamic realworld setting is lacking we propose a data center carbon footprint reduction dccfr multiagent reinforcement learning marl framework that optimizes data centers for the multiple objectives of carbon footprint reduction energy consumption and energy cost the results show that the dccfr marl agents effectively resolved the complex interdependencies in optimizing cooling load shifting and energy storage in realtime for various locations under realworld dynamic weather and grid carbon intensity conditions dccfr significantly outperformed the industry standard ashrae controller with a considerable reduction in carbon emissions 145 energy usage 144 and energy cost 137 when evaluated over one year across multiple geographical regions,0
decades of progress in energyefficient and lowpower design have successfully reduced the operational carbon footprint in the semiconductor industry however this has led to an increase in embodied emissions encompassing carbon emissions arising from design manufacturing packaging and other infrastructural activities while existing research has developed tools to analyze embodied carbon at the computer architecture level for traditional monolithic systems these tools do not apply to nearmainstream heterogeneous integration hi technologies hi systems offer significant potential for sustainable computing by minimizing carbon emissions through two key strategies reducing computation by reusing predesigned chiplet ip blocks and adopting hierarchical approaches to system design the reuse of chiplets across multiple designs even spanning multiple generations of integrated circuits ics can substantially reduce embodied carbon emissions throughout the operational lifespan this paper introduces a carbon analysis tool specifically designed to assess the potential of hi systems in facilitating greener vlsi system design and manufacturing approaches the tool takes into account scaling chiplet and packaging yields design complexity and even carbon overheads associated with advanced packaging techniques employed in heterogeneous systems experimental results demonstrate that hi can achieve a reduction of embodied carbon emissions up to 70 compared to traditional large monolithic systems these findings suggest that hi can pave the way for sustainable computing practices contributing to a more environmentally conscious semiconductor industry,0
food systems are responsible for a third of global anthropogenic greenhouse gas emissions central to global warming and climate change increasing awareness of the environmental impact of foodcentric emissions has led to the carbon footprint quantification of food products however food consumption is dictated by traditional dishes the cultural capsules that encode traditional protocols for culinary preparations carbon footprint estimation of recipes will provide actionable insights into the environmental sustainability of culturally influenced patterns in recipe compositions by integrating the carbon footprint data of food products with a goldstandard repository of recipe compositions we show that the ingredient constitution dictates the carbon load of recipes beyond the prevalent focus on individual food products our analysis quantifies the carbon footprint of recipes within the cultural contexts that shape culinary protocols while emphasizing the widely understood harms of animalsourced ingredients this article presents a nuanced perspective on the environmental impact of culturally influenced dietary practices along with the grasp of taste and nutrition correlates such an understanding can help design palatable and environmentally sustainable recipes systematic compilation of finegrained carbon footprint data is the way forward to address the challenge of sustainably feeding an anticipated population of 10 billion,0
distributed information sharing dish is a new cooperative approach to designing multichannel mac protocols it aids nodes in their decision making processes by compensating for their missing information via information sharing through other neighboring nodes this approach was recently shown to significantly boost the throughput of multichannel mac protocols however a critical issue for ad hoc communication devices ie energy efficiency has yet to be addressed in this paper we address this issue by developing simple solutions which 1 reduce the energy consumption 2 without compromising the throughput performance and meanwhile 3 maximize cost efficiency we propose two energyefficient strategies insitu energy conscious dish which uses existing nodes only and altruistic dish which needs additional nodes called altruists we compare five protocols with respect to the strategies and identify altruistic dish to be the right choice in general it 1 conserves 4080 of energy 2 maintains the throughput advantage gained from the dish approach and 3 more than doubles the cost efficiency compared to protocols without applying the strategy on the other hand our study shows that insitu energy conscious dish is suitable only in certain limited scenarios,0
the rapid increase in computing demand and its corresponding energy consumption have focused attention on computings impact on the climate and sustainability prior work proposes metrics that quantify computings carbon footprint across several lifecycle phases including its supply chain operation and endoflife industry uses these metrics to optimize the carbon footprint of manufacturing hardware and running computing applications unfortunately prior work on optimizing datacenters carbon footprint often succumbs to the emphsunk cost fallacy by considering embodied carbon emissions a sunk cost when making operational decisions ie job scheduling and placement which leads to operational decisions that do not always reduce the total carbon footprint in this paper we evaluate carbonaware job scheduling and placement on a given set of servers for a number of carbon accounting metrics our analysis reveals stateoftheart carbon accounting metrics that include embodied carbon emissions when making operational decisions can actually increase the total carbon footprint of executing a set of jobs we study the factors that affect the added carbon cost of such suboptimal decisionmaking we then use a realworld case study from a datacenter to demonstrate how the sunk carbon fallacy manifests itself in practice finally we discuss the implications of our findings in better guiding effective carbonaware scheduling in onpremise and cloud datacenters,0
the large particle physics laboratory directors group ldg established the working group on the sustainability assessment of future accelerators in 2024 with the mandate to develop guidelines and a list of key parameters for the assessment of the sustainability of future accelerators in particle physics while focused on accelerator projects much of the work will also be relevant to other current and future research infrastructures the development and continuous update of such a framework aim to enable a coherent communication amongst scientists and adequately convey the information to a broader set of stakeholders this document outlines the major findings and recommendations from the ldg sustainability wg report a summary of current best practices recommended to be adopted by new research infrastructures the full report will be available in june 2025 at not all of sustainability topics are addressed at the same level the assessment process is complex largely under development and a homogeneous evaluation of all the aspects deserves a strategy to be pursued over time,0
as the penetration of distributed energy resources der and renewable energy sources res increases carbon footprint tracking requires more granular analysis results existing carbon footprint tracking methods focus on deterministic steadystate analysis where the high uncertainties of res cannot be considered considering the deficiency of the existing deterministic method this paper proposes two stochastic carbon footprint tracking methods to cope with the impact of res uncertainty on loadside carbon footprint tracing the first method introduces probabilistic analysis in the framework of carbon emissions flow cef to provide a global reference for the spatial characteristic of the power system component carbon intensity distribution considering that the cef network expands with the increasing penetration of ders the second method can effectively improve the computational efficiency over the first method while ensuring the computational accuracy on the large power systems these proposed models are tested and compared in a synthetic 1004bus test system in the case study to demonstrate the performance of the two proposed methods,0
the rapid growth in demand for hpc systems has led to a rise in carbon footprint which requires urgent intervention in this work we present a comprehensive analysis of the carbon footprint of highperformance computing hpc systems considering the carbon footprint during both the hardware manufacturing and system operational stages our work employs hpc hardware component carbon footprint modeling regional carbon intensity analysis and experimental characterization of the system life cycle to highlight the importance of quantifying the carbon footprint of hpc systems,0
federated learning fl distributes machine learning ml training across edge devices to reduce data transfer overhead and protect data privacy since fl model training may span hundreds of devices and is thus resource and energyintensive it has a significant carbon footprint importantly since energys carbonintensity differs substantially by up to 60times across locations training on the same device using the same amount of energy but at different locations can incur widely different carbon emissions while prior work has focused on improving fls resource and energyefficiency by optimizing timetoaccuracy it implicitly assumes all energy has the same carbon intensity and thus does not optimize carbon efficiency ie work done per unit of carbon emitted to address the problem we design ecolearn which minimizes fls carbon footprint without significantly affecting model accuracy or training time ecolearn achieves a favorable tradeoff by integrating carbon awareness into multiple aspects of fl training including i selecting clients with high data utility and low carbon ii provisioning more clients during the initial training rounds and iii mitigating stragglers by dynamically adjusting client overprovisioning based on carbon we implement ecolearn and its carbonaware fl training policies in the flower framework and show that it reduces the carbon footprint of training by up to 108times while maintaining model accuracy and training time within sim1 compared to stateoftheart approaches,0
in recent years largescale adoption of cloud storage solutions has revolutionized the way we think about digital data storage however the exponential increase in data volume especially images has raised environmental concerns regarding power and resource consumption as well as the rising digital carbon footprint emissions the aim of this research is to propose a methodology for cloudbased image storage by integrating image compression technology with superresolution generative adversarial networks srgan rather than storing images in their original format directly on the cloud our approach involves initially reducing the image size through compression and downsizing techniques before storage upon request these compressed images will be retrieved and processed by srgan to generate images the efficacy of the proposed method is evaluated in terms of psnr and ssim metrics additionally a mathematical analysis is given to calculate power consumption and carbon footprint assesment the proposed data compression technique provides a significant solution to achieve a reasonable trade off between environmental sustainability and industrial efficiency,0
the carbon footprint of bitcoin has drawn wide attention but bitcoins longterm impact on the climate remains uncertain here we present a framework to overcome uncertainties in previous estimates and project bitcoins electricity consumption and carbon footprint in the long term if we assume bitcoins market capitalization grows in line with the one of gold we find that the annual electricity consumption of bitcoin may increase from 60 to 400 twh between 2020 and 2100 the future carbon footprint of bitcoin strongly depends on the decarbonization pathway of the electricity sector if the electricity sector achieves carbon neutrality by 2050 bitcoins carbon footprint has peaked already however in the businessasusual scenario emissions sum up to 2 gigatons until 2100 an amount comparable to 7 of global emissions in 2019 the bitcoin price spike at the end of 2020 shows however that progressive development of market capitalization could yield an electricity consumption of more than 100 twh already in 2021 and lead to cumulative emissions of over 5 gigatons by 2100 therefore we also discuss policy instruments to reduce bitcoins future carbon footprint,0
environmental sustainability particularly in relation to climate change is a key concern for consumers producers and policymakers the carbon footprint based on greenhouse gas emissions is a standard metric for quantifying the contribution to climate change of activities and is often assessed using life cycle assessment lca however conducting lca is complex due to opaque and global supply chains as well as fragmented data this paper presents a methodology that combines advances in lca and publicly available databases with knowledgeaugmented ai techniques including retrievalaugmented generation to estimate cradletogate carbon footprints of food products we introduce a chatbot interface that allows users to interactively explore the carbon impact of composite meals and relate the results to familiar activities a live web demonstration showcases our proofofconcept system with arbitrary food items and followup questions highlighting both the potential and limitations such as database uncertainties and ai misinterpretations of delivering lca insights in an accessible format,0
the carbon footprint of astronomical research is an increasingly topical issue from a comparison of existing literature we infer an annual per capita carbon footprint of several tens of tonnes of co2 equivalents for an average person working in astronomy astronomical observatories contribute significantly to the carbon footprint of astronomy and we examine the related sources of greenhouse gas emissions as well as lever arms for their reduction comparison with other scientific domains illustrates that astronomy is not the only field that needs to accomplish significant carbon footprint reductions of their research facilities we show that limiting global warming to 15c or 2c implies greenhouse gas emission reductions that can only be reached by a systemic change of astronomical research activities and we argue that a new narrative for doing astronomical research is needed if we want to keep our planet habitable,0
the carbon footprint of astronomical research is an increasingly topical issue with first estimates of research institute and national community footprints having recently been published as these assessments have typically excluded the contribution of astronomical research infrastructures we complement these studies by providing an estimate of the contribution of astronomical space missions and groundbased observatories using greenhouse gas emission factors that relates cost and payload mass to carbon footprint we find that worldwide active astronomical research infrastructures currently have a carbon footprint of 203pm33 mtco2 equivalent co2e and an annual emission of 1169pm249 ktco2e yr1 corresponding to a footprint of 366pm140 tco2e per year per astronomer compared with contributions from other aspects of astronomy research activity our results suggest that research infrastructures make the single largest contribution to the carbon footprint of an astronomer we discuss the limitations and uncertainties of our method and explore measures that can bring greenhouse gas emissions from astronomical research infrastructures towards a sustainable level,0
carbon footprint accounting is crucial for quantifying greenhouse gas emissions and achieving carbon neutralitythe dynamic nature of processes accounting rules carbonrelated policies and energy supply structures necessitates realtime updates of cfa traditional life cycle assessment methods rely heavily on human expertise making nearrealtime updates challenging this paper introduces a novel approach integrating large language models llms with retrievalaugmented generation technology to enhance the realtime professional and economical aspects of carbon footprint information retrieval and analysis by leveraging llms logical and language understanding abilities and rags efficient retrieval capabilities the proposed method llmsragcfa can retrieve more relevant professional information to assist llms enhancing the models generative abilities this method offers broad professional coverage efficient realtime carbon footprint information acquisition and accounting and costeffective automation without frequent llms parameter updates experimental results across five industriesprimary aluminum lithium battery photovoltaic new energy vehicles and transformersdemonstrate that the llmsragcfa method outperforms traditional methods and other llms achieving higher information retrieval rates and significantly lower information deviations and carbon footprint accounting deviations the economically viable design utilizes rag technology to balance realtime updates with costeffectiveness providing an efficient reliable and costsaving solution for realtime carbon emission management thereby enhancing environmental sustainability practices,0
sustainability is a promise by agile development as it is part of both the agile alliances and the scrum alliances vision thus far not much has been delivered on this promise this paper explores the agile manifesto and points out how agility could contribute to sustainability in its three dimensions social economic and environmental additionally this paper provides some sample cases of companies focusing on both sustainability partially or holistically and agile development,0
turbulent flow physics regulates the aerodynamic properties of lifting surfaces the thermodynamic efficiency of vapor power systems and exchanges of natural and anthropogenic quantities between the atmosphere and ocean to name just a few applications the dynamics of turbulent flows are described via numerical integration of the nonlinear navierstokes equation a procedure known as computational fluid dynamics cfd at the dawn of scientific computing in the late 1950s it would be many decades before terms such as carbon footprint or sustainability entered the lexicon and longer still before these themes attained national priority throughout advanced economies this paper introduces a framework designed to calculate the carbon footprint of cfd and its contribution to carbon emission reduction strategies we will distinguish between hero and routine calculations noting that the carbon footprint of hero calculations is largely determined by the energy source mix utilized we will also review cfd of flows where turbulence effects are modeled thus reducing the degrees of freedom estimates of the carbon footprint are presented for such fully and partiallyresolved simulations as functions of turbulence activity and calculation year demonstrating a reduction in carbon emissions by two to five orders of magnitude at practical conditions beyond analyzing co2 emissions we quantify the benefits of applying cfd towards overall carbon emission reduction the communitys effort to avoid redundant calculations via turbulence databases merits particular attention with estimates indicating that a single database could potentially reduce co2 emissions by approximately o1 million metric tons additionally implementing cfd in the fluids industry has markedly decreased dependence on wind tunnel testing which is anticipated to lead to co2 emission reduction,0
while researchers in both industry and academia are racing to build quantum computing qc platforms with viable performance and functionality the environmental impacts of this endeavor such as its carbon footprint ewaste generation mineral use and water and energy consumption remain largely unknown a similar oversight occurred during the semiconductor revolution and continues to have disastrous consequences for the health of our planet as we build the quantum computing stack from the ground up it is crucial to comprehensively assess it through an environmental sustainability lens for its entire lifecycle production use and disposal in this paper we highlight the need and challenges in establishing a qc sustainability benchmark that enables researchers to make informed architectural design decisions and celebrate the potential quantum environmental advantage we propose a carbonaware quantum computing cqc framework that provides the foundational methodology and open research questions for calculating the total lifecycle carbon footprint of a qc platform our call to action to the research community is the establishment of a new research direction known as sustainable quantum computing that promotes both quantum computing for sustainabilityoriented applications and the sustainability of quantum computing,0
the rise of machine learning ml systems has exacerbated their carbon footprint due to increased capabilities and model sizes however there is scarce knowledge on how the carbon footprint of ml models is actually measured reported and evaluated in light of this the paper aims to analyze the measurement of the carbon footprint of 1417 ml models and associated datasets on hugging face which is the most popular repository for pretrained ml models the goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ml models the study includes the first repository mining study on the hugging face hub api on carbon emissions this study seeks to answer two research questions 1 how do ml model creators measure and report carbon emissions on hugging face hub and 2 what aspects impact the carbon emissions of training ml models the study yielded several key findings these include a stalled proportion of carbon emissionsreporting models a slight decrease in reported carbon footprint on hugging face over the past 2 years and a continued dominance of nlp as the main application domain furthermore the study uncovers correlations between carbon emissions and various attributes such as model size dataset size and ml application domains these results highlight the need for software measurements to improve energy reporting practices and promote carbonefficient model development within the hugging face community in response to this issue two classifications are proposed one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency the aim of these classification proposals is to foster transparency and sustainable model development within the ml community,0
we estimate the carbon footprint of astronomical research infrastructures including space telescopes and probes and groundbased observatories our analysis suggests annual greenhouse gas emissions of 12pm02 mtco2e yr1 due to construction and operation of the worldfleet of astronomical observatories corresponding to a carbon footprint of 366pm140 tco2e per year and average astronomer we show that decarbonising astronomical facilities is compromised by the continuous deployment of new facilities suggesting that a significant reduction in the deployment pace of new facilities is needed to reduce the carbon footprint of astronomy we propose measures that would bring astronomical activities more in line with the imperative to reduce the carbon footprint of all human activities,0
due to increased computing use data centers consume and emit a lot of energy and carbon these contributions are expected to rise as big data analytics digitization and large ai models grow and become major components of daily working routines to reduce the environmental impact of software development green sustainable coding and claims that ai models can improve energy efficiency have grown in popularity furthermore in the automotive industry where software increasingly governs vehicle performance safety and user experience the principles of green coding and aidriven efficiency could significantly contribute to reducing the sectors environmental footprint we present an overview of green coding and metrics to measure ai model sustainability awareness this study introduces llm as a service and uses a generative commercial ai language model github copilot to autogenerate code using sustainability metrics to quantify these ai models sustainability awareness we define the codes embodied and operational carbon,0
the ict information communication technologies ecosystem is estimated to be responsible as of today for 10 of the total worldwide energy demand equivalent to the combined energy production of germany and japan cloud storage mainly operated through large and denselypacked data centers constitutes a nonnegligible part of it however since the cloud is a fastinflating market and the energyefficiency of data centers is mostly an insensitive issue for the collectivity its carbon footprint shows no signs of slowing down in this paper we analyze a novel paradigm for cloud storage implemented by cubbit in which data are stored and distributed over a network of p2pinteracting armbased singleboard devices we compare cubbits distributed cloud to the traditional centralized solution in terms of environmental footprint and energy efficiency we demonstrate that compared to the centralized cloud the distributed architecture of cubbit has a carbon footprint reduced of a 77 factor for data storage and of a 50 factor for data transfers these results provide an example of how a radical paradigm shift in a largereach technology can benefit both the final consumer as well as our society as a whole,0
deep learning has experienced significant growth in recent years resulting in increased energy consumption and carbon emission from the use of gpus for training deep neural networks dnns answering the call for sustainability conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity however moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations moreover postponing training can negatively impact application service quality because the dnns backing the service are not updated in a timely fashion in this work we present a practical solution that reduces the carbon footprint of dnn training without migrating or postponing jobs specifically our solution observes realtime carbon intensity shifts during training and controls the energy consumption of gpus thereby reducing carbon footprint while maintaining training performance furthermore in order to proactively adapt to shifting carbon intensity we propose a lightweight machine learning algorithm that predicts the carbon intensity of the upcoming time frame our solution chase reduces the total carbon footprint of training resnet50 on imagenet by 136 while only increasing training time by 25,0
the energy demand of modern cloud services particularly those related to generative ai is increasing at an unprecedented pace to date carbonaware computing strategies have primarily focused on batch process scheduling or geodistributed load balancing however such approaches are not applicable to services that require constant availability at specific locations due to latency privacy data or infrastructure constraints in this paper we explore how the carbon footprint of energyintensive services can be reduced by adjusting the fraction of requests served by different service quality tiers we show that adapting this quality of responses with respect to grid carbon intensity can lead to additional carbon savings beyond resource and energy efficiency and introduce a forecastbased multihorizon optimization that reaches closetooptimal carbon savings,0
mini data centres have become increasingly prevalent in diverse organizations in recent years they can be easily deployed at large scale with high resilience they are also costeffective and provide highsecurity protection on the other hand it technologies have resulted in the development of ever more energyefficient servers leading to the periodic replacement of oldergeneration servers in mini data centres however the disposal of older servers has resulted in electronic waste that further aggravates the already critical ewaste problem furthermore despite the shift towards more energyefficient servers many mini data centres still rely heavily on highcarbon energy sources this contributes to data centres overall carbon footprint all these issues are concerns for sustainability in order to address this sustainability issue this paper proposes an approach to extend the lifespan of oldergeneration servers in mini data centres this is made possible thanks to a novel solarpowered computing technology named genesis that compensates for the energy overhead generated by older servers as a result electronic waste can be reduced while improving system sustainability by reusing functional server hardware moreover genesis does not require server cooling which reduces energy and water requirements analytical reasoning is applied to compare the efficiency of typical conventional mini data centre designs against alternative genesisbased designs in terms of energy carbon emissions and exploitation costs,0
we present an assessment of the greenhouse gases emissions of the institute for research in astrophysics and planetology irap located in toulouse france it was performed following the established bilan carbone methodology over a large scope compared to similar previous studies including in particular the contribution from the purchase of goods and services as well as iraps use of external research infrastructures such as groundbased observatories and spaceborne facilities the carbon footprint of the institute for the reference year 2019 is 7400 900 tco2e if we exclude the contribution from external research infrastructures to focus on a restricted perimeter over which the institute has some operational control iraps emissions in 2019 amounted to 3300 400 tco2e over the restricted perimeter the contribution from purchasing goods and services is dominant about 40 of the total slightly exceeding the contribution from professional travel including hotel stays which accounts for 38 local infrastructures make a smaller contribution to iraps carbon footprint about 25 over the restricted perimeter we note that this repartition may be specific to irap since the energy used to produce the electricity and heating has a relatively low carbon footprint over the full perimeter the large share from the use of groundbased observatories and spaceborne facilities and the fact that the majority of irap purchases are related to instrument development indicate that research infrastructures represent the most significant challenge for reducing the carbon footprint of research at our institute with 260 staff members employed our results imply that performing research in astronomy and astrophysics at irap according to the standards of 2019 produces average ghg emissions of 28 tco2eyr per person involved in that activity abridged,0
organizations increasingly need to reassess their supply chain strategies in the rapidly modernizing world towards sustainability this is particularly true in the united states where supply chains are very extensive and consume a large number of resources this research paper discusses how ai can support decisionmaking for sustainable supply chains with a special focus on carbon footprints these ai technologies including machine learning predictive analytics and optimization algorithms will enable companies to be more efficient reduce emissions and display regulatory and consumer demands for sustainability among other aspects the paper reviews challenges and opportunities regarding implementing aidriven solutions to promote sustainable supply chain practices in the usa,0
climate change is a critical concern for hpc systems but ghg protocol carbonemission accounting methodologies are difficult for a single system and effectively infeasible for a collection of systems as a result there is no hpcwide carbon reporting and even the largest hpc sites do not do ghg protocol reporting we assess the carbon footprint of hpc focusing on the top 500 systems the key challenge lies in modeling the carbon footprint with limited data availability with the disclosed top500 website data and using a new tool easyc we were able to model the operational carbon of 391 hpc systems and the embodied carbon of 283 hpc systems we further show how this coverage can be enhanced by exploiting additional public information with improved coverage then interpolation is used to produce the first carbon footprint estimates of the top 500 hpc systems they are 14 million mt co2e operational carbon 1 year and 19 million mt co2e embodied carbon we also project how the top 500s carbon footprint will increase through 2030 a key enabler is the easyc tool which models carbon footprint with only a few data metrics we explore availability of data and enhancement showing that coverage can be increased to 98 of top 500 systems for operational and 808 of the systems for embodied emissions,0
realizing a shared responsibility between providers and consumers is critical to manage the sustainability of hpc however while cost may motivate efficiency improvements by infrastructure operators broader progress is impeded by a lack of user incentives we conduct a survey of hpc users that reveals fewer than 30 percent are aware of their energy consumption and that energy efficiency is among users lowest priority concerns one explanation is that existing pricing models may encourage users to prioritize performance over energy efficiency we propose two transparent multiresource pricing schemes energy and carbonbased accounting that seek to change this paradigm by incentivizing more efficient user behavior these two schemes charge for computations based on their energy consumption or carbon footprint respectively rewarding users who leverage efficient hardware and software we evaluate these two pricing schemes via simulation in a prototype and a user study,0
carbon footprint optimization cfo is important for sustainable heavyduty etruck transportation we consider the cfo problem for timely transportation of etrucks where the truck travels from an origin to a destination across a national highway network subject to a deadline the goal is to minimize the carbon footprint by orchestrating path planning speed planning and intermediary charging planning we first show that it is nphard even just to find a feasible cfo solution we then develop a 1f 1 bicriteria approximation algorithm that achieves a carbon footprint within a ratio of 1f to the minimum with no deadline violation and at most a ratio of 1 battery capacity violation for any positive f and  its time complexity is polynomial in the size of the highway network 1f and 1 such algorithmic results are among the best possible unless pnp simulation results based on realworld traces show that our scheme reduces up to 11 carbon footprint as compared to baseline alternatives considering only energy consumption but not carbon footprint,0
sustainable business models also offer banks competitive advantages such as increasing brand reputation and cost reduction however no framework is presented to evaluate the sustainability of banking business models to bridge this theoretical gap the current study using a delphianalytic hierarchy process method firstly developed a sustainable business model to evaluate the sustainability of the business model of banks in the second step the sustainability performance of sixteen banks from eight european countries including norway the uk poland hungary germany france spain and italy assessed the proposed business model components of this study were ranked in terms of their impact on achieving sustainability goals consequently the proposed model components of this study based on their impact on sustainability are respectively value proposition core competencies financial aspects business processes target customers resources technology customer interface and partner network the results of the comparison of the banks studied by each country disclosed that the sustainability of the norwegian and german banks business models is higher than in other counties the studied banks of hungary and spain came in second the banks of the uk poland and france ranked third and finally the italian banks ranked fourth in the sustainability of their business models,0
research institutions are bound to contribute to greenhouse gas emission ghg reduction efforts for several reasons first part of the scientific communitys research deals with climate change issues second scientists contribute to students education they must be consistent and role models third the literature on the carbon footprint of researchers points to the high level of some individual footprints in a quest for consistency and role models scientists teams of scientists or universities have started to quantify their carbon footprints and debate on reduction options indeed measuring the carbon footprint of research activities requires tools designed to tackle its specific features in this paper we present an opensource web application ges 1point5 developed by an interdisciplinary team of scientists from several research labs in france ges 1point5 is specifically designed to estimate the carbon footprint of research activities in france it operates at the scale of research labs ie laboratoires which are the social structures around which research is organized in france and the smallest decision making entities in the french research system the application allows french research labs to compute their own carbon footprint along a standardized open protocol the data collected in a rapidly growing network of labs will be used as part of the labos 1point5 project to estimate frances research carbon footprint at the time of submitting this manuscript 89 research labs had engaged with ges 1point5 to estimate their greenhouse gas emissions we expect that an international adoption of ges 1point5 adapted to fit domestic specifics could contribute to establishing a global understanding of the drivers of the research carbon footprint worldwide and the levers to decrease it,0
large language models llm have significantly transformed various domains including software development these models assist programmers in generating code potentially increasing productivity and efficiency however the environmental impact of utilising these ai models is substantial given their high energy consumption during both training and inference stages this research aims to compare the energy consumption of manual software development versus an llmassisted approach using codeforces as a simulation platform for software development the goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using llm in software development our results show that the llmassisted code generation leads on average to 3272 higher carbon footprint than the manual one moreover there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches,0
despite impressive results deep learningbased technologies also raise severe privacy and environmental concerns induced by the training procedure often conducted in data centers in response alternatives to centralized training such as federated learning fl have emerged perhaps unexpectedly fl is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and social groups advocating for privacy protection textithowever the potential environmental impact related to fl remains unclear and unexplored this paper offers the firstever systematic study of the carbon footprint of fl first we propose a rigorous model to quantify the carbon footprint hence facilitating the investigation of the relationship between fl design and carbon emissions then we compare the carbon footprint of fl to traditional centralized learning our findings show that depending on the configuration fl can emit up to two order of magnitude more carbon than centralized machine learning however in certain settings it can be comparable to centralized learning due to the reduced energy consumption of embedded devices we performed extensive experiments across different types of datasets settings and various deep learning models with fl finally we highlight and connect the reported results to the future challenges and trends in fl to reduce its environmental impact including algorithms efficiency hardware capabilities and stronger industry transparency,0
this paper undertakes an analysis of deforestation in the amazon area using a pathwaysbased approach to sustainability we ground the analysis primarily in the sustainability transitions literature but also draw a bridge with socioecological concepts which helps us to understand the nature of transitions in this context the concept of a deforestation system is developed by examining the interplay of infrastructure technologies narratives and institutions drawing on a literature review and an indepth case study of puerto maldonado in madre de dios peru the paper identifies three pathways for addressing deforestation optimisation natural capital and regenerative change we suggest that while the optimisation pathway provides partial solutions through mitigation and compensation strategies it often reinforces extractivist logics the study also underscores the limitations of natural capital frameworks which tend to rely on centralised governance and marketbased instruments while lacking broader social engagement in contrast our findings emphasise the potential of regenerative strategies rooted in local agency communityled experimentation and contextsensitive institutional arrangements the paper contributes to ongoing debates on biodiversity governance by illustrating how the spatial and longterm dynamics of deforestation interact and why inclusive territorially grounded pathways are crucial for bending the curve of biodiversity loss,0
carbon footprint quantification is key to wellinformed decision making over carbon reduction potential both for individuals and for companies many carbon footprint case studies for products and services have been circulated recently due to the complex relationships within each scenario however the underlying assumptions often are difficult to understand also reusing and adapting a scenario to local or individual circumstances is not a straightforward task to overcome these challenges we propose an open and linked data model for carbon footprint scenarios which improves data quality and transparency by design we demonstrate the implementation of our idea with a webbased data interpreter prototype,0
the carbon footprint associated with large language models llms is a significant concern encompassing emissions from their training inference experimentation and storage processes including operational and embodied carbon emissions an essential aspect is accurately estimating the carbon impact of emerging llms even before their training which heavily relies on gpu usage existing studies have reported the carbon footprint of llm training but only one tool mlco2 can predict the carbon footprint of new neural networks prior to physical training however mlco2 has several serious limitations it cannot extend its estimation to dense or mixtureofexperts moe llms disregards critical architectural parameters focuses solely on gpus and cannot model embodied carbon footprints addressing these gaps we introduce textitcarb an endtoend carbon footprint projection model designed for both dense and moe llms compared to mlco2 carbsignificantly enhances the accuracy of carbon footprint estimations for various llms the source code is released at url,0
the increasing energy consumption and carbon footprint of deep learning dl due to growing compute requirements has become a cause of concern in this work we focus on the carbon footprint of developing dl models for medical image analysis mia where volumetric images of high spatial resolution are handled in this study we present and compare the features of four tools from literature to quantify the carbon footprint of dl using one of these tools we estimate the carbon footprint of medical image segmentation pipelines we choose nnunet as the proxy for a medical image segmentation pipeline and experiment on three common datasets with our work we hope to inform on the increasing energy costs incurred by mia we discuss simple strategies to cutdown the environmental impact that can make model selection and training processes more efficient,0
the annual meeting of the european astronomical society took place in lyon france in 2019 but in 2020 it was held online only due the covid19 pandemic the carbon footprint of the virtual meeting was roughly 3000 times smaller than the facetoface one providing encouragement for more ecologically minded conferencing,0
this paper introduces an infrastructureaware benchmarking framework for quantifying the environmental footprint of llm inference across 30 stateoftheart models in commercial datacenters the framework combines public api performance data with companyspecific environmental multipliers and statistical inference of hardware configurations we additionally utilize crossefficiency data envelopment analysis dea to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes modellevel energy water and carbon metrics results show the most energyintensive models exceed 29 wh per long prompt over 65 times the most efficient systems even a 042 wh short query when scaled to 700m queriesday aggregates to annual electricity comparable to 35000 us homes evaporative freshwater equal to the annual drinking needs of 12m people and carbon emissions requiring a chicagosized forest to offset these findings highlight a growing paradox as ai becomes cheaper and faster global adoption drives disproportionate resource consumption our methodology offers a standardized empirically grounded basis for sustainability benchmarking and accountability in ai deployment,0
deep learning dl can achieve impressive results across a wide variety of tasks but this often comes at the cost of training models for extensive periods on specialized hardware accelerators this energyintensive workload has seen immense growth in recent years machine learning ml may become a significant contributor to climate change if this exponential trend continues if practitioners are aware of their energy and carbon footprint then they may actively take steps to reduce it whenever possible in this work we present carbontracker a tool for tracking and predicting the energy and carbon footprint of training dl models we propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like carbontracker we hope this will promote responsible computing in ml and encourage research into energyefficient deep neural networks,0
digitalization appears as a lever to enhance agriculture sustainability however existing works on digital agricultures own sustainability remain scarce disregarding the environmental effects of deploying digital devices on a largescale we propose a bottomup method to estimate the carbon footprint of digital agriculture scenarios considering deployment of devices over a diversity of farm sizes it is applied to two usecases and demonstrates that digital agriculture encompasses a diversity of devices with heterogeneous carbon footprints and that more complex devices yield higher footprints not always compensated by better performances or scaling gains by emphasizing the necessity of considering the multiplicity of devices and the territorial distribution of farm sizes when modelling digital agriculture deployments this study highlights the need for further exploration of the firstorder effects of digital technologies in agriculture,0
recent advances in distributed learning raise environmental concerns due to the large energy needed to train and move data tofrom data centers novel paradigms such as federated learning fl are suitable for decentralized model training across devices or silos that simultaneously act as both data producers and learners unlike centralized learning cl techniques relying on bigdata fusion and analytics located in energy hungry data centers in fl scenarios devices collaboratively train their models without sharing their private data this article breaks down and analyzes the main factors that influence the environmental footprint of fl policies compared with classical clbigdata algorithms running in data centers the proposed analytical framework takes into account both learning and communication energy costs as well as the carbon equivalent emissions in addition it models both vanilla and decentralized fl policies driven by consensus the framework is evaluated in an industrial setting assuming a realworld robotized workplace results show that fl allows remarkable endtoend energy savings 3040 for wireless systems characterized by low bitjoule efficiency 50 kbitjoule or lower consensusdriven fl does not require the parameter server and further reduces emissions in mesh networks 200 kbitjoule on the other hand all fl policies are slower to converge when local data are unevenly distributed often 2x slower than cl energy footprint and learning loss can be traded off to optimize efficiency,0
deep neural network models are used today in various applications of artificial intelligence the strengthening of which in the face of adversarial attacks is of particular importance an appropriate solution to adversarial attacks is adversarial training which reaches a tradeoff between robustness and generalization this paper introduces a novel framework layer sustainability analysis lsa for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks lsa can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis the lsa framework identifies a list of most vulnerable layers mvl list of the given network the relative error as a comparison measure is used to evaluate representation sustainability of each layer against adversarial inputs the proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layerwise regularization lr over lsa proposals for adversarial training at ie the atlr procedure atlr could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches the proposed idea performs well theoretically and experimentally for stateoftheart multilayer perceptron and convolutional neural network architectures compared with the atlr and its corresponding base adversarial training the classification accuracy of more significant perturbations increased by 1635 2179 and 10730 on moon mnist and cifar10 benchmark datasets respectively the lsa framework is available and published at,0
the need for efficient and sustainable software to improve business and achieve goals cannot be overemphasized sustainable digital services and product delivery cannot be achieved without embracing sustainable software design practices despite the current research progress on software sustainability most software development practitioners in developing countries are unclear about what constitutes software sustainability and often lack the proper understanding of how to implement it in their specific industry domain research efforts from software engineering focused on promoting software sustainability awareness in developed countries and fewer efforts have been channeled to studying the same awareness in developing countries this has affected the level of awareness about sustainable software design practices in most developing countries this research investigates the awareness of software sustainability in the nigerian pension industry and its challenges among practitioners the software development practitioners were engaged and interviewed we offered ways to mitigate the identified challenges and promote the awareness of software sustainability in the pension industry our findings further show that with the right sustainability knowledge the software practitioners in the pension industry have the potential to support their organizations sustainable culture and improve the efficiency of product design and service delivery,0
scientific workflows facilitate the automation of data analysis and are used to process increasing amounts of data therefore they tend to be resourceintensive and longrunning leading to significant energy consumption and carbon emissions with everincreasing emissions from the ict sector it is crucial to quantify and understand the carbon footprint of scientific workflows however existing tooling requires significant effort from users such as setting up power monitoring before executing workloads or translating monitored metrics into the carbon footprints postexecution in this paper we introduce a system to estimate the carbon footprint of nextflow scientific workflows that enables posthoc estimation based on existing workflow traces power models for computational resources utilised and carbon intensity data aligned with the execution time we discuss our automated power modelling approach and compare it with commonly used estimation methodologies furthermore we exemplify several potential use cases and evaluate our energy consumption estimation approach finding its estimation error to be between 39103 outperforming both baseline methodologies,0
the rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact while existing approaches to energy optimization focus on architectural improvements or hardware acceleration there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups we propose an adaptation of kaplan scaling laws to predict gpu energy consumption for diffusion models based on computational complexity flops our approach decomposes diffusion model inference into text encoding iterative denoising and decoding components with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps we conduct comprehensive experiments across four stateoftheart diffusion models stable diffusion 2 stable diffusion 35 flux and qwen on three gpu architectures nvidia a100 a4000 a6000 spanning various inference configurations including resolution 256x256 to 1024x1024 precision fp16fp32 step counts 1050 and classifierfree guidance settings our energy scaling law achieves high predictive accuracy within individual architectures rsquared 09 and exhibits strong crossarchitecture generalization maintaining high rank correlations across models and enabling reliable energy estimation for unseen modelhardware combinations these results validate the computebound nature of diffusion inference and provide a foundation for sustainable ai deployment planning and carbon footprint estimation,0
much debate nowadays is devoted to the impacts of modern information and communication technology on global carbon emissions green information and communication technology is a paradigm creating a sustainable and environmentally friendly computing field that tries to minimize the adverse effects on the environment green information and communication technology are under constant development nowadays thus in this paper we undertake the problem of performance bugs that until recently have never been studied so profoundly we assume that inappropriate software implementations can have a crucial influence on global carbon emissions here we classify those performance bugs and develop inappropriate implementations of four programs written in c to mitigate these simulated performance bugs measuring software and hardware methods that can estimate the increased carbon footprint properly were proposed,0
we explain the mathematical theory of the inputoutput method for carbon footprints computations,0
taiwan plans to rapidly increase its industrial production capacity of electronic components while concurrently setting policies for its ecological transition given that the island is responsible for the manufacturing of a significant part of worldwide electronics components the sustainability of the taiwanese electronics industry is therefore of critical interest in this paper we survey the environmental footprint of 16 taiwanese electronic components manufacturers ecm using corporate sustainability responsibility reports csr based on data from 2015 to 2020 this study finds out that our sample of 16 manufacturers increased its greenhouse gases ghg emissions by 75 per year its final energy and electricity consumption by 88 and 89 and the water usage by 61 we show that the volume of manufactured electronic components and the environmental footprints compiled in this study are strongly correlated which suggests that relative efficiency gains are not sufficient to curb the environmental footprint at the national scale given the critical nature of electronics industry for taiwans geopolitics and economics the observed increase of energy consumption and the slow renewable energy rollout these industrial activities could create a carbon lockin blocking the taiwanese government from achieving its carbon reduction goals and its sustainability policies besides the european union the usa or even china aim at developing an industrial ecosystem targeting sub10nm cmos technology nodes similar to taiwan this study thus provides important insights regarding the environmental implications associated with such a technology roadmap all data and calculation models used in this study are provided as supplementary material,0
aligning with the global mandates pushing towards advanced technologies with reduced resource consumption and environmental impacts the sustainability of wireless networks becomes a significant concern in 6g systems to address this concern a native integration of sustainability into the operations of nextgeneration networks through novel designs and metrics is necessary nevertheless existing wireless sustainability efforts remain limited to energyefficient network designs which fail to capture the environmental impact of such systems in this paper a novel sustainability metric is proposed that captures emissions per bit providing a rigorous measure of the environmental footprint associated with energy consumption in 6g networks this metric also captures how energy computing and communication resource parameters influence the reduction of emissions per bit then the problem of allocating the energy computing and communication resources is posed as a multiobjective mo optimization problem to solve the resulting nonconvex problem our framework leverages mo reinforcement learning morl to maximize the novel sustainability metric alongside minimizing energy consumption and average delays in successfully delivering the data all while adhering to constraints on energy resource capacity the proposed morl methodology computes a global policy that achieves a paretooptimal tradeoff among multiple objectives thereby balancing environmental sustainability with network performance simulation results show that the proposed approach reduces the average emissions per bit by around 26 compared to stateoftheart methods that do not explicitly integrate carbon emissions into their control objectives,0
as deep neural networks dnns continue to drive advancements in artificial intelligence the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes 3d integration improves performance but introduces sustainability challenges making carbonaware optimization essential in this work we propose a carbonefficient design methodology for 3d dnn accelerators leveraging approximate computing and genetic algorithmbased design space exploration to optimize carbon delay product cdp by integrating areaefficient approximate multipliers into multiplyaccumulate mac units our approach effectively reduces silicon area and fabrication overhead while maintaining high computational accuracy experimental evaluations across three technology nodes 45nm 14nm and 7nm show that our method reduces embodied carbon by up to 30 with negligible accuracy drop,0
we propose a statistical model to understand peoples perception of their carbon footprint driven by the observation that few people think of co2 impact in absolute terms we design a system to probe peoples perception from simple pairwise comparisons of the relative carbon footprint of their actions the formulation of the model enables us to take an activelearning approach to selecting the pairs of actions that are maximally informative about the model parameters we define a set of 18 actions and collect a dataset of 2183 comparisons from 176 users on a university campus the early results reveal promising directions to improve climate communication and enhance climate mitigation,0
to improve privacy and ensure qualityofservice qos deep learning dl models are increasingly deployed on internet of things iot devices for data processing significantly increasing the carbon footprint associated with dl on iot covering both operational and embodied aspects existing operational energy predictors often overlook quantized dl models and emerging neural processing units npus while embodied carbon footprint modeling tools neglect noncomputing hardware components common in iot devices creating a gap in accurate carbon footprint modeling tools for iotenabled dl this paper introduces textitcarb an endtoend tool for precise carbon footprint estimation in iotenabled dl with deviations as low as 5 for operational and 323 for embodied carbon footprints compared to actual measurements across various dl models additionally practical applications of carbare showcased through multiple user case studies,0
given recent algorithm software and hardware innovation computing has enabled a plethora of new applications as computing becomes increasingly ubiquitous however so does its environmental impact this paper brings the issue to the attention of computersystems researchers our analysis built on industryreported characterization quantifies the environmental effects of computing in terms of carbon emissions broadly carbon emissions have two sources operational energy consumption and hardware manufacturing and infrastructure although carbon emissions from the former are decreasing thanks to algorithmic software and hardware innovations that boost performance and power efficiency the overall carbon footprint of computer systems continues to grow this work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and datacenter equipment come from hardware manufacturing and infrastructure we therefore outline future directions for minimizing the environmental impact of computing systems,0
transportation plays a critical role in supply chain networks directly impacting cost efficiency delivery reliability and environmental sustainability this study provides an enhanced optimization model for transportation planning emphasizing environmental sustainability and costefficiency an integer linear programming ilp model was developed to minimize the total transportation costs by considering organizational and thirdparty vehicles operational and rental costs while incorporating constraints on carbon emissions the model incorporates multimodal transportation routing and emission caps to select the optimized number of organizational and rental vehicles of different modes in each route to ensure adherence to sustainability goals key innovations include adding carbon emission constraints and optimizing route selection to reduce overall emissions the model was implemented using the gurobi solver and numerical analysis reveals a tradeoff between cost minimization and carbon footprint reduction the results indicate that adopting tight environmental policies increases the costs by around 8 on average while more than 95 of the vehicles utilized will be rented these insights provide actionable guidance for industries aiming to enhance both economic performance and environmental responsibility,0
the commodity and widespread use of online shopping are having an unprecedented impact on climate with emission figures from key actors that are easily comparable to those of a largescale metropolis despite online shopping being fueled by recommender systems recsys algorithms the role and potential of the latter in promoting more sustainable choices is little studied one of the main reasons for this could be attributed to the lack of a dataset containing carbon footprint emissions for the items while building such a dataset is a rather challenging task its presence is pivotal for opening the doors to novel perspectives evaluations and methods for recsys research in this paper we target this bottleneck and study the environmental role of recsys algorithms first we mine a dataset that includes carbon footprint emissions for its items then we benchmark conventional recsys algorithms in terms of accuracy and sustainability as two faces of the same coin we find that recsys algorithms optimized for accuracy overlook greenness and that longer recommendation lists are greener but less accurate then we show that a simple reranking approach that accounts for the items carbon footprint can establish a better tradeoff between accuracy and greenness this reranking approach is modular ready to use and can be applied to any recsys algorithm without the need to alter the underlying mechanisms or retrain models our results show that a small sacrifice of accuracy can lead to significant improvements of recommendation greenness across all algorithms and list lengths arguably this accuracygreenness tradeoff could even be seen as an enhancement of user satisfaction particularly for purposedriven users who prioritize the environmental impact of their choices we anticipate this work will serve as the starting point for studying recsys for more sustainable recommendations,0
in recent years large language models llm such as chatgpt copilot and gemini have been widely adopted in different areas as the use of llms continues to grow many efforts have focused on reducing the massive training overheads of these models but it is the environmental impact of handling user requests to llms that is increasingly becoming a concern recent studies estimate that the costs of operating llms in their inference phase can exceed training costs by 25x per year as llms are queried incessantly the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase further estimates indicate that 500 ml of fresh water is expended for every 2050 requests to llms during inference to address these important sustainability issues with llms we propose a novel framework called slit to cooptimize llm quality of service timetofirst token carbon emissions water usage and energy costs the framework utilizes a machine learning ml based metaheuristic to enhance the sustainability of llm hosting across geodistributed cloud datacenters such a framework will become increasingly vital as llms proliferate,0
large language models llms like gpt3 and bert have revolutionized natural language processing nlp yet their environmental costs remain dangerously overlooked this article critiques the sustainability of llms quantifying their carbon footprint water usage and contribution to ewaste through case studies of models such as gpt4 and energyefficient alternatives like mistral 7b training a single llm can emit carbon dioxide equivalent to hundreds of cars driven annually while data centre cooling exacerbates water scarcity in vulnerable regions systemic challenges corporate greenwashing redundant model development and regulatory voids perpetuate harm disproportionately burdening marginalized communities in the global south however pathways exist for sustainable nlp technical innovations eg model pruning quantum computing policy reforms carbon taxes mandatory emissions reporting and cultural shifts prioritizing necessity over novelty by analysing industry leaders google microsoft and laggards amazon this work underscores the urgency of ethical accountability and global cooperation without immediate action ais ecological toll risks outpacing its societal benefits the article concludes with a call to align technological progress with planetary boundaries advocating for equitable transparent and regenerative ai systems that prioritize both human and environmental wellbeing,0
ambitious scenarios of carbon emission redistribution for mitigating climate change in line with the paris agreement and reaching the sustainable development goal of eradicating poverty have been proposed recently they imply a strong reduction in carbon footprint inequality by 2030 that effectively halves the gini coefficient to about 025 this paper examines feasibility of these scenarios by analyzing the historical evolution of both weighted international inequality in co2 emissions attributed territorially and global inequality in carbon footprints attributed to end consumers for the latter a new dataset is constructed that is more comprehensive than existing ones in both cases we find a decreasing trend in global inequality partially attributed to the move of china from the lower to the middle part of the distribution with footprints more unequal than territorial emissions these results show that realization of the redistributive scenarios would require an unprecedented reduction in global inequality far below historical levels moreover the territorial emissions data available for more recent years up to 2017 show a saturation of the decreasing gini coefficient at a level of 05 this observation confirms an earlier prediction based on maximal entropy reasoning that the lorenz curve converges to the exponential distribution this saturation further undermines feasibility of the redistributive scenarios which are also hindered by structural tendencies that reinforce carbon footprint inequality under global capitalism one way out of this conundrum is a fast decarbonization of the global energy supply in order to decrease global carbon emissions without relying crucially on carbon inequality reduction,0
research software is a class of software developed to support research today a wealth of such software is created daily in universities government and commercial research enterprises worldwide the sustainability of this software faces particular challenges due at least in part to the type of people who develop it these research software engineers rses face challenges in developing and sustaining software that differ from those faced by the developers of traditional software as a result professional associations have begun to provide support advocacy and resources for rses these benefits are critical to sustaining rses especially in environments where their contributions are often undervalued and not rewarded this paper focuses on how professional associations such as the united states research software engineer association usrse can provide this,0
this paper discusses a phd research project testing the hypothesis that using the united nations sustainable development goalssdg as explicit inputs to drive the software requirements engineering process will result in requirements with improved sustainability benefits the research has adopted the design science research method dsrm to test a process named sdg assessment for requirements elicitation sdgare three dsrm cycles are being used to test the hypothesis in safetycritical highprecision softwareintensive systems in aerospace and healthcare initial results from the first two dsrm cycles support the hypothesis however these cycles are in a plandriven waterfall development context and future research agenda would be a similar application in an agile development context,0
progress in machine learning ml comes with a cost to the environment given that training ml models requires significant computational resources energy and materials in the present article we aim to quantify the carbon footprint of bloom a 176billion parameter language model across its life cycle we estimate that blooms final training emitted approximately 247 tonnes ofcarboneqif we consider only the dynamic power consumption and 505 tonnes if we account for all processes ranging from equipment manufacturing to energybased operational consumption we also study the energy requirements and carbon emissions of its deployment for inference via an api endpoint receiving user queries in realtime we conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ml models and future research directions that can contribute towards improving carbon emissions reporting,0
throughout its lifecycle a large language model llm generates a substantially larger carbon footprint during inference than training llm inference requests vary in batch size prompt length and token generation number while cloud providers employ different gpu types and quantities to meet diverse servicelevel objectives for accuracy and latency it is crucial for both users and cloud providers to have a tool that quickly and accurately estimates the carbon impact of llm inferences based on a combination of inference request and hardware configurations before execution estimating the carbon footprint of llm inferences is more complex than training due to lower and highly variable model flops utilization rendering previous equationbased models inaccurate additionally existing machine learning ml prediction methods either lack accuracy or demand extensive training data as they inadequately handle the distinct prefill and decode phases overlook hardwarespecific features and inefficiently sample uncommon inference configurations we introduce coo a graph neural network gnnbased model that greatly improves the accuracy of llm inference carbon footprint predictions compared to previous methods,0
sustainability is focussed on avoiding the longterm depletion of natural resources under the terms of a government plan to tackle climate change a driver for improved sustainability is the cut of greenhouse gas emissions in the uk to almost zero by 2050 with this type of change new themes are continuously being developed which drive complex projects such as the development of new power generation methods which encompass challenging lead times and demanding requirements consideration of the implementation of strategies and key concepts which may engender sustainability within complex projects therefore presents an opportunity for further critical debate review and application through a project management lens sustainability incorporation in project management has been documented in academic literature with this emerging field providing new challenges for example project management education can provide a holistic base for the inculcation of sustainability factors to a range of industries including complex projects likewise practitioner interest and approaches to sustainability in project management are being driven by the recently chartered association for project management apm whilst this body makes a significant contribution to the uk economy across many sectors it also addresses ongoing sustainability challenges therefore by drawing on research and practitioner developments the authors argue that by connecting with the next generation through practice simulation approaches and embedding sustainability issues within project management tools and methods improved focus on sustainability in complex project management may be achieved,0
lowearth orbit leo satellites are increasingly proposed for communication and inorbit computing achieving lowlatency global services however their sustainability remains largely unexamined this paper investigates the carbon footprint of computing in space focusing on lifecycle emissions from launch over orbital operation to reentry we present espas a lightweight tool for estimating carbon intensities across cpu usage memory and networking in orbital vs terrestrial settings three worked examples compare i launch technologies stateoftheart rocket vs potential next generation and ii operational emissions of data center workloads in orbit and on the ground results show that even under optimistic assumptions inorbit systems incur significantly higher carbon costs up to an order of magnitude more than terrestrial equivalents primarily due to embodied emissions from launch and reentry our findings advocate for carbonaware design principles and regulatory oversight in developing sustainable digital infrastructure in orbit,0
followthesun fts is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads it involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels with the significant power consumption of artificial intelligence ai being a subject of extensive debate fts is proposed as a strategy to mitigate the carbon footprint of training ai models however the literature lacks scientific evidence on the advantages of fts to mitigate the carbon footprint of ai workloads in this paper we present the results of an experiment conducted in a partial synthetic scenario to address this research gap we benchmarked four ai algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases no strategy fts and two strategies previously introduced in the state of the art namely flexible start and pause and resume to conduct our experiment we utilized historical carbon intensity data from the year 2021 for seven european cities our results demonstrate that the fts strategy not only achieves average reductions of up to 146 in carbon emissions with peaks of 163 but also helps in preserving the time needed for training,0
climate change is profoundly affecting nearly all aspects of life on earth including human societies economies and health various human activities are responsible for significant greenhouse gas emissions including data centres and other sources of largescale computation although many important scientific milestones have been achieved thanks to the development of highperformance computing the resultant environmental impact has been underappreciated in this paper we present a methodological framework to estimate the carbon footprint of any computational task in a standardised and reliable way based on the processing time type of computing cores memory available and the efficiency and location of the computing facility metrics to interpret and contextualise greenhouse gas emissions are defined including the equivalent distance travelled by car or plane as well as the number of treemonths necessary for carbon sequestration we develop a freely available online tool green algorithms which enables a user to estimate and report the carbon footprint of their computation the green algorithms tool easily integrates with computational processes as it requires minimal information and does not interfere with existing code while also accounting for a broad range of cpus gpus cloud computing local servers and desktop computers finally by applying green algorithms we quantify the greenhouse gas emissions of algorithms used for particle physics simulations weather forecasts and natural language processing taken together this study develops a simple generalisable framework and freely available tool to quantify the carbon footprint of nearly any computation combined with a series of recommendations to minimise unnecessary co2 emissions we hope to raise awareness and facilitate greener computation,0
highperformance computing hpc systems are becoming increasingly waterintensive due to their reliance on waterbased cooling and the energy used in power generation however the water footprint of hpc remains relatively underexploredespecially in contrast to the growing focus on carbon emissions in this paper we present thirstyflops a comprehensive water footprint analysis framework for hpc systems our approach incorporates regionspecific metrics including water usage effectiveness power usage effectiveness and energy water factor to quantify water consumption using realworld data using four representative hpc systems marconi fugaku polaris and frontier as examples we provide implications for hpc system planning and management we explore the impact of regional water scarcity and nuclearbased energy strategies on hpc sustainability our findings aim to advance the development of wateraware environmentally responsible computing infrastructures,0
the demand in computing power has never stopped growing over the years today the performance of the most powerful systems exceeds the exascale unfortunately this growth also comes with everincreasing energy costs leading to a high carbon footprint this paper investigates the evolution of high performance systems in terms of carbon emissions a lot of studies focus on top500 and green500 as the tip of an iceberg to identify trends in the domain in terms of computing performance we propose here to go further in considering the whole span life of several large scale systems and to link the evolution with trajectory toward 2030 more precisely we introduce the energy mix in the analysis of top500 systems and we derive a predictive model for estimating the weight of hpc for the next 5 years,0
growing concerns about climate change and sustainability are driving manufacturers to take significant steps toward reducing their carbon footprints for these manufacturers a first step towards this goal is to identify the environmental impact of the individual components of their products we propose a system leveraging large language models llms to automatically map components from manufacturer bills of materials boms to life cycle assessment lca database entries by using llms to expand on available component information our approach reduces the need for manual data processing paving the way for more accessible sustainability practices,0
effective climate mitigation strategies in cities rely on understanding and mapping urban carbon footprints one significant source of carbon is a product of lifestyle choices and travel behaviors of urban residents although previous research addressed consumption and homerelated footprints activitybased footprints of urban dwellers have garnered less attention this study relies on deidentified human trajectory data from 5 million devices to examine the activitybased carbon footprint in harris county texas our analysis of the heterogeneity of footprints based on places visited and distance traveled reveals significant inequality 10 of users account for 88 of visitationbased footprints and 71 of distancetraveled footprints we also identify the influence of income on activitybased carbon footprint gap of users related to their travel behavior and lifestyle choices with highincome users having larger footprints due to lifestyle choices while low to mediumincome users footprints are due to limited access our findings underscore the need for urban design adjustments to reduce carbonintensive behaviors and to improve facility distribution our conclusions highlight the importance of addressing urban design parameters that shape carbonintensive lifestyle choices and facility distribution decisions which have implications for developing interventions to reduce carbon footprints caused by human activities,0
in this paper a solution for sustainable cloud system is proposed and then implemented on a real testbed the solution composes of optimization of a profit model and introduction of virtual carbon tax to limit environmental footprint of the cloud the proposed multicriteria optimizer of the cloud system suggests new optimum cpu frequencies for cpucores when the local grid energy mix or the cloud workload changes the cloud system is implemented on a blade system and proper middlewares are developed to interact with the blades the experimental results show that it is possible to significantly decrease the targeted environmental footprint of the system and keep it profitable,0
cryptocurrencies are gaining more popularity due to their security making counterfeits impossible however these digital currencies have been criticized for creating a large carbon footprint due to their algorithmic complexity and decentralized system design for proof of work and mining we hypothesize that the carbon footprint of cryptocurrency transactions has a higher dependency on carbonrich fuel sources than green or renewable fuel sources we provide a machine learning framework to model such transactions and correlate them with the electricity generation patterns to estimate and analyze their carbon cost,0
the sustainable foraging problem is a dynamic environment testbed for exploring the forms of agent cognition in dealing with social dilemmas in a multiagent setting the agents need to resist the temptation of individual rewards through foraging and choose the collective longterm goal of sustainability we investigate methods of online learning in neuroevolution and deep recurrent qnetworks to enable agents to attempt the problem oneshot as is often required by wicked social problems we further explore if learning temporal dependencies with long shortterm memory may be able to aid the agents in developing sustainable foraging strategies in the long term it was found that the integration of long shortterm memory assisted agents in developing sustainable strategies for a single agent however failed to assist agents in managing the social dilemma that arises in the multiagent scenario,0
we study the carbon footprint optimization cfo of a heavyduty etruck traveling from an origin to a destination across a national highway network subject to a hard deadline by optimizing path planning speed planning and intermediary charging planning such a cfo problem is essential for carbonfriendly etruck operations however it is notoriously challenging to solve due to i the hard deadline constraint ii positive battery stateofcharge constraints iii nonconvex carbon footprint objective and iv enormous geographical and temporal charging options with diverse carbon intensity indeed we show that the cfo problem is nphard as a key contribution we show that under practical settings it is equivalent to finding a generalized restricted shortest path on a stageexpanded graph which extends the original transportation graph to model charging options compared to alternative approaches our formulation incurs low model complexity and reveals a problem structure useful for algorithm design we exploit the insights to develop an efficient dualsubgradient algorithm that always converges as another major contribution we prove that i each iteration only incurs polynomialtime complexity albeit it requires solving an integer charging planning problem optimally and ii the algorithm generates optimal results if a condition is met and solutions with bounded optimality loss otherwise extensive simulations based on realworld traces show that our scheme reduces up to 28 carbon footprint compared to baseline alternatives the results also demonstrate that etruck reduces 56 carbon footprint than internal combustion engine trucks,0
product sustainability reports provide valuable insights into the environmental impacts of a product and are often distributed in pdf format these reports often include a combination of tables and text which complicates their analysis the lack of standardization and the variability in reporting formats further exacerbate the difficulty of extracting and interpreting relevant information from large volumes of documents in this paper we tackle the challenge of answering questions related to carbon footprints within sustainability reports available in pdf format unlike previous approaches our focus is on addressing the difficulties posed by the unstructured and inconsistent nature of text extracted from pdf parsing to facilitate this analysis we introduce carbonpdfqa an opensource dataset containing questionanswer pairs for 1735 product report documents along with humanannotated answers our analysis shows that gpt4o struggles to answer questions with data inconsistencies to address this limitation we propose carbonpdf an llmbased technique specifically designed to answer carbon footprint questions on such datasets we develop carbonpdf by finetuning llama 3 with our training data our results show that our technique outperforms current stateoftheart techniques including questionanswering qa systems finetuned on table and text data,0
global emissions from fossil fuel combustion and cement production were recorded in 2022 signaling a resurgence to prepandemic levels and providing an apodictic indication that emission peaks have not yet been achieved significant contributions to this upward trend are made by the information and communication technology ict industry due to its substantial energy consumption this shows the need for further exploration of swarm intelligence applications to measure and optimize the carbon footprint within ict all causative factors are evaluated based on the quality of data collection variations from each source are quantified and an objective function related to carbon footprint in ict energy management is optimized emphasis is placed on the asyndetic integration of data sources to construct a convex optimization problem an apodictic necessity to prevent the erosion of accuracy in carbon footprint assessments is addressed complexity percentages ranged from 525 for the bat algorithm to 787 for fast bacterial swarming indicating significant fluctuations in resource intensity among algorithms these findings suggest that we were able to quantify the environmental impact of various swarm algorithms,0
transformative changes in our production and consumption habits are needed to halt biodiversity loss organizations are the way we humans have organized our everyday life and much of our negative environmental impacts also called carbon and biodiversity footprints are caused by organizations here we explore how the accounts of any organization can be exploited to develop an integrated carbon and biodiversity footprint account as a metric we utilize spatially explicit potential global loss of species across all ecosystem types and argue that it can be understood as the biodiversity equivalent the utility of the biodiversity equivalent for biodiversity could be like what carbon dioxide equivalent is for climate we provide a global country specific dataset that organizations experts and researchers can use to assess consumptionbased biodiversity footprints we also argue that the current integration of financial and environmental accounting is superficial and provide a framework for a more robust financial valuetransforming accounting model to test the methodologies we utilized a finnish university as a living lab assigning an offsetting cost to the footprints significantly altered the financial value of the organization we believe such valuetransforming accounting is needed to draw the attention of senior executives and investors to the negative environmental impacts of their organizations,0
the rapid advancement of generative artificial intelligence genai across diverse sectors raises significant environmental concerns notably the carbon emissions from their cloud and high performance computing hpc infrastructure this paper presents sprout an innovative framework designed to address these concerns by reducing the carbon footprint of generative large language model llm inference services sprout leverages the innovative concept of generation directives to guide the autoregressive generation process thereby enhancing carbon efficiency our proposed method meticulously balances the need for ecological sustainability with the demand for highquality generation outcomes employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator sprout demonstrates a significant reduction in carbon emissions by over 40 in realworld evaluations using the llama2 llm and global electricity grid data this research marks a critical step toward aligning ai technology with sustainable practices highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence,0
in this work we examine the potential of physical unclonable functions pufs that have been implemented on nand flash memories using programming disturbances to act as sustainable primitives for the purposes of lightweight cryptography in particular we investigate the ability of such pufs to tolerate temperature and voltage variations and examine the current shortcomings of existing nandflashmemory pufs that are based on programming disturbances as well as how these could potentially be addressed in order to provide more robust and more sustainable security solutions,0
urban areas consume over twothirds of the worlds energy and account for more than 70 percent of global co2 emissions as stated in ipccs global warming of 15c report achieving carbon neutrality by 2050 requires a clear understanding of urban geometry highquality building footprint generation from satellite images can accelerate this predictive process and empower municipal decisionmaking at scale however previous deep learningbased approaches face consequential issues such as scale invariance and defective footprints partly due to everpresent classwise imbalance additionally most approaches require supplemental data such as point cloud data building height information and multiband imagery which has limited availability and are tedious to produce in this paper we propose a modified deeplabv3 module with a dilated resnet backbone to generate masks of building footprints from threechannel rgb satellite imagery only furthermore we introduce an fbeta measure in our objective function to help the model account for skewed class distributions and prevent falsepositive footprints in addition to fbeta we incorporate an exponentially weighted boundary loss and use a crossdataset training strategy to further increase the quality of predictions as a result we achieve stateoftheart performances across three public benchmarks and demonstrate that our rgbonly method produces higher quality visual results and is agnostic to the scale resolution and urban density of satellite imagery,0
neural scaling laws have driven the development of increasingly large language models llms by linking accuracy improvements to growth in parameter count dataset size and compute however these laws overlook the carbon emissions that scale exponentially with llm size this paper presents textitcarbonscaling an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in llm training by integrating models for neural scaling gpu hardware evolution parallelism optimization and carbon estimation textitcarbonscaling quantitatively connects model accuracy to carbon footprint results show that while a powerlaw relationship between accuracy and carbon holds realworld inefficiencies significantly increase the scaling factor hardware technology scaling reduces carbon emissions for small to midsized models but offers diminishing returns for extremely large llms due to communication overhead and underutilized gpus training optimizationsespecially aggressive critical batch size scalinghelp alleviate this inefficiency textitcarbonscaling offers key insights for training more sustainable and carbonefficient llms,0
sustainability has been defined as meeting the needs of the present without compromising the ability of future generations to meet their own needs but what are the needs of the present and are they met from the poor performance of the 2030 sustainable development goals sdg defined by the un in 2015 not even the collective needs of the present seem to be met how to expect not to compromise the needs of the future is the achievement of global world goals incompatible with the characteristic processes of human evolution as some authors have recently suggested simple mathematical models cannot capture the whole breadth of human experience and destiny but on the other hand one should not neglect whatever insights they may provide and what these models teach us is how the behavior pattern parochial cooperation conflict growth was reached and how this pattern in addition to leading to several types of crises is also on the way of the global governance needed to achieve the sdgs,0
the paper presents a cradletogate sustainability assessment methodology specifically designed to evaluate aircraft components in a robust and systematic manner this methodology integrates multicriteria decisionmaking mcdm analysis across ten criteria categorized under environmental impact cost and performance environmental impact is analyzed through life cycle assessment and cost through life cycle costing with both analyses facilitated by simapro software performance is measured in terms of component mass and specific stiffness the robustness of this methodology is tested through various mcdm techniques normalization approaches and objective weighting methods to demonstrate the methodology the paper assesses the sustainability of a fuselage panel comparing nine variants that differ in materials joining techniques and part thicknesses all approaches consistently identify thermoplastic cfrp panels as the most sustainable option with the geometric mean aggregation of weights providing balanced criteria consideration across environmental cost and performance aspects the adaptability of this proposed methodology is illustrated showing its applicability to any aircraft component with the requisite data this structured approach offers critical insights to support sustainable decisionmaking in aircraft component design and procurement,0
cloud computing has become a critical infrastructure for modern society like electric power grids and roads as the backbone of the modern economy it offers subscriptionbased computing services anytime anywhere on a payasyougo basis its use is growing exponentially with the continued development of new classes of applications driven by a huge number of emerging networked devices however the success of cloud computing has created a new global energy challenge as it comes at the cost of vast energy usage currently data centres hosting cloud services worldwide consume more energy than most countries globally by 2025 they are projected to consume 20 of global electricity and emit up to 55 of the worlds carbon emissions in addition a significant part of the energy consumed is transformed into heat which leads to operational problems including a reduction in system reliability and the life expectancy of devices and escalation in cooling requirements therefore for future generations of cloud computing to address the environmental and operational consequences of such significant energy usage they must become energyefficient and environmentally sustainable while continuing to deliver highquality services in this paper we propose a vision for learningcentric approach for the integrated management of new generation cloud computing environments to reduce their energy consumption and carbon footprint while delivering service quality guarantees in this paper we identify the dimensions and key issues of integrated resource management and our envisioned approaches to address them we present a conceptual architecture for energyefficient new generation clouds and early results on the integrated management of resources and workloads that evidence its potential benefits towards energy efficiency and sustainability,0
with the evergrowing adoption of artificial intelligence ai aibased software and its negative impact on the environment are no longer negligible and studying and mitigating this impact has become a critical area of research however it is currently unclear which role environmental sustainability plays during ai adoption in industry and how ai regulations influence green ai practices and decisionmaking in industry we therefore aim to investigate the green ai perception and management of industry practitioners to this end we conducted a total of 11 interviews with participants from 10 different organizations that adopted aibased software the interviews explored three main themes ai adoption current efforts in mitigating the negative environmental impact of ai and the influence of the eu ai act and the corporate sustainability reporting directive csrd our findings indicate that 9 of 11 participants prioritized business efficiency during ai adoption with minimal consideration of environmental sustainability monitoring and mitigation of ais environmental impact were very limited only one participant monitored negative environmental effects regarding applied mitigation practices six participants reported no actions with the others sporadically mentioning techniques like prompt engineering relying on smaller models or not overusing ai awareness and compliance with the eu ai act are low with only one participant reporting on its influence while the csrd drove sustainability reporting efforts primarily in larger companies all in all our findings reflect a lack of urgency and priority for sustainable ai among these companies we suggest that current regulations are not very effective which has implications for policymakers additionally there is a need to raise industry awareness but also to provide userfriendly techniques and tools for green ai practices,0
the energy consumption of any of the rm ee higgs factory projects that can credibly operate immediately after the end of lhc namely three linear colliders clic operating at sqrts380gev and ilc and rm c3 operating at sqrts250 gev and two circular colliders cepc and fccee operating at sqrts240 gev will be everything but negligible future higgs boson studies may therefore have a significant environmental impact this note proposes to include the carbon footprint for a given physics performance as a toplevel gauge for the design optimization and eventually the choice of the future facility the projected footprints per higgs boson produced evaluated using the 2021 carbon emission of available electricity are found to vary by a factor 100 depending on the considered higgs factory project,0
dnn inference known for its significant energy consumption and the resulting high carbon footprint can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day our heuristic algorithm uses larger highaccuracy models during lowintensity periods and smaller loweraccuracy ones during highintensity periods we also introduce a metric carbonemission efficiency which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint the evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80,0
conceived and developed by christopher alexander through his lifes work the nature of order wholeness is defined as a mathematical structure of physical space in our surroundings yet there was no mathematics as alexander admitted then that was powerful enough to capture his notion of wholeness recently a mathematical model of wholeness together with its topological representation has been developed that is capable of addressing not only why a space is good but also how much goodness the space has this paper develops a structural perspective on goodness of space both large and smallscale in order to bridge two basic concepts of space and place through the very concept of wholeness the wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view a space is good genuinely and objectively if its adjacent spaces are good the larger space to which it belongs is good and what is contained in the space is also good eventually goodness of space sustainability of space is considered a matter of fact rather than of opinion under the new view of space space is neither lifeless nor neutral but a living structure capable of being more living or less living or more sustainable or less sustainable under the new view of space geography or architecture will become part of complexity science not only for understanding complexity but also for making and remaking complex or living structures keywords scaling law headtail breaks living structure beauty streets cities,0
in upcoming years the number of internetofthings iot devices is expected to surge up to tens of billions of physical objects however while the iot is often presented as a promising solution to tackle environmental challenges the direct environmental impacts generated over the life cycle of the physical devices are usually overlooked it is implicitly assumed that their environmental burden is negligible compared to the positive impacts they can generate in this paper we present a parametric framework based on hardware profiles to evaluate the cradletogate carbon footprint of iot edge devices we exploit our framework in three ways first we apply it on four use cases to evaluate their respective production carbon footprint then we show that the heterogeneity inherent to iot edge devices must be considered as the production carbon footprint between simple and complex devices can vary by a factor of more than 150x finally we estimate the absolute carbon footprint induced by the worldwide production of iot edge devices through a macroscopic analysis over a 10year period results range from 22 to 562 mtco2eqyear in 2027 depending on the deployment scenarios however the truncation error acknowledged for lca bottomup approaches usually lead to an undershoot of the environmental impacts we compared the results of our use cases with the few reports available from google and apple which suggest that our estimates could be revised upwards by a factor around 2x to compensate for the truncation error worstcase scenarios in 2027 would therefore reach more than 1000 mtco2eqyear this truly stresses the necessity to consider environmental constraints when designing and deploying iot edge devices,0
increased usage of generative ai genai in humancomputer interaction hci research induces a climate impact from carbon emissions due to energy consumption of the hardware used to develop and run genai models and systems the exact energy usage and and subsequent carbon emissions are difficult to estimate in hci research because hci researchers most often use cloudbased services where the hardware and its energy consumption are hidden from plain view the hci genai co2st calculator is a tool designed specifically for the hci research pipeline to help researchers estimate the energy consumption and carbon footprint of using generative ai in their research either a priori allowing for mitigation strategies or experimental redesign or post hoc allowing for transparent documentation of carbon footprint in written reports of the research,0
due to global warming and its detrimental effect every country is responsible to join the global effort to reduce carbon emissions in order to improve the mitigation plan of climate change accurate estimates of carbon emissions population and electricity consumption are critical carbon footprint is significantly linked to the socioeconomic development of the country which can be reflected in the citys infrastructure and urbanization we may be able to estimate the carbon footprint population growth and electricity consumption of a city by observing the nighttime light reflecting its urbanization this is more challenging in oilproducing countries where urbanization can be more complicated in this study we are therefore investigating the possibility of correlating the remotely sensed nppviirs nighttime light ntl estimation with the aforementioned socioeconomic indicators daily nppviirs ntl were obtained for the period between 2012 to 2021 for the united arab emirates uae which is one of the top oil producing countries the socioeconomic indicators of the uae including the population electricity consumption and carbon dioxide emissions have been obtained for the same period the analysis of the correlation between the ntls and the population indicates that there is a high correlation of more than 09 there is also a very good correlation of 07 between ntls and carbon emissions and electricity consumption however these correlations differ from one city to another for example dubai has shown the highest correlation between population and ntls r2 08 however the correlation was the lowest in alain a rural city r2 04 with maximum electricity consumption of 11e04 gwh these results demonstrate that ntls can be considered as a promising proxy for carbon footprint and urbanization in oilproducing regions,0
the distributed consensus mechanism is the backbone of the rapidly developing blockchain network blockchain platforms consume vast amounts of electricity based on the current consensus mechanism of proof of work here we point out an advanced consensus mechanism named proof of stake that can eliminate the extensive energy consumption of the current powbased blockchain we comprehensively elucidate the current and projected energy consumption and carbon footprint of the pow and pos based bitcoin and ethereum blockchain platforms,0
the product carbon footprint pcf is crucial for decarbonizing the supply chain as it measures the direct and indirect greenhouse gas emissions caused by all activities during the products life cycle however pcf accounting often requires expert knowledge and significant time to construct life cycle models in this study we test and compare the emergent ability of five large language models llms in modeling the cradletogate life cycles of products and generating the inventory data of inputs and outputs revealing their limitations as a generalized pcf knowledge database by utilizing llms we propose an automatic aidriven pcf accounting framework called autopcf which also applies deep learning algorithms to automatically match calculation parameters and ultimately calculate the pcf the results of estimating the carbon footprint for three case products using the autopcf framework demonstrate its potential in achieving automatic modeling and estimation of pcf with a large reduction in modeling time from days to minutes,0
connectivity onthego has been one of the most impressive technological achievements in the 2010s decade however multiple studies show that this has come at an expense of increased carbon footprint that also rivals the entire aviation sectors carbon footprint the two major contributors of this increased footprint are a smartphone batteries which affect the embodied footprint and b basestations that occupy everincreasing energy footprint to provide the last mile wireless connectivity to smartphones the rootcause of both these turn out to be the same which is communicating over the lastmile lossy wireless medium we show in this paper titled densquer how basestation densification which is to replace a single larger basestation with multiple smaller ones reduces the effect of the lastmile wireless and in effect conquers both these adverse sources of increased carbon footprint backed by a opensource raytracing computation framework sionna we show how a strategic densification strategy can minimize the number of required smaller basestations to practically achievable numbers which lead to about 3x powersavings in the basestation network also densquer is able to also reduce the required deployment height of basestations to as low as 15m that makes the smaller cells easily deployable on treesstreet poles instead of requiring a dedicated tower further by utilizing newly introduced hardware power rails in google pixel 7a and above phones we also show that this strategic densified network leads to reduction in mobile transmit power by 1015 db leading to about 3x reduction in total cellular power consumption and about 50 increase in smartphone battery life when it communicates data via the cellular network,0
in response to the european commissions aim of cutting carbon emissions by 2050 there is a growing need for cuttingedge solutions to promote lowcarbon energy consumption in public infrastructures this paper introduces a proof of concept poc that integrates the transparency and immutability of blockchain and the internet of things iot to enhance energy efficiency in tangible governmentheld public assets focusing on curbing carbon emissions our system design utilizes a forecasting and optimization framework inscribing the scheduled operations of heat pumps on a public sector blockchain registering usage metrics on the blockchain facilitates the verification of energy conservation allows transparency in public energy consumption and augments public awareness of energy usage patterns the system finetunes the operations of electric heat pumps prioritizing their use during lowcarbon emission periods in power systems occurring during high renewable energy generations adaptive temperature configuration and schedules enable energy management in public venues but blockchains processing power and latency may represent bottlenecks setting scalability limits however the proofofconcept weakness and other barriers are surpassed by the public sector blockchain advantages leading to future research and tech innovations to fully exploit the synergies of blockchain and iot in harnessing sustainable lowcarbon energy in the public domain,0
the scientific data about the state of our planet presented at the 2012 rio20 summit documented that todays human family lives even less sustainably than it did in 1992 the data indicate furthermore that the environmental impacts from our current economic activities are so large that we are approaching situations where potentially controllable regional problems can easily lead to uncontrollable global disasters assuming that 1 the majority of the human family once adequately informed wants to achieve a sustainable way of life and 2 that the development towards sustainability roadmap will be based on scientific principles one must begin with unambiguous and quantifiable definitions of these goals as will be demonstrated the well known scientific method to define abstract and complex issues by their negation satisfies these requirements following this new approach it also becomes possible to decide if proposed and actual policies changes will make our way of life less unsustainable and thus move us potentially into the direction of sustainability furthermore if potentially dangerous tipping points are to be avoided the transition roadmap must include some minimal speed requirements combining the negation method and the time evolution of that remaining natural capital in different domains the transition speed for a development towards sustainability can be quantified at local regional and global scales the presented ideas allow us to measure the rate of natural capital depletion and the rate of restoration that will be required if humanity is to avoid reaching a sustainable future by a collapse transition,0
efficient and dynamic path planning has become an important topic for urban areas with larger density of connected vehicles cv which results in reduction of travel time and directly contributes to environmental sustainability through reducing energy consumption cvs exploit the cellular wireless vehicletoeverything cv2x communication technology to disseminate the vehicletoinfrastructure v2i messages to the basestation bs to improve situation awareness on urban roads in this paper we investigate radio resource management rrm in such a framework to minimize the age of information aoi so as to enhance path planning results we use the fact that v2i messages with lower aoi value result in less error in estimating the road capacity and more accurate path planning through simulations we compare road travel times and volume over capacity vc against different levels of aoi and demonstrate the promising performance of the proposed framework,0
in this paper energy efficient resource allocation is considered for an uplink hybrid system where nonorthogonal multiple access noma is integrated into orthogonal multiple access oma to ensure the quality of service for the users a minimum rate requirement is predefined for each user we formulate an energy efficiency ee maximization problem by jointly optimizing the user clustering channel assignment and power allocation to address this hard problem a manytoone bipartite graph is first constructed considering the users and resource blocks rbs as the two sets of nodes based on swap matching a joint userrb association and power allocation scheme is proposed which converges within a limited number of iterations moreover for the power allocation under a given userrb association we first derive the feasibility condition if feasible a lowcomplexity algorithm is proposed which obtains optimal ee under any successive interference cancellation sic order and an arbitrary number of users in addition for the special case of two users per cluster analytical solutions are provided for the two sic orders respectively these solutions shed light on how the power is allocated for each user to maximize the ee numerical results are presented which show that the proposed joint userrb association and power allocation algorithm outperforms other hybrid multiple access based and omabased schemes,0
this work introduces ecolife the first carbonaware serverless function scheduler to cooptimize carbon footprint and performance ecolife builds on the key insight of intelligently exploiting multigeneration hardware to achieve high performance and lower carbon footprint ecolife designs multiple novel extensions to particle swarm optimization pso in the context of serverless execution environment to achieve high performance while effectively reducing the carbon footprint,0
blockchain technologies are considered one of the most disruptive innovations of the last decade enabling secure decentralized trustbuilding however in recent years with the rapid increase in the energy consumption of blockchainbased computations for cryptocurrency mining there have been growing concerns about their sustainable operation in electric grids this paper investigates the trifactor impact of such large loads on carbon footprint grid reliability and electricity market price in the texas grid we release opensource highresolution data to enable highresolution modeling of influencing factors such as location and flexibility we reveal that the permegawatthour carbon footprint of cryptocurrency mining loads across locations can vary by as much as 50 of the crude system average estimate we show that the flexibility of mining loads can significantly mitigate power shortages and market disruptions that can result from the deployment of mining loads these findings suggest policymakers to facilitate the participation of large mining facilities in wholesale markets and require them to provide mandatory demand response,0
at present analytical labonchip devices find their usage in different facets of chemical analysis biological analysis point of care analysis biosensors etc in addition graphene has already established itself as an essential component of advanced labonchip devices graphenebased labonchip devices have achieved appreciable admiration because of their peerless performance in comparison to others however to accomplish a sustainable future a device must undergo greenscreening to check its environmental compatibility thus extensive research is carried out globally to make the graphenebased labonchip green though it is yet to be achieved nevertheless as a ray of hope there are few existing strategies that can be stitched together for feasible fabrication of environmentfriendly green graphenebased analytical labonchip and those prospective pathways are reviewed in this paper,0
photonic integrated circuits are finding use in a variety of applications including optical transceivers lidar biosensing photonic quantum computing and machine learning ml in particular with the exponentially increasing sizes of ml models photonicsbased accelerators are getting special attention as a sustainable solution because they can perform ml inferences with multiple orders of magnitude higher energy efficiency than cmosbased accelerators however recent studies have shown that hardware manufacturing and infrastructure contribute significantly to the carbon footprint of computing devices even surpassing the emissions generated during their use for example the manufacturing process accounts for 74 of the total carbon emissions from apple in 2019 this prompts us to ask if we consider both the embodied manufacturing and operational carbon cost of photonics is it indeed a viable avenue for a sustainable future so in this paper we build a carbon footprint model for photonic chips and investigate the sustainability of photonicsbased accelerators by conducting a case study on adept a photonicsbased accelerator for deep neural network inference our analysis shows that photonics can reduce both operational and embodied carbon footprints with its high energy efficiency and at least 4times less fabrication carbon cost per unit area than 28 nm cmos,0
recent advancements in large language models llms have led to their widespread adoption and largescale deployment across various domains however their environmental impact particularly during inference has become a growing concern due to their substantial energy consumption and carbon footprint existing research has focused on inference computation alone overlooking the analysis and optimization of carbon footprint in networkaided llm service systems to address this gap we propose aolo a framework for analysis and optimization for lowcarbon oriented wireless llm services aolo introduces a comprehensive carbon footprint model that quantifies greenhouse gas emissions across the entire llm service chain including computational inference and wireless communication furthermore we formulate an optimization problem aimed at minimizing the overall carbon footprint which is solved through joint optimization of inference outputs and transmit power under qualityofexperience and system performance constraints to achieve this joint optimization we leverage the energy efficiency of spiking neural networks snns by adopting snn as the actor network and propose a lowcarbonoriented optimization algorithm ie snnbased deep reinforcement learning sdrl comprehensive simulations demonstrate that sdrl algorithm significantly reduces overall carbon footprint achieving an 1877 reduction compared to the benchmark soft actorcritic highlighting its potential for enabling more sustainable llm inference services,0
one important goal in sustainability is making technologies available to the maximum possible number of individuals and especially to those living in less developed areas goal 9 of sdg however the diffusion of technical knowledge is hindered by a number of factors among which the intellectual property rights ipr system plays a primary role while opinions about the real effect of iprs in stimulating and disseminating innovation differ there is a growing number of authors arguing that a different approach may be more effective in promoting global development the success of the open source os model in the field of software has led analysts to speculate whether this paradigm can be extended to other fields key to this model are both free access to knowledge and the right to use other peoples results abstract after reviewing the main features of the os model we explore different areas where it can be profitably applied such as hardware design and production we finally discuss how academical institutions can and should help diffusing the os philosophy and practice widespread use of os software fostering of research projects aimed to use and develop os software and hardware the use of open education tools and a strong commitment to open access publishing are some of the discussed examples,0
as global warming soars the need to assess and reduce the environmental impact of recommender systems is becoming increasingly urgent despite this the recommender systems community hardly understands addresses and evaluates the environmental impact of their work in this study we examine the environmental impact of recommender systems research by reproducing typical experimental pipelines based on our results we provide guidelines for researchers and practitioners on how to minimize the environmental footprint of their work and implement green recommender systems recommender systems designed to minimize their energy consumption and carbon footprint our analysis covers 79 papers from the 2013 and 2023 acm recsys conferences comparing traditional good oldfashioned ai models with modern deep learning models we designed and reproduced representative experimental pipelines for both years measuring energy consumption using a hardware energy meter and converting it into co2 equivalents our results show that papers utilizing deep learning models emit approximately 42 times more co2 equivalents than papers using traditional models on average a single deep learningbased paper generates 2909 kilograms of co2 equivalents more than the carbon emissions of a person flying from new york city to melbourne or the amount of co2 sequestered by one tree over 260 years this work underscores the urgent need for the recommender systems and wider machine learning communities to adopt green ai principles balancing algorithmic advancements and environmental responsibility to build a sustainable future with aipowered personalization,0
research infrastructures have been identified as an important source of greenhouse gas emissions of astronomical research based on a comprehensive inventory of 1211 groundbased observatories and space missions we assessed the evolution of the number of astronomical facilities and their carbon footprint from 1945 to 2022 we found that space missions dominate greenhouse gas emissions in astronomy showing an important peak at the end of the 1960ies followed by a decrease that has turned again into a rise over the last decade extrapolating past trends we predict that greenhouse gas emissions from astronomical facilities will experience no strong decline in the future and may even rise substantially unless research practices are changed we demonstrate that a continuing growth in the number of operating astronomical facilities is not environmentally sustainable these findings should motivate the astronomical community to reflect about the necessary evolutions that would put astronomical research on a sustainable path,0
contemporary developments in generative ai are rapidly transforming the field of medical ai these developments have been predominantly driven by the availability of large datasets and high computing power which have facilitated a significant increase in model capacity despite their considerable potential these models demand substantially high power leading to high carbon dioxide co2 emissions given the harm such models are causing to the environment there has been little focus on the carbon footprints of such models this study analyzes carbon emissions from 2d and 3d latent diffusion models ldms during training and data generation phases revealing a surprising finding the synthesis of large images contributes most significantly to these emissions we assess different scenarios including model sizes image dimensions distributed training and data generation steps our findings reveal substantial carbon emissions from these models with training 2d and 3d models comparable to driving a car for 10 km and 90 km respectively the process of data generation is even more significant with co2 emissions equivalent to driving 160 km for 2d models and driving for up to 3345 km for 3d synthesis additionally we found that the location of the experiment can increase carbon emissions by up to 94 times and even the time of year can influence emissions by up to 50 these figures are alarming considering they represent only a single training and data generation phase for each model our results emphasize the urgent need for developing environmentally sustainable strategies in generative ai,0
we present a pioneering estimate of the global yearly greenhouse gas emissions of a largescale astrophysics experiment over several decades the giant array for neutrino detection grand the project aims at detecting ultrahigh energy neutrinos with a 200000 radio antenna array over 200000km2 as of the 2030s with a fully transparent methodology based on open source data we calculate the emissions related to three unavoidable sources travel digital technologies and hardware equipment we find that these emission sources have a different impact depending on the stages of the experiment digital technologies and travel prevail for the smallscale prototyping phase grandproto300 whereas hardware equipment material production and transportation and data transferstorage largely outweigh the other emission sources in the largescale phase grand200k in the midscale phase grand10k the three sources contribute equally this study highlights the considerable carbon footprint of a largescale astrophysics experiment but also shows that there is room for improvement we discuss various lines of actions that could be implemented the grand project being still in its prototyping stage our results provide guidance to the future collaborative practices and instrumental design in order to reduce its carbon footprint,0
development of modern deep learning methods has been driven primarily by the push for improving model efficacy accuracy metrics this sole focus on efficacy has steered development of largescale models that require massive resources and results in considerable carbon footprint across the model lifecycle in this work we explore how physics inductive biases can offer useful tradeoffs between model efficacy and model efficiency compute energy and carbon we study a variety of models for spatiotemporal forecasting a task governed by physical laws and wellsuited for exploring different levels of physics inductive bias we show that embedding physics inductive biases into the model design can yield substantial efficiency gains while retaining or even improving efficacy for the tasks under consideration in addition to using standard physicsinformed spatiotemporal models we demonstrate the usefulness of more recent models like flow matching as a general purpose method for spatiotemporal forecasting our experiments show that incorporating physics inductive biases offer a principled way to improve the efficiency and reduce the carbon footprint of machine learning models we argue that model efficiency along with model efficacy should become a core consideration driving machine learning model development and deployment,0
despite the popularity of the pretrain then finetune paradigm in the nlp community existing work quantifying energy costs and associated carbon emissions has largely focused on language model pretraining although a single pretraining run draws substantially more energy than finetuning finetuning is performed more frequently by many more individual actors and thus must be accounted for when considering the energy and carbon footprint of nlp in order to better characterize the role of finetuning in the landscape of energy and carbon emissions in nlp we perform a careful empirical study of the computational costs of finetuning across tasks datasets hardware infrastructure and measurement modalities our experimental results allow us to place finetuning energy and carbon costs into perspective with respect to pretraining and inference and outline recommendations to nlp researchers and practitioners who wish to improve their finetuning energy efficiency,0
in the context of global sustainability buildings are significant consumers of energy emphasizing the necessity for innovative strategies to enhance efficiency and reduce environmental impact this research leverages extensive raw data from building infrastructures to uncover energy consumption patterns and devise strategies for optimizing resource use we investigate the factors influencing energy efficiency and cost reduction in buildings utilizing lasso regression decision tree and random forest models for accurate energy use forecasting our study delves into the factors affecting energy utilization focusing on primary fuel and electrical energy and discusses the potential for substantial cost savings and environmental benefits significantly we apply metaheuristic techniques to enhance the decision tree algorithm resulting in improved predictive precision this enables a more nuanced understanding of the characteristics of buildings with high and low energy efficiency potential our findings offer practical insights for reducing energy consumption and operational costs contributing to the broader goals of sustainable development and cleaner production by identifying key drivers of energy use in buildings this study provides a valuable framework for policymakers and industry stakeholders to implement cleaner and more sustainable energy practices,0
machine learning ml workloads have rapidly grown in importance but raised concerns about their carbon footprint four best practices can reduce ml training energy by up to 100x and co2 emissions up to 1000x by following best practices overall ml energy use across research development and production held steady at 15 of googles total energy use for the past three years if the whole ml field were to adopt best practices total carbon emissions from training would reduce hence we recommend that ml papers include emissions explicitly to foster competition on more than just model quality estimates of emissions in papers that omitted them have been off 100x100000x so publishing emissions has the added benefit of ensuring accurate accounting given the importance of climate change we must get the numbers right to make certain that we work on its biggest challenges,0
climate change which is now considered one of the biggest threats to humanity is also the reason behind various other environmental concerns continued negligence might lead us to an irreparably damaged environment after the partial failure of the paris agreement it is quite evident that we as individuals need to come together to bring about a change on a large scale to have a significant impact this paper discusses our approach towards obtaining a realistic measure of the carbon footprint index being consumed by a user through daytoday activities performed via a smart phone app and offering incentives in weekly and monthly leader board rankings along with a reward system the app helps ease out decision makings on tasks like travel shopping electricity consumption and gain a different and rather numerical perspective over the daily choices,0
there is an urgent need to control global warming caused by humans to achieve a sustainable future co2 levels are rising steadily and while countries worldwide are actively moving toward the sustainability goals proposed during the paris agreement in 2015 we are still a long way to go from achieving a sustainable mode of global operation the increased popularity of cryptocurrencies since the introduction of bitcoin in 2009 has been accompanied by an increasing trend in greenhouse gas emissions and high electrical energy consumption popular energy tracking studies eg digiconomist and the cambridge bitcoin energy consumption index cbeci have estimated energy consumption ranges of 2996 twh to 13512 twh and 2641 twh to 17698 twh respectively for bitcoin as of july 2021 which are equivalent to the energy consumption of countries such as sweden and thailand the latest estimate by digiconomist on carbon footprints shows a 6418 mtco2 emission by bitcoin as of july 2021 close to the emissions by greece and oman this review compiles estimates made by various studies from 2018 to 2021 we compare with the energy consumption and carbon footprints of these cryptocurrencies with countries around the world and centralized transaction methods such as visa we identify the problems associated with cryptocurrencies and propose solutions that can help reduce their energy usage and carbon footprints finally we present case studies on cryptocurrency networks namely ethereum 20 and pi network with a discussion on how they solve some of the challenges we have identified,0
in this paper we study the energy efficiency ee maximization problem for an uplink millimeter wave massive multipleinput multipleoutput system with nonorthogonal multiple access noma multiple twouser clusters are formed according to their channel correlation and gain difference and noma is applied within each cluster then a hybrid analogdigital beamforming scheme is designed to lower the number of radio frequency chains at the base station bs on this basis we formulate a power allocation pa problem to maximize the ee under users quality of service requirements an iterative algorithm is proposed to obtain the pa moreover an enhanced noma scheme is also proposed by exploiting the global information at the bs numerical results show that the proposed noma schemes achieve superior ee when compared with the conventional orthogonal multiple access scheme,0
cloud platforms are increasing their emphasis on sustainability and reducing their operational carbon footprint a common approach for reducing carbon emissions is to exploit the temporal flexibility inherent to many cloud workloads by executing them in periods with the greenest energy and suspending them at other times since such suspendresume approaches can incur long delays in job completion times we present a new approach that exploits the elasticity of batch workloads in the cloud to optimize their carbon emissions our approach is based on the notion of carbon scaling similar to cloud autoscaling where a job dynamically varies its server allocation based on fluctuations in the carbon cost of the grids energy we develop a greedy algorithm for minimizing a jobs carbon emissions via carbon scaling that is based on the wellknown problem of marginal resource allocation we implement a carbonscaler prototype in kubernetes using its autoscaling capabilities and an analytic tool to guide the carbonefficient deployment of batch applications in the cloud we then evaluate carbonscaler using realworld machine learning training and mpi jobs on a commercial cloud platform and show that it can yield i 51 carbon savings over carbonagnostic execution ii 37 over a stateoftheart suspendresume policy and iii 8 over the best static scaling policy,0
the relentless growth of largescale artificial intelligence ai has created unprecedented demand for computational power straining the energy bandwidth and scaling limits of conventional electronic platforms electronicphotonic integrated circuits epics have emerged as a compelling platform for nextgeneration ai systems offering inherent advantages in ultrahigh bandwidth low latency and energy efficiency for computing and interconnection beyond performance epics also hold unique promises for sustainability fabricated in relaxed process nodes with fewer metal layers and lower defect densities photonic devices naturally reduce embodied carbon footprint cfp compared to advanced digital electronic integrated circuits while delivering ordersofmagnitude higher computing performance and interconnect bandwidth to further advance the sustainability of photonic ai systems we explore how electronicphotonic design automation epda and crosslayer codesign methodologies can amplify these inherent benefits we present how advanced epda tools enable more compact layout generation reducing both chip area and metal layer usage we will also demonstrate how crosslayer devicecircuitarchitecture codesign unlocks new sustainability gains for photonic hardware ultracompact photonic circuit designs that minimize chip area cost reconfigurable hardware topology that adapts to evolving ai workloads and intelligent resilience mechanisms that prolong lifetime by tolerating variations and faults by uniting intrinsic photonic efficiency with epda and codesigndriven gains in area efficiency reconfigurability and robustness we outline a vision for lifelongsustainable electronicphotonic ai systems this perspective highlights how epic ai systems can simultaneously meet the performance demands of modern ai and the urgent imperative for sustainable computing,0
terahertz thz communication is widely deemed the next frontier of wireless networks owing to the abundant spectrum resources in the thz band whilst thz signals suffer from severe propagation losses a massive antenna array can be deployed at the base station bs to mitigate those losses through beamforming nevertheless a very large number of antennas increases the bss hardware complexity and power consumption and hence it can lead to poor energy efficiency ee to surmount this fundamental problem we propose a novel array design based on superdirectivity and nonuniform interelement spacing specifically we exploit the mutual coupling between closely spaced elements to form superdirective pairs a unique property of them is that all require the same excitation amplitude and thus can be driven by a single radio frequency chain akin to conventional phased arrays moreover they facilitate multiport impedance matching which ensures maximum power transfer for any beamforming angle after addressing the implementation issues of superdirectivity we show that the number of bs antennas can be effectively reduced without sacrificing the achievable rate simulation results demonstrate that our design offers huge ee gains compared to uncoupled arrays with uniform spacing and hence could be a radical solution for future thz systems,0
to meet the urgent requirements for the climate change mitigation several proactive measures of energy efficiency have been implemented in maritime industry many of these practices depend highly on the onboard data of vessels operation and environmental conditions in this paper a high resolution onboard data from passenger vessels in shortsea shipping sss have been collected and preprocessed we first investigated the available data to deploy it effectively to model the physics of the vessel and hence the vessel performance since in sss the weather measurements and forecasts might have not been in temporal and spatial resolutions that accurately representing the actual environmental conditions then we proposed a datadriven modeling approach for vessel energy efficiency this approach addresses the challenges of data representation and energy modeling by combining and aggregating data from multiple sources and seamlessly integrates explainable artificial intelligence xai to attain clear insights about the energy efficiency for a vessel in sss after that the developed model of energy efficiency has been utilized in developing a framework for optimizing the vessel voyage to minimize the fuel consumption and meeting the constraint of arrival time moreover we developed a spatial clustering approach for labeling the vessel paths to detect the paths for vessels with operating routes of repeatable and semirepeatable paths,0
with the use of the appropriate technology such as photovoltaics and seawater desalination humans have the ability to sustainably increase their production of food and energy while minimising detrimental impacts on the earth system,0
in this paper we study the resource allocation for an intelligent reflecting surface irsassisted uplink system where the base station is equipped with multiple antennas we propose to jointly optimize the transmit power of the users active beamforming at the base station and passive beamforming at the irs to maximize the overall system energy efficiency while maintaining users minimum rate constraints this problem belongs to a highly intractable nonconvex biquadratic programming problem for which an iterative solution based on block coordinate descent is proposed extensive simulations are conducted to demonstrate the effectiveness of the proposed scheme and the benefits of having more elements at the irs,0
in sprite the stateoftheart significantly reducing carbon footprint cf in communications systems remains urgent we address this challenge in the context of edge computing the carbon intensity of electricity supply largely varies spatially as well as temporally this together with energy sharing via a battery management system bms justifies the potential of cforiented task offloading by redistributing the computational tasks in time and space in this paper we consider optimal task scheduling and offloading as well as battery charging to minimize the total cf we formulate this cf minimization problem as an integer linear programming model however we demonstrate that via a graphbased reformulation the problem can be cast as a minimumcost flow problem this finding reveals that global optimum can be admitted in polynomial time numerical results using realworld data show that optimization can reduce up to 833 of the total cf,0
artificial intelligence ai is currently spearheaded by machine learning ml methods such as deep learning which have accelerated progress on many tasks thought to be out of reach of ai these recent ml methods are often compute hungry energy intensive and result in significant green house gas emissions a known driver of anthropogenic climate change additionally the platforms on which ml systems run are associated with environmental impacts that go beyond the energy consumption driven carbon emissions the primary solution lionized by both industry and the ml community to improve the environmental sustainability of ml is to increase the compute and energy efficiency with which ml systems operate in this perspective we argue that it is time to look beyond efficiency in order to make ml more environmentally sustainable we present three highlevel discrepancies between the many variables that influence the efficiency of ml and the environmental sustainability of ml firstly we discuss how compute efficiency does not imply energy efficiency or carbon efficiency second we present the unexpected effects of efficiency on operational emissions throughout the ml model life cycle and finally we explore the broader environmental impacts that are not accounted by efficiency these discrepancies show as to why efficiency alone is not enough to remedy the adverse environmental impacts of ml instead we argue for systems thinking as the next step towards holistically improving the environmental sustainability of ml,0
in recent years there has been an increased emphasis on reducing the carbon emissions from electricity consumption many organizations have set ambitious targets to reduce the carbon footprint of their operations as a part of their sustainability goals the carbon footprint of any consumer of electricity is computed as the product of the total energy consumption and the carbon intensity of electricity thirdparty carbon information services provide information on carbon intensity across regions that consumers can leverage to modulate their energy consumption patterns to reduce their overall carbon footprint in addition to accelerate their decarbonization process large electricity consumers increasingly acquire power purchase agreements ppas from renewable power plants to obtain renewable energy credits that offset their brown energy consumption there are primarily two methods for attributing carbonfree energy or renewable energy credits to electricity consumers locationbased and marketbased these two methods yield significantly different carbon intensity values for various consumers as there is a lack of consensus which method to use for carbonfree attribution a concurrent application of both approaches is observed in practice in this paper we show that such concurrent applications can cause discrepancies in the carbon savings reported by carbon optimization techniques our analysis across three stateoftheart carbon optimization techniques shows possible overestimation of up to 551 in the carbon reductions reported by the consumers and even increased emissions for consumers in some cases we also find that carbon optimization techniques make different decisions under the marketbased method and locationbased method and the marketbased method can yield up to 282 less carbon savings than those claimed by the locationbased method for consumers without ppas,0
the objective of this article is to investigate the effect of activelearning pedagogy on learners academic achievement and their attitude toward mathematics using both quantitative and qualitative methods we cultivated sustainable learning in mathematics education for college freshmen n 55 by exposing them to both the conventional teaching method ctm and flipped classroom pedagogy fcp by splitting them into control and experimental groups alternately n1 24 n2 31 and by selecting the four most challenging topics in college algebra we measured their cognitive gains quantitatively via a sequence of pre and posttests both groups improved academically over time across all these four topics with statistically very significant outcomes p 0001 although they were not always statistically significant p 005 in some topics the posttest results suggest that generally the fcp trumps the ctm in cognitive gains except for the first topic on factorization where the opposite is true with a very statistically significant mean difference p 0001 by examining noncognitive gains qualitatively we analyzed the students feedback on the fcp and their responses to a perception inventory the finding suggests a favorable response toward the fcp with primary improvements in the attitudes toward mathematics and increased levels of cooperation among students since these students are so happy to have control of their own learning they were more relaxed motivated confident active and responsible in learning under the fcp we are confident that although this study is relatively small in scale it will yield incremental and longlasting effects not only for the learners themselves but also for other roletakers in education sectors who aspire in nurturing sustainable longlife learning and achieving sustainable development goals successfully,0
as computing power advances the environmental cost of semiconductor manufacturing and operation has become a critical concern however current sustainability metrics fail to quantify carbon emissions at the transistor level the fundamental building block of modern processors this paper introduces a carbon per transistor cpt formula a novel approach and green implementation metric to measuring the co2 footprint of semiconductor chips from fabrication to endoflife by integrating emissions from silicon crystal growth wafer production chip manufacturing and operational power dissipation the cpt formula provides a scientifically rigorous benchmark for evaluating the sustainability of computing hardware using realworld data from intel core i913900k amd ryzen 9 7950x and apple m1m2m3 processors we reveal a startling insightmanufacturing emissions dominate contributing 60125 kg co2 per cpu far exceeding operational emissions over a typical device lifespan notably apples hightransistorcount mseries chips despite their energy efficiency exhibit a significantly larger carbon footprint than traditional processors due to extensive fabrication impact this research establishes a critical reference point for green computing initiatives enabling industry leaders and researchers to make datadriven decisions in reducing semiconductorrelated emissions and get correct estimates for the green factor of the information technology process the proposed formula paves the way for carbonaware chip design regulatory standards and future innovations in sustainable computing,0
the carbon footprint concern in the development and deployment of 5g new radio systems has drawn the attention to several stakeholders in this article we analyze the critical power consuming component of all candidate 5g system architecturesthe power amplifier paand propose pacentric resource management solutions for green 5g communications we discuss the impact of ongoing trends in cellular communications on sustainable green networking and analyze two communications architectures that allow exploiting the extra degreesoffreedom dof from multiantenna and massive antenna deployments small cellsdistributed antenna network and massive mimo for small cell systems with a moderate number of antennas we propose a peak to average power ratioaware resource allocation scheme for joint orthogonal frequency and space division multiple access for massive mimo systems we develop a highly parallel recurrent neural network for energyefficient precoding simulation results for representative 5g deployment scenarios demonstrate an energy efficiency improvement of one order of magnitude or higher with respect to current stateoftheart solutions,0
advances in artificial intelligence need to become more resourceaware and sustainable this requires clear assessment and reporting of energy efficiency tradeoffs like sacrificing fast running time for higher predictive performance while first methods for investigating efficiency have been proposed we still lack comprehensive results for popular methods and data sets in this work we attempt to fill this information gap by providing empiric insights for popular ai benchmarks with a total of 100 experiments our findings are evidence of how different data sets all have their own efficiency landscape and show that methods can be more or less likely to act efficiently,0
this study explores the intersection of technological innovation and environmental sustainability in the context of bitcoin mining with bitcoins growing adoption concerns surrounding the energy consumption and environmental impact of mining activities have intensified the study examines the core process of bitcoin mining focusing on its energyintensive proofofwork mechanism and provides a detailed analysis of its ecological footprint especially in terms of carbon emissions and electronic waste various models estimate that bitcoins energy consumption rivals that of entire nations highlighting serious sustainability concerns to address these issues the paper unearths potential technological innovations such as energyefficient mining hardware and the integration of renewable energy sources as viable strategies to reduce environmental impact additionally the study reviews current sustainability initiatives including efforts to lower carbon footprints and manage electronic waste effectively regulatory developments and marketbased approaches are also discussed as possible pathways to mitigate the environmental harm associated with bitcoin mining ultimately the paper advocates for a balanced approach that fosters technological innovation while promoting environmental responsibility suggesting that with appropriate policy and technological interventions bitcoin mining can evolve to be both innovative and sustainable,0
it power usage is a significant concern data center energy consumption is estimated to account for 1 to 15 of all energy consumption worldwide hardware designers data center designers and other members of the it community have been working to improve energy efficiency across many parts of the it infrastructure however little attention has been paid to the energy efficiency of software components indeed energy efficiency is currently not a common performance criteria for software in this work we attempt to quantify the potential for gains in energy efficiency in software based on a set of examples drawn from common everyday decisions made by software developers and enterprise architects our results show that there is potential for significant energy savings through energyconscious choices at software development and selection time making the software and it artifact sustainable and environment friendly,0
this paper sketches a new approach using fuzzy cognitive maps fcms to operably map and simulate digital transformation in architecture and urban planning today these processes are poorly understood many current studies on digital transformation are only treating questions of economic efficiency sustainability and social impact only play a minor role decisive definitions concepts and terms stay unclear therefore this paper develops an open experimental testbed for sustainable and innovative environments etsie for three different digital transformation scenarios using fcms a traditional growthoriented scenario a covid19 scenario and an innovative and sustainable covid19 scenario are modeled and tested all three scenarios have the same number of components connections and the same driver components only the initial state vectors are different and the internal correlations are weighted differently this allows for comparing all three scenarios on an equal basis the mental modeler software is used gray et al 2013 this paper presents one of the first applications of fcms in the context of digital transformation it is shown that the traditional growthoriented scenario is structurally very similar to the current covid19 scenario the current pandemic is able to accelerate digital transformation to a certain extent but the pandemic does not guarantee for a distinct sustainable and innovative future development only by changing the initial state vectors and the weights of the connections an innovative and sustainable turnaround in a third scenario becomes possible,0
the increasing integration of renewable energy sources results in fluctuations in carbon intensity throughout the day to mitigate their carbon footprint datacenters can implement demand response dr by adjusting their load based on grid signals however this presents challenges for private datacenters with diverse workloads and services one of the key challenges is efficiently and fairly allocating power curtailment across different workloads in response to these challenges we propose the carbon responder framework the carbon responder framework aims to reduce the carbon footprint of heterogeneous workloads in datacenters by modulating their power usage unlike previous studies carbon responder considers both online and batch workloads with different service level objectives and develops accurate performance models to achieve performanceaware power allocation the framework supports three alternative policies efficient dr fair and centralized dr and fair and decentralized dr we evaluate carbon responder polices using production workload traces from a private hyperscale datacenter our experimental results demonstrate that the efficient carbon responder policy reduces the carbon footprint by around 2x as much compared to baseline approaches adapted from existing methods the fair carbon responder policies distribute the performance penalties and carbon reduction responsibility fairly among workloads,0
new facilities of the 2020s such as the high luminosity large hadron collider hllhc will be relevant through at least the 2030s this means that their software efforts and those that are used to analyze their data need to consider sustainability to enable their adaptability to new challenges longevity and efficiency over at least this period this will help ensure that this software will be easier to develop and maintain that it remains available in the future on new platforms that it meets new needs and that it is as reusable as possible this report discusses a virtual halfday workshop on software sustainability and high energy physics that aimed 1 to bring together experts from hep as well as those from outside to share their experiences and practices and 2 to articulate a vision that helps the institute for research and innovation in software for high energy physics irishep to create a work plan to implement elements of software sustainability software sustainability practices could lead to new collaborations including elements of hep software being directly used outside the field and as has happened more frequently in recent years to hep developers contributing to software developed outside the field rather than reinventing it a focus on and skills related to sustainable software will give hep software developers an important skill that is essential to careers in the realm of software inside or outside hep the report closes with recommendations to improve software sustainability in hep aimed at the hep community via irishep and the hep software foundation hsf,0
mlenabled systems that are deployed in a production environment typically suffer from decaying model prediction quality through concept drift ie a gradual change in the statistical characteristics of a certain realworld domain to combat this a simple solution is to periodically retrain ml models which unfortunately can consume a lot of energy one recommended tactic to improve energy efficiency is therefore to systematically monitor the level of concept drift and only retrain when it becomes unavoidable different methods are available to do this but we know very little about their concrete impact on the tradeoff between accuracy and energy efficiency as these methods also consume energy themselves to address this we therefore conducted a controlled experiment to study the accuracy vs energy efficiency tradeoff of seven common methods for concept drift detection we used five synthetic datasets each in a version with abrupt and one with gradual drift and trained six different ml models as base classifiers based on a full factorial design we tested 420 combinations 7 drift detectors 5 datasets 2 types of drift 6 base classifiers and compared energy consumption and drift detection accuracy our results indicate that there are three types of detectors a detectors that sacrifice energy efficiency for detection accuracy kswin b balanced detectors that consume low to medium energy with good accuracy hddmw adwin and c detectors that consume very little energy but are unusable in practice due to very poor accuracy hddma pagehinkley ddm eddm by providing rich evidence for this energy efficiency tactic our findings support ml practitioners in choosing the best suited method of concept drift detection for their mlenabled systems,0
in this research paper we propose a new type of energyefficient green ai architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems we introduce a multilayered framework and metaarchitecture that integrates stateoftheart machine learning algorithms energyconscious computational models and optimization techniques to facilitate decisionmaking for resource reuse waste reduction and sustainable productionwe tested the framework on realworld datasets from lithiumion battery recycling and urban waste management systems demonstrating its practical applicability notably the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency quantitative optimization was based on mathematical models such as mixedinteger linear programming and lifecycle assessments moreover ai algorithms improved classification accuracy on urban waste by 20 percent while optimized logistics reduced transportation emissions by 30 percent we present graphical analyses and visualizations of the developed framework illustrating its impact on energy efficiency and sustainability as reflected in the simulation results this paper combines the principles of green ai with practical insights into how such architectural models contribute to circular economies presenting a fully scalable and scientifically rooted solution aligned with applicable un sustainability goals worldwide these results open avenues for incorporating newly developed ai technologies into sustainable management strategies potentially safeguarding local natural capital while advancing technological progress,0
despite their potential in many respects blockchain and distributed ledger technology dlt technology have been the target of criticism for the energy intensity of the proofofwork pow consensus algorithm in general and of bitcoin mining in particular however mining is also believed to have the potential to drive net decarbonization and renewable penetration in the energy grid by providing ancillary and other services in this paper we systematize the state of the art in this regard although not completely absent from the literature the extent to which flexible load response flr through pow mining may support grid decarbonization remains insufficiently studied and hence contested we approach this research gap by systematizing both the strengths and the limitations of mining to provide flr services for energy grids we find that a netdecarbonizing effect led by renewablebased mining is indeed plausible,0
achieving carbon neutrality has become a critical goal in mitigating the environmental impacts of human activities particularly in the face of global climate challenges inputoutput io devices such as keyboards mice displays and printers contribute significantly to greenhouse gas emissions through their manufacturing operation and disposal processes in this paper we explores sustainable strategies for achieving carbon neutrality in io devices emphasizing the importance of environmentally conscious design and development through a comprehensive review of existing literature and best approaches we introduces a framework to outline approaches for reducing the carbon footprint of io devices the result underscore the necessity of integrating sustainability into the lifecycle of io devices to support global carbon neutrality goals and promote longterm environmental sustainability,0
the growing energy consumption of information and communication technology ict has raised concerns about its environmental impact however the carbon efficiency of data transmission over the internet has so far received little attention this carbon efficiency can be enhanced effectively by sending traffic over carbonefficient interdomain paths however challenges in estimating and disseminating carbon intensity of interdomain paths have prevented carbonaware path selection from becoming a reality in this paper we take advantage of pathaware network architectures to overcome these challenges in particular we design ciro a system for forecasting the carbon intensity of interdomain paths and disseminating them across the internet we implement a proof of concept for ciro on the codebase of the scion pathaware internet architecture and test it on the scionlab global research testbed further we demonstrate the potential of ciro for reducing the carbon footprint of endpoints and end domains through largescale simulations we show that ciro can reduce the carbon intensity of communications by at least 47 for half of the domain pairs and the carbon footprint of internet usage by at least 50 for 87 of end domains,0
serverless computing is an emerging cloud computing paradigm that can reduce costs for cloud providers and their customers however serverless cloud platforms have stringent performance requirements due to the need to execute short duration functions in a timely manner and a growing carbon footprint traditional carbonreducing techniques such as shutting down idle containers can reduce performance by increasing coldstart latencies of containers required in the future this can cause higher violation rates of service level objectives slos conversely traditional latencyreduction approaches of prewarming containers or keeping them alive when not in use can improve performance but increase the associated carbon footprint of the serverless cluster platform to strike a balance between sustainability and performance in this paper we propose a novel carbon and sloaware framework called casa to schedule and autoscale containers in a serverless cloud computing cluster experimental results indicate that casa reduces the operational carbon footprint of a faas cluster by up to 26x while also reducing the slo violation rate by up to 14x compared to the stateoftheart,0
the boom in mobile apps has changed the traditional landscape of software development by introducing new challenges due to the limited resources of mobile devices eg memory cpu network bandwidth and battery the energy consumption of mobile apps is nowadays a hot topic and researchers are actively investigating the role of coding practices on energy efficiency recent studies suggest that design quality can conflict with energy efficiency therefore it is important to take into account energy efficiency when evolving the design of a mobile app the research community has proposed approaches to detect and remove antipatterns ie poor solutions to design and implementation problems in software systems but to the best of our knowledge none of these approaches have included antipatterns that are specific to mobile apps andor considered the energy efficiency of apps in this paper we fill this gap in the literature by analyzing the impact of eight type of antipatterns on a testbed of 59 android apps extracted from fdroid first we 1 analyze the impact of antipatterns in mobile apps with respect to energy efficiency then 2 we study the impact of different types of antipatterns on energy efficiency we found that then energy consumption of apps containing antipatterns and not refactored apps is statistically different moreover we find that the impact of refactoring antipatterns can be positive 7 type of antipatterns or negative 2 type of antipatterns therefore developers should consider the impact on energy efficiency of refactoring when applying maintenance activities,0
major cloud providers such as microsoft google facebook and amazon rely heavily on datacenters to support the everincreasing demand for their computational and application services however the financial and carbon footprint related costs of running such large infrastructure negatively impacts the sustainability of cloud services most of existing efforts primarily focus on minimizing the energy consumption of servers in this paper we devise a conceptual model and practical design guidelines for holistic management of all resources including servers networks storage cooling systems to improve the energy efficiency and reduce carbon footprints in cloud data centers cdcs furthermore we discuss the intertwined relationship between energy and reliability for sustainable cloud computing where we highlight the associated research issues finally we propose a set of future research directions in the field and setup grounds for further practical developments,0
heating ventilation and air conditioning hvac systems account for approximately 38 of building energy consumption globally making them one of the most energyintensive services the increasing emphasis on energy efficiency and sustainability combined with the need for enhanced occupant comfort presents a significant challenge for traditional hvac systems these systems often fail to dynamically adjust to realtime changes in electricity market rates or individual comfort preferences leading to increased energy costs and reduced comfort in response we propose a humanintheloop hitl artificial intelligence framework that optimizes hvac performance by incorporating realtime user feedback and responding to fluctuating electricity prices unlike conventional systems that require predefined information about occupancy or comfort levels our approach learns and adapts based on ongoing user input by integrating the occupancy prediction model with reinforcement learning the system improves operational efficiency and reduces energy costs in line with electricity market dynamics thereby contributing to demand response initiatives through simulations we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort this feedbackdriven approach ensures personalized comfort control without the need for predefined settings offering a scalable solution that balances individual preferences with economic and environmental goals,0
mitigating climate change and its impacts is one of the sustainable development goals sdgs required by united nations for an urgent action increasing carbon emissions due to human activities is the root cause to climate change telecommunication networks that provide service connectivity to mobile users contribute great amount of carbon emissions by consuming lots of nonrenewable energy sources beyond the improvement on energy efficiency to reduce the carbon footprint telecom operators are increasing their adoption of renewable energy eg wind power the high variability of renewable energy in time and location however creates difficulties for operators when utilizing renewables for the reduction of carbon emissions in this paper we consider a heterogeneous network consisted of one macro base station mbs and multiple small base stations sbss where each base station bs is powered by both of renewable and nonrenewable energy different from the prior works that target on the total power consumption we propose a novel scheme to minimize the carbon footprint of networks by dynamically switching the onoff modes of sbss and adjusting the association between users and bss to access renewables as much as possible our numerical analysis shows that the proposed scheme significantly reduces up to 86 of the nonrenewable energy consumption compared to two representative baselines,0
sustainable food is among the many challenges associated with climate change the resources required to grow or gather the food and the distance it travels to reach the consumer are two key factors of an ingredients sustainability food that is grown locally and is currently inseason will have a lower carbon footprint but when dining out these details unfortunately may not affect ones ordering preferences we introduce dinar as an immersive experience to make this information more accessible and to encourage better dining choices through friendly competition with a leaderboard of sustainability scores our study measures the effectiveness of immersive ar experiences on impacting consumer preferences towards sustainability,0
ecosystems and other naturally resilient systems exhibit allometric scaling in the distribution of sizes of their elements in this paper we define an allometry inspired scaling indicator for cities that is a first step towards quantifying the resilience borne of a complex systems hierarchical structural composition the scaling indicator is calculated using large census datasets and is analogous to fractal dimension in spatial analysis lack of numerical rigor and the resulting variation in scaling indicators inherent in the use of box counting mechanism for fractal dimension calculation for cities has been one of the hindrances in the adoption of fractal dimension as an urban indicator of note the intraurban indicator of scaling in population density distribution developed here is calculated for 58 us cities using a methodology that produces replicable results employing large censusblock wise population datasets from the 2010 us census 2010 and the 2007 us economic census we show that rising disparity as measured by the proposed indicator of population density distribution in census blocks in metropolitan statistical areas using us census 2010 data adversely affects energy consumption efficiency and carbon emissions in cities and leads to a higher urban carbon footprint we then define a planning plane as a visual and analytic tool for incorporation of scaling indicator analysis into policy and decisionmaking,0
modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability while hardware accelerators and renewable energy have been utilized to enhance sustainability addressing quality of service qos degradation caused by renewable energy supply and hardware recycling remains challenging 1 prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations 2 integrating recycled nand flash chips in data centers poses challenges due to their short lifetime increasing energy consumption 3 the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact this study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional nand flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation we also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices we present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain,0
technology solutions must effectively balance economic growth social equity and environmental integrity to achieve a sustainable society notably although the internet of things iot paradigm constitutes a key sustainability enabler critical issues such as the increasing maintenance operations energy consumption and manufacturingdisposal of iot devices have longterm negative economic societal and environmental impacts and must be efficiently addressed this calls for selfsustainable iot ecosystems requiring minimal external resources and intervention effectively utilizing renewable energy sources and recycling materials whenever possible thus encompassing energy sustainability in this work we focus on energysustainable iot during the operation phase although our discussions sometimes extend to other sustainability aspects and iot lifecycle phases specifically we provide a fresh look at energysustainable iot and identify energy provision transfer and energy efficiency as the three main energyrelated processes whose harmonious coexistence pushes toward realizing selfsustainable iot systems their main related technologies recent advances challenges and research directions are also discussed moreover we overview relevant performance metrics to assess the energysustainability potential of a certain technique technology device or network and list some target values for the next generation of wireless systems overall this paper offers insights that are valuable for advancing sustainability goals for present and future generations,0
the growing reliance on largescale data centers to run resourceintensive workloads has significantly increased the global carbon footprint underscoring the need for sustainable computing solutions while container orchestration platforms like kubernetes help optimize workload scheduling to reduce carbon emissions existing methods often depend on centralized machine learning models that raise privacy concerns and struggle to generalize across diverse environments in this paper we propose a federated learning approach for energy consumption prediction that preserves data privacy by keeping sensitive operational data within individual enterprises by extending the kubernetes efficient power level exporter kepler our framework trains xgboost models collaboratively across distributed clients using flowers fedxgbbagging aggregation using a bagging strategy eliminating the need for centralized data sharing experimental results on the specpower benchmark dataset show that our flbased approach achieves 117 percent lower mean absolute error compared to a centralized baseline this work addresses the unresolved tradeoff between data privacy and energy prediction efficiency in prior systems such as kepler and casper and offers enterprises a viable pathway toward sustainable cloud computing without compromising operational privacy,0
you hear a lot about environmental issues you may have initiated positive personal attitudes you have a group will in your unit and you are wondering about the environmental impact of digital technology in your professional environmentecodiag is here for youthe cnrs ecoinfo gds has found that assessing the environmental impacts of digital technology is complex and can discourage the best will which is why we offer you a simple and effective method to estimate the carbon footprint of your fleet through this new service based on our experience we chose an indicator that everyone could understand co2e emissions co2e equivalent based on an inventory of the digital services used and the units computer equipment our methodology and expertise will allow you to establish a global estimate of ghg emissions this evaluation will be accompanied by recommendations and resources sheets guide material to integrate this work into a hcrestype report or use them as part of a more global carbon assessment for your structure the provision of advice will allow you to post a policy to reduce ghg emissions generated by digital technology in the following yearsmeet us in front of the poster to discover and leave with our tool for assessing the environmental impacts of your units digital activities you will have the tools and a recommendation guide to control and reduce these impacts in a continuous improvement process concrete examples from two units will illustrate our approach and demonstrate the ease of use of the ecodiag service,0
over the years the chip industry has consistently developed highperformance processors to address the increasing demands across diverse applications however the rapid expansion of chip production has significantly increased carbon emissions raising critical concerns about environmental sustainability while researchers have previously modeled the carbon footprint cfp at both system and processor levels a holistic analysis of sustainability trends encompassing the entire chip lifecycle remains lacking this paper presents carbonset a comprehensive dataset integrating sustainability and performance metrics for cpus and gpus over the past decade carbonset aims to benchmark and assess the design of nextgeneration processors leveraging this dataset we conducted detailed analysis of flagship processors sustainability trends over the last decade this paper further highlights that modern processors are not yet sustainably designed with total carbon emissions increasing more than 50times in the past three years due to the surging demand driven by the ai boom power efficiency remains a significant concern while advanced process nodes pose new challenges requiring to effectively amortize the dramatically increased manufacturing carbon emissions,0
the energy consumption of data centers dcs is a very important figure for the telecommunications operators not only in terms of cost but also in terms of operational reliability a relation between the energy consumption and the weather conditions would indicate that weather forecast models could be used for predicting energy consumption of dcs a reliable forecast would result in a more efficient management of the available energy and would make it easier to take advantage of the modern types of powergrid based on renewable energy resources in this paper we exploit the capabilities provided by the fiestaiot platform in order to investigate the correlation between the weather conditions and the energy consumption in dcs then by using multivariable linear regression process we model this correlation between the energy consumption and the dominant weather conditions parameters in order to effectively forecast the energy consumption based on the weather forecast we have validated our results through live measurements from the realdc testbed results from our proposed approach indicate that forecasting of energy consumption based on weather conditions could help not only dc operators in managing their cooling systems and power usage but also electricity companies in optimizing their power distribution systems,0
in this paper we explore the intersection of privacy security and environmental sustainability in cloudbased office solutions focusing on quantifying user and networkside energy use and associated carbon emissions we hypothesise that privacyfocused services are typically more energyefficient than those funded through data collection and advertising to evaluate this we propose a framework that systematically measures environmental costs based on energy usage and network data traffic during welldefined automated usage scenarios to test our hypothesis we first analyse how underlying architectures and business models such as monetisation through personalised advertising contribute to the environmental footprint of these services we then explore existing methodologies and tools for software environmental impact assessment we apply our framework to three mainstream email services selected to reflect different privacy policies from adsupported trackingintensive models to privacyfocused designs microsoft outlook google mail gmail and proton mail we extend this comparison to a selfhosted email solution evaluated with and without endtoend encryption we show that the selfhosted solution even with 14 of device energy and 15 of emissions overheads from pgp encryption remains the most energyefficient saving up to 33 of emissions per session compared to gmail among commercial providers proton mail is the most efficient saving up to 01 gco2 e per session compared to outlook whose emissions can be further reduced by 2 through adblocking,0
c2cnt carbon dioxide to carbon nanotube cement plants have been introduced and analyzed which provide a significant economic incentive to eliminate the massive co2 greenhouse gas emissions of current plants and serves as a template for carbon mitigation in other industrial manufacturing processes rather than regarding co2 as a costly pollutant this is accomplished by treating co2 as a feedstock resource to generate valuable products carbon nanotubes the exhaust from partial and full oxyfuel cement plant configurations are coupled to the inlet of a c2cnt chamber in which co2 is transformed by electrolysis in a molten carbonate electrolyte at a steel cathode and a nickel anode in this high yield 4e per co2 process the co2 is transformed into carbon nanotubes at the cathode and pure oxygen at the anode that is looped back in improving the cement line energy efficiency and rate of production a partial oxyfuel process looping the pure oxygen back in through the plant calcinator has been compared to a full oxyfuel process in which it is looped back in through the plant kiln the second provides the advantage of easier retrofit while the first maximizes efficiency by minimizing the volume of gas throughput eliminating n2 from air an upper limit to the electrical cost to drive c2cnt electrolysis is usd70 based on texas wind power costs but will be lower with fuel expenses when oxyfuel plant energy improvements are taken account the current value of a ton of carbon nanotubes is substantially in excess of a ton of cement hence a c2cnt cement plant consumes usd50 of electricity emits no co2 and produces usd100 of cement and usd60000 of carbon nanotubes per ton of co2 avoided as the cement product ages co2 is spontaneously absorbed this is a powerful economic incentive rather than economic cost to mitigate climate change through a carbon negative process,0
training largescale artificial intelligence ai models demands significant computational power and energy leading to increased carbon footprint with potential environmental repercussions this paper delves into the challenges of training ai models across geographically distributed geodistributed data centers emphasizing the balance between learning performance and carbon footprint we consider federated learning fl as a solution which prioritizes model parameter exchange over raw data ensuring data privacy and compliance with local regulations given the variability in carbon intensity across regions we propose a new framework called cafe short for carbonaware federated learning to optimize training within a fixed carbon footprint budget our approach incorporates coreset selection to assess learning performance employs the lyapunov driftpluspenalty framework to address the unpredictability of future carbon intensity and devises an efficient algorithm to address the combinatorial complexity of the data center selection through extensive simulations using realworld carbon intensity data we demonstrate the efficacy of our algorithm highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact,0
the sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environments future the growing internet of things iot has the potential to exacerbate this issue however an emerging area known as tiny machine learning tinyml has the opportunity to help address these environmental challenges through sustainable computing practices tinyml the deployment of machine learning ml algorithms onto lowcost lowpower microcontroller systems enables ondevice sensor analytics that unlocks numerous alwayson ml applications this article discusses both the potential of these tinyml applications to address critical sustainability challenges as well as the environmental footprint of this emerging technology through a complete life cycle analysis lca we find that tinyml systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors nevertheless when globally scaled the carbon footprint of tinyml systems is not negligible necessitating that designers factor in environmental impact when formulating new devices finally we outline research directions to enable further sustainable contributions of tinyml,0
prominent works in the field of natural language processing have long attempted to create new innovative models by improving upon previous model training approaches altering model architecture and developing more indepth datasets to better their performance however with the quickly advancing field of nlp comes increased greenhouse gas emissions posing concerns over the environmental damage caused by training llms gaining a comprehensive understanding of the various costs particularly those pertaining to environmental aspects that are associated with artificial intelligence serves as the foundational basis for ensuring safe ai models currently investigations into the co2 emissions of ai models remain an emerging area of research and as such in this paper we evaluate the co2 emissions of wellknown large language models which have an especially high carbon footprint due to their significant amount of model parameters we argue for the training of llms in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions furthermore we discuss how the choice of hardware affects co2 emissions by contrasting the co2 emissions during model training for two widely used gpus based on our results we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe ai models without sacrificing their robustness and performance,0
sustainability became the most important component of world development as countries worldwide fight the battle against the climate change to understand the effects of climate change the ecological footprint along with the biocapacity should be observed the big part of the ecological footprint the carbon footprint is most directly associated with the energy and specifically fuel sources this paper develops a time series vector autoregression prediction model of the ecological footprint based on energy parameters the objective of the paper is to forecast the ef based solely on energy parameters and determine the relationship between the energy and the ef the dataset included global yearly observations of the variables for the period 19712014 predictions were generated for every variable that was used in the model for the period 20152024 the results indicate that the ecological footprint of consumption will continue increasing as well as the primary energy consumption from different sources however the energy consumption from coal sources is predicted to have a declining trend,0
the soaring energy demands of largescale software ecosystems and cloud data centers accelerated by the intensive training and deployment of large language models have driven energy consumption and carbon footprint to unprecedented levels in response both industry and academia are increasing efforts to reduce the carbon emissions associated with cloud computing through more efficient task scheduling and infrastructure orchestration in this work we present a systematic review of various kubernetes scheduling strategies categorizing them into hardwarecentric and softwarecentric annotating each with its sustainability objectives and grouping them according to the algorithms they use we propose a comprehensive taxonomy for cloud task scheduling studies with a particular focus on the environmental sustainability aspect we analyze emerging research trends and open challenges and our findings provide critical insight into the design of sustainable scheduling solutions for nextgeneration cloud computing systems,0
the increasing electric vehicle ev adoption challenges the energy management of charging stations css due to the large number of evs and the underlying uncertainties moreover the carbon footprint of css is growing significantly due to the rising charging power demand this makes it important for css to properly manage their energy usage and ensure their carbon footprint stay within their carbon emission quotas this paper proposes a twostage online algorithm for this purpose considering the different time scales of energy management and carbon trading in the first stage the cs characterizes the realtime aggregate ev power flexibility in terms of upper and lower bounds on the total charging power by a lyapunov optimizationbased online algorithm in the second stage the cs cooptimizes energy management and carbon trading with ev charging power chosen within the aggregate flexibility region provided by the first stage a generalized battery model is proposed to capture the dynamic carbon footprint changes and carbon trading a virtual carbon queue is designed to develop an online algorithm for the second stage which can ensure the carbon footprint of cs be within its carbon emission quota and its total operation cost is nearly offline optimal case studies validate the effectiveness and advantages of the proposed algorithm,0
the growing carbon footprint of artificial intelligence ai has been undergoing public scrutiny nonetheless the equally important water withdrawal and consumption footprint of ai has largely remained under the radar for example training the gpt3 language model in microsofts stateoftheart us data centers can directly evaporate 700000 liters of clean freshwater but such information has been kept a secret more critically the global ai demand is projected to account for 4266 billion cubic meters of water withdrawal in 2027 which is more than the total annual water withdrawal of 46 denmark or half of the united kingdom this is concerning as freshwater scarcity has become one of the most pressing challenges to respond to the global water challenges ai can and also must take social responsibility and lead by example by addressing its own water footprint in this paper we provide a principled methodology to estimate the water footprint of ai and also discuss the unique spatialtemporal diversities of ais runtime water efficiency finally we highlight the necessity of holistically addressing water footprint along with carbon footprint to enable truly sustainable ai,0
todays cloud data centers are often distributed geographically to provide robust data services but these geodistributed data centers gddcs have a significant associated environmental impact due to their increasing carbon emissions and water usage which needs to be curtailed moreover the energy costs of operating these data centers continue to rise this paper proposes a novel framework to cooptimize carbon emissions water footprint and energy costs of gddcs using a hybrid workload management framework called shield that integrates machine learning guided local search with a decompositionbased evolutionary algorithm our framework considers geographical factors and timebased differences in power generationuse costs and environmental impacts to intelligently manage workload distribution across gddcs and data center operation experimental results show that shield can realize 344x speedup and 21x improvement in pareto hypervolume while reducing the carbon footprint by up to 37x water footprint by up to 18x energy costs by up to 13x and a cumulative improvement across all objectives carbon water cost of up to 48x compared to the stateoftheart,0
obstacle avoidance path planning for uncrewed aerial vehicles uavs or drones is rarely addressed in most flight path planning schemes despite obstacles being a realistic condition obstacle avoidance can also be energyintensive making it a critical factor in efficient pointtopoint drone flights to address these gaps we propose ecoflight an energyefficient pathfinding algorithm that determines the lowestenergy route in 3d space with obstacles the algorithm models energy consumption based on the drone propulsion system and flight dynamics we conduct extensive evaluations comparing ecoflight with directflight and shortestdistance schemes the simulation results across various obstacle densities show that ecoflight consistently finds paths with lower energy consumption than comparable algorithms particularly in highdensity environments we also demonstrate that a suitable flying speed can further enhance energy savings,0
current proposals for ai regulation in the eu and beyond aim to spur ai that is trustworthy eg ai act and accountable eg ai liability what is missing however is a robust regulatory discourse and roadmap to make ai and technology more broadly environmentally sustainable this paper aims to take first steps to fill this gap the ict sector contributes up to 39 percent of global greenhouse gas ghg emissionsmore than global air travel at 25 percent the carbon footprint and water consumption of ai especially largescale generative models like gpt4 raise significant sustainability concerns the paper is the first to assess how current and proposed technology regulations including eu environmental law the general data protection regulation gdpr and the ai act could be adjusted to better account for environmental sustainability the gdpr for instance could be interpreted to limit certain individual rights like the right to erasure if these rights significantly conflict with broader sustainability goals in a second step the paper suggests a multifaceted approach to achieve sustainable ai regulation it advocates for transparency mechanisms such as disclosing the ghg footprint of ai systems as laid out in the proposed eu ai act however sustainable ai regulation must go beyond mere transparency the paper proposes a regulatory toolkit comprising coregulation sustainabilitybydesign principles restrictions on training data and consumption caps including integration into the eu emissions trading scheme finally the paper argues that this regulatory toolkit could serve as a blueprint for regulating other highemission technologies and infrastructures like blockchain metaverse applications and data centers the framework aims to cohesively address the crucial dual challenges of our era digital transformation and climate change mitigation,0
background transitions towards healthier more environmentally sustainable diets would require large shifts in consumption patterns cost and affordability can be barriers to consuming healthy sustainable diets objective this study provides the first worldwide test of how retail food prices relate to empirically estimated environmental footprints and nutritional profile scores between and within food groups methods we use 48316 prices for 860 retail food items commonly sold in 181 countries during 2011 and 2017 matched to estimated carbon and water footprints and nutritional profiles to test whether healthier and more sustainable foods are more expensive between and within food groups results prices environmental footprints and nutritional profiles differ between food groups within almost all groups more expensive items have significantly larger carbon and water footprints associations are strongest for animal source foods where each 10 increment in price is associated with 21 grams higher carbon footprint and 5 liters higher water footprint per 100kcal of food there is no such gradient for price and nutritional profile as more expensive items are sometimes healthier and sometimes less healthy depending on the food group price range and nutritional attribute of interest conclusions our finding that higherpriced items have larger environmental footprints is contrary to expectations that a more sustainable diet would be more expensive instead we find that within each food group meeting dietary needs with lower environmental footprints is possible by choosing items with a lower unit price these findings are consistent with prior observations that higherpriced items typically use more resources including energy and water but may or may not be healthful as measured by nutrient profile scores,0
embodied carbon footprint modeling has become an area of growing interest due to its significant contribution to carbon emissions in computing however the deterministic nature of the existing models fail to account for the spatial and temporal variability in the semiconductor supply chain the absence of uncertainty modeling limits system designers ability to make informed carbonaware decisions we introduce carbonclarity a probabilistic framework designed to model embodied carbon footprints through distributions that reflect uncertainties in energyperarea gasperarea yield and carbon intensity across different technology nodes our framework enables a deeper understanding of how design choices such as chiplet architectures and new vs old technology node selection impact emissions and their associated uncertainties for example we show that the gap between the mean and 95th percentile of embodied carbon per cm2 can reach up to 16x for the 7nm technology node additionally we demonstrate through case studies that i carbonclarity is a valuable resource for device provisioning help maintaining performance under a tight carbon budget and ii chiplet technology and mature nodes not only reduce embodied carbon but also significantly lower its associated uncertainty achieving an 18 reduction in the 95th percentile compared to monolithic designs for the mobile application,0
the research efforts on cellular vehicletoeverything v2x communications are gaining momentum with each passing year it is considered as a paradigmaltering approach to connect a large number of vehicles with minimal cost of deployment and maintenance this article aims to further push the stateoftheart of cellular v2x communications by providing an optimization framework for wireless charging power allocation and resource block assignment specifically we design a network model where roadside objects use wireless power from rf signals of electric vehicles for charging and information processing moreover due to the resourceconstraint nature of cellular v2x the power allocation and resource block assignment are performed to efficiently use the resources the proposed optimization framework shows an improvement in terms of the overall energy efficiency of the network when compared with the baseline technique the performance gains of the proposed solution clearly demonstrate its feasibility and utility for cellular v2x communications,0
tackling climate change by reducing and eventually eliminating carbon emissions is a significant milestone on the path toward establishing an environmentally sustainable society as we transition into the exascale era marked by an increasing demand and scale of hpc resources the hpc community must embrace the challenge of reducing carbon emissions from designing and operating modern hpc systems in this position paper we describe challenges and highlight different opportunities that can aid hpc sites in reducing the carbon footprint of modern hpc systems,0
the latest trends in the adoption of cloud edge and distributed computing as well as a rise in applying aiml workloads have created a need to measure monitor and reduce the carbon emissions of these computeintensive workloads and the associated communication costs the data movement over networks has considerable carbon emission that has been neglected due to the difficulty in measuring the carbon footprint of a given endtoend network path we present a novel network carbon footprint measuring mechanism and propose three ways in which users can optimize scheduling networkintensive tasks to enable carbon savings through shifting tasks in time space and overlay networks based on the geographic carbon intensity,0
cloud computing drives innovation but also poses significant environmental challenges due to its highenergy consumption and carbon emissions data centers account for 24 of global energy usage and the ict sectors share of electricity consumption is projected to reach 40 by 2040 as the goal of achieving netzero emissions by 2050 becomes increasingly urgent there is a growing need for more efficient and transparent solutions particularly for private cloud infrastructures which are utilized by 87 of organizations despite the dominance of publiccloud systems this study evaluates the maizx framework designed to optimize cloud operations and reduce carbon footprint by dynamically ranking resources including data centers edge computing nodes and multicloud environments based on realtime and forecasted carbon intensity power usage effectiveness pue and energy consumption leveraging a flexible ranking algorithm maizx achieved an 8568 reduction in co2 emissions compared to baseline hypervisor operations tested across geographically distributed data centers the framework demonstrates scalability and effectiveness directly interfacing with hypervisors to optimize workloads in private hybrid and multicloud environments maizx integrates realtime data on carbon intensity power consumption and carbon footprint as well as forecasted values into cloud management providing a robust tool for enhancing climate performance potential while maintaining operational efficiency,0
many organizations including governments utilities and businesses have set ambitious targets to reduce carbon emissions for their environmental social and governance esg goals to achieve these targets these organizations increasingly use power purchase agreements ppas to obtain renewable energy credits which they use to compensate for the brown energy consumed from the grid however the details of these ppas are often private and not shared with important stakeholders such as grid operators and carbon information services who monitor and report the grids carbon emissions this often results in incorrect carbon accounting where the same renewable energy production could be factored into grid carbon emission reports and separately claimed by organizations that own ppas such double counting of renewable energy production could lead organizations with ppas to understate their carbon emissions and overstate their progress toward sustainability goals and also provide significant challenges to consumers using common carbon reduction measures to decrease their carbon footprint unfortunately there is no consensus on accurately computing the grids carbon intensity by properly accounting for ppas the goal of our work is to shed quantitative and qualitative light on the renewable energy attribution and the incorrect carbon intensity estimation problems,0
the reliability of machine learning ml software systems is heavily influenced by changes in data over time for that reason ml systems require regular maintenance typically based on model retraining however retraining requires significant computational demand which makes it energyintensive and raises concerns about its environmental impact to understand which retraining techniques should be considered when designing sustainable ml applications in this work we study the energy consumption of common retraining techniques since the accuracy of ml systems is also essential we compare retraining techniques in terms of both energy efficiency and accuracy we showcase that retraining with only the most recent data compared to all available data reduces energy consumption by up to 25 being a sustainable alternative to the status quo furthermore our findings show that retraining a model only when there is evidence that updates are necessary rather than on a fixed schedule can reduce energy consumption by up to 40 provided a reliable data change detector is in place our findings pave the way for better recommendations for ml practitioners guiding them toward more energyefficient retraining techniques when designing sustainable ml software systems,0
organizations are increasingly offloading their workloads to cloud platforms for workloads with relaxed deadlines this presents an opportunity to reduce the total carbon footprint of these computations by moving workloads to datacenters with access to lowcarbon power recently published results have shown that the carbon footprint of the widearea network wan can be a significant share of the total carbon output of executing the workload itself and so careful selection of the time and place where these computations are offloaded is critical in this paper we propose an approach to geographic workload migration that uses highfidelity maps of physical internet infrastructure to better estimate the carbon costs of wan transfers our findings show that spaceshifting workloads can achieve much higher carbon savings than timeshifting alone if accurate estimates of wan carbon costs are taken into account,0
footprints provide a way to estimate the relative impact of processes and products on the global climate including footprint analysis in a course on energy simultaneously provides students with an understanding of this tool and a quantitative guide to approaches that address climate change collegelevel classroom activities for primarily processbased life cycle carbon footprinting are discussed,0
the cloud computing paradigm offers ondemand services over the internet and supports a wide variety of applications with the recent growth of internet of things iot based applications the usage of cloud services is increasing exponentially the next generation of cloud computing must be energyefficient and sustainable to fulfil the enduser requirements which are changing dynamically presently cloud providers are facing challenges to ensure the energy efficiency and sustainability of their services the usage of large number of cloud datacenters increases cost as well as carbon footprints which further effects the sustainability of cloud services in this paper we propose a comprehensive taxonomy of sustainable cloud computing the taxonomy is used to investigate the existing techniques for sustainability that need careful attention and investigation as proposed by several academic and industry groups further the current research on sustainable cloud computing is organized into several categories application design sustainability metrics capacity planning energy management virtualization thermalaware scheduling cooling management renewable energy and waste heat utilization the existing techniques have been compared and categorized based on the common characteristics and properties a conceptual model for sustainable cloud computing has been proposed along with discussion on future research directions,0
with infrastructure systems lasting for decades even centuries there is growing need to assess sustainability impacts however compared to energy or transportation networks which each contribute roughly one third of global emissions broadband networks have arguably received less attention due to their much smaller footprint 1839 of global emissions nevertheless many countries are looking to provide universal mobile broadband over the next decade to meet goal 9 of the sustainable development goals sdgs therefore this paper evaluates the future sustainability impacts of providing either 4g or 5g mobile broadband quantifying the carbon and other environmental emissions associated with each universal broadband strategy this paper contributes the first ex ante sustainability assessment of global universal mobile broadband strategies aimed at delivering sdg goal 9,0
this paper presents a solution to the challenge of mitigating carbon emissions from hosting largescale machine learning ml inference services ml inference is critical to modern technology products but it is also a significant contributor to carbon footprint we introduce clover a carbonfriendly ml inference service runtime system that balances performance accuracy and carbon emissions through mixedquality models and gpu resource partitioning our experimental results demonstrate that clover is effective in substantially reducing carbon emissions while maintaining high accuracy and meeting service level agreement sla targets,0
cloud platforms growing energy demand and carbon emissions are raising concern about their environmental sustainability the current approach to enabling sustainable clouds focuses on improving energyefficiency and purchasing carbon offsets these approaches have limits many cloud data centers already operate near peak efficiency and carbon offsets cannot scale to near zero carbon where there is little carbon left to offset instead enabling sustainable clouds will require applications to adapt to when and where unreliable lowcarbon energy is available applications cannot do this today because their energy use and carbon emissions are not visible to them as the energy system provides the rigid abstraction of a continuous reliable energy supply this vision paper instead advocates for a carbon first approach to cloud design that elevates carbonefficiency to a firstclass metric to do so we argue that cloud platforms should virtualize the energy system by exposing visibility into and softwaredefined control of it to applications enabling them to define their own abstractions for managing energy and carbon emissions based on their own requirements,0
the waste heat potential from industrial processes is huge and if it can be utilized it may contribute significantly to the mitigation of climate change a packed bed thermal energy storage system is a lowcost storage technology that can be employed to enable the utilization of waste heat from industrial processes this system can be used to store excess heat and release this energy when it is needed at a later time to ensure the efficient operation of a packed bed thermal energy storage its characteristics in standby mode need to be studied in great detail in the present study the standby efficiency and thermocline degradation of a labscale packed bed thermal energy storage in standby mode is experimentally investigated for different flow directions of the heat transfer fluid during the preceding charging period results show that for long standby periods the standby efficiency is significantly affected by the flow direction the maximum entropy generation rate for a 22hour standby process with the flow direction of the heat transfer fluid from bottom to top in the preceding charging process is twice as high as for the same process with the reverse flow direction energy and exergy efficiencies are lower for the process with reverse flow direction by 5 and 18 respectively,0
the growing demand for radio access networks rans driven by advanced wireless technology and the everincreasing mobile traffic faces significant energy consumption challenges that threaten sustainability to address this an architecture referring to the vertical heterogeneous network vhetnet has recently been proposed our study seeks to enhance network operations in terms of energy efficiency and sustainability by examining a vhetnet configuration comprising a high altitude platform station haps acting as a super macro base station smbs along with a macro base station mbs and a set of small base stations sbss in a densely populated area,0
there has been a significant societal push towards sustainable practices including in computing modern interactive workloads such as geodistributed webservices exhibit various spatiotemporal and performance flexibility enabling the possibility to adapt the location time and intensity of processing to align with the availability of renewable and lowcarbon energy an example is a web application hosted across multiple cloud regions each with varying carbon intensity based on their local electricity mix distributed loadbalancing enables the exploitation of lowcarbon energy through load migration across regions reducing web applications carbon footprint in this paper we present casper a carbonaware scheduling and provisioning system that primarily minimizes the carbon footprint of distributed web services while also respecting their service level objectives slo we formulate casper as an multiobjective optimization problem that considers both the variable carbon intensity and latency constraints of the network our evaluation reveals the significant potential of casper in achieving substantial reductions in carbon emissions compared to baseline methods casper demonstrates improvements of up to 70 with no latency performance degradation,0
as largescale data processing workloads continue to grow their carbon footprint raises concerns prior research on carbonaware schedulers has focused on shifting computation to align with availability of lowcarbon energy but these approaches assume that each task can be executed independently in contrast data processing jobs have precedence constraints ie outputs of one task are inputs for another that complicate decisions since delaying an upstream bottleneck task to a lowcarbon period will also block downstream tasks impacting the entire jobs completion time in this paper we show that carbonaware scheduling for data processing benefits from knowledge of both timevarying carbon and precedence constraints our main contribution is textttpcaps a carbonaware scheduler that interfaces with modern ml scheduling policies to explicitly consider the precedencedriven importance of each task in addition to carbon to illustrate the gains due to finegrained task information we also study textttcap a wrapper for any carbonagnostic scheduler that adapts the key provisioning ideas of textttpcaps our schedulers enable a configurable priority between carbon reduction and job completion time and we give analytical results characterizing the tradeoff between the two furthermore our spark prototype on a 100node kubernetes cluster shows that a moderate configuration of textttpcaps reduces carbon footprint by up to 329 without significantly impacting the clusters total efficiency,0
energy system optimization models esoms are designed to examine the potential effects of a proposed policy but often represent energyefficient technologies and policies in an overly simplified way most esoms include different enduse technologies with varying efficiencies and select technologies for deployment based solely on leastcost optimization which drastically oversimplifies consumer decisionmaking in this paper we change the structure of an existing esom to model energy efficiency in way that is consistent with microeconomic theory the resulting model considers the effectiveness of energyefficient technologies in meeting energy service demands and their potential to substitute electricity usage by conventional technologies to test the revised model we develop a simple hypothetical case and use it to analyze the welfare gain from an energy efficiency subsidy versus a carbon tax policy in the simple test case the maximum recovered welfare from an efficiency subsidy is less than 50 of the firstbest carbon tax policy,0
there have been growing discussions on estimating and subsequently reducing the operational carbon footprint of enterprise data centers the design and intelligent control for data centers have an important impact on data center carbon footprint in this paper we showcase pydcm a python library that enables extremely fast prototyping of data center design and applies reinforcement learningenabled control with the purpose of evaluating key sustainability metrics including carbon footprint energy consumption and observing temperature hotspots we demonstrate these capabilities of pydcm and compare them to existing works in energyplus for modeling data centers pydcm can also be used as a standalone gymnasium environment for demonstrating sustainabilityfocused data center control,0
mixed reality mr is becoming ubiquitous as it finds its applications in education healthcare and other sectors beyond leisure while mr end devices such as headsets have low energy intensity the total number of devices and resource requirements of the entire mr ecosystem which includes cloud and edge endpoints can be significant the resulting operational and embodied carbon footprint of mr has led to concerns about its environmental implications recent research has explored reducing the carbon footprint of mr devices by exploring hardware design space or network optimizations however many additional avenues for enhancing mrs sustainability remain open including energy savings in nonprocessor components and carbonaware optimizations in collaborative mr ecosystems in this paper we aim to identify key challenges existing solutions and promising research directions for improving mr sustainability we explore adjacent fields of embedded and mobile computing systems for insights and outline mrspecific problems requiring new solutions we identify the challenges that must be tackled to enable researchers developers and users to avail themselves of these opportunities in collaborative mr systems,0
the computation demand for machine learning ml has grown rapidly recently which comes with a number of costs estimating the energy cost helps measure its environmental impact and finding greener strategies yet it is challenging without detailed information we calculate the energy use and carbon footprint of several recent large modelst5 meena gshard switch transformer and gpt3and refine earlier estimates for the neural architecture search that found evolved transformer we highlight the following opportunities to improve energy efficiency and co2 equivalent emissions co2e large but sparsely activated dnns can consume 110th the energy of large dense dnns without sacrificing accuracy despite using as many or even more parameters geographic location matters for ml workload scheduling since the fraction of carbonfree energy and resulting co2e vary 5x10x even within the same country and the same organization we are now optimizing where and when large models are trained specific datacenter infrastructure matters as cloud datacenters can be 142x more energy efficient than typical datacenters and the mloriented accelerators inside them can be 25x more effective than offtheshelf systems remarkably the choice of dnn datacenter and processor can reduce the carbon footprint up to 1001000x these large factors also make retroactive estimates of energy cost difficult to avoid miscalculations we believe ml papers requiring large computational resources should make energy consumption and co2e explicit when practical we are working to be more transparent about energy use and co2e in our future research to help reduce the carbon footprint of ml we believe energy usage and co2e should be a key metric in evaluating models and we are collaborating with mlperf developers to include energy usage during training and inference in this industry standard benchmark,0
software sustainability is a key multifaceted nonfunctional requirement that encompasses environmental social and economic concerns yet its integration into the development of machine learning mlenabled systems remains an open challenge while previous research has explored highlevel sustainability principles and policy recommendations limited empirical evidence exists on how sustainability is practically managed in ml workflows existing studies predominantly focus on environmental sustainability eg carbon footprint reduction while missing the broader spectrum of sustainability dimensions and the challenges practitioners face in realworld settings to address this gap we conduct an empirical study to characterize sustainability in mlenabled systems from a practitioners perspective we investigate 1 how ml engineers perceive and describe sustainability 2 the software engineering practices they adopt to support it and 3 the key challenges hindering its adoption we first perform a qualitative analysis based on interviews with eight experienced ml engineers followed by a largescale quantitative survey with 203 ml practitioners our key findings reveal a significant disconnection between sustainability awareness and its systematic implementation highlighting the need for more structured guidelines measurement frameworks and regulatory support,0
in this talk recent simulation results on the single high energy electron reconstruction with the beam calorimeter for the ild detector are presented guinea pig is used to generate the ee pair background and geant4 for the simulation of electron showers in the calorimeter an algorithm was developed for the shee reconstruction on top of the large ee background the efficiency of the shee reconstruction is estimated for the nominal and sb2009 ilc beam parameters,0
data centers are large electricity consumers due to the high consumption needs of servers and their cooling systems given the current cryptocurrency and artificial intelligence trends the data center electricity demand is bound to grow significantly with the electricity sector being responsible for a large share of global greenhouse gas ghg emissions it is important to lower the carbon footprint of data centers to meet ghg emissions targets set by international agreements moreover uncontrolled integration of data centers in power distribution grids contributes to increasing the stochasticity of the power system demand thus increasing the need for capacity reserves which leads to economic and environmental inefficiencies in the power grid operation this work provides a method to size a photovoltaic pv system and an energy storage system ess for an existing data center looking to reduce both its carbon footprint and demand stochasticity via dispatching the proposed scenariobased optimization framework allows to size the ess and the pv system to minimize the expected operational and capital costs along with the carbon footprint of the data center complex the life cycle assessment of the resources as well as the dynamic carbon emissions of the upstream power distribution grid are accounted for while computing the dayahead planning of the data center aggregated demand and pv generation case studies in different swiss cantons and regions of germany emphasize the need for locationaware sizing processes since the obtained optimal solutions strongly depend on the local electricity carbon footprint cost and on the local irradiance conditions some regions show potential in carbon footprint reduction while other regions do not,0
investigations have been performed into using clustering methods in data mining timeseries data from smart meters the problem is to identify patterns and trends in energy usage profiles of commercial and industrial customers over 24hour periods and group similar profiles we tested our method on energy usage data provided by several us power utilities the results show accurate grouping of accounts similar in their energy usage patterns and potential for the method to be utilized in energy efficiency programs,0
reconfigurable intelligent surfaces riss are widely considered a promising technology for future wireless communication systems as an important indicator of risassisted communication systems in green wireless communications energy efficiency ee has recently received intensive research interest as an optimization target however most previous works have ignored the different power consumption between on and off states of the pin diodes attached to each ris element this oversight results in extensive unnecessary power consumption and reduction of actual ee due to the inaccurate power model to address this issue in this paper we first utilize a practical power model for a risassisted multiuser multipleinput singleoutput mumiso communication system which takes into account the difference in power dissipation caused by onoff states of riss pin diodes based on this model we formulate a more accurate ee optimization problem however this problem is nonconvex and has mixedinteger properties which poses a challenge for optimization to solve the problem an effective alternating optimization ao algorithm framework is utilized to optimize the base station and ris beamforming precoder separately to obtain the essential ris beamforming precoder we develop two effective methods based on maximum gradient search and sdp relaxation respectively theoretical analysis shows the exponential complexity of the original problem has been reduced to polynomial complexity simulation results demonstrate that the proposed algorithm outperforms the existing ones leading to a significant increase in ee across a diverse set of scenarios,0
federated learning fl methods adopt efficient communication technologies to distribute machine learning tasks across edge devices reducing the overhead in terms of data storage and computational complexity compared to centralized solutions rather than moving large data volumes from producers sensors machines to energyhungry data centers raising environmental concerns due to resource demands fl provides an alternative solution to mitigate the energy demands of several learning tasks while enabling new artificial intelligence of things aiot applications this paper proposes a framework for realtime monitoring of the energy and carbon footprint impacts of fl systems the carbon tracking tool is evaluated for consensus fully decentralized and classical fl policies for the first time we present a quantitative evaluation of different computationally and communication efficient fl methods from the perspectives of energy consumption and carbon equivalent emissions suggesting also general guidelines for energyefficient design results indicate that consensusdriven fl implementations should be preferred for limiting carbon emissions when the energy efficiency of the communication is low ie 25 kbitjoule besides quantization and sparsification operations are shown to strike a balance between learning performances and energy consumption leading to sustainable fl designs,0
accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research we introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions as well as generating standardized online appendices utilizing this framework we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning finally based on case studies using our framework we propose strategies for mitigation of carbon emissions and reduction of energy consumption by making accounting easier we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms,0
broadband connectivity is a tool for catalyzing socioeconomic development and reducing the societal inequalities recent studies have investigated the supporting role of broadband in addressing sustainable development goals sdgs relationally emerging ultradense broadband networks such as 56g have been linked to increased power consumption and more carbon footprint with sdgs recognized as interdependent and addressing one should not jeopardize the achievement of the other there is need for sustainability research despite the need to narrow the digital divide and address the sdgs by 2030 it is surprising that limited comprehensive studies exist on broadband sustainability to this end we review 113 peer reviewed journals focusing on six key areas sdgs addressed application areas country income technology methodology and spatial focus we further discuss our findings before making four key recommendations on broadband sustainability research to fasttrack sdg achievement by 2030 especially for developing economies,0
the rapid growth of machine learning ml has increased demand for dnn hardware accelerators but their embodied carbon footprint poses significant environmental challenges this paper leverages approximate computing to design sustainable accelerators by minimizing the carbon delay product cdp using gatelevel pruning and precision scaling we generate areaaware approximate multipliers and optimize the accelerator design with a genetic algorithm results demonstrate reduced embodied carbon while meeting performance and accuracy requirements,0
the rapid expansion of data centers dcs has intensified energy and carbon footprint incurring a massive environmental computing cost while carbonaware workload migration strategies have been examined existing approaches often overlook reliability metrics such as server lifetime degradation and qualityofservice qos that substantially affects both carbon and operational efficiency of dcs hence this paper proposes a comprehensive optimization framework for spatiotemporal workload migration across distributed dcs that jointly minimizes operational and embodied carbon emissions while complying with servicelevel agreements sla a key contribution is the development of an embodied carbon emission model based on servers expected lifetime analysis which explicitly considers server heterogeneity resulting from aging and utilization conditions these issues are accommodated using new server dispatch strategies and backup resource allocation model accounting hardware software and workloadinduced failure the overall model is formulated as a mixedinteger optimization problem with multiple linearization techniques to ensure computational tractability numerical case studies demonstrate that the proposed method reduces total carbon emissions by up to 21 offering a pragmatic approach to sustainable dc operations,0
data centers are carbonintensive enterprises due to their massive energy consumption and it is estimated that data center industry will account for 8 of global carbon emissions by 2030 however both technological and policy instruments for reducing or even neutralizing data center carbon emissions have not been thoroughly investigated to bridge this gap this survey paper proposes a roadmap towards carbonneutral data centers that takes into account both policy instruments and technological methodologies we begin by presenting the carbon footprint of data centers as well as some insights into the major sources of carbon emissions following that carbon neutrality plans for major global cloud providers are discussed to summarize current industrial efforts in this direction in what follows we introduce the carbon market as a policy instrument to explain how to offset data center carbon emissions in a costefficient manner on the technological front we propose achieving carbonneutral data centers by increasing renewable energy penetration improving energy efficiency and boosting energy circulation simultaneously a comprehensive review of existing technologies on these three topics is elaborated subsequently based on this a multipronged approach towards carbon neutrality is envisioned and a digital twinpowered industrial artificial intelligence ai framework is proposed to make this solution a reality furthermore three key scientific challenges for putting such a framework in place are discussed finally several applications for this framework are presented to demonstrate its enormous potential,0
owing to large industrial energy consumption industrial production has brought a huge burden to the grid in terms of renewable energy access and power supply due to the coupling of multiple energy sources and the uncertainty of renewable energy and demand centralized methods require large calculation and coordination overhead thus this paper proposes a multienergy management framework achieved by decentralized execution and centralized training for an industrial park the energy management problem is formulated as a partiallyobservable markov decision process which is intractable by dynamic programming due to the lack of the prior knowledge of the underlying stochastic process the objective is to minimize longterm energy costs while ensuring the demand of users to solve this issue and improve the calculation speed a novel multiagent deep reinforcement learning algorithm is proposed which contains the following key points counterfactual baseline for facilitating contributing agents to learn better policies soft actorcritic for improving robustness and exploring optimal solutions a novel reward is designed by lagrange multiplier method to ensure the capacity constraints of energy storage in addition considering that the increase in the number of agents leads to performance degradation due to large observation spaces an attention mechanism is introduced to enhance the stability of policy and enable agents to focus on important energyrelated information which improves the exploration efficiency of soft actorcritic numerical results based on actual data verify the performance of the proposed algorithm with high scalability indicating that the industrial park can minimize energy costs under different demands,0
as the climate crisis intensifies understanding the environmental impact of professional activities is paramount especially in sectors with historically significant resource utilisation this includes high energy physics hep and related fields which investigate the fundamental laws of our universe as members of the young high energy physicists yhep association we investigate the co2equivalent emissions generated by heprelated research on a personalised perresearcher level for four distinct categories experiments tied to collaborations with substantial infrastructure institutional representing the resource consumption of research institutes and universities computing focussing on simulations and data analysis and travel covering professional trips to conferences etc the findings are integrated into a tool for selfevaluation the knowyourfootprint kyf calculator allowing the assessment of the personal and professional footprint and optionally sharing the data with the yhep association the study aims to heighten awareness foster sustainability and inspire the community to adopt more environmentally responsible research practices urgently,0
with highperformance computing systems now running at exascale optimizing powerscaling management and resource utilization has become more critical than ever this paper explores runtime powercapping optimizations that leverage integrated cpugpu power management on architectures like the nvidia gh200 superchip we evaluate energyperformance metrics that account for simultaneous cpu and gpu powercapping effects by using two complementary approaches speedupenergydelay and a euclidean distancebased multiobjective optimization method by targeting a mostly computebound exascale science application the locally selfconsistent multiple scattering lsms we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications and we recognize that even modest reductions in energy consumption can have significant overall impacts our results highlight how gpu taskspecific dynamic powercap adjustments combined with integrated cpugpu power steering can improve the energy utilization of certain gpu tasks thereby laying the groundwork for future adaptive optimization strategies,0
in recent years cloud service providers have been building and hosting datacenters across multiple geographical locations to provide robust services however the geographical distribution of datacenters introduces growing pressure to both local and global environments particularly when it comes to water usage and carbon emissions unfortunately efforts to reduce the environmental impact of such datacenters often lead to an increase in the cost of datacenter operations to cooptimize the energy cost carbon emissions and water footprint of datacenter operation from a global perspective we propose a novel framework for multiobjective sustainable datacenter management mosaic that integrates adaptive local search with a collaborative decompositionbased evolutionary algorithm to intelligently manage geographical workload distribution and datacenter operations our framework sustainably allocates workloads to datacenters while taking into account multiple geography and timebased factors including renewable energy sources variable energy costs power usage efficiency carbon factors and water intensity in energy our experimental results show that compared to the bestknown prior work frameworks mosaic can achieve 2745x speedup and 153x improvement in pareto hypervolume while reducing the carbon footprint by up to 133x water footprint by up to 309x and energy costs by up to 140x in the simultaneous threeobjective cooptimization scenario mosaic achieves a cumulative improvement across all objectives carbon water cost of up to 461x compared to the stateofthearts,0
supercapacitors are increasingly used as energy storage elements unlike batteries their state of charge has a considerable influence on their voltage in normal operation allowing them to work from zero to their maximum voltage in this work a theoretical and practical analysis is proposed of the energy efficiency of these devices according to their working voltages to this end several supercapacitors were subjected to charge and discharge cycles until the measurements of current and voltage stabilized at this point their energy efficiency was calculated these chargedischarge cycles were carried out i without rest between charging and discharging and ii with a rest of several minutes between the two stages using the information obtained from the tests the energy efficiency is shown plotted against the minimum and maximum working voltages by consulting the data and the graphs the ideal working voltages to optimize the energy efficiency of these devices can be obtained,0
the growing demand for internet of things iot networks has sparked interest in sustainable zeroenergy designs through energy harvesting eh to extend the lifespans of iot sensors visible light communication vlc is particularly promising integrating signal transmission with optical power harvesting to enable both data exchange and energy transfer in indoor network nodes vlc indoor channels however can be unstable due to their lineofsight nature and indoor movements in conventional ehbased iot networks maximum energy storage es capacity might halt further harvesting or waste excess energy leading to resource inefficiency addressing these issues this paper proposes a novel vlcbased wpans concept that enhances both data and energy harvesting efficiency the architecture employs densely distributed nodes and a central controller for simultaneous data and energy network operation ensuring efficient energy exchange and resource optimisation this approach with centralised control and energystateaware nodes aims for longterm energy autonomy the feasibility of the dataenergy networkingenabled lightbased internet of things deliot concept is validated through real hardware implementation demonstrating its sustainability and practical applicability results show significant improvements in the lifetime of resourcelimited nodes confirming the effectiveness of this new data and energy networking model in enhancing sustainability and resource optimisation in vlcbased wpans,0
the rapid advancement of artificial intelligence ai has led to unprecedented computational demands raising significant environmental and ethical concerns this paper critiques the prevailing reliance on largescale static datasets and monolithic training paradigms advocating for a shift toward humaninspired sustainable ai solutions we introduce a novel framework human ai hai which emphasizes incremental learning carbonaware optimization and humanintheloop collaboration to enhance adaptability efficiency and accountability by drawing parallels with biological cognition and leveraging dynamic architectures hai seeks to balance performance with ecological responsibility we detail the theoretical foundations system design and operational principles that enable ai to learn continuously and contextually while minimizing carbon footprints and human annotation costs our approach addresses pressing challenges in active learning continual adaptation and energyefficient model deployment offering a pathway toward responsible humancentered artificial intelligence,0
the end of dennard scaling and the slowing of moores law has put the energy use of datacenters on an unsustainable path datacenters are already a significant fraction of worldwide electricity use with application demand scaling at a rapid rate we argue that substantial reductions in the carbon intensity of datacenter computing are possible with a softwarecentric approach by making energy and carbon visible to application developers on a finegrained basis by modifying system apis to make it possible to make informed trade offs between performance and carbon emissions and by raising the level of application programming to allow for flexible use of more energy efficient means of compute and storage we also lay out a research agenda for systems software to reduce the carbon footprint of datacenter computing,0
the square kilometer array ska will be the largest global science project of the next two decades it will encompass a sensor network dedicated to radioastronomy covering two continents it will be constructed in remote areas of south africa and australia spreading over 3000km in high solar irradiance latitudes solar power supply is therefore an option to power supply the ska and contribute to a zero carbon footprint next generation telescope here we outline the major characteristics of the ska and some innovation approaches on thermal solar energy integration with ska prototypes,0
we examine the relationship among photovoltaic pv investments energy production and environmental impact using a dynamic optimization model our findings show that increasing investment in renewables supports both energy generation and ecological sustainability with the optimal path depending on policy priorities our analysis demonstrates that the economic and technological conditions for a transition to pv energy are already in place challenging the idea that renewables will only become competitive in the future we also account for the fact that pv optimality conditions improve over time as storage technology efficiency increases and production costs decrease in this perspective we find that energy storage may be a more effective policy tool than carbon taxation for cutting emissions as it faces less political resistance and further strengthens the longterm viability of renewable energy policy insights of the paper capture the evolving competitiveness of pv and its role in accelerating the energy transition they also provide policymakers with strategies to align economic growth with longterm sustainability through renewable energy investments,0
llms have transformed nlp yet deploying them on edge devices poses great carbon challenges prior estimators remain incomplete neglecting peripheral energy use distinct prefilldecode behaviors and soc design complexity this paper presents co2meter a unified framework for estimating operational and embodied carbon in llm edge inference contributions include 1 equationbased peripheral energy models and datasets 2 a gnnbased predictor with phasespecific llm energy data 3 a unitlevel embodied carbon model for soc bottleneck analysis and 4 validation showing superior accuracy over prior methods case studies show co2meters effectiveness in identifying carbon hotspots and guiding sustainable llm design on edge platforms source code,0
ai research is increasingly moving toward complex problem solving where models are optimized not only for pattern recognition but for multistep reasoning historically computings global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand but as efficiency improvements are approaching physical limits emerging reasoning ai lacks comparable saturation points performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference this paper argues that efficiency alone will not lead to sustainable reasoning ai and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems,0
improving energy efficiency in industrial foundry processes is a critical challenge as these operations are highly energyintensive and marked by complex interdependencies among process variables correlationbased analyses often fail to distinguish true causal drivers from spurious associations limiting their usefulness for decisionmaking this paper applies a timeseries causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting using production data from a danish foundry the study integrates timeseries clustering to segment melting cycles into distinct operational modes with the pcmci algorithm a stateoftheart causal discovery method to uncover causeeffect relationships within each mode across clusters robust causal relations among energy consumption furnace temperature and material weight define the core drivers of efficiency while voltage consistently influences cooling water temperature with a delayed response clusterspecific differences further distinguish operational regimes efficient clusters are characterized by stable causal structures whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies the contributions of this study are twofold first it introduces an integrated clusteringcausal inference pipeline as a methodological innovation for analyzing energyintensive processes second it provides actionable insights that enable foundry operators to optimize performance reduce energy consumption and lower emissions,0
this paper studies the energy efficiency of the cloud radio access network cran specifically focusing on two fundamental and different downlink transmission strategies namely the datasharing strategy and the compression strategy in the datasharing strategy the backhaul links connecting the central processor cp and the basestations bss are used to carry user messages each users messages are sent to multiple bss the bss locally form the beamforming vectors then cooperatively transmit the messages to the user in the compression strategy the user messages are precoded centrally at the cp which forwards a compressed version of the analog beamformed signals to the bss for cooperative transmission this paper compares the energy efficiencies of the two strategies by formulating an optimization problem of minimizing the total network power consumption subject to user target rate constraints where the total network power includes the bs transmission power bs activation power and loaddependent backhaul power to tackle the discrete and nonconvex nature of the optimization problems we utilize the techniques of reweighted ell1 minimization and successive convex approximation to devise provably convergent algorithms our main finding is that both the optimized datasharing and compression strategies in cran achieve much higher energy efficiency as compared to the nonoptimized coordinated multipoint transmission but their comparative effectiveness in energy saving depends on the user target rate at low user target rate datasharing consumes less total power than compression however as the user target rate increases the backhaul power consumption for datasharing increases significantly leading to better energy efficiency of compression at the high user rate regime,0
selfsustaining nonlinear oscillators of practically any type can function as latches and registers if boolean logic states are represented physically as the phase of oscillatory signals combinational operations on such phaseencoded logic signals can be implemented using arithmetic negation and addition followed by amplitude limiting with these generalpurpose boolean computation using a wide variety of natural and engineered oscillators becomes potentially possible such phaseencoded logic shows promise for energy efficient computing it also has inherent noise immunity advantages over traditional levelbased logic,0
the rapid adoption of large language models llms has led to significant energy consumption and carbon emissions posing a critical challenge to the sustainability of generative ai technologies this paper explores the integration of energyefficient optimization techniques in the deployment of llms to address these environmental concerns we present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of llms without compromising their operational effectiveness experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization making them particularly suitable for resourceconstrained environments the findings provide actionable insights for achieving sustainability in ai while maintaining high levels of accuracy and responsiveness,0
the rapid advancement of large language models llms has significantly impacted humancomputer interaction epitomized by the release of gpt4o which introduced comprehensive multimodality capabilities in this paper we first explored the deployment strategies economic considerations and sustainability challenges associated with the stateoftheart llms more specifically we discussed the deployment debate between retrievalaugmented generation rag and finetuning highlighting their respective advantages and limitations after that we quantitatively analyzed the requirement of xpus in training and inference additionally for the tokenomics of llm services we examined the balance between performance and cost from the quality of experience qoes perspective of end users lastly we envisioned the future hybrid architecture of llm processing and its corresponding sustainability concerns particularly in the environmental carbon footprint impact through these discussions we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of llms,0
technology companies have been leading the way to a renewable energy transformation by investing in renewable energy sources to reduce the carbon footprint of their datacenters in addition to helping build new solar and wind farms companies make power purchase agreements or purchase carbon offsets rather than relying on renewable energy every hour of the day every day of the week 247 relying on renewable energy 247 is challenging due to the intermittent nature of wind and solar energy inherent variations in solar and wind energy production causes excess or lack of supply at different times to cope with the fluctuations of renewable energy generation multiple solutions must be applied these include capacity sizing with a mix of solar and wind power energy storage options and carbon aware workload scheduling however depending on the region and datacenter workload characteristics the carbonoptimal solution varies existing work in this space does not give a holistic view of the tradeoffs of each solution and often ignore the embodied carbon cost of the solutions in this work we provide a framework carbon explorer to analyze the multidimensional solution space by taking into account operational and embodided footprint of the solutions to help make datacenters operate on renewable energy 247 the solutions we analyze include capacity sizing with a mix of solar and wind power battery storage and carbon aware workload scheduling which entails shifting the workloads from times when there is lack of renewable supply to times with abundant supply,0
this paper addresses the critical challenge posed by the increasing energy consumption in mobile networks particularly with the advent of sixth generation 6g technologies we propose an adaptive network management framework that leverages the open radio access network oran architecture to enhance network adaptability and energy efficiency by utilizing orans open interfaces and intelligent controllers our approach implements dynamic resource management strategies that respond to fluctuating user demands while maintaining the quality of service we design and implement orancompliant applications to validate our framework demonstrating significant improvements in energy efficiency without compromising network performance our study offers a comprehensive guide for utilizing orans open architecture to achieve sustainable and energyefficient 6g networks aligning with global efforts to reduce the environmental impact of mobile communication systems,0
research on life cycle assessment lca is being conducted in various sectors from analyzing building materials and components to comprehensive evaluations of entire structures however reviews of the existing literature have been unable to provide a comprehensive overview of research in this field leaving scholars without a definitive guideline for future investigations this paper aims to fill this gap mapping more than twenty years of research using an innovative methodology that combines social network analysis and text mining the paper examined 8024 scientific abstracts the authors identified seven key thematic groups building and sustainability clusters bscs to assess their significance in the broader discourse on building and sustainability the semantic brand score sbs indicator was applied additionally building and sustainability trends were tracked focusing on the lca concept the major research topics mainly relate to building materials and energy efficiency in addition to presenting an innovative approach to reviewing extensive literature domains the article also provides insights into emerging and underdeveloped themes outlining crucial future research directions,0
as the world enters the journey toward the 6th generation 6g of wireless technology the promises of ultrahigh data rates unprecedented low latency and a massive surge in connected devices require crucial exploration of network energy saving nes solutions to minimize the carbon footprint and overall energy usage of future cellular networks on the other hand networkcontrolled repeaters ncrs have been introduced by 3rd generation partnership project 3gpp as a costeffective solution to improve network coverage however their impact on network power consumption and energy efficiency has not been thoroughly investigated this paper studies nes schemes for nextgeneration 6g networks aided by ncrs and proposes optimal nes strategies aiming at maximizing the overall energy efficiency of the network repeaters are shown to allow for power savings at nextgeneration nodeb gnb and offer higher overall energy efficiency ee and spectral efficiency se thus providing an energyefficient and costefficient alternative to increase the performance of future 6g networks,0
conventional cellular wireless networks were designed with the purpose of providing high throughput for the user and high capacity for the service provider without any provisions of energy efficiency as a result these networks have an enormous carbon footprint in this paper we describe the sources of the inefficiencies in such networks first we present results of the studies on how much carbon footprint such networks generate we also discuss how much more mobile traffic is expected to increase so that this carbon footprint will even increase tremendously more we then discuss specific sources of inefficiency and potential sources of improvement at the physical layer as well as at higher layers of the communication protocol hierarchy in particular considering that most of the energy inefficiency in cellular wireless networks is at the base stations we discuss multitier networks and point to the potential of exploiting mobility patterns in order to use base station energy judiciously we then investigate potential methods to reduce this inefficiency and quantify their individual contributions by a consideration of the combination of all potential gains we conclude that an improvement in energy consumption in cellular wireless networks by two orders of magnitude or even more is possible,0
hybrid intelligence aims to enhance decisionmaking problemsolving and overall system performance by combining the strengths of both human cognitive abilities and artificial intelligence with the rise of large language models llm progressively participating as smart agents to accelerate machine learning development hybrid intelligence is becoming an increasingly important topic for effective interaction between humans and machines this paper presents an approach to leverage hybrid intelligence towards sustainable and energyaware machine learning when developing machine learning models final model performance commonly rules the optimization process while the efficiency of the process itself is often neglected moreover in recent times energy efficiency has become equally crucial due to the significant environmental impact of complex and largescale computational processes the contribution of this work covers the interactive inclusion of secondary knowledge sources through humanintheloop hitl and llm agents to stress out and further resolve inefficiencies in the machine learning development process,0
vibration energy harvesting is an emerging technology aimed at turning mechanical energy from vibrations into electricity to power microsystems of the future most of present vibration energy harvesters are based on a mass spring structure introducing a resonance phenomenon that allows to increase the output power compared to nonresonant systems but limits the working frequency bandwidth therefore they are not able to harvest energy when ambient vibrations frequencies shift to follow shifts of ambient vibration frequencies and to increase the frequency band where energy can be harvested one solution consists in using nonlinear springs we present in this paper a model of adjustable nonlinear springs hshaped springs and their benefits to improve velocitydamped vibration energy harvesters veh output powers a simulation on a real vibration source proves that the output power can be higher in nonlinear devices compared to linear systems up to 48,0
machine learning solutions are rapidly adopted to enable a variety of key use cases from conversational ai assistants to scientific discovery this growing adoption is expected to increase the associated lifecycle carbon footprint including both emphoperational carbon from training and inference and emphembodied carbon from ai hardware manufacturing we introduce ourframework the first carbonaware cooptimization framework for transformerbased models and hardware accelerators by integrating both operational and embodied carbon into earlystage design space exploration ourframework enables sustainabilitydriven model architecture and hardware accelerator codesign that reveals fundamentally different tradeoffs than latency or energycentric approaches evaluated across a range of transformer models ourframework consistently demonstrates the potential to reduce total carbon emissions by up to 30 while maintaining accuracy and latency we further highlight its extensibility through a focused case study on multimodal models our results emphasize the need for holistic optimization methods that prioritize carbon efficiency without compromising model capability and execution time performance the source code of ourframework is available at smallhref,0
the importance of promoting sustainable and environmentally responsible practices is becoming increasingly recognized in all domains including tourism the impact of tourism extends beyond its immediate stakeholders and affects passive participants such as the environment local businesses and residents city trips in particular offer significant opportunities to encourage sustainable tourism practices by directing travelers towards destinations that minimize environmental impact while providing enriching experiences tourism recommender systems trs can play a critical role in this by integrating sustainability features in trs travelers can be guided towards destinations that meet their preferences and align with sustainability objectives this paper investigates how different user interface design elements affect the promotion of sustainable city trip choices we explore the impact of various features on user decisions including sustainability labels for transportation modes and their emissions popularity indicators for destinations seasonality labels reflecting crowd levels for specific months and an overall sustainability composite score through a user study involving mockups participants evaluated the helpfulness of these features in guiding them toward more sustainable travel options our findings indicate that sustainability labels significantly influence users towards lowercarbon footprint options while popularity and seasonality indicators guide users to less crowded and more seasonally appropriate destinations this study emphasizes the importance of providing users with clear and informative sustainability information which can help them make more sustainable travel choices it lays the groundwork for future applications that can recommend sustainable destinations in realtime,0
from ridehailing to car rentals consumers are often presented with ecofriendly options beyond highlighting a green vehicle and co2 emissions co2 equivalencies have been designed to provide understandable amounts we ask which equivalencies will lead to ecofriendly decisions we conducted five ridehailing scenario surveys where participants picked between regular and ecofriendly options testing equivalencies social features and valencebased interventions further we tested a carrental embodiment to gauge how an individual needing a car for several days might behave versus the immediate ridehailing context we find that participants are more likely to choose green rides when presented with additional information about emissions co2 by weight was found to be the most effective further we found that information framing be it individual or collective footprint positive or negative valence had an impact on participants choices finally we discuss how our findings inform the design of effective interventions for reducing carbased carbonemissions,0
unmanned aerial vehicles uavs especially fixedwing ones that withstand strong winds have great potential for oceanic exploration and research this paper studies a uavaided maritime data collection system with a fixedwing uav dispatched to collect data from marine buoys we aim to minimize the uavs energy consumption in completing the task by jointly optimizing the communication time scheduling among the buoys and the uavs flight trajectory subject to wind effect which is a nonconvex problem and difficult to solve optimally existing techniques such as the successive convex approximation sca method provide efficient suboptimal solutions for collecting smallmoderate data volume whereas the solution heavily relies on the trajectory initialization and has not explicitly considered the wind effect while the computational complexity and resulted trajectory complexity both become prohibitive for the task with large data volume to this end we propose a new cyclical trajectory design framework that can handle arbitrary data volume efficiently subject to wind effect specifically the proposed uav trajectory comprises multiple cyclical laps each responsible for collecting only a subset of data and thereby significantly reducing the computationaltrajectory complexity which allows searching for better trajectory initialization that fits the buoys topology and the wind numerical results show that the proposed cyclical scheme outperforms the benchmark oneflightonly scheme in general moreover the optimized cyclical 8shape trajectory can proactively exploit the wind and achieve lower energy consumption compared with the case without wind,0
embodied carbon is the total carbon released from the processes associated with a product from cradle to gate in many industry sectors embodied carbon dominates the overall carbon footprint embodied carbon accounting ie to estimate the embodied carbon of a product has become an important research topic existing studies derive the embodied carbon through life cycle analysis lca reports current lca reports only provide the carbon emission of a product class eg 28nm cpu yet a product instance can be manufactured from diverse regions and in diverse time periods eg a winter period of ireland intel it is known that the carbon emission depends on the electricity generation process which has spatial and temporal dynamics therefore the embodied carbon of a specific product instance can largely differ from its product class in this paper we present new spatialtemporal embodied carbon models for embodied carbon accounting we observe significant differences between current embodied carbon models and our spatialtemporal embodied carbon models eg for 7nm cpu the difference can be 1369,0
the transition to 6g is expected to bring significant advancements including much higher data rates enhanced reliability and ultralow latency compared to previous generations although 6g is anticipated to be 100 times more energy efficient this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability network sustainability encompasses a broader scope integrating business viability environmental sustainability and social responsibility this paper explores the sustainability requirements for 6g and proposes open ran as a key architectural solution by enabling network diversification fostering open and continuous innovation and integrating aiml open ran can promote sustainability in 6g the paper identifies high energy consumption and ewaste generation as critical sustainability challenges and discusses how open ran can address these issues through softwarisation edge computing and ai integration,0
as the global focus on combating environmental pollution intensifies the transition to sustainable energy sources particularly in the form of electric vehicles evs has become paramount this paper addresses the pressing need for smart charging for evs by developing a comprehensive mathematical model aimed at optimizing charging station management the model aims to efficiently allocate the power from charging sockets to evs prioritizing cost minimization and avoiding energy waste computational simulations demonstrate the efficacy of the mathematical optimization model which can unleash its full potential when the number of evs at the charging station is high,0
detailed scheduling has traditionally been optimized for the reduction of makespan and manufacturing costs however growing awareness of environmental concerns and increasingly stringent regulations are pushing manufacturing towards reducing the carbon footprint of its operations scope 2 emissions which are the indirect emissions related to the production and consumption of grid electricity are in fact estimated to be responsible for more than onethird of the global ghg emissions in this context carbonaware scheduling can serve as a powerful way to reduce manufacturings carbon footprint by considering the timedependent carbon intensity of the grid and the availability of onsite renewable electricity this study introduces a carbonaware permutation flowshop scheduling model designed to reduce scope 2 emissions the model is formulated as a mixedinteger linear problem taking into account the forecasted grid generation mix and available onsite renewable electricity along with the set of jobs to be scheduled and their corresponding power requirements the objective is to find an optimal dayahead schedule that minimizes scope 2 emissions the problem is addressed using a dedicated memetic algorithm combining evolutionary strategy and local search results from computational experiments confirm that by considering the dynamic carbon intensity of the grid and onsite renewable electricity availability substantial reductions in carbon emissions can be achieved,0
energyefficient ventilation control plays a vital role in reducing building energy consumption while ensuring occupant health and comfort while computational fluid dynamics cfd simulations provide detailed and physically accurate representation of indoor airflow their high computational cost limits their use in realtime building control in this work we present a neural operator learning framework that combines the physical accuracy of cfd with the computational efficiency of machine learning to enable building ventilation control with the highfidelity fluid dynamics models our method jointly optimizes the airflow supply rates and vent angles to reduce energy use and adhere to air quality constraints we train an ensemble of neural operator transformer models to learn the mapping from building control actions to airflow fields using highresolution cfd data this learned neural operator is then embedded in an optimizationbased control framework for building ventilation control experimental results show that our approach achieves significant energy savings compared to maximum airflow rate control rulebased control as well as datadriven control methods using spatially averaged co2 prediction and deep learning based reduced order model while consistently maintaining safe indoor air quality these results highlight the practicality and scalability of our method in maintaining energy efficiency and indoor air quality in realworld buildings,0
from its conception 6g is being designed with a particular focus on sustainability the general philosophy of the h2020 hexax project work on sustainability in 6g is based on two principles to reduce direct negative life cycle impacts of 6g systems as much as possible sustainable 6g and to analyze use cases that maximize positive environmental social and economic effects in other sectors of society 6g for sustainability or its enablement effect to apply this philosophy hexax is designing 6g with three sustainability objectives in mind to enable the reduction of emissions in 6gpowered sectors of society to reduce the total cost of ownership and to improve energy efficiency this paper describes these objectives their associated kpis and quantitative targets and the levers to reach them furthermore to maximize the positive effects of 6g through the enablement effect a link between 6g and the united nations sustainable development goals un sdgs framework is proposed and illustrated by hexax use case families,0
energy management decreases energy expenditures and consumption while simultaneously increasing energy efficiency reducing carbon emissions and enhancing operational performance smart grids are a type of sophisticated energy infrastructure that increase the generation and distribution of electricitys sustainability dependability and efficiency by utilizing digital communication technologies they combine a number of cuttingedge techniques and technology to improve energy resource management a large amount of research study on the topic of smart grids for energy management has been completed in the last several years the authors of the present study want to cover a number of topics including smart grid benefits and components technical developments integrating renewable energy sources using artificial intelligence and data analytics cybersecurity and privacy smart grids for energy management are an innovative field of study aiming at tackling various difficulties and magnifying the efficiency dependability and sustainability of energy systems including 1 renewable sources of power like solar and wind are intermittent and unpredictable 2 defending smart grid system from various cyberattacks 3 incorporating an increasing number of electric vehicles into the system of power grid without overwhelming it additionally it is proposed to use ai and data analytics for better performance on the grid reliability and energy management it also looks into how ai and data analytics can be used to optimize grid performance enhance reliability and improve energy management the authors will explore these significant challenges and ongoing research lastly significant issues in this field are noted and recommendations for further work are provided,0
motivated by an imperative to reduce the carbon emissions of cloud data centers this paper studies the online carbonaware resource scaling problem with unknown job lengths ocsu and applies it to carbonaware resource scaling for executing computing workloads the task is to dynamically scale resources eg the number of servers assigned to a job of unknown length such that it is completed before a deadline with the objective of reducing the carbon emissions of executing the workload the total carbon emissions of executing a job originate from the emissions of running the job and excess carbon emitted while switching between different scales eg due to checkpoint and resume prior work on carbonaware resource scaling has assumed accurate job length information while other approaches have ignored switching losses and require carbon intensity forecasts these assumptions prohibit the practical deployment of prior work for online carbonaware execution of scalable computing workload we propose lacs a theoretically robust learningaugmented algorithm that solves ocsu to achieve improved practical averagecase performance lacs integrates machinelearned predictions of job length to achieve solid theoretical performance lacs extends the recent theoretical advances on online conversion with switching costs to handle a scenario where the job length is unknown our experimental evaluations demonstrate that on average the carbon footprint of lacs lies within 12 of the online baseline that assumes perfect job length information and within 16 of the offline baseline that in addition to the job length also requires accurate carbon intensity forecasts furthermore lacs achieves a 32 reduction in carbon footprint compared to the deadlineaware carbonagnostic execution of the job,0
classical heating of residential areas is very energyintensive so alternatives are needed including renewable energies and advanced heating technologies thus the present paper introduces a new methodology for comprehensive variant analysis for future district heating planning aiming at optimizing emissions and costs for this an extensive modelicabased modeling study comprising models of heating center heat grid pipelines and heating interface units to buildings are coupled in cosimulations these enable a comparative analysis of the economic feasibility and sustainability for various technologies and energy carriers to be carried out the new modular and highly parameterizable building model serves for validation of the introduced heat grid model the results show that biomethane as an energy source reduces carbon equivalent emissions by nearly 70 compared to conventional natural gas heating and the use of hydrogen as an energy source reduces carbon equivalent emissions by 77 when equipped with a heat pump in addition the use of ground source heat pumps has a high economic viability when economic benefits are taken into account the study findings highlight the importance of strategic planning and flexible design in the early stages of district development in order to achieve improved energy efficiency and a reduced carbon footprint,0
future wireless cellular networks will utilize millimeterwave and subthz frequencies and deploy smallcell base stations to achieve data rates on the order of hundreds of gigabits per second per user the move to subthz frequencies will require attention to sustainability and reduction of power whenever possible to reduce the carbon footprint while maintaining adequate battery life for the massive number of resourceconstrained devices to be deployed this article analyzes power consumption of future wireless networks using a new metric the power waste factor w which shows promise for the study and development of green g green technology for future wireless networks using w power efficiency can be considered by quantifying the power wasted by all devices on a signal path in a cascade we then show that the consumption efficiency factor cef defined as the ratio of the maximum data rate achieved to the total power consumed is a novel and powerful measure of power efficiency that shows less energy per bit is expended as the cell size shrinks and carrier frequency and channel bandwidth increase our findings offer a standard approach to calculating and comparing power consumption and energy efficiency,0
the transportation sector accounts for about 25 of global greenhouse gas emissions therefore an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint efficiency is typically measured in terms of energy use per traveled distance eg liters of fuel per kilometer leading factors that impact the energy efficiency are the type of vehicle environment driver behavior and weather conditions these varying factors introduce uncertainty in estimating the vehicles energy efficiency we propose in this paper an ensemble learning approach based on deep neural networks enn that is designed to reduce the predictive uncertainty and to output measures of such uncertainty we evaluated it using the publicly available vehicle energy dataset ved and compared it with several baselines per vehicle and energy type the results showed a high predictive performance and they allowed to output a measure of predictive uncertainty,0
in recent years sustainability in software systems has gained significant attention especially with the rise of cloud computing and the shift towards cloudbased architectures this shift has intensified the need to identify sustainability in architectural discussions to take informed architectural decisions one source to see these decisions is in online qa forums among practitioners discussions however recognizing sustainability concepts within software practitioners discussions remains challenging due to the lack of clear and distinct guidelines for this task to address this issue we introduce the notion of sustainability flags as pointers in relevant discussions developed through thematic analysis of multiple sustainability best practices from cloud providers this study further evaluates the effectiveness of these flags in identifying sustainability within cloud architecture posts using a controlled experiment preliminary results suggest that the use of flags results in classifying fewer posts as sustainabilityrelated compared to a control group with moderately higher certainty and significantly improved performance moreover sustainability flags are perceived as more useful and understandable than relying solely on definitions for identifying sustainability,0
telecommunication technologies are important enablers for both digital and ecological transitions by offering digital alternatives to traditional modes of transportation and communication they help reduce carbon footprints while improving access to fundamental services particularly in rural and remote areas telecommunications facilitate access to education healthcare and employment helping to bridge the digital divide additionally telecommunications can promote sustainability by supporting renewable energy usage gender equality and circular economies however defining the role of telecommunications in sustainability remains complex due to the historical focus on performance rather than longterm societal goals given the significance of this theme this paper aims to provide the reader with a deeper look at the concept of sustainability within the telecommunications sector by examining relevant initiatives and projects it reviews the major approaches for measuring sustainability and outlines practical approaches for implementing these assessments furthermore the paper explores the proposed network architectures that incorporate key value indicators and discusses major technologies in this area such as network digital twins and intentbased networking through this analysis the paper aims to contribute to creating sustainable telecommunication networks and broader industries,0
in alignment with the paris agreement the city of oxford in the uk aims to become carbon neutral by 2040 renewable energy help achieve this target by reducing the reliance on carbonintensive grid electricity this research seeks to optimally size solar photovoltaic and lithium battery storage systems reducing oxfords grid electricity reliance in buildings the analysis starts with modeling the electricity demand the model uses elexon electricity settlement profiles and assembles them into the demand profile according to the quantity and types of buildings in oxford then solar generation is modeled using pfenninger and staffells method solar photovoltaic and lithium storage systems are sized using a hybridized analytical and iterative method first the method calculates the solar system size search range then iterates through the range at each solar size the method calculates and iterates through the storage system size search range within each iteration the renewable system is simulated using demand and generation data with a simplified system setup and the conventional operation strategy the method outputs combinations of solar system capacity storage system capacity and grid electricity import each combinations levelized cost of electricity is calculated and the lowest cost combination is the optimal sizing solar and storage system costs are projected from 2019 to 2100 and the optimal sizing is calculated for each year the result shows that solar photovoltaic is economically competitive but lithium storage cost is still too high as solar and storage prices continue to drop they will take up greater portions of the energy system however there will always be a need for the grid as it provides flexibility and can meet demands that are too costly for solar and storage,0
environmental sustainability in systemsofsystems sos is an emerging field that seeks to integrate technological solutions to promote the efficient management of natural resources while systematic reviews address sustainability in the context of smart cities a category of sos a systematic study synthesizing the existing knowledge on environmental sustainability applied to sos in general does not exist although literature includes other types of sustainability such as financial and social this study focuses on environmental sustainability analyzing how sos contribute to sustainable practices such as carbon emission reduction energy efficiency and biodiversity conservation we conducted a systematic mapping study to identify the application domains of sos in sustainability the challenges faced and research opportunities we planned and executed a research protocol including an automated search over four scientific databases of 926 studies retrieved we selected analyzed and reported the results of 39 relevant studies our findings reveal that most studies focus on smart cities and smart grids while applications such as sustainable agriculture and wildfire prevention are less explored we identified challenges such as system interoperability scalability and data governance finally we propose future research directions for sos and environmental sustainability,0
the ict sector responsible for 2 of global carbon emissions is under scrutiny calling for methodologies and tools to design and develop software in an environmentally sustainablebydesign manner however the software engineering solutions for designing and developing carbonefficient software are currently scattered over multiple different pieces of literature which makes it difficult to consult the body of knowledge on the topic in this article we precisely conduct a systematic literature review on stateoftheart proposals for designing and developing carbonefficient software we identify and analyse 65 primary studies by classifying them through a taxonomy aimed at answering the 5w1h questions of carbonefficient software design and development we first provide a reasoned overview and discussion of the existing guidelines reference models measurement solutions and techniques for measuring reducing or minimising the carbon footprint of software ultimately we identify open challenges and research gaps offering insights for future work in this field,0
a community integrated energy system cies is an important carrier of the energy internet and smart city in geographical and functional terms its emergence provides a new solution to the problems of energy utilization and environmental pollution to coordinate the integrated demand response and uncertainty of renewable energy generation rgs a datadriven twostage distributionally robust optimization dro model is constructed a comprehensive norm consisting of the 1norm and infinitynorm is used as the uncertainty probability distribution information set thereby avoiding complex probability density information to address multiple uncertainties of rgs a generative adversarial network based on the wasserstein distance with gradient penalty is proposed to generate rg scenarios which has wide applicability to further tap the potential of the demand response we take into account the ambiguity of human thermal comfort and the thermal inertia of buildings thus an integrated demand response mechanism is developed that effectively promotes the consumption of renewable energy the proposed method is simulated in an actual cies in north china in comparison with traditional stochastic programming and robust optimization it is verified that the proposed dro model properly balances the relationship between economical operation and robustness while exhibiting stronger adaptability furthermore our approach outperforms other commonly used dro methods with better operational economy lower renewable power curtailment rate and higher computational efficiency,0
the proliferation of software and ai comes with a hidden risk its growing energy and carbon footprint as concerns regarding environmental sustainability come to the forefront understanding and optimizing how software impacts the environment becomes paramount in this paper we present a stateoftheart review of methods and tools that enable the measurement of software and airelated energy andor carbon emissions we introduce a taxonomy to categorize the existing work as monitoring estimation or blackbox approaches we delve deeper into the tools and compare them across different dimensions and granularity for example whether their measurement encompasses energy and carbon emissions and the components considered like cpu gpu ram etc we present our observations on the practical use component wise consolidation of approaches as well as the challenges that we have identified across the current stateoftheart as we start an initiative to address these challenges we emphasize active collaboration across the community in this important field,0
rapid adoption of machine learning ml technologies has led to a surge in power consumption across diverse systems from tiny iot devices to massive datacenter clusters benchmarking the energy efficiency of these systems is crucial for optimization but presents novel challenges due to the variety of hardware platforms workload characteristics and systemlevel interactions this paper introduces mlperf power a comprehensive benchmarking methodology with capabilities to evaluate the energy efficiency of ml systems at power levels ranging from microwatts to megawatts developed by a consortium of industry professionals from more than 20 organizations mlperf power establishes rules and best practices to ensure comparability across diverse architectures we use representative workloads from the mlperf benchmark suite to collect 1841 reproducible measurements from 60 systems across the entire range of ml deployment scales our analysis reveals tradeoffs between performance complexity and energy efficiency across this wide range of systems providing actionable insights for designing optimized ml solutions from the smallest edge devices to the largest cloud infrastructures this work emphasizes the importance of energy efficiency as a key metric in the evaluation and comparison of the ml system laying the foundation for future research in this critical area we discuss the implications for developing sustainable ai solutions and standardizing energy efficiency benchmarking for ml systems,0
we study sensor networks with energy harvesting nodes the generated energy at a node can be stored in a buffer a sensor node periodically senses a random field and generates a packet these packets are stored in a queue and transmitted using the energy available at that time at the node for such networks we develop efficient energy management policies first for a single node we obtain policies that are throughput optimal ie the data queue stays stable for the largest possible data rate next we obtain energy management policies which minimize the mean delay in the queue we also compare performance of several easily implementable suboptimal policies a greedy policy is identified which in low snr regime is throughput optimal and also minimizes mean delay next using the results for a single node we develop efficient mac policies,0
by constructing digital twins dt of an integrated energy system ies one can benefit from dts predictive capabilities to improve coordinations among various energy converters hence enhancing energy efficiency cost savings and carbon emission reduction this paper is motivated by the fact that practical iess suffer from multiple uncertainty sources and complicated surrounding environment to address this problem a novel dtbased dayahead scheduling method is proposed the physical ies is modelled as a multivector energy system in its virtual space that interacts with the physical ies to manipulate its operations a deep neural network is trained to make statistical costsaving scheduling by learning from both historical forecasting errors and dayahead forecasts case studies of iess show that the proposed dtbased method is able to reduce the operating cost of ies by 635 comparing to the existing forecastbased scheduling methods it is also found that both electric vehicles and thermal energy storages play proactive roles in the proposed method highlighting their importance in future energy system integration and decarbonisation,0
while the transition to electric vehicles evs is essential for decarbonizing the transportation system the production and distribution of evs entail substantial carbon costs to ensure these emissions are accurately accounted for and effectively mitigated this research introduces a digital twin of the evs supply chain addressing a critical gap in current ev life cycle analyses and providing the first comprehensive quantification of its environmental sustainability and resilience this simulation model replicates global market dynamics and captures the complexity and uncertainty of the ev supply chain enabling a thorough evaluation of its carbon footprint sustainability resilience and whatif counterfactual scenarios for alternative market structures the results reveal that average supply chain emissions range from 642 to 694 kg eco2kwh across different battery technologies additionally the mass flow analysis shows unbalanced dependencies at all supply phases with one geographical region significantly dominating the supply chain structure highlighting the current supply chain architectures low resilience and high vulnerability in light of these findings the study introduces an optimization model for hub and resource allocation configuration effectively reducing vulnerability levels and supply chain emissions by up to 80,0
multiuncertainties from power sources and loads have brought significant challenges to the stable demand supply of various resources at islands to address these challenges a comprehensive scheduling framework is proposed by introducing a modelfree deep reinforcement learning drl approach based on modeling an island integrated energy system ies in response to the shortage of freshwater on islands in addition to the introduction of seawater desalination systems a transmission structure of hydrothermal simultaneous transmission hst is proposed the essence of the ies scheduling problem is the optimal combination of each units output which is a typical timing control problem and conforms to the markov decisionmaking solution framework of deep reinforcement learning deep reinforcement learning adapts to various changes and timely adjusts strategies through the interaction of agents and the environment avoiding complicated modeling and prediction of multiuncertainties the simulation results show that the proposed scheduling framework properly handles multiuncertainties from power sources and loads achieves a stable demand supply for various resources and has better performance than other realtime scheduling methods especially in terms of computational efficiency in addition the hst model constitutes an active exploration to improve the utilization efficiency of island freshwater,0
decentralized federated learning dfl is an emerging paradigm that enables collaborative model training without centralized data and model aggregation enhancing privacy and resilience however its sustainability remains underexplored as energy consumption and carbon emissions vary across different system configurations understanding the environmental impact of dfl is crucial for optimizing its design and deployment this work aims to develop a comprehensive and operational framework for assessing the sustainability of dfl systems to address it this work provides a systematic method for quantifying energy consumption and carbon emissions offering insights into improving the sustainability of dfl this work proposes greendfl a fully implementable framework that has been integrated into a realworld dfl platform greendfl systematically analyzes the impact of various factors including hardware accelerators model architecture communication medium data distribution network topology and federation size on the sustainability of dfl systems besides a sustainabilityaware aggregation algorithm greendflsa and a node selection algorithm greendflsn are developed to optimize energy efficiency and reduce carbon emissions in dfl training empirical experiments are conducted on multiple datasets measuring energy consumption and carbon emissions at different phases of the dfl lifecycle the proposed greendfl provides a comprehensive and practical approach for assessing the sustainability of dfl systems furthermore it offers best practices for improving environmental efficiency in dfl making sustainability considerations more actionable in realworld deployments,0
future highly renewable energy systems might require substantial storage deployment at the current stage the technology portfolio of dominant storage options is limited to pumpedhydro storage and liion batteries it is uncertain which storage design will be able to compete with these options considering europe as a case study we derive the cost and efficiency requirements of a generic storage technology which we refer to as storagex to be deployed in the costoptimal system this is performed while including existing pumpedhydro facilities and accounting for the competition from stationary liion batteries flexible generation technology and flexible demand in a highly renewable sectorcoupled energy system based on a sample space of 724 storage configurations we show that energy capacity cost and discharge efficiency largely determine the optimal storage deployment in agreement with previous studies here we show that charge capacity cost is also important due to its impact on renewable curtailment a significant deployment of storagex in a costoptimal system requires a discharge efficiency of at least 95 b discharge efficiency of at least 50 together with low energy capacity cost 10eurkwh or c discharge efficiency of at least 25 with very low energy capacity cost 2eurkwh comparing our findings with seven emerging technologies reveals that none of them fulfill these requirements thermal energy storage tes is however on the verge of qualifying due to its low energy capacity cost and concurrent low charge capacity cost exploring the space of storage designs reveals that system cost reduction from storagex deployment can reach 9 at its best but this requires high roundtrip efficiency 90 and low charge capacity cost 35eurkw,0
the significant carbon footprint of the ict sector calls for methodologies to contain carbon emissions of running software this article proposes a novel framework for implementing configuring and assessing carbonaware interactive software services first we propose a methodology to implement carbonaware services leveraging the strategy design pattern to feature alternative service versions with different energy consumption then we devise a bilevel optimisation scheme to configure which version to use at different times of the day based on forecasts of carbon intensity and service requests pursuing the twofold goal of minimising carbon emissions and maintaining average output quality above a desired setpoint last an opensource prototype of such optimisation scheme is used to configure a software service implemented as per our methodology and assessed against traditional nonadaptive implementations of the same service results show the capability of our framework to control the average quality of output results of carbonaware services and to reduce carbon emissions from 8 to 50,0
this work focuses on the high carbon emissions generated by deep learning model training specifically addressing the core challenge of balancing algorithm performance and energy consumption it proposes an innovative twodimensional sustainability evaluation system different from the traditional single performanceoriented evaluation paradigm this study pioneered two quantitative indicators that integrate energy efficiency ratio and accuracy the sustainable harmonic mean fms integrates accumulated energy consumption and performance parameters through the harmonic mean to reveal the algorithm performance under unit energy consumption the area under the sustainability curve asc constructs a performancepower consumption curve to characterize the energy efficiency characteristics of the algorithm throughout the cycle to verify the universality of the indicator system the study constructed benchmarks in various multimodal tasks including image classification segmentation pose estimation and batch and online learning experiments demonstrate that the system can provide a quantitative basis for evaluating crosstask algorithms and promote the transition of green ai research from theory to practice our sustainability evaluation framework code can be found here providing methodological support for the industry to establish algorithm energy efficiency standards,0
an increasing share of consumers care about the carbon footprint of their electricity this paper proposes to integrate consumer carbon preferences in the electricity marketclearing through consumerbased carbon costs specifically consumers can submit not only bids for power but also assign a cost to the carbon emissions incurred by their electricity use we start from a centralized market clearing that maximizes social welfare under consideration of generation costs consumer utility and consumer carbon costs we then derive an equivalent equilibrium formulation which incorporates a carbon allocation problem and gives rise to a set of carbonadjusted electricity prices for both consumers and generators we prove that the carbonadjusted prices are higher for lowemitting generators and consumers with high carbon costs further we prove that this new paradigm satisfies the same desirable market properties as standard electricity markets based on locational marginal prices namely revenue adequacy and individual rationality and demonstrate that a carbon tax on generators is equivalent to imposing a uniform carbon cost on consumers using a simplified threebus system and the rtsgmlc system we illustrate that consumerbased carbon costs contribute to greener electricity market clearing both through generation redispatch and reductions in demand,0
scientific workflows are widely used to automate scientific data analysis and often involve computationally intensive processing of large datasets on compute clusters as such their execution tends to be longrunning and resourceintensive resulting in substantial energy consumption and depending on the energy mix carbon emissions meanwhile a wealth of carbonaware computing methods have been proposed yet little work has focused specifically on scientific workflows even though they present a substantial opportunity for carbonaware computing because they are often significantly delay tolerant efficiently interruptible highly scalable and widely heterogeneous in this study we first exemplify the problem of carbon emissions associated with running scientific workflows and then show the potential for carbonaware workflow execution for this we estimate the carbon footprint of seven realworld nextflow workflows executed on different cluster infrastructures using both average and marginal carbon intensity data furthermore we systematically evaluate the impact of carbonaware temporal shifting and the pausing and resuming of the workflow moreover we apply resource scaling to workflows and workflow tasks finally we report the potential reduction in overall carbon emissions with temporal shifting capable of decreasing emissions by over 80 and resource scaling capable of decreasing emissions by 67,0
we study the properties of plasmas containing a low energy thermal photon component at comoving temperature equiv ktme c2 sim 105 102 interacting with an energetic electron component characteristic of eg the dissipation phase of relativistic outflows in gammaray bursts grbs xray flashes and blazars we show that for scattering optical depths larger than a few balance between compton and inversecompton scattering leads to the accumulation of electrons at values of  015 03 for optical depths larger than 100 this leads to a peak in the comoving photon spectrum at 110 kev very weakly dependent on the values of the free parameters in particular these results are applicable to the internal shock model of grb as well as to slow dissipation models eg as might be expected from reconnection if the dissipation occurs at a subphotospheric radii for grb bulk lorentz factors 100 this results in observed spectral peaks clustering in the 011 mev range with conversion efficiencies of electron into photon energy in the batse range of 30,0
scientific workflows are widely used to automate scientific data analysis and often involve processing large quantities of data on compute clusters as such their execution tends to be longrunning and resource intensive leading to significant energy consumption and carbon emissions meanwhile a wealth of carbonaware computing methods have been proposed yet little work has focused specifically on scientific workflows even though they present a substantial opportunity for carbonaware computing because they are inherently delay tolerant efficiently interruptible and highly scalable in this study we demonstrate the potential for carbonaware workflow execution for this we estimate the carbon footprint of two realworld nextflow workflows executed on cluster infrastructure we use a linear power model for energy consumption estimates and realworld average and marginal ci data for two regions we evaluate the impact of carbonaware temporal shifting pausing and resuming and resource scaling our findings highlight significant potential for reducing emissions of workflows and workflow tasks,0
we analyse a device aimed at the conversion of heat into electrical energy based on a closed cycle in which a distiller generates two solutions at different concentrations and an electrochemical cell consumes the concentration difference converting it into electrical current we first study an ideal model of such a process we show that if the device works at a single fixed pressure ie with a single effect then the efficiency of the conversion of heat into electrical power can approach the efficiency of a reversible carnot engine operating between the boiling temperature of the concentrated solution and that of the pure solvent when two heat reservoirs with a higher temperature difference are available the overall efficiency can be incremented by employing an arrangement of multiple cells working at different pressures multiple effects we find that a given efficiency can be achieved with a reduced number of effects by using solutions with a high boiling point elevation,0
the demand for computing is continuing to grow exponentially this growth will translate to exponential growth in computings energy consumption unless improvements in its energyefficiency can outpace increases in its demand yet after decades of research further improving energyefficiency is becoming increasingly challenging as it is already highly optimized as a result at some point increases in computing demand are likely to outpace increases in its energyefficiency potentially by a wide margin such exponential growth if left unchecked will position computing as a substantial contributor to global carbon emissions while prominent technology companies have recognized the problem and sought to reduce their carbon emissions they understandably focus on their successes which has the potential to inadvertently convey the false impression that this is now or will soon be a solved problem such false impressions can be counterproductive if they serve to discourage further research in this area since as we discuss eliminating computings and more generally societys carbon emissions is far from a solved problem to better understand the problems scope this paper distills the fundamental trends that determine computings carbon footprint and their implications for achieving sustainable computing,0
like esg investing climate change is an important concern for asset managers and owners and a new challenge for portfolio construction until now investors have mainly measured carbon risk using fundamental approaches such as with carbon intensity metrics nevertheless it has not been proven that asset prices are directly impacted by these fundamentalbased measures in this paper we focus on another approach which consists in measuring the sensitivity of stock prices with respect to a carbon risk factor in our opinion carbon betas are marketbased measures that are complementary to carbon intensities or fundamentalbased measures when managing investment portfolios because carbon betas may be viewed as an extension or forwardlooking measure of the current carbon footprint in particular we show how this new metric can be used to build minimum variance strategies and how they impact their portfolio construction,0
microservices are a popular architectural style adopted by the industry when it comes to deploying software that requires scalability maintainability and agile development there is an increasing demand for improving the sustainability of microservice systems in the industry this rapid review gathers 22 peerreviewed studies and synthesizes architectural tactics that improve the environmental sustainability of microservices from them we list 6 tactics that are presented in an actionable way and categorized according to their sustainability aspects and context the sustainability aspects include energy efficiency carbon efficiency and resource efficiency among which resource efficiency is the most researched one while energy efficiency and carbon efficiency are still in the early stage of study the context categorization including serverless platforms decentralized networks etc helps to identify the tactics that we can use in a specific setting additionally we present how the evidence of optimization after adopting these tactics is presented like the measurement unit and statistical methods and how experiments are generally set up so that this review is both instructive for our future study and our industrial practitioners interest we further study the insufficiencies of the current study and hope to provide insight for other researchers and the industry,0
concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint cf these protocols are manual workintensive and expensive all of the above have led to a shift towards automatic datadriven approaches to estimate the cf including machine learning ml solutions unfortunately the decisionmaking processes involved in these solutions lack transparency from the end users point of view who must blindly trust their outcomes compared to intelligible traditional manual approaches in this research manual and automatic methodologies for cf estimation were reviewed taking into account their transparency limitations this analysis led to the proposal of a new explainable ml solution for automatic cf calculations through bank transaction classification consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose for classification different ml models have been employed based on their promising performance in the literature such as support vector machine random forest and recursive neural networks the results obtained were in the 90 range for accuracy precision and recall evaluation metrics from their decision paths the proposed solution estimates the co2 emissions associated with bank transactions the explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models the explainability terms were automatically validated using a similarity metric over the descriptions of the target categories conclusively the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions,0
sustainability is no longer a matter of choice but is invariably linked to the survival of the entire ecosystem of our planet earth as robotics technology is growing at an exponential rate it is crucial to examine its implications for sustainability our focus is on social sustainability specifically analyzing the role of robotics technology in this domain by identifying six distinct ways robots influence social sustainability,0
this paper studies the longterm energy management of a microgrid coordinating hybrid hydrogenbattery energy storage we develop an approximate semiempirical hydrogen storage model to accurately capture the powerdependent efficiency of hydrogen storage we introduce a predictionfree twostage coordinated optimization framework which generates the annual stateofcharge soc reference for hydrogen storage offline during online operation it updates the soc reference online using kernel regression and makes operation decisions based on the proposed adaptive virtualqueuebased online convex optimization oco algorithm we innovatively incorporate penalty terms for longterm pattern tracking and experttracking for step size updates we provide theoretical proof to show that the proposed oco algorithm achieves a sublinear bound of dynamic regret without using prediction information numerical studies based on the elia and north china datasets show that the proposed framework significantly outperforms the existing online optimization approaches by reducing the operational costs and loss of load by around 30 and 80 respectively these benefits can be further enhanced with optimized settings for the penalty coefficient and step size of oco as well as more historical references,0
ubiquitous technology platforms have been created to track and improve health and fitness similar technologies can help individuals monitor and reduce their carbon footprints this paper proposes carbonkit a platform combining technology markets and incentives to empower and reward people for reducing their carbon footprint we argue that a goalandreward behavioral feedback loop can be combined with the big data available from tracked activities apps and social media to make carbonkit an integral part of individuals daily lives carbonkit comprises five modules that link personal carbon tracking health and fitness social media and economic incentives protocols for safeguarding security privacy and individuals control over their own data are essential to the design of the carbonkit initially carbonkit would operate on a voluntary basis but such a system can also serve as part of a mandatory regionwide initiative we use the example of the british columbia to illustrate the regulatory framework and participating stakeholders that would be required to support the carbonkit in specific jurisdictions,0
deploying large language models llms on edge devices presents significant challenges due to computational constraints memory limitations inference speed and energy consumption model quantization has emerged as a key technique to enable efficient llm inference by reducing model size and computational overhead in this study we conduct a comprehensive analysis of 28 quantized llms from the ollama library which applies by default posttraining quantization ptq and weightonly quantization techniques deployed on an edge device raspberry pi 4 with 4gb ram we evaluate energy efficiency inference performance and output accuracy across multiple quantization levels and task types models are benchmarked on five standardized datasets commonsenseqa bigbench hard truthfulqa gsm8k and humaneval and we employ a highresolution hardwarebased energy measurement tool to capture realworld power consumption our findings reveal the tradeoffs between energy efficiency inference speed and accuracy in different quantization settings highlighting configurations that optimize llm deployment for resourceconstrained environments by integrating hardwarelevel energy profiling with llm benchmarking this study provides actionable insights for sustainable ai bridging a critical gap in existing research on energyaware llm deployment,0
the carbon footprint of data centers has recently become a critical concern so far most carbonaware strategies have focused on leveraging the flexibility of scheduling decisions for batch processing by shifting the time and location of workload executions however such approaches cannot be applied to serviceoriented cloud applications since they have to be reachable at every point in time and often at low latencies we propose a carbonaware approach for operating microservices under hourly carbon budgets by choosing the most appropriate version and horizontal scaleout for each microservice our strategy maximizes user experience and revenue while staying within budget constraints experiments across various application configurations and carbon budgets demonstrate that the approach adapts properly to changing workloads and carbon intensities,0
energyefficient software helps improve mobile device experiences and reduce the carbon footprint of data centers however energy goals are often deprioritized in order to meet other requirements we take inspiration from recent work exploring the use of large language models llms for different software engineering activities we propose a novel application of llms as code optimizers for energy efficiency we describe and evaluate a prototype finding that over 6 small programs our system can improve energy efficiency in 3 of them up to 2x better than compiler optimizations alone from our experience we identify some of the challenges of energyefficient llm code optimization and propose a research agenda,0
simultaneous wireless information and power transfer swipt provides a promising solution for enabling perpetual wireless networks as energy efficiency ee is an im portant evaluation of system performance this thesis studies energyefficient resource allocation algorithm designs in swipt systems we first investigate the tradeoff between the ee for information transmission the ee for power transfer and the total transmit power in a basic swipt system with separated receivers a multiobjective optimization problem is formulated under the constraint of maximum transmit power we propose an algorithm which achieves flexible resource allocation for energy efficiencies maxi mization and transmit power minimization the tradeoff region of the system design objectives is shown in simulation results further we consider secure communication in a swipt system with power splitting receivers artificial noise is injected to the com munication channel to combat the eavesdropping capability of potential eavesdroppers a powerefficient resource allocation algorithm is developed when multiple legitimate information receivers and multiantenna potential eavesdroppers coexist in the system simulation results demonstrate a significant performance gain by the proposed optimal algorithm compared to suboptimal baseline schemes,0
in the past decade global warming made several headlines and turned the attention of the whole world to it carbon footprint is the main factor that drives greenhouse emissions up and results in the temperature increase of the planet with dire consequences while the attention of the public is turned to reducing carbon emissions by transportation food consumption and household activities we ignore the contribution of co2eq emissions produced by online activities in the current information era we spend a big amount of our days browsing online this activity consumes electricity which in turn produces co2eq while website browsing contributes to the production of greenhouse gas emissions the impact of the internet on the environment is further exacerbated by the webtracking practice indeed most webpages are heavily loaded by tracking content used mostly for advertising data analytics and usability improvements this extra content implies big data transmissions which results in higher electricity consumption and thus higher greenhouse gas emissions in this work we focus on the overhead caused by web tracking and analyse both its network and carbon footprint by leveraging the browsing telemetry of 100k users and the results of a crawling experiment of 27m websites we find that web tracking increases data transmissions upwards of 21 which in turn implies the additional emission of around 11 mt of greenhouse gases in the atmosphere every year we find such contribution to be far from negligible and comparable to many activities of modern life such as meat production transportation and even cryptocurrency mining our study also highlights that there exist significant inequalities when considering the footprint of different countries website categories and tracking organizations with a few actors contributing to a much greater extent than the remaining ones,0
as organizations face increasing pressure to understand their corporate and products carbon footprints artificial intelligence aiassisted calculation systems for footprinting are proliferating but with widely varying levels of rigor and transparency standards and guidance have not kept pace with the technology evaluation datasets are nascent and statistical approaches to uncertainty analysis are not yet practical to apply to scaled systems we present a set of criteria to validate aiassisted systems that calculate greenhouse gas ghg emissions for products and materials we implement a threestep approach 1 identification of needs and constraints 2 draft criteria development and 3 refinements through pilots the process identifies three use cases of ai applications case 1 focuses on aiassisted mapping to existing datasets for corporate ghg accounting and product hotspotting automating repetitive manual tasks while maintaining mapping quality case 2 addresses ai systems that generate complete product models for corporate decisionmaking which require comprehensive validation of both component tasks and endtoend performance we discuss the outlook for case 3 applications systems that generate standardscompliant models we find that credible ai systems can be built and that they should be validated using systemlevel evaluations rather than lineitem review with metrics such as benchmark performance indications of data quality and uncertainty and transparent documentation this approach may be used as a foundation for practitioners auditors and standards bodies to evaluate aiassisted environmental assessment tools by establishing evaluation criteria that balance scalability with credibility requirements our approach contributes to the fields efforts to develop appropriate standards for aiassisted carbon footprinting systems,0
solar sensorbased monitoring systems have become a crucial agricultural innovation advancing farm management and animal welfare through integrating sensor technology internetofthings and edge and cloud computing however the resilience of these systems to cyberattacks and their adaptability to dynamic and constrained energy supplies remain largely unexplored to address these challenges we propose a sustainable smart farm network designed to maintain highquality animal monitoring under various cyber and adversarial threats as well as fluctuating energy conditions our approach utilizes deep reinforcement learning drl to devise optimal policies that maximize both monitoring effectiveness and energy efficiency to overcome drls inherent challenge of slow convergence we integrate transfer learning tl and decision theory dt to accelerate the learning process by incorporating dtguided strategies we optimize monitoring quality and energy sustainability significantly reducing training time while achieving comparable performance rewards our experimental results prove that dtguided drl outperforms tlenhanced drl models improving system performance and reducing training runtime by 475,0
we studied the central regions of the galactic centre to determine if the circumnuclear disk cnd acts as an absorber or a barrier for the central xrays diffuse emission after reprocessing 46ms of chandra observations we were able to detect for the first time a depression in the xray luminosity of the diffuse emission whose size and location correspond to those of the cnd we extracted the xray spectra for various regions inside the cnd footprint as well as for the region where the footprint is observed and for a region located outside the footprint we simultaneously fitted these spectra as an optically thin plasma whose absorption by the interstellar medium and by the local plasma were fitted independently using the mcmc method the hydrogen column density of the ism is 75x1022 cm2 the xray diffuse emission inside the cnd footprint is formed by a 2t plasma of 1 and 4kev with slightly supersolar abundances except for the iron and carbon which are subsolar the plasma from the cnd in turn is better described by a 1t model with abundances and local hydrogen column density which are very different to those of the innermost regions the large iron abundance in this region confirms that the cnd is dominated by the shockheated ejecta of the sgr a east supernova remnant we deduced that the cnd rather acts as a barrier for the galactic centre plasma and that the plasma located outside the cnd may correspond to the collimated outflow possibly created by sgr a or the interaction between the wind of massive stars and the minispiral material,0
the rapid advancement of ai particularly large language models llms has raised significant concerns about the energy use and carbon emissions associated with model training and inference however existing tools for measuring and reporting such impacts are often fragmented lacking systematic metric integration and offering limited support for correlation analysis among them this paper presents aimeter a comprehensive software toolkit for the measurement analysis and visualization of energy use power draw hardware performance and carbon emissions across ai workloads by seamlessly integrating with existing ai frameworks aimeter offers standardized reports and exports finegrained timeseries data to support benchmarking and reproducibility in a lightweight manner it further enables indepth correlation analysis between hardware metrics and model performance and thus facilitates bottleneck identification and performance enhancement by addressing critical limitations in existing tools aimeter encourages the research community to weigh environmental impact alongside raw performance of ai workloads and advances the shift toward more sustainable green ai practices the code is available at,0
this is the 1st part of the dissertation for my master degree and compares the power consumption using the default floating point 32bit and nvidia mixed precision 16bit and 32bit while training a classification ml model a custom pc with specific hardware was built to perform the experiments and different ml hyperparameters such as batch size neurons and epochs were chosen to build deep neural networks dnn additionally various software was used during the experiments to collect the power consumption data in watts from the graphics processing unit gpu central processing unit cpu random access memory ram and manually from a wattmeter connected to the wall a benchmarking test with default hyper parameter values for the dnn was used as a reference while the experiments used a combination of different settings the results were recorded in excel and descriptive statistics were chosen to calculate the mean between the groups and compare them using graphs and tables the outcome was positive when using mixed precision combined with specific hyperparameters compared to the benchmarking the optimisation for the classification reduced the power consumption between 7 and 11 watts similarly the carbon footprint is reduced because the calculation uses the same power consumption data still a consideration is required when configuring hyperparameters because it can negatively affect hardware performance however this research required inferential statistics specifically anova and ttest to compare the relationship between the means furthermore tests indicated no statistical significance of the relationship between the benchmarking and experiments however a more extensive implementation with a cluster of gpus can increase the sample size significantly as it is an essential factor and can change the outcome of the statistical analysis,0
the sustainable internet of things iot is becoming a promising solution for the green living and smart industries in this article we investigate the practical issues in the radio energy harvesting and data communication systems through extensive field experiments a number of important characteristics of energy harvesting circuits and communication modules have been studied including the nonlinear energy consumption of the communication system relative to the transmission power the wakeup time associated with the payload and the varying system power during consecutive packet transmissions in order to improve the efficiency of energy harvest and energy utilization we propose a new model to accurately describe the energy harvesting process and the power consumption for sustainable iot devices experiments are performed using commercial iot devices and rf energy harvesters to verify the accuracy of the proposed model the experiment results show that the new model matches the performance of sustainable iot devices very well in the real scenario,0
as recommender systems become increasingly prevalent the environmental impact and energy efficiency of training largescale models have come under scrutiny this paper investigates the potential for energyefficient algorithm performance by optimizing dataset sizes through downsampling techniques in the context of green recommender systems we conducted experiments on the movielens 100k 1m 10m and amazon toys and games datasets analyzing the performance of various recommender algorithms under different portions of dataset size our results indicate that while more training data generally leads to higher algorithm performance certain algorithms such as funksvd and biasedmf particularly with unbalanced and sparse datasets like amazon toys and games maintain highquality recommendations with up to a 50 reduction in training data achieving ndcg10 scores within approximately 13 of full dataset performance these findings suggest that strategic dataset reduction can decrease computational and environmental costs without substantially compromising recommendation quality this study advances sustainable and green recommender systems by providing insights for reducing energy consumption while maintaining effectiveness,0
barrosos seminal contributions in energyproportional warehousescale computing launched an era where modern datacenters have become more energy efficient and cost effective than ever before at the same time modern ai applications have driven everincreasing demands in computing highlighting the importance of optimizing efficiency across the entire deep learning model development cycle this paper characterizes the carbon impact of ai including both operational carbon emissions from training and inference as well as embodied carbon emissions from datacenter construction and hardware manufacturing we highlight key efficiency optimization opportunities for cuttingedge ai technologies from deep learning recommendation models to multimodal generative ai tasks to scale ai sustainably we must also go beyond efficiency and optimize across the life cycle of computing infrastructures from hardware manufacturing to datacenter operations and endoflife processing for the hardware,0
the traditional framework of quantum metrology commonly assumes unlimited access to resources overlooking resource constraints in realistic scenarios as such the optimal strategies therein can be infeasible in practice here we investigate quantum metrology where the total energy consumption of the probe state preparation intermediate control operations and the final measurement is subject to a constraint we establish a comprehensive theoretical framework for characterizing energyconstrained multistep quantum processes based on which we develop a general optimization method for energyconstrained quantum metrology that determines both the optimal precision and the corresponding strategy using the method we determine the ultimate precision limit of energyconstrained phase estimation and identify a novel advantage of quantum superpositions of causal orders in enhancing the energy efficiency of adaptive quantum estimation,0
fullspectrum ranging from sub 6 ghz to thz and visible light will be exploited in 6g in order to reach unprecedented keyperformanceindicators kpis however extraordinary amount of energy will be consumed by network infrastructure while functions of massively deployed internet of everything ioe devices are limited by embedded batteries therefore energy selfsustainable 6g is proposed in this article first of all it may achieve networkwide energy efficiency by exploiting cellfree and airborne access networks as well as by implementing intelligent holographic environments secondly by exploiting radiofrequencyvisible light signals for providing ondemand wireless power transfer wpt and for enabling passive backscatter communication zeroenergy devices may become a reality furthermore ioe devices actively adapt their transceivers for better performance to a dynamic environment this article aims to provide a first glance at primary designing principles of energy selfsustainable 6g,0
we investigate the dynamical behavior of strange quark matter sqm objects such as stars and planets when subjected to radial oscillations induced by tidal interactions in stellar systems our study demonstrates that sqm objects can efficiently convert mechanical energy into hadronic energy due to the critical mass density at their surfaces of 471014 gcm3 below which sqm becomes unstable and decays into photons hadrons and leptons we show that even smallamplitude radial oscillations with a radius change of as little as 01 can result in significant excitation energies near the surface of sqm stars this excitation energy is rapidly converted into electromagnetic energy over short timescales approximately 1 ms potentially leading to observable astrophysical phenomena higher amplitude oscillations may cause fragmentation or dissolution of sqm stars which has important implications for the evolution of binary systems containing sqm objects and the emission of gravitational waves,0
with increasing concerns over climate change scientists must imperatively acknowledge their share in co2 emissions considering the large emissions associated with scientific traveling especially international conferences initiatives to mitigate such impact are blooming with the covid19 pandemic shattering our notion of privateprofessional interactions the moment should be seized to reinvent science conferences and collaborations with a model respectful of the environment yet despite efforts to reduce the footprint of conferences there is a lack of a robust approach based on reliable numbers emissions carbon offsettingremovals etc to accompany this shift of paradigm here considering a representative scientific society the international adsorption society we report on a case study of the problem making conferences carbon neutral while respecting the needs of scientists we first provide a quantitative analysis of the co2 emissions for the ias conference in 2022 related to accommodation catering flights etc second we conduct two surveys probing our community view on the carbon footprint of our activities these surveys mirror each other and were distributed two years before and in the aftermath of our triennial conference also corresponding to prepost covid times by combining the different parts we propose ambitious recommendations to shape the future of conferences,0
future sixth generation 6g wireless communication networks face the need to similarly meet unprecedented quality of service qos demands while also providing a larger energy efficiency ee to minimize their carbon footprint moreover due to the diverseness of network participants mixed criticality qos levels are assigned to the users of such networks in this work with a focus on a cloudradio access network cran the fulfillment of desired qos and minimized transmit power use is optimized jointly within a ratesplitting paradigm thereby the optimization problem is nonconvex hence a lowcomplexity algorithm is proposed based on fractional programming numerical results validate that there is a tradeoff between the qos fulfillment and power minimization moreover the energy efficiency of the proposed ratesplitting algorithm is larger than in comparative schemes especially with mixed criticality,0
this report documents the process that led to the nsf workshop on sustainable computing for sustainability held in april 2024 at nsf in alexandria va and reports on its findings the workshops primary goals were to i advance the development of research initiatives along the themes of both sustainable computing and computing for sustainability while also ii helping develop and sustain the interdisciplinary teams those initiatives would need the workshops findings are in the form of recommendations grouped in three categories general recommendations that cut across both themes of sustainable computing and computing for sustainability and recommendations that are specific to sustainable computing and computing for sustainability respectively,0
carbon emissions significantly contribute to climate change and carbon credits have emerged as a key tool for mitigating environmental damage and helping organizations manage their carbon footprint despite their growing importance across sectors fully leveraging carbon credits remains challenging this study explores engineering practices and fintech solutions to enhance carbon emission management we first review the negative impacts of carbon emission nondisclosure revealing its adverse effects on financial stability and market value organizations are encouraged to actively manage emissions and disclose relevant data to mitigate risks next we analyze factors influencing carbon prices and review advanced prediction algorithms that optimize carbon credit purchasing strategies reducing costs and improving efficiency additionally we examine corporate carbon emission prediction models which offer accurate performance assessments and aid in planning future carbon credit needs by integrating carbon price and emission predictions we propose research directions including corporate carbon management cost forecasting this study provides a foundation for future quantitative research on the financial and market impacts of carbon management practices and is the first systematic review focusing on computing solutions and engineering practices for carbon credits,0
artificial intelligence ai increasingly influences critical decisionmaking across sectors federated learning fl as a privacypreserving collaborative ai paradigm not only enhances data protection but also holds significant promise for intelligent network management including distributed monitoring adaptive control and edge intelligence although the trustworthiness of fl systems has received growing attention the sustainability dimension remains insufficiently explored despite its importance for scalable realworld deployment to address this gap this work introduces sustainability as a distinct pillar within a comprehensive trustworthy fl taxonomy consistent with aihleg guidelines this pillar includes three key aspects hardware efficiency federation complexity and the carbon intensity of energy sources experiments using the federatedscope framework under diverse scenarios including varying participants system complexity hardware and energy configurations validate the practicality of the approach results show that incorporating sustainability into fl evaluation supports environmentally responsible deployment enabling more efficient adaptive and trustworthy network services and management ai models,0
an increasing amount of data is being injected into the network from iot internet of things applications many of these applications developed to improve societys quality of life are latencycritical and inject large amounts of data into the network these requirements of iot applications trigger the emergence of edge computing paradigm currently data centers are responsible for a global energy use between 2 and 3 however this trend is difficult to maintain as bringing computing infrastructures closer to the edge of the network comes with its own set of challenges for energy efficiency in this paper we propose our approach for the sustainability of future computing infrastructures to provide i an energyefficient and economically viable deployment ii a faulttolerant automated operation and iii a collaborative resource management to improve resource efficiency we identify the main limitations of applying cloudbased approaches close to the data sources and present the research challenges to edge sustainability arising from these constraints we propose twophase immersion cooling formal modeling machine learning and energycentric federated management as edgeenabling technologies we present our early results towards the sustainability of an edge infrastructure to demonstrate the benefits of our approach for future computing environments and deployments,0
we propose a novel energyaware federated learning flbased system namely susfl for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors this system equips animals such as cattle with solar sensors with computational capabilities including raspberry pis to train a local deeplearning model on health data these sensors periodically update long range lora gateways forming a wireless sensor network wsn to detect diseases like mastitis our proposed susfl system incorporates mechanism design a game theory concept for intelligent client selection to optimize monitoring quality while minimizing energy use this strategy ensures the systems sustainability and resilience against adversarial attacks including data poisoning and privacy threats that could disrupt fl operations through extensive comparative analysis using realtime datasets we demonstrate that our flbased monitoring system significantly outperforms existing methods in prediction accuracy operational efficiency system reliability ie mean time between failures or mtbf and social welfare maximization by the mechanism designer our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms the experimental results show that susfl significantly improves system performance including a 10 reduction in energy consumption a 15 increase in social welfare and a 34 rise in mean time between failures mtbf alongside a marginal increase in the global models prediction accuracy,0
the internet of things iot can support the evolution towards a digital and green future however the introduction of the technology clearly has in itself a direct adverse ecological impact this paper assesses this impact at both the iotnode and at the network side for the nodes we show that the electronics production of devices comes with a carbon footprint that can be much higher than during operation phase we highlight that the inclusion of iot support in existing cellular networks comes with a significant ecological penalty raising overall energy consumption by more than 15 these results call for novel design approaches for the nodes and for early consideration of the support for iot in future networks raising the vehicle or bandit question on the nature of iot in the broader sense of sustainability we illustrate the need for multidisciplinary cooperation to steer applications in desirable directions,0
in this essay we will defend the thesis that the multicomputational paradigm is a natural way of thinking about the fourth industrial revolution this will be done considering the geometry that emerges as the continuum limit of multiway systems,0
motive and electrical energy has played a crucial role in human civilization since ancient times motive energy played a primary role in agricultural and industrial production as well as transportation at that time motive energy was provided by work of humans and draft animals later work of water and wind power was harnessed during the 19textth century steam power became the main source of motive energy in usa and britain modern transportation and industry depend on the work of heat engines that use fossil fuel a brief history of different sources of energy is presented in this work the energy consumptions in preindustrial and industrial societies are calculated the lost opportunities for the second industrial revolution such as fast breeder reactors and thermonuclear power stations are discussed the case that the solar power will become the main source of energy by the second half of this century is presented it is calculated that the solar power has the potential to bring about the new industrial revolution based on material and energy resources available in the solar system it is demonstrated that the solar system civilization supporting a population of 10 quadrillion with a high standard of living is possible,0
energy production plays a primary role in industrial capacity and thus material standard of living of any civilization the industrial revolution was engendered by a vast growth of motive energy production solar power revolution has the potential of increasing global energy production by a factor of 160 and engendering a new technological revolution in industry food production and transportation electric energy can be used for vast expansion of industry electric energy can be used to extract vast amount of hydrocarbon motor fuel and chemicals from unconventional oil resources and coal it can also be used to produce liquid hydrocarbon fuel chemicals and animal feed from water and carbon dioxide electric energy can be used to connect the world by a network of rapid transit such as magnetic levitation trains maglev the solar power revolution will enable earth to sustain a population of 50 billion at material living standards much higher than corresponding standards in usa 2020 global civilization which harvests the maximum possible fraction of solar energy falling on earth is called kardashev 1 civilization it is also the stepping stone toward the final frontier of humankind kardashev 2 civilization which has colonized the solar system,0
we introduce the economic productivity of energy epe gdp generated per unit of energy consumed as a quantitative lens to assess the sustainability of the artificial intelligence ai revolution historical evidence shows that the first industrial revolution prescientific in the sense that technological adoption preceded scientific understanding initially disrupted this ratio epe collapsed as profits outpaced efficiency with poorly integrated technologies and recovered only with the rise of scientific knowledge and societal adaptation later industrial revolutions such as electrification and microelectronics grounded in established scientific theory did not exhibit comparable declines todays ai revolution highly profitable yet energyintensive remains prescientific and may follow a similar trajectory in epe we combine this conceptual discussion with crosscountry epe data spanning the last three decades we find that the advanced economies exhibit a consistent linear growth in epe those countries are the ones that contribute most to global gdp production and energy consumption and are expected to be the most affected by the ai transition therefore we advocate for regular monitoring of epe transparent reporting of airelated energy use and productivitylinked incentives can expose hidden energy costs and prevent efficiencyblind economic expansion embedding epe within sustainability frameworks would help align technological innovation with energy productivity a critical condition for sustainable growth,0
the purpose of this note is to explain what is analytical history a modular and testable analysis of historical events introduced in a book published in 2002 roehner and syme 2002 broadly speaking it is a comparative methodology for the analysis of historical events comparison is the keystone and hallmark of science for instance the extrasolar planets are crucial for understanding our own solar system until their discovery astronomers could observe only one instance single instances can be described but they cannot be understood in a testable way in other words if one accepts that as many historians say historical events are unique then no testable understanding can be developed,0
the work is divided into three sections the first one describes the historical evolution of the main arguments presented about the plurality of inhabited worlds from the presocratics to the birth of modern science the second section analyzes the race to define the search for life beyond earth as a scientific activity under a specific name finally the third part presents a brief description of the social history of science that allowed the early development of astrobiology in iberoamerica,0
lifestyle politics emerge when activities that have no substantive relevance to ideology become politically aligned and polarized homophily and social influence are able generate these fault lines on their own however social identities from demographics may serve as coordinating mechanisms through which lifestyle politics are mobilized are spread using a dataset of 137661886 observations from 299327 facebook interests aggregated across users of different racialethnic education age gender and income demographics we find that the most extreme instances of lifestyle politics are those which are highly confounded by demographics such as raceethnicity eg black artists and performers after adjusting political alignment for demographic effects lifestyle politics decreased by 2736 toward the political center and demographically confounded interests were no longer among the most polarized interests instead after demographic deconfounding we found that the most liberal interests included electric cars planned parenthood and liberal satire while the most conservative interests included the republican party and conservative commentators we validate our measures of political alignment and lifestyle politics using the general social survey and find similar demographic entanglements with lifestyle politics existed before social media such as facebook were ubiquitous giving us strong confidence that our results are not due to echo chambers or filter bubbles likewise since demographic characteristics exist prior to ideological values we argue that the demographic confounding we observe is causally responsible for the extreme instances of lifestyle politics that we find among the aggregated interests we conclude our paper by relating our results to simpsons paradox cultural omnivorousness and network autocorrelation,0
seti is not a usual point of departure for environmental humanities however this paper argues that theories originating in this field have direct implications for how we think about viable inhabitation of the earth to demonstrate setis impact on environmental humanities this paper introduces fermi paradox as a speculative tool to probe possible trajectories of planetary history and especially the sustainability solution proposed by jacob haqqmisra and seth baum this solution suggests that sustainable coupling between extraterrestrial intelligences and their planetary environments is the major factor in the possibility of their successful detection by remote observation by positing that exponential growth is not a sustainable development pattern this solution rules out spacefaring civilizations colonizing solar systems or galaxies this paper elaborates on haqqmisras and baums arguments and discusses speculative implications of the sustainability solution thus rethinking three concepts in environmental humanities technosphere planetary history and sustainability the paper advocates that 1 technosphere is a transitory layer that shall fold back into biosphere 2 planetary history must be understood in a generic perspective that abstracts from terrestrial particularities and 3 sustainability is not sufficient vector of viable human inhabitation of the earth suggesting instead habitability and genesity as better candidates,0
the sustainability of the global academic ecosystem relies on researcher demographics and gender balance yet assessing these dynamics in a timely manner for policy is challenging here we propose a researcher population pyramids framework for tracking global demographic and gender trajectories using publication data this framework provides a timely snapshot of historical and present demographics and gender balance revealing three contrasting research systems emerging systems eg arab countries exhibit high researcher inflows with widening gender gaps in cumulative productivity mature systems eg the united states show modest inflows with narrowing gender gaps and rigid systems eg japan lag in both furthermore by simulating future scenarios the framework makes potential trajectories visible if 2023 demographic patterns persist arab countries systems could resemble mature or even rigid ones by 2050 our framework provides a robust diagnostic tool for policymakers worldwide to foster sustainable talent pipelines and gender equality in academia,0
users of social media sites like facebook and twitter rely on crowdsourced content recommendation systems eg trending topics to retrieve important and useful information contents selected for recommendation indirectly give the initial users who promoted by liking or posting the content an opportunity to propagate their messages to a wider audience hence it is important to understand the demographics of people who make a content worthy of recommendation and explore whether they are representative of the media sites overall population in this work using extensive data collected from twitter we make the first attempt to quantify and explore the demographic biases in the crowdsourced recommendations our analysis focusing on the selection of trending topics finds that a large fraction of trends are promoted by crowds whose demographics are significantly different from the overall twitter population more worryingly we find that certain demographic groups are systematically underrepresented among the promoters of the trending topics to make the demographic biases in twitter trends more transparent we developed and deployed a webbased service whomakestrends at twitterappmpiswsorgwhomakestrends,0
is collective intelligence just individual intelligence writ large or are there fundamental differences this position paper argues that a cognitive history methodology can shed light into the nature of collective intelligence and its differences from individual intelligence to advance this proposed area of research a small case study on the structure of argument and proof is presented quantitative metrics from network science are used to compare the artifacts of deduction from two sources the first is the work of archimedes of syracuse putatively an individual and of other ancient greek mathematicians the second is work of the polymath project a massively collaborative mathematics project that used blog posts and comments to prove new results in combinatorics,0
this is an editorial report on the outcomes of an international conference sponsored by a grant from the national science foundation nsf reese1205273 to the school of education at boston university and the center for philosophy and history of science at boston university for a conference titled how can the history and philosophy of science contribute to contemporary us science teaching the presentations of the conference speakers and the reports of the working groups are reviewed multiple themes emerged for k16 education from the perspective of the history and philosophy of science key ones were that students need to understand that central to science is argumentation criticism and analysis students should be educated to appreciate science as part of our culture students should be educated to be science literate what is meant by the nature of science as discussed in much of the science education literature must be broadened to accommodate a science literacy that includes preparation for socioscientific issues teaching for science literacy requires the development of new assessment tools and it is difficult to change what science teachers do in their classrooms the principal conclusions drawn by the editors are that to prepare students to be citizens in a participatory democracy science education must be embedded in a liberal arts education science teachers alone cannot be expected to prepare students to be scientifically literate and to educate students for scientific literacy will require a new curriculum that is coordinated across the humanities historysocial studies and science classrooms,0
novae and supernovae are rare astronomical events that would have had an influence on the skywatching peoples who witnessed them although several bright novaesupernovae have been visible during recorded human history there are many proposed but no confirmed accounts of supernovae in oral traditions or material culture criteria are established for confirming novaesupernovae in oral and material culture and claims from around the world are discussed to determine if they meet these criteria australian aboriginal traditions are explored for possible descriptions of novaesupernovae although representations of supernovae may exist in indigenous traditions and an account of a nova in aboriginal traditions has been confirmed there are currently no confirmed accounts in indigenous oral or material traditions,0
voter demographics and socioeconomic factors like age sex ethnicity education level income and other measurable factors like behaviour in previous elections or referenda are of key importance in modelling opinion formation dynamics here we revisit the kinetic opinion formation model from dring and wright 2022 and compare in more detail the influence of different choices of characteristic demographic factors and initial conditions the model is based on the kinetic opinion formation model by toscani 2006 and the leaderfollower model of dring et al 2009 which leads to a system of fokkerplancktype partial differential equations in the runup to the 2024 general election in the united kingdom we consider in our numerical experiments the situation in the socalled red wall in north and midlands of england and compare our simulation results to other election forecasts,0
the australian national curriculum promotes indigenous culture in school education programs to foster a broader appreciation of cultural astronomy to utilise the unique astronomical heritage of the site and to develop an educational program within the framework of the national curriculum sydney observatory launched dreamtime astronomy a program incorporating australian indigenous culture astronomy and sydneys astronomical history and heritage this paper reviews the development and implementation of this program and discusses modifications following an evaluation by schools,0
with cyrano voltaire and verne france provided important milestones in the history of early science fiction however even if the genre was not very common a few centuries ago there were numerous additional contributions by frenchspeaking writers in this paper we review two cases of interplanetary novels written in the second half of the eighteenth century and sharing a rare particularity their authors were female voyages de milord ceton was imagined by marieanne de roumierrobert whereas cornelie wouters de wasse conceived le char volant while their personal lives were very different and their writing style too both authors share in these novels a common philosophy in which equality between ranks but also between genders takes an important place their works thus clearly fit into the context of the enlightenment,0
this study examines the complex interplay of gender and other demographics on continuation rates in high school physics using a diverse dataset that combines demographics from the canadian census and eleven years of gendered enrolment data from the ontario ministry of education we track student cohorts as they transition from mandatory science to elective physics courses we then employ hierarchical linear modelling to quantify the interaction effects between gender and other demographics providing a detailed perspective on the on continuation in physics our results indicate the racial demographics of a schools neighbourhood have a limited impact on continuation once controlling for other factors such as socioeconomic status though neighbourhoods with a higher black population were a notable exception consistently exhibiting significantly lower continuation rates for both male and female students a potential role model effect related to parental education was also found as the proportion of parents with stem degrees correlates positively with increased continuation whereas an increase in nonstem degrees corresponds with a reduced scr the most pronounced effects are schoollevel factors continuation rates in physics are very strongly correlated with continuation in chemistry or calculus effects which are much stronger for male than female students conversely continuation in biology positively correlates with the continuation of female students in physics with little to no effect found for male students nevertheless the effect sizes observed for chemistry and calculus markedly outweigh that for biology this is further evidence that considering stem as a homogeneous subject when examining gender disparities is misguided these insights can guide future education policies and initiatives to increase continuation rates and foster greater gender equity in physics education,0
science is about facts and truth yet sometimes the truth and facts are not obvious for example in the field of mri magnetic resonance imaging there has been a longlasting debate about who were the major contributors in its development particularly there was a strong dispute between the followers of two scientists r damadian and p lauterbur in this review we carefully trace the major developments in applying nmr for cancer detection starting almost 50 years ago the research records show that the truth was beyond the claims of either research camps the development of nmr for cancer detection involved multiple research groups who made critical contributions at different junctures,0
based on palaeoenvironmental historical and archaeological data the paper proposes possible climatic impacts on the history of the avar khaganate which comprised the carpathian basin between the late 6th and the early 9th century ad while the establishment of the avars in east central europe took place within a period characterised by cold and dry climatic conditions recently identified as late antique little ice age more stable climatic parameters may have favoured the stabilisation of avar rule after a crisis in the aftermath of 626 ad data indicates growth of settlement and agricultural activity up to the mid8th century these developments did not necessarily strengthen central power but may have contributed to a greater autonomy of various groups on the basis of increased resources the khaganate quickly disintegrated faced by the carolingian advance of the 790s the last decades of documented avar presence were again accompanied by environmental vicissitudes,0
social media offers a unique lens to observe largescale spatialtemporal patterns of users reactions toward critical events however social media use varies across demographics with younger users being more prevalent compared to older populations this difference introduces biases in data representativeness and analysis based on social media without proper adjustment will lead to overlooking the voices of digitally marginalized communities and inaccurate estimations this study explores solutions to pinpoint and alleviate the demographic biases in social media analysis through a case study estimating the public sentiment about covid19 using twitter data we analyzed the pandemicrelated twitter data in the us during 20202021 to 1 elucidate the uneven social media usage among demographic groups and the disparities of their sentiments toward covid19 2 construct an adjusted public sentiment measurement based on social media the sentiment adjusted by demographics sad index to evaluate the spatiotemporal varying public sentiment toward covid19 the results show higher proportions of female and adolescent twitter users expressing negative emotions to covid19 the sad index unveils that the public sentiment toward covid19 was most negative in january and february 2020 and most positive in april 2020 vermont and wyoming were the most positive and negative states toward covid19,0
the application of leslie matrices in demographic research is considered in this paper the leslie matrix is first proposed in the 1940s and gained popularity in the mid1960s becoming fundamental tool for predicting population dynamics the leslie matrix allows to categorize individuals based on various attributes and calculate the expected population sizes for various demographic categories in subsequent time intervals the universality of the leslie matrix extends to diverse life cycles in plants and animals making it ubiquitous tool in nonhuman species in the paper is presented detailed application of leslie matrices to the problem of the two countries demonstrating their practical value in solving real demographic problems in conclusion the leslie matrix remains a cornerstone of demographic analysis reflecting the complexity of population dynamics and providing a robust framework for understanding the intricate interplay of factors shaping human society its enduring relevance and adaptability make it an essential component in the toolkit of demographers and ecologists,0
growth dynamic of real networks because of emerging complexities is an open and interesting question indeed it is not realistic to ignore history impact on the current events the mystery behind that complexity could be in the role of history in some how to regard this point the average effect of history has been included by a kernel function in differential equation of barabasi albert ba model this approach leads to a fractional order ba differential equation as a generalization of ba model as opposed to unlimited growth for degree of nodes our results show that over time the memory impact will cause a decay for degrees this gives a higher chance to younger members for turning to a hub in fact in a real network there are two competitive processes on one hand based on preferential attachment mechanism nodes with higher degree are more likely to absorb links on the other hand node history through aging process prevents new connections our findings from simulating a network grown by considering these effects also from studying a real network of collaboration between hollywood movie actors conforms the results and significant effects of history and time on dynamic,0
the structure of an evolving network contains information about its past extracting this information efficiently however is in general a difficult challenge we formulate a fast and efficient method to estimate the most likely history of growing trees based on exact results on root finding we show that our lineartime algorithm produces the exact stepwise most probable history in a broad class of tree growth models our formulation is able to treat very large trees and therefore allows us to make reliable numerical observations regarding the possibility of root inference and history reconstruction in growing trees we obtain the general formula langle ln mathcaln rangle cong n ln n cn for the sizedependence of the mean logarithmic number of possible histories of a given tree a quantity that largely determines the reconstructability of tree histories we also reveal an uncertainty principle a relationship between the inferrability of the root and that of the complete history indicating that there is a tradeoff between the two tasks the root and the complete history cannot both be inferred with high accuracy at the same time,0
the possibility of modeling and therefore predicting the trend of demographic mortality is of great scientific and social interest the article presents and discusses the hypothesis that the demographic distribution of mortality in advanced ages converges asymptotically to an ssystem distribution as lifespan increases the statistical distribution of the ssystem was introduced by the author in a 2022 paper and was derived by applying the methods of fermi statistics to a cellular automaton acting as an arbitrary oscillator this distribution is here recalled and formalized analytically and its characteristics are described the conjecture is based on two case studies mortality in the united states from 1900 to 2017 and mortality in italy from 1974 to 2019 the conjecture applied to both case studies appears reasonable tables and comparison figures are provided to support this finally an attempt to predict demographic mortality behavior and limitations for the years to come is provided,0
in this paper we present a novel approach to predict crime in a geographic space from multiple data sources in particular mobile phone and demographic data the main contribution of the proposed approach lies in using aggregated and anonymized human behavioral data derived from mobile network activity to tackle the crime prediction problem while previous research efforts have used either background historical knowledge or offenders profiling our findings support the hypothesis that aggregated human behavioral data captured from the mobile network infrastructure in combination with basic demographic information can be used to predict crime in our experimental results with real crime data from london we obtain an accuracy of almost 70 when predicting whether a specific area in the city will be a crime hotspot or not moreover we provide a discussion of the implications of our findings for datadriven crime analysis,0
this article considers the current problem of investigation and development of the method of webmembers sociodemographic characteristics profile validation based on analysis of sociodemographic characteristics the topicality of the paper is determined by the necessity to identify the webcommunity member by means of computerlinguistic analysis of their information track all information about webcommunity members which posted on the internet the formal model of basic sociodemographic characteristics of virtual communities member is formed the algorithm of these characteristics verification is developed,0
sicily has played an important role in the development of the new research area named econophysics in fact some key ideas supporting this new hybrid discipline were originally formulated in a pioneering work of the sicilian born physicist ettore majorana the article he wrote was entitled the value of statistical laws in physics and social sciences i will discuss its origin and history that has been recently discovered in the study of stefano roncoroni this recent study documents the true reasons and motivations that triggered the pioneering work of majorana it also shows that the description of this work provided by edoardo amaldi was shallow and misleading in the second part of the talk i will recollect the first years of development of econophysics and in particular the role of the international workshop on econophysics and statistical finance held in palermo on 2830 september 1998 and the setting in 1999 of the observatory of complex systems the research group on econophysics of palermo university and istituto nazionale di fisica della materia,0
world population and the number of cultural artifacts are growing exponentially or faster while cultural interaction approaches the fidelity of a global nervous system every day hundreds of millions of images are loaded into social networks by users all over the world as this myriad of new artifacts veils the view into the past like city lights covering the night sky it is easy to forget that there is more than one starry night the painting by van gogh like in ecology where saving rare species may help us in treating disease art and architectural history can reveal insights into the past which may hold keys to our own future with humanism under threat facing the challenge of understanding the structure and dynamics of art and culture both qualitatively and quantitatively is more crucial now than it ever was the purpose of this article is to provide perspective in the aim of figuring out the process of art history not art history as a discipline but the actual history of all made things in the spirit of george kubler and marcel duchamp in other words this article deals with the grand challenge of developing a systematic science of art and culture no matter what and no matter how,0
we conduct a detailed investigation of correlations between realtime expressions of individuals made across the united states and a wide range of emotional geographic demographic and health characteristics we do so by combining 1 a massive geotagged data set comprising over 80 million words generated over the course of several recent years on the social network service twitter and 2 annuallysurveyed characteristics of all 50 states and close to 400 urban populations among many results we generate taxonomies of states and cities based on their similarities in word use estimate the happiness levels of states and cities correlate highlyresolved demographic characteristics with happiness levels and connect word choice and message length with urban characteristics such as education levels and obesity rates our results show how social media may potentially be used to estimate realtime levels and changes in populationlevel measures such as obesity rates,0
the social connections people form online affect the quality of information they receive and their online experience although a host of socioeconomic and cognitive factors were implicated in the formation of offline social ties few of them have been empirically validated particularly in an online setting in this study we analyze a large corpus of georeferenced messages or tweets posted by social media users from a major us metropolitan area we linked these tweets to us census data through their locations this allowed us to measure emotions expressed in the tweets posted from an area the structure of social connections and also use that areas socioeconomic characteristics in analysis we extracted the structure of online social interactions from the people mentioned in tweets from that area we find that at an aggregate level places where social media users engage more deeply with less diverse social contacts are those where they express more negative emotions like sadness and anger demographics also has an impact these places have residents with lower household income and education levels conversely places where people engage less frequently but with diverse contacts have happier more positive messages posted from them and also have better educated younger more affluent residents results suggest that cognitive factors and offline characteristics affect the quality of online interactions our work highlights the value of linking social media data to traditional data sources such as us census to drive novel analysis of online behavior,0
when studying a nationallevel population demographers can safely ignore the effect of individuallevel randomness on agesex structure when studying a single community or group of communities however the potential importance of individuallevel randomness is less clear we seek to measure the effect of individuallevel randomness in births and deaths on standard summary indicators of agesex structure for populations of different sizes focusing on on demographic conditions typical of historical populations we conduct a microsimulation experiment where we simulate events and agesex structure under a range of settings for demographic rates and population size the experiment results suggest that individuallevel randomness strongly affects agesex structure for populations of about 100 but has a much smaller effect on populations of 1000 and a negligible effect on populations of 10000 our conclusion is that analyses of agesex structure in historical populations with sizes on the order 100 must account for individuallevel randomness in demographic events analyses of populations with sizes on the order of 1000 may need to make some allowance for individuallevel variation but other issues such as measurement error probably deserve more attention analyses of populations of 10000 can safely ignore individuallevel variation,0
a quantitative understanding of cities demographic dynamics is becoming a potentially useful tool for planning sustainable growth the concomitant theory should reveal details of the cities past and also of its interaction with nearby urban conglomerates for providing a reasonably complete picture using the exhaustive database of the census bureau in a time window of 170 years we exhibit here empirical evidence for time and space correlations in the demographic dynamics of us counties with a characteristic memorytime of 25 years and typical distances of interaction of 200 km these correlations are much larger than those observed in an european country spain giving to the us a more coherent evolution we also measure the resilience of us cities to historical events finding a demographical posttraumatic amnesia after wars as the civil war or economic crisis as the 1929 stock market crash,0
social factors such as demographic traits and institutional prestige structure the creation and dissemination of ideas in academic publishing one place these effects can be observed is in how central or peripheral a researcher is in the coauthorship network here we investigate inequities in network centrality in a handcollected data set of 5670 usbased faculty employed in phdgranting computer science departments and their dblp coauthorship connections we introduce algorithms for combining name and perceptionbased demographic labels by maximizing alignment with selfreported demographics from a survey of faculty from our census we find that women and individuals with minoritized race identities are less central in the computer science coauthorship network implying worse access to and ability to spread information centrality is also highly correlated with prestige such that faculty in topranked departments are at the core and those in lowranked departments are in the peripheries of the computer science coauthorship network we show that these disparities can be mitigated using simulated edge interventions interpreted as facilitated collaborations our intervention increases the centrality of target individuals chosen independently of the network structure by linking them with researchers from highly ranked institutions when applied to scholars during their phd the intervention also improves the predicted rank of their placement institution in the academic job market this work was guided by an ameliorative approach uncovering social inequities in order to address them by targeting scholars for intervention based on institutional prestige we are able to improve their centrality in the coauthorship network that plays a key role in job placement and longerterm academic success,0
demographic heterogeneity is often studied through the geographical lens therefore it is considered at a predetermined spatial resolution which is a suitable choice to understand scalefull phenomena spatial autocorrelation indices are well established for this purpose yet complex systems are often scalefree and thus studying the scaling behavior of demographic heterogeneity may provide valuable insights furthermore migration processes are not necessarily influenced by the physical landscape which is accounted for by the spatial autocorrelation indices the migration process may be more influenced by the socioeconomic landscape which is better reflected by the hierarchical demographic data here we explore the scaling behavior of variability measures in the united kingdom 2011 census data set as expected all of the considered variability measures decrease as the hierarchical scale becomes coarser though the nonmonotonicity is observed it can be explained by accounting for the imperfect hierarchical relationships we show that the scaling behavior of variability measures can be qualitatively understood in terms of schellings segregation model and kawasakiising,0
recent years have seen tremendous growth of many online social networks such as facebook linkedin and myspace people connect to each other through these networks forming large social communities providing researchers rich datasets to understand model and predict social interactions and behaviors new contacts in these networks can be formed due to an individuals demographic attributes such as age group gender geographic location or due to a networks structural dynamics such as triadic closure and preferential attachment or a combination of both demographic and structural characteristics a number of network generation models have been proposed in the last decade to explain the structure evolution and processes taking place in different types of networks and notably social networks network generation models studied in the literature primarily consider structural properties and in some cases an individuals demographic profile in the formation of new social contacts these models do not present a mechanism to combine both structural and demographic characteristics for the formation of new links in this paper we propose a new network generation algorithm which incorporates both these characteristics to model network formation we use different publicly available facebook datasets as benchmarks to demonstrate the correctness of the proposed network generation model the proposed model is flexible and thus can generate networks with varying demographic and structural properties,0
network growth processes can be understood as generative models of the structure and history of complex networks this point of view naturally leads to the problem of network archaeology reconstructing all the past states of a network from its structurea difficult permutation inference problem in this paper we introduce a bayesian formulation of network archaeology with a generalization of preferential attachment as our generative mechanism we develop a sequential monte carlo algorithm to evaluate the posterior averages of this model as well as an efficient heuristic that uncovers a history well correlated with the true one in polynomial time we use these methods to identify and characterize a phase transition in the quality of the reconstructed history when they are applied to artificial networks generated by the model itself despite the existence of a norecovery phase we find that nontrivial inference is possible in a large portion of the parameter space as well as on empirical data,0
demographic transition characterized by declines in fertility and mortality is a global phenomenon associated with modernization while typical patterns of fertility decline are observed in western countries their applicability to other regions and the underlying mechanisms remain unclear by analyzing demographic data from 195 countries over 200 years this study identifies two universal pathways in the changes in the crude birth rate ie births per 1000 individuals  and life expectancy at birth e0 characterized by the conservation of either e0 or expe018 these pathways define two distinct phases governed by different mechanisms phase i characterized by the conservation of e0 dominated until the mid20th century with high child mortality and steady population growth in contrast phase ii conserving expe018 has prevailed since 1950 featuring low child mortality and steady gdp per capita growth a theoretical model considering the tradeoff between reproduction and education elucidates the transition between these phases demonstrating that population size is prioritized in phase i while productivity is maximized in phase ii modernization processes such as declining educational costs and increasing social mobility are identified as key accelerators of the transition to phase ii the findings suggest that reducing educational costs can foster fertility recovery without compromising educational standards offering potential policy interventions this study provides a novel theoretical framework for understanding demographic transition by applying principles from statistical physics to uncover universal macroscopic laws and their underlying mechanisms,0
we evaluate homophily and heterophily among ideological and demographic groups in a typical opinion formation context online discussions of current news we analyze user interactions across five years in the rnews community on reddit one of the most visited websites in the united states then we estimate demographic and ideological attributes of these users thanks to a comparison with a carefullycrafted network null model we establish which pairs of attributes foster interactions and which ones inhibit them individuals prefer to engage with the opposite ideological side which contradicts the echo chamber narrative instead demographic groups are homophilic as individuals tend to interact within their own group even in an online setting where such attributes are not directly observable in particular we observe age and income segregation consistently across years users tend to avoid interactions when belonging to different groups these results persist after controlling for the degree of interest by each demographic group in different news topics our findings align with the theory that affective polarization the difficulty in socializing across political boundariesis more connected with an increasingly divided society rather than ideological echo chambers on social media we publicly release our anonymized data set and all the code to reproduce our results,0
instagram is a relatively new form of communication where users can instantly share their current status by taking pictures and tweaking them using filters it has seen a rapid growth in the number of users as well as uploads since it was launched in october 2010 inspite of the fact that it is the most popular photo sharing application it has attracted relatively less attention from the web and social media research community in this paper we present a largescale quantitative analysis on millions of users and pictures we crawled over 1 month from instagram our analysis reveals several insights on instagram which were never studied before 1 its social network properties are quite different from other popular social media like twitter and flickr 2 people typically post once a week and 3 people like to share their locations with friends to the best of our knowledge this is the first indepth analysis of user activities demographics social network structure and usergenerated content on instagram,0
a dangerously brief history of the developments of the main ideas in economics as observed by a physicist is given this was published in econophysics of stock and other markets eds a chatterjee b k chakrabarti new economic windows series springer milan 2006 pp219224,0
characterizing human mobility patterns is essential for understanding human behaviors and the interactions with socioeconomic and natural environment with the continuing advancement of location and web 20 technologies locationbased social media lbsm have been gaining widespread popularity in the past few years with an access to locations of users profiles and the contents of the social media posts the lbsm data provided a novel modality of data source for human mobility study by exploiting the explicit location footprints and mining the latent demographic information implied in the lbsm data the purpose of this paper is to investigate the spatiotemporal characteristics of human mobility with a particular focus on the impact of demography we first collect geotagged twitter feeds posted in the conterminous united states area and organize the collection of feeds using the concept of spacetime trajectory corresponding to each twitter user commonly human mobility measures including detected home and activity centers are derived for each user trajectory we then select a subset of twitter users that have detected home locations in the city of chicago as a case study and apply name analysis to the names provided in user profiles to learn the implicit demographic information of twitter users including raceethnicity gender and age finally we explore the spatiotemporal distribution and mobility characteristics of chicago twitter users and investigate the demographic impact by comparing the differences across three demographic dimensions raceethnicity gender and age we found that although the human mobility measures of different demographic groups generally follow the generic laws eg power law distribution the demographic information particular the raceethnicity group significantly affects the urban human mobility patterns,0
this paper analyzes some economic and demographic features of italians living in cities containing a saint name in their appellation hagiotoponyms demographic data come from the surveys done in the 15th 2011 italian census while the economic wealth of such cities is explored through their recent aggregated tax income ati this cultural problem is treated from various points of view first the exact list of hagiotoponyms is obtained through linguistic and religiosity criteria next it is examined how such cities are distributed in the italian regions demographic and economic perspectives are also offered at the saint level ie calculating the cumulated values of the number of inhabitants and the ati per saint as well as the corresponding relative values taking into account the saint popularity on one hand frequencysize plots and cumulative distribution function plots and on the other hand scatter plots and ranksize plots between the various quantities are shown and discussed in order to find the importance of correlations between the variables it is concluded that rankrank correlations point to a strong saint effect which explains what actually saintbased toponyms imply in terms of comparing economic and demographic data,0
physics graduate studies are substantial efforts on the part of individual students departments and institutions of higher education understanding the factors that lead to student success and attrition is crucial for improving these programs one factor that has recently started to be investigated is the broadly defined students experiences related to support structures the aspects of student experience scale ases a likertstyle survey was developed by researchers to do just that in this study we leverage the network approach for likertstyle surveys nals methodology to provide a unique interpretation of responses to the ases instrument for welldefined demographic groups we confirm the validity of our findings by studying the stability of the nals themes and investigating how they are expressed within demographicbased networks we find that for all four themes in the original ases study certain thematic trends capturing students experiences vary across the demographicbased networks in meaningful ways we also reveal that for some demographic groups there is an interesting interplay between and mixing of the original themes finally our study showcases how nals can be applied to other likertstyle datasets,0
the sarscov2 pandemic has caused significant mortality and morbidity worldwide sparing almost no community as the disease will likely remain a threat for years to come an understanding of the precise influences of human demographics and settlement as well as the dynamic factors of climate susceptible depletion and intervention on the spread of localized epidemics will be vital for mounting an effective response we consider the entire set of local epidemics in the united states a broad selection of demographic population density and climate factors and local mobility data tracking social distancing interventions to determine the key factors driving the spread and containment of the virus assuming first a linear model for the rate of exponential growth or decay in casesmortality we find that populationweighted density humidity and median age dominate the dynamics of growth and decline once interventions are accounted for a focus on distinct metropolitan areas suggests that some locales benefited from the timing of a nearly simultaneous nationwide shutdown andor the regional climate conditions in midmarch while others suffered significant outbreaks prior to intervention using a firstprinciples model of the infection spread we then develop predictions for the impact of the relaxation of social distancing and local climate conditions a few regions where a significant fraction of the population was infected show evidence that the epidemic has partially resolved via depletion of the susceptible population ie herd immunity while most regions in the united states remain overwhelmingly susceptible these results will be important for optimal management of intervention strategies which can be facilitated using our online dashboard,0
covid19 had a strong and disruptive impact on our society and yet further analyses on most relevant factors explaining the spread of the pandemic are needed interdisciplinary studies linking epidemiological mobility environmental and sociodemographic data analysis can help understanding how historical conditions concurrent social policies and environmental factors impacted on the evolution of the pandemic crisis this work deals with a regression analysis linking covid19 mortality to sociodemographic mobility and environmental data in the us during the first half of 2020 ie during the covid19 pandemic first wave this study can provide very useful insights about risk factors enhancing mortality rates before nonpharmaceutical interventions or vaccination campaigns took place our crosssectional ecological regression analysis demonstrates that when considering the entire us area the sociodemographic variables globally play the most important role with respect to environmental and mobility variables in describing covid19 mortality compared to the complete generalized linear model considering all sociodemographic mobility and environmental data the regression based only on sociodemographic data provides a better approximation and proves to be a better explanatory model when compared to the mobilitybased and environmentalbased models however when looking at single entries within each of the three groups we see that the mobility data can become relevant descriptive predictors at local scale as in new jersey where the time spent at work is one of the most relevant explanatory variables while environmental data play contradictory roles,0
it is argued that the present lognormal distribution of language sizes is to a large extent a consequence of demographic dynamics within the population of speakers of each language a twoparameter stochastic multiplicative process is proposed as a model for the population dynamics of individual languages and applied over a period spanning the last ten centuries the model disregards language birth and death a straightforward fitting of the two parameters which statistically characterize the population growth rate predicts a distribution of language sizes in excellent agreement with empirical data numerical simulations and the study of the size distribution within language families validate the assumptions at the basis of the model,0
population growth is in general constrained by food production which in turn depends on the access to water resources at a country level some populations use more water than they control because of their ability to import food and the virtual water required for its production here we investigate the dependence of demographic growth on available water resources for exporting and importing nations by quantifying the carrying capacity of nations based on calculations of the virtual water available through the food trade network we point to the existence of a global water unbalance we suggest that current export rates will not be maintained and consequently we question the longrun sustainability of the food trade system as a whole water rich regions are likely to soon reduce the amount of virtual water they export thus leaving importdependent regions without enough water to sustain their populations we also investigate the potential impact of possible scenarios that might mitigate these effects through 1 cooperative interactions among nations whereby water rich countries maintain a tiny fraction of their food production available for export 2 changes in consumption patterns and 3 a positive feedback between demographic growth and technological innovations we find that these strategies may indeed reduce the vulnerability of watercontrolled societies,0
this article examines the importance of graphic representations in the social sciences and particularly in medieval history taking as its starting point a reflection by tiennejules marey a physiologist and pioneer of 19thcentury photography and cinema marey believed that the visual should replace language in many fields indeed the twentieth and early twentyfirst centuries saw an exponential multiplication of visual media particularly with the advent of digital technology however this graphics revolution has not affected all disciplines equally significant differences remain between scientific fields such as astrophysics anthropology chemistry and medieval history despite their shared commitment to describing dynamic processes and changes of state yet while historians have already digitized a large part of the cultural heritage from antiquity to the 10th13th centuries exploration of this corpus using visualizations remains limited there is therefore untapped potential in this fieldthis article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history it examines two distinct intellectual approaches 1 the use of visuals to support a scientific discourse majority and 2 the construction of a historical discourse based on observations made from visual figures with the aim of modeling phenomena invisible to the naked eye the author thus examines the use of images in medievalism focusing on the annual volumes of the socit des historiens mdivistes de lenseignement suprieur shmesp up to 2006 two other parts of the text look at the stillrare forms of visual representation in medieval history particularly those with a heuristic vocation using iconographic objects parchments buildings and digitized texts the article suggests various visualization techniques such as network analysis the creation of stemmas 20 and interactive chronologies which could benefit the discipline these methods could potentially profoundly change our understanding of ancient societies by showing the dynamic relationships between different aspects of these societies one of the most important advances expected from these visual methods is a better understanding of the patterns of development in medieval europe which varied from region to region the hypothesis is that the scarcity of heuristic graphics in medieval history stems from the relationship with ancient documents and the historical method based on narration and exemplarity the article thus questions the value of visual modelling in medieval history and highlights the challenges associated with the widespread adoption of this approach in the humanities and social sciences finally the text invites us to reflect on the nature and functioning of heuristic visual devices by comparing medieval images and contemporary scientific visuals in both cases the point is to materialize the invisible in order to show something that exists beyond the visual the author suggests that this way of approaching visuals could play a growing role in the decades to come particularly in the field of data science,0
we present a linkcentric approach to study variation in the mobile phone communication patterns of individuals unlike most previous research on call detail records that focused on the variation of phone usage across individual users we examine how the calling and texting patterns obtained from call detail records vary among pairs of users and how these patterns are affected by the nature of relationships between users to demonstrate this linkcentric perspective we extract factors that contribute to the variation in the mobile phone communication patterns and predict demographicsrelated quantities for pairs of users the time of day and the channel of communication calls or texts are found to explain most of the variance among pairs that frequently call each other furthermore we find that this variation can be used to predict the relationship between the pairs of users as inferred from their age and gender as well as the age of the younger user in a pair from the classifier performance across different age and gender groups as well as the inherent class overlap suggested by the estimate of the bounds of the bayes error we gain insights into the similarity and differences of communication patterns across different relationships,0
recently numerous approaches have emerged in the social sciences to exploit the opportunities made possible by the vast amounts of data generated by online social networks osns having access to information about users on such a scale opens up a range of possibilities all without the limitations associated with often slow and expensive paperbased polls a question that remains to be satisfactorily addressed however is how demography is represented in the osn content here we study language use in the us using a corpus of text compiled from over half a billion geotagged messages from the online microblogging platform twitter our intention is to reveal the most important spatial patterns in language use in an unsupervised manner and relate them to demographics our approach is based on latent semantic analysis lsa augmented with the robust principal component analysis rpca methodology we find spatially correlated patterns that can be interpreted based on the words associated with them the main language features can be related to slang use urbanization travel religion and ethnicity the patterns of which are shown to correlate plausibly with traditional census data our findings thus validate the concept of demography being represented in osn language use and show that the traits observed are inherently present in the word frequencies without any previous assumptions about the dataset thus they could form the basis of further research focusing on the evaluation of demographic data estimation from other big data sources or on the dynamical processes that result in the patterns found here,0
spatiotemporal patterns of population changes within and across countries have various implications different geographical demographic and econosocietal factors seem to contribute to migratory decisions made by individual inhabitants focussing on internal ie domestic migration we ask whether individuals may take into account the information on the population density in distant locations to make migratory decisions we analyse population census data in japan recorded with a high spatial resolution ie cells of size 500 m times 500 m for the entirety of the country and simulate demographic dynamics induced by the gravity model and its variants we show that in the census data the population growth rate in a cell is positively correlated with the population density in nearby cells up to a radius of 20 km as well as that of the focal cell the ordinary gravity model does not capture this empirical observation we then show that the empirical observation is better accounted for by extensions of the gravity model such that individuals are assumed to perceive the attractiveness approximated by the population density of the source or destination cell of migration as the spatial average over a radius of approx 1 km,0
it is arguable whether history is made by great men and women or vice versa but undoubtably social connections shape history analysing wikipedia a global collective memory place we aim to understand how social links are recorded across cultures starting with the set of biographies in the english wikipedia we focus on the networks of links between these biographical articles on the 15 largest language wikipedias we detect the most central characters in these networks and point out culturerelated peculiarities furthermore we reveal remarkable similarities between distinct groups of language wikipedias and highlight the shared knowledge about connections between persons across cultures,0
many dynamic microsimulation models have shown their ability to reasonably project detailed population and households using nondata based household formation and dissolution rules although those rules allow modellers to simplify changes in the household construction they typically fall short in replicating household projections or if applied retrospectively the observed household numbers consequently such models with biased estimation for household size and other household related attributes lose their usefulness in applications that are sensitive to household size such as in travel demand and housing demand modelling nonetheless these demographic microsimulation models with their associated shortcomings have been commonly used to assess various planning policies which can result in misleading judgements in this paper we contribute to the literature of population microsimulation by introducing a fully integrated system of models for different life event where a household alignment method adjusts household size distribution to closely align with any given target distribution furthermore some demographic events that are generally difficult to model such as incorporating immigrant families into a population can be included we illustrated an example of the household alignment method and put it to test in a dynamic microsimulation model that we developed using dymiumcore a generalpurpose microsimulation toolkit in r to show potential improvements and weaknesses of the method the implementation of this model has been made publicly available on github,0
the paper presents a recursive function able to mimic demographic mortality curves this function is not a fitting algorithm and depends only from one parameter that has a precise meaning in a cellular automaton model this model is also presented for the function definition the fermi statistics method of calculation has been used resulting in similarities with known statistical distribution curves a continuous representation of the recursive equations is also provided implications with life span and more general life cycle concepts are outlined a correlation with a more recent study using a multiomics approach is pointed out,0
with the severity of the covid19 outbreak we characterize the nature of the growth trajectories of counties in the united states using a novel combination of spectral clustering and the correlation matrix as the us and the rest of the world are experiencing a severe second wave of infections the importance of assigning growth membership to counties and understanding the determinants of the growth are increasingly evident subsequently we select the demographic features that are most statistically significant in distinguishing the communities lastly we effectively predict the future growth of a given county with an lstm using three social distancing scores this comprehensive study captures the nature of counties growth in cases at a very microlevel using growth communities demographic factors and social distancing performance to help government agencies utilize known information to make appropriate decisions regarding which potential counties to target resources and funding to,0
timely pre and postdiagnosis checkups are critical for cancer patients across all cancer types as these often lead to better outcomes several sociodemographic properties have been identified as strongly connected with both cancers clinical dynamics and indirectly with different individual checkup behaviors unfortunately existing checkup policies typically consider only the former association explicitly in this work we propose a novel framework accompanied by a highresolution computer simulation to investigate and optimize sociodemographicbased sms reminder campaigns for cancer checkups we instantiate our framework and simulation for the case of bladder cancer the 10th most prevalent cancer today using extensive realworld data our results indicate that optimizing an sms reminder campaign based solely on simple sociodemographic features can bring about a statistically significant reduction in mortality rate compared to alternative campaigns by up to 58,0
mobile phone usage provides a wealth of information which can be used to better understand the demographic structure of a population in this paper we focus on the population of mexican mobile phone users we first present an observational study of mobile phone usage according to gender and age groups we are able to detect significant differences in phone usage among different subgroups of the population we then study the performance of different machine learning ml methods to predict demographic features namely age and gender of unlabeled users by leveraging individual calling patterns as well as the structure of the communication graph we show how a specific implementation of a diffusion model harnessing the graph structure has significantly better performance over other nodebased standard ml methods we provide details of the methodology together with an analysis of the robustness of our results to changes in the model parameters furthermore by carefully examining the topological relations of the training nodes seed nodes to the rest of the nodes in the network we find topological metrics which have a direct influence on the performance of the algorithm,0
urban analytics combines spatial analysis statistics computer science and urban planning to understand and shape city futures while it promises better policymaking insights concerns exist around its epistemological scope and impacts on privacy ethics and social control this chapter reflects on the history and trajectory of urban analytics as a scholarly and professional discipline in particular it considers the direction in which this field is going and whether it improves our collective and individual welfare it first introduces early theories models and deductive methods from which the field originated before shifting toward induction it then explores urban network analytics that enrich traditional representations of spatial interaction and structure next it discusses urban applications of spatiotemporal big data and machine learning finally it argues that privacy and ethical concerns are too often ignored as ubiquitous monitoring and analytics can empower social repression it concludes with a call for a more critical urban analytics that recognizes its epistemological limits emphasizes human dignity and learns from and supports marginalized communities,0
although north koreas nuclear program has been the subject of extensive scrutiny estimates of its fissile material stockpiles remain fraught with uncertainty in potential future disarmament agreements inspectors may need to use nuclear archaeology methods to verify or gain confidence in a north korean fissile material declaration this study explores the potential utility of a bayesian inferencebased analysis of the isotopic composition of reprocessing waste to reconstruct the operating history of the 5 mwe reactor and estimate its plutonium production history we simulate several scenarios that reflect different assumptions and varying levels of prior knowledge about the reactor the results show that correct prior assumptions can be confirmed and incorrect prior information or a false declaration can be detected model comparison techniques can distinguish between scenarios with different numbers of core discharges a capability that could provide important insights into the early stages of operation of the 5 mwe reactor using these techniques a weighted plutonium estimate can be calculated even in cases where the number of core discharges is not known with certainty,0
in this paper we propose a new mathematical model capable of merging darwinian evolution human history and seti into a single mathematical scheme 1 darwinian evolution over the last 35 billion years is defined as one particular realization of a certain stochastic process called geometric brownian motion gbm this gbm yields the fluctuations in time of the number of species living on earth its mean value curve is an increasing exponential curve ie the exponential growth of evolution 2 in 2008 this author provided the statistical generalization of the drake equation yielding the number n of communicating et civilizations in the galaxy n was shown to follow the lognormal probability distribution 3 we call blognormals those lognormals starting at any positive time b birth larger than zero then the exponential growth curve becomes the geometric locus of the peaks of a oneparameter family of blognormals this is our way to redefine cladistics 4 blognormals may be also be interpreted as the lifespan of any living being a cell or an animal a plant a human or even the historic lifetime of any civilization applying this new mathematical apparatus to human history leads to the discovery of the exponential progress between ancient greece and the current usa as the envelope of all blognormals of western civilizations over a period of 2500 years 5 we then invoke shannons information theory the blognormals entropy turns out to be the index of development level reached by each historic civilization we thus get a numerical estimate of the entropy difference between any two civilizations like the aztecspaniard difference in 1519 6 in conclusion we have derived a mathematical scheme capable of estimating how much more advanced than humans an alien civilization will be when the seti scientists will detect the first hints about ets,0
the evolution processes of complex systems carry key information in the systems functional properties applying machine learning algorithms we demonstrate that the historical formation process of various networked complex systems can be extracted including proteinprotein interaction ecology and social network systems the recovered evolution process has demonstrations of immense scientific values such as interpreting the evolution of proteinprotein interaction network facilitating structure prediction and particularly revealing the key coevolution features of network structures such as preferential attachment community structure local clustering degreedegree correlation that could not be explained collectively by previous theories intriguingly we discover that for large networks if the performance of the machine learning model is slightly better than a random guess on the pairwise order of links reliable restoration of the overall network formation process can be achieved this suggests that evolution history restoration is generally highly feasible on empirical networks,0
we present a general framework to describe the evolutionary dynamics of an arbitrary number of types in finite populations based on stochastic differential equations sde for large but finite populations this allows to include demographic noise without requiring explicit simulations instead the population size only rescales the amplitude of the noise moreover this framework admits the inclusion of mutations between different types provided that mutation rates  are not too small compared to the inverse population size 1n this ensures that all types are almost always represented in the population and that the occasional extinction of one type does not result in an extended absence of that type for nll1 this limits the use of sdes but in this case there are well established alternative approximations based on time scale separation we illustrate our approach by a rockscissorspaper game with mutations where we demonstrate excellent agreement with simulation based results for sufficiently large populations in the absence of mutations the excellent agreement extends to small population sizes,0
we study human dynamics by analyzing linux history files the goodnessoffit test shows that most of the collected datasets belong to the universality class suggested in the literature by a variablelength queueing process based on priority in order to check the validity of this model we design two tests based on mutual information between time intervals and a mathematical relationship known as the arcsine law since the previously suggested queueing process fails to pass these tests the result suggests that the modelling of human dynamics should properly consider the statistical dependency in the temporal dimension,0
has ideological polarization actually increased in the last decades or have voters simply sorted themselves into parties matching their ideology more closely we present a novel methodology to quantify multidimensional ideological polarization by embedding the respondents to a wide variety of political social and economic topics from the american national election studies anes into a twodimensional ideological space by identifying several demographic attributes of the anes respondents we chart how political and socioeconomic groups move through the ideological space in time we observe that income and especially racial groups align into parties but their ideological distance has not increased over time instead democrats and republicans have become ideologically more distant in the last 30 years both parties moved away from the center at different rates furthermore democratic voters have become ideologically more heterogeneous after 2010 indicating that partisan sorting has declined in the last decade,0
understanding influencing factors is essential for the surveillance and prevention of infectious diseases and the factors are likely to vary spatially and temporally as the disease progresses taking daily cases and deaths data during the coronavirus disease 2019 covid19 outbreak in the us as a case study we develop a mobilityaugmented geographically and temporally weighted regression mgtwr model to quantify the spatiotemporal impacts of socialdemographic factors and human activities on the covid19 dynamics different from the base gtwr model we incorporate a mobilityadjusted distance weight matrix where travel mobility is used in addition to the spatial adjacency to capture the correlations among local observations the model residuals suggest that the proposed model achieves a substantial improvement over other benchmark methods in addressing the spatiotemporal nonstationarity our results reveal that the impacts of socialdemographic and human activity variables present significant spatiotemporal heterogeneity in particular a 1 increase in population density may lead to 063 and 071 more daily cases and deaths and a 1 increase in the mean commuting time may result in 022 and 095 increases in daily cases and deaths although increased human activities will in general intensify the disease outbreak we report that the effects of grocery and pharmacyrelated activities are insignificant in areas with high population density and activities at the workplace and public transit are found to either increase or decrease the number of cases and deaths depending on particular locations the results of our study establish a quantitative framework for identifying influencing factors during a disease outbreak and the obtained insights may have significant implications in guiding the policymaking against infectious diseases,0
the report discusses the emergence of the socioeconomic soft matter sesm as the result of interactions between physics and economy first demographic changes since the industrial revolution onset are tested using soft matter science tools notable in the support of innovative derivativebased and distortionssensitive analytic tools it revealed the weibull type powered exponential increase with a notably lesser rising rate since the crossover detected near the year 1970 subsequently demographic sesm patterns are tested for rapa nui easter island model case and for four large hallmark cities where the rise and decay phases have occurred they are detroit and cleveland in the usa and lodz former textile industry center and bytom former coal mining center in poland the analysis explicitly revealed scaling patterns for demographic changes influenced by the historical and socioeconomic backgrounds and the longlasting determinism in population changes universalistic features of demographic changes are discussed within the socioeconomic soft matter concept,0
art is the ultimate expression of human creativity that is deeply influenced by the philosophy and culture of the corresponding historical epoch the quantitative analysis of art is therefore essential for better understanding human cultural evolution here we present a largescale quantitative analysis of almost 140 thousand paintings spanning nearly a millennium of art history based on the local spatial patterns in the images of these paintings we estimate the permutation entropy and the statistical complexity of each painting these measures map the degree of visual order of artworks into a scale of orderdisorder and simplicitycomplexity that locally reflects qualitative categories proposed by art historians the dynamical behavior of these measures reveals a clear temporal evolution of art marked by transitions that agree with the main historical periods of art our research shows that different artistic styles have a distinct average degree of entropy and complexity thus allowing a hierarchical organization and clustering of styles according to these metrics we have further verified that the identified groups correspond well with the textual content used to qualitatively describe the styles and that the employed complexityentropy measures can be used for an effective classification of artworks,0
political regimes have been changing throughout human history after the apparent triumph of liberal democracies at the end of the twentieth century francis fukuyama and others have been arguing that humankind is approaching an end of history eoh in the form of a universality of liberal democracies this view has been challenged by recent developments that seem to indicate the rise of defective democracies across the globe there has been no attempt to quantify the expected eoh with a statistical approach in this study we model the transition between political regimes as a markov process and using a bayesian inference approach we estimate the transition probabilities between political regimes from timeseries data describing the evolution of political regimes from 18002018 we then compute the steady state for this markov process which represents a mathematical abstraction of the eoh and predicts that approximately 46 of countries will be full democracies furthermore we find that under our model the fraction of autocracies in the world is expected to increase for the next halfcentury before it declines using randomwalk theory we then estimate survival curves of different types of regimes and estimate characteristic lifetimes of democracies and autocracies of 244 years and 69 years respectively quantifying the expected eoh allows us to challenge common beliefs about the nature of political equilibria specifically we find no statistical evidence that the eoh constitutes a fixed complete omnipresence of democratic regimes,0
various processes can be modelled as quasireaction systems of stochastic differential equations such as cell differentiation and disease spreading since the underlying data of particle interactions such as reactions between proteins or contacts between people are typically unobserved statistical inference of the parameters driving these systems is developed from concentration data measuring each unit in the system over time while observing the continuous time process at a time scale as fine as possible should in theory help with parameter estimation the existing local linear approximation lla methods fail in this case due to numerical instability caused by small changes of the system at successive time points on the other hand one may be able to reconstruct the underlying unobserved interactions from the observed count data motivated by this we first formalise the latent event history model underlying the observed count process we then propose a computationally efficient expectationmaximation algorithm for parameter estimation with an extended kalman filtering procedure for the prediction of the latent states a simulation study shows the performance of the proposed method and highlights the settings where it is particularly advantageous compared to the existing lla approaches finally we present an illustration of the methodology on the spreading of the covid19 pandemic in italy,0
the sloan digital sky survey sdss is one of the largest international astronomy organizations we present demographic data based on surveys of its members from 2014 2015 and 2016 during the fourth phase of sdss sdssiv we find about half of sdssiv collaboration members were based in north america a quarter in europe and the remainder in asia and central and south america overall 2636 are women from 2014 to 2016 up to 2 report nonbinary genders 1114 report that they are racial or ethnic minorities where they live the fraction of women drops with seniority and is also lower among collaboration leadership men in sdssiv were more likely to report being in a leadership role and for the role to be funded and formally recognized sdssiv collaboration members are twice as likely to have a parent with a college degree than the general population and are ten times more likely to have a parent with a phd this trend is slightly enhanced for female collaboration members despite this the fraction of first generation college students fgcs is significant 31 this fraction increased among collaboration members who are racial or ethnic minorities 4050 and decreased among women 1525 sdssiv implemented many inclusive policies and established a dedicated committee the committee on inclusiveness in sdss coins more than 60 of the collaboration agree that the collaboration is inclusive however collaboration leadership more strongly agree with this than the general membership in this paper we explain these results in full including the history of inclusive efforts in sdssiv we conclude with a list of suggested recommendations based on our findings which can be used to improve equity and inclusion in large astronomical collaborations which we argue is not only moral but will also optimize their scientific output,0
the journal impact factor jif is by far the most discussed bibliometric indicator since its introduction over 40 years ago it has had enormous effects on the scientific ecosystem transforming the publishing industry shaping hiring practices and the allocation of resources and as a result reorienting the research activities and dissemination practices of scholars given both the ubiquity and impact of the indicator the jif has been widely dissected and debated by scholars of every disciplinary orientation drawing on the existing literature as well as on original research this chapter provides a brief history of the indicator and highlights wellknown limitationssuch as the asymmetry between the numerator and the denominator differences across disciplines the insufficient citation window and the skewness of the underlying citation distributions the inflation of the jif and the weakening predictive power is discussed as well as the adverse effects on the behaviors of individual actors and the research enterprise alternative journalbased indicators are described and the chapter concludes with a call for responsible application and a commentary on future developments in journal indicators,0
understanding human activities and movements on the web is not only important for computational social scientists but can also offer valuable guidance for the design of online systems for recommendations caching advertising and personalization in this work we demonstrate that people tend to follow routines on the web and these repetitive patterns of web visits increase their browsing behaviors achievable predictability we present an informationtheoretic framework for measuring the uncertainty and theoretical limits of predictability of human mobility on the web we systematically assess the impact of different design decisions on the measurement we apply the framework to a web tracking dataset of german internet users our empirical results highlight that individuals routines on the web make their browsing behavior predictable to 85 on average though the value varies across individuals we observe that these differences in the users predictabilities can be explained to some extent by their demographic and behavioral attributes,0
we consider all matches played by professional tennis players between 1968 and 2010 and on the basis of this data set construct a directed and weighted network of contacts the resulting graph shows complex features typical of many real networked systems studied in literature we develop a diffusion algorithm and apply it to the tennis contact network in order to rank professional players jimmy connors is identified as the best player of the history of tennis according to our ranking procedure we perform a complete analysis by determining the best players on specific playing surfaces as well as the best ones in each of the years covered by the data set the results of our technique are compared to those of two other well established methods in general we observe that our ranking method performs better it has a higher predictive power and does not require the arbitrary introduction of external criteria for the correct assessment of the quality of players the present work provides a novel evidence of the utility of tools and methods of network theory in real applications,0
the rapid increase of population and settlement structures in the global south during recent decades motivates the development of suitable models to describe their formation and evolution such settlement formation has been previously suggested to be dynamically driven by simple patternforming mechanisms here we explore the use of a datadriven whitebox approach called sindy to discover differential equation models directly from available spatiotemporal demographic data for three representative regions of the global south we show that the current resolution and observation time of the available data is insufficient to uncover relevant patternforming mechanisms in settlement development using synthetic data generated with a generic patternforming model the allencahn equation we characterize what the requirements are on spatial and temporal resolution as well as observation time to successfully identify possible model system equations overall the study provides a theoretical framework for the analysis of largescale geographicalecological systems and it motivates further improvements in optimization approaches and data collection,0
we study the structure of the social graph of mobile phone users in the country of mexico with a focus on demographic attributes of the users more specifically the users age we examine assortativity patterns in the graph and observe a strong age homophily in the communications preferences we propose a graph based algorithm for the prediction of the age of mobile phone users the algorithm exploits the topology of the mobile phone network together with a subset of known users ages seeds to infer the age of remaining users we provide the details of the methodology and show experimental results on a network gt with more than 70 million users by carefully examining the topological relations of the seeds to the rest of the nodes in gt we find topological metrics which have a direct influence on the performance of the algorithm in particular we characterize subsets of users for which the accuracy of the algorithm is 62 when predicting between 4 age categories whereas a pure random guess would yield an accuracy of 25 we also show that we can use the probabilistic information computed by the algorithm to further increase its inference power to 72 on a significant subset of users,0
in this article we propose a network spread model for hiv epidemics wherein each individual is represented by a node of the transmission network and the edges are the connections between individuals along which the infection may spread the sexual activity of each individual measured by its degree is not homogeneous but obeys a powerlaw distribution due to the heterogeneity of activity the infection can persistently exist at a very low prevalence which has been observed in real data but can not be illuminated by previous models with homogeneous mixing hypothesis furthermore the model displays a clear picture of hierarchical spread in the early stage the infection is adhered to these highrisk persons and then diffuses toward lowrisk population the prediction results show that the development of epidemics can be roughly categorized into three patterns for different countries and the pattern of a given country is mainly determined by the average sexactivity and transmission probability per sexual partner in most cases the effect of hiv epidemics on demographic structure is very small however for some extremely countries like botswana the number of sexactive people can be depressed to nearly a half by aids,0
at least since priestleys 1765 chart of biography large numbers of individual person records have been used to illustrate aggregate patterns of cultural history wikidata the structured database sister of wikipedia currently contains about 27 million explicit person records across all language versions of the encyclopedia these individuals notable according to wikipedia editing criteria are connected via millions of hyperlinks between their respective wikipedia articles this situation provides us with the chance to go beyond the illustration of an idiosyncratic subset of individuals as in the case of priestly in this work we summarize the overlap of nationalities and occupations based on their cooccurrence in wikidata individuals we construct networks of cooccurring nationalities and occupations provide insights into their respective community structure and apply the results to select and color chronologically structured subsets of a large network of individuals connected by wikipedia hyperlinks while the imagined communities of nationality are much more discrete in terms of cooccurrence than occupations our quantifications reveal the existing overlap of nationality as much less clearcut than in case of occupational domains our work contributes to a growing body of research using biographies of notable persons to analyze cultural processes,0
how to enable efficient analytics over such data has been an increasingly important research problem given the sheer size of such social networks many existing studies resort to sampling techniques that draw random nodes from an online social network through its restrictive webapi interface almost all of them use the exact same underlying technique of random walk a markov chain monte carlo based method which iteratively transits from one node to its random neighbor random walk fits naturally with this problem because for most online social networks the only query we can issue through the interface is to retrieve the neighbors of a given node ie no access to the full graph topology a problem with random walks however is the burnin period which requires a large number of transitionsqueries before the sampling distribution converges to a stationary value that enables the drawing of samples in a statistically valid manner in this paper we consider a novel problem of speeding up the fundamental design of random walks ie reducing the number of queries it requires without changing the stationary distribution it achieves thereby enabling a more efficient dropin replacement for existing samplingbased analytics techniques over online social networks our main idea is to leverage the history of random walks to construct a higherordered markov chain we develop two algorithms circulated neighbors and groupby neighbors random walk cnrw and gnrw and prove that no matter what the social network topology is cnrw and gnrw offer better efficiency than baseline random walks while achieving the same stationary distribution we demonstrate through extensive experiments on realworld social networks and synthetic graphs the superiority of our techniques over the existing ones,0
in modern society people are being exposed to numerous information with some of them being frequently repeated or more disruptive than others in this paper we use a model of opinion dynamics to study how this news impact the society in particular our study aims to explain how the exposure of the society to certain events deeply change peoples perception of the present and future the evolution of opinions which we consider is influenced both by external information and the pressure of the society the latter includes imitation differentiation homophily and its opposite xenophobia the combination of these ingredients gives rise to a collective memory effect which is triggered by external information in this paper we focus our attention on how this memory arises when the order of appearance of external news is random we will show which characteristics a piece of news needs to have in order to be embedded in the societys memory we will also provide an analytical way to measure how many information a society can remember when an extensive number of news items is presented finally we will show that when a certain piece of news is present in the societys history even a distorted version of it is sufficient to trigger the memory of the originally stored information,0
in the global move toward urbanization making sure the people remaining in rural areas are not left behind in terms of development and policy considerations is a priority for governments worldwide however it is increasingly challenging to track important statistics concerning this sparse geographically dispersed population resulting in a lack of reliable uptodate data in this study we examine the usefulness of the facebook advertising platform which offers a digital census of over two billions of its users in measuring potential ruralurban inequalities we focus on italy a country where about 30 of the population lives in rural areas first we show that the population statistics that facebook produces suffer from instability across time and incomplete coverage of sparsely populated municipalities to overcome such limitation we propose an alternative methodology for estimating facebook ads audiences that nearly triples the coverage of the rural municipalities from 19 to 55 and makes feasible finegrained subpopulation analysis using official national census data we evaluate our approach and confirm known significant urbanrural divides in terms of educational attainment and income extending the analysis to facebookspecific user interests and behaviors we provide further insights on the divide for instance finding that rural areas show a higher interest in gambling notably we find that the most predictive features of income in rural areas differ from those for urban centres suggesting researchers need to consider a broader range of attributes when examining rural wellbeing the findings of this study illustrate the necessity of improving existing tools and methodologies to include underrepresented populations in digital demographic studies the failure to do so could result in misleading observations conclusions and most importantly policies,0
spatial distribution of the human population is distinctly heterogeneous eg showing significant difference in the population density between urban and rural areas in the historical perspective ie on the timescale of centuries the emergence of the densely populated areas at their present locations is widely believed to be linked to more favourable environmental and climatic conditions in this paper we challenge this point of view we first identify a few areas at different parts of the world where the environmental conditions quantified by the temperature precipitation and elevation are approximately uniform over thousands of miles we then examine the population distribution across those areas to show that in spite of the homogeneity of the environment it exhibits a clear nearlyperiodic spatial pattern based on this apparent disagreement we hypothesize that there exists an inherent mechanism that can lead to pattern formation even in a uniform environment we consider a mathematical model of the coupled demographiceconomic dynamics and show that its spatially uniform locally stable steady state can give rise to a periodic spatial pattern due to the turing instability using computer simulations we show that interestingly the emergence of the turing patterns eventually leads to the system collapse,0
wikipedia is a free internet encyclopedia with an enormous amount of content this encyclopedia is written by volunteers with various backgrounds in a collective fashion anyone can access and edit most of the articles this openediting nature may give us prejudice that wikipedia is an unstable and unreliable source yet many studies suggest that wikipedia is even more accurate and selfconsistent than traditional encyclopedias scholars have attempted to understand such extraordinary credibility but usually used the number of edits as the unit of time without consideration of real time in this work we probe the formation of such collective intelligence through a systematic analysis using the entire history of 34534110 english wikipedia articles between 2001 and 2014 from this massive data set we observe the universality of both timewise and lengthwise editing scales which suggests that it is essential to consider the realtime dynamics by considering real time we find the existence of distinct growth patterns that are unobserved by utilizing the number of edits as the unit of time to account for these results we present a mechanistic model that adopts the article editing dynamics based on both editoreditor and editorarticle interactions the model successfully generates the key properties of real wikipedia articles such as distinct types of articles for the editing patterns characterized by the interrelationship between the numbers of edits and editors and the article size in addition the model indicates that infrequently referred articles tend to grow faster than frequently referred ones and articles attracting a high motivation to edit counterintuitively reduce the number of participants we suggest that this decay of participants eventually brings inequality among the editors which will become more severe with time,0
the structure of interconnected systems and its impact on the system dynamics is a muchstudied crossdisciplinary topic although various critical phenomena have been found in different models the study on the connections between different percolation transitions is still lacking here we propose a unified framework to study the origins of the discontinuous transitions of the percolation process on interacting networks the model evolves in generations with the result of the present percolation depending on the previous state and thus is historydependent both theoretical analysis and monte carlo simulations reveal that the nature of the transition remains the same at finite generations but exhibits an abrupt change for the infinite generation we use brain functional correlation and morphological similarity data to show that our model also provides a general method to explore the network structure and can contribute to many practical applications such as detecting the abnormal structures of human brain networks,0
the origin of nonpoissonian or bursty temporal patterns observed in various datasets for human social dynamics has been extensively studied yet its understanding still remains incomplete considering the fact that humans are social beings a fundamental question arises is the bursty human dynamics dominated by individual characteristics or by interaction between individuals in this paper we address this question by analyzing the wikipedia edit history to see how spontaneous individual editors are in initiating bursty periods of editing ie individualdriven burstiness and to what extent such editors behaviors are driven by interaction with other editors in those periods ie interactiondriven burstiness we quantify the degree of initiative doi of an editor of interest in each wikipedia article by using the statistics of bursty periods containing the editors edits the integrated value of the doi over all relevant timescales reveals which is dominant between individualdriven and interactiondriven burstiness we empirically find that this value tends to be larger for weaker temporal correlations in the editors editing behavior andor stronger editorial correlations these empirical findings are successfully confirmed by deriving an analytic form of the doi from a model capturing the essential features of the edit sequence thus our approach provides a deeper insight into the origin and underlying mechanisms of bursts in human social dynamics,0
social contact patterns are key drivers of infectious disease transmission during the covid19 pandemic differences between precovid and covidera contact rates were widely attributed to nonpharmaceutical interventions such as lockdowns however the factors that drive changes in the distribution of contacts between different subpopulations remain poorly understood here we present a clustering analysis of 33 contact matrices generated from surveys conducted before and during the covid19 pandemic and analyse key features distinguishing their topological structures while we expected to identify aspects of pandemic scenarios responsible for these features our analysis demonstrates that they can be explained by differences in study design and longterm demographic trends our results caution against using survey data from different studies in counterfactual analysis of epidemic mitigation strategies doing so risks attributing differences stemming from methodological choices or longterm changes to the shortterm effects of interventions,0
in this paper the history of the founding and of the development of the amaldi conferences is described with special reference to the following aspects and questions 1 the origin 2 the vision of a european cisac committee on international security and arms control 3 changes in the political landscape and their consequences 4 discussions on widening the scope of the amaldi conferences 5 the amaldi guidelines 6 are the amaldi conferences still serving their initial purpose 7 are there new chances for a european cisac after the progress in european unification,0
this report provides insights into global population dynamics since the beginning of the anthropocene focusing on empirical data and minimizing a priori the impact of model assumptions it explores the relative growth rate concept introduced recently to global population studies by lehman et al and subsequently extended to its analytical counterpart the analysis reveals a general nonmonotonic growth pattern in the anthropocene emphasizing the uniqueness of the industrial revolution era for the first 290 years the doomsday critical scaling provides a superior description for population changes with a singularity at 2026 this is followed by the crossover to an exceptional reversed criticality patterm which has held over the last six decades to the present day the analysis suggests that the evolution of the human real intelligence ri population during the innovationsdriven industrial revolution times a period of rapidly increasing connectivity and complexity can serve as a model counterpart for the puzzling dynamics of artificial intelligence ai growth the final conclusion is positive the catastropic doomsday singularity can be avoided due to the generic system constraints both for ri and ai,0
many introductory articles and books about nanotechnology have been written to disseminate this apparently new technology which investigate and manipulates matter at dimension of a billionth of a meter however these texts show in general a common feature there is very little about the origins of this multidisciplinary field if anything is mentioned at all a few dates facts and characters are reinforced which under the scrutiny of a careful historical digging do not sustain as really founding landmarks of the field nevertheless in spite of these flaws such historical narratives bring up important elements to understand and contextualize this human endeavor as well as the corresponding dissemination among the public would nanotechnology be a cultural imperative,0
the italian government has been one of the most responsive to covid19 emergency through the adoption of quick and increasingly stringent measures to contain the outbreak despite this italy has suffered a huge human and social cost especially in lombardy the aim of this paper is dual i first to investigate the reasons of the case fatality rate cfr differences across italian 20 regions and 107 provinces using a multivariate ols regression approach and ii second to build a taxonomy of provinces with similar mortality risk of covid19 by using the ward hierarchical agglomerative clustering method i considered health system metrics environmental pollution climatic conditions demographic variables and three ad hoc indexes that represent the health system saturation the results showed that overall health care efficiency physician density and average temperature helped to reduce the cfr by the contrary population aged 70 and above car and firm density level of air pollutants no2 o3 pm10 and pm25 relative average humidity covid19 prevalence and all three indexes of health system saturation were positively associated with the cfr population density social vertical integration and altitude were not statistically significant in particular the risk of dying increases with age as 90 years old and above had a threefold greater risk than the 80 to 89 years old and fourfold greater risk than 70 to 79 years old moreover the cluster analysis showed that the highest mortality risk was concentrated in the north of the country while the lowest risk was associated with southern provinces finally since prevalence and health system saturation indexes played the most important role in explaining the cfr variability a significant part of the latter may have been caused by the massive stress of the italian health system,0
historydependent processes are ubiquitous in natural and social systems many such stochastic processes especially those that are associated with complex systems become more constrained as they unfold meaning that their samplespace or their set of possible outcomes reduces as they age we demonstrate that these samplespace reducing ssr processes necessarily lead to zipfs law in the rank distributions of their outcomes we show that by adding noise to ssr processes the corresponding rank distributions remain exact powerlaws pxsim x where the exponent directly corresponds to the mixing ratio of the ssr process and noise this allows us to give a precise meaning to the scaling exponent in terms of the degree to how much a given process reduces its samplespace as it unfolds noisy ssr processes further allow us to explain a wide range of scaling exponents in frequency distributions ranging from  2 to infty we discuss several applications showing how ssr processes can be used to understand zipfs law in word frequencies and how they are related to diffusion processes in directed networks or ageing processes such as in fragmentation processes ssr processes provide a new alternative to understand the origin of scaling in complex systems without the recourse to multiplicative preferential or selforganised critical processes,0
migration plays a crucial role in urban growth over time individuals opting to relocate led to vast metropolises like london and paris during the industrial revolution shanghai and karachi during the last decades and thousands of smaller settlements here we analyze the impact that migration has on population redistribution we use a model of citytocity migration as a process that occurs within a network where the nodes represent cities and the edges correspond to the flux of individuals we analyze metrics characterizing the urban distribution and show how a slight preference for some destinations might result in the observed distribution of the population,0
the frequency distribution of personal given names offers important evidence about the information economy this paper presents data on the popularity of the most frequent personal given names first names in england and wales over the past millennium the popularity of a name is its frequency relative to the total name instances sampled the data show that the popularity distribution of names like the popularity of other symbols and artifacts associated with the information economy can be helpfully viewed as a power law moreover the data on name popularity suggest that historically distinctive changes in the information economy occurred in conjunction with the industrial revolution,0
the digital information landscape has introduced a new dimension to understanding how we collectively react to new information and preserve it at the societal level this together with the emergence of platforms such as wikipedia has challenged traditional views on the relationship between current events and historical accounts of events with an evershrinking divide between news and history wikipedias place as the internets primary reference work thus poses the question of how it represents both traditional encyclopaedic knowledge and evolving important news stories in other words how is information on and attention towards current events integrated into the existing topical structures of wikipedia to address this we develop a temporal community detection approach towards topic detection that takes into account both short term dynamics of attention as well as long term article network structures we apply this method to a dataset of one year of current events on wikipedia to identify clusters distinct from those that would be found solely from page view time series correlations or static network structure we are able to resolve the topics that more strongly reflect unfolding current events vs more established knowledge by the relative importance of collective attention dynamics vs link structures we also offer important developments by identifying and describing the emergent topics on wikipedia this work provides a means of distinguishing how these information and attention clusters are related to wikipedias twin faces of encyclopaedic knowledge and current events crucial to understanding the production and consumption of knowledge in the digital age,0
sedentism was a decisive moment in the history of humankind in a review article kay and kaplan quantified land use for early human settlements and found that sedentism and the emergence of farming go hand in hand for these settlements two primary land use categories farming and living can be identified whereas for hunter gatherer societies no distinct differences can be made it is natural to search for this in the behavior of two different groups settlers and farmers the development of two distinct zones and the two groups lead us to the hypothesis that the emergence of settlements is the result of diffusiondriven turing instability in this short communication we further specify this and show that this results in a regular settlement arrangement as can still be seen today in agricultural regions,0
the intent of this paper is to discuss the history and origins of lagrangian hydrodynamic methods for simulating shock driven flows the majority of the pioneering research occurred within the manhattan project a range of lagrangian hydrodynamic schemes were created between 1943 and 1948 by john von neumann rudolf peierls tony skyrme and robert richtmyer these schemes varied significantly from each other however they all used a staggeredgrid and finite difference approximations of the derivatives in the governing equations where the first scheme was by von neumann these groundbreaking schemes were principally published in los alamos laboratory reports that were eventually declassified many decades after authorship which motivates us to document the work and describe the accompanying history in a paper that is accessible to the broader scientific community furthermore we seek to correct historical omissions on the pivotal contributions made by peierls and skyrme to creating robust lagrangian hydrodynamic methods for simulating shock driven flows understanding the history of lagrangian hydrodynamic methods can help explain the origins of many modern schemes and may inspire the pursuit of new schemes,0
power generation and distribution remains an important topic of discussion since the industrial revolution as the system continues to grow it needs to evolve both in infrastructure robustness and its resilience to deal with failures one such potential failure that we target in this work is the cascading failure this avalanche effect propagates through the network and we study this propagation by percolation theory and implement some solutions for mitigation we have extended the percolation theory as given in mark newman networks an introductionfor random nodes to targeted nodes having high load bearing which is eliminated from the network to study the cascade effect we also implement mitigation strategy to improve the network performance,0
i present a brief review of the history of the instituto argentino de radioastronoma a description of its current facilities and projects and a view of his prospects for the future,0
researchers working in lattice field theory constitute an established community since the early 1990s and around the same time the online openaccess eprint repository arxiv was created the fact that this field has a specific arxiv section heplat which is comprehensively used provides a unique opportunity for a statistical study of its evolution over the last three decades we present data for the number of entries e published papers p and citations c in total and separated by nations we compare them to six other arxiv sections hepph hepth grqc nuclth quantph condmat and to two socioeconomic indices of the nations involved the gross domestic product gdp and the education index ei we present rankings which are based either on the hirsch index h or on the linear combination  e p 005 c we consider both extensive and intensive national statistics ie absolute and relative to the population or to the gdp,0
an interesting yet unknown episode concerning the effective permeation of the scientific revolution in the xviii century kingdom of naples and more generally italy is recounted the quite intriguing story of watts steam engine prepared for serving a royal estate of the king of naples in carditello reveals a fascinating piece of the history of that kingdom as well as an unknown step in the history of watts steam engine whose final entrepreneurial success for the celebrated boulton watt company was a direct consequence that story unveils that contrary to what claimed in the literature the first introduction in italy of the most important technological innovation of the xviii century did not take place with the construction of the first steamship of the mediterranean sea but rather 30 years before that thanks to the incomparable work of giuseppe saverio poli a leading scholar and a very influential figure in the kingdom of naples the tragic epilogue of polis engine testifies for its vanishing in the historical memory,0
this article presents an overview and recent history of studies of gender gaps in the mathematicallyintensive sciences included are several statistics about gender differences in science and about public resources aimed at addressing them we then examine the role that gender differences in creativity play in explaining the recent and current gender differences in the mathematical sciences and identify several constructive suggestions aimed at improving analytical creativity output in research institutions,0
the history and advances of neutronics calculations at los alamos during the manhattan project through the present is reviewed we briefly summarize early simpler and more approximate neutronics methods we then motivate the need to better predict neutronics behavior through consideration of theoretical equations models and algorithms experimental measurements and available computing capabilities and their limitations these coupled with increasing postwar defense needs and the invention of electronic computing led to the creation of monte carlo neutronics transport as a part of the history we note the crucial role that the scientific comradery between the great los alamos scientists played in the process we focus heavily on these early developments and the subsequent successes of monte carlo and its applications to problems of national defense at los alamos we cover the early methods algorithms and computers electronic and women pioneers that enabled monte carlo to spread to all areas of science,0
we explore aboriginal oral traditions that relate to australian meteorite craters using the literature firsthand ethnographic records and fieldtrip data we identify oral traditions and artworks associated with four impact sites gosses bluff henbury liverpool and wolfe creek oral traditions describe impact origins for gosses bluff henbury and wolfe creek craters and nonimpact origins of liverpool crater with wolfe creek and henbury having both impact and nonimpact origins in oral tradition three impact sites that are believed to have formed during human habitation of australia dalgaranga veevers and boxhole do not have associated oral traditions that are reported in the literature,0
this article describes the history of the computing facility at los alamos during the manhattan project 1944 to 1946 the hand computations are briefly discussed but the focus is on the ibm punch card accounting machines pcam during wwii the los alamos facility was one of most advanced pcam facilities both in the machines and in the problems being solved,0
australian indigenous astronomical traditions hint at a relationship between animals in the skyworld and the behaviour patterns of their terrestrial counterparts in our continued study of aboriginal astronomical traditions from the great victoria desert south australia we investigate the relationship between animal behaviour and stellar positions we develop a methodology to test the hypothesis that the behaviour of these animals is predicted by the positions of their celestial counterparts at particular times of the day of the twelve animals identified in the ooldean sky the nine stellar ie nonplanet or nongalactic associations were analysed and each demonstrated a close connection between animal behaviour and stellar positions we suggest that this may be a recurring theme in aboriginal astronomical traditions requiring further development of the methodology,0
assessing the potential influence of vocational education and training vet courses on creating job opportunities and nurturing work skills has been considered challenging due to the ambiguity in defining their complex relationships and connections with the local economy here we quantify the potential influence of vet courses and explain it with future economy and specialization by constructing a network of more than 17000 courses jobs and skills in singapores skillsfuture data based on their text similarities captured by a text embedding technique sentence transformer we find that vet courses associated with singapores 4th industrial revolution economy demonstrate higher influence than those related to other future economies the course influence varies greatly across different sectors attributed to the level of specificity of the skills covered lastly we show a notable concentration of vet supply in certain occupation sectors requiring general skills underscoring a disproportionate distribution of education supply for the labor market,0
we make a short study of the history and evolution of scientific publications in order to explain the format for nearterm eprints servers proposing a new scientific publication scheme via digital network and exploring the new dynamics of publication,0
we explore 50 australian aboriginal accounts of lunar and solar eclipses to determine how aboriginal groups understood this phenomenon we summarise the literature on aboriginal references to eclipses showing that many aboriginal groups viewed eclipses negatively frequently associating them with bad omens evil magic disease blood and death in many communities elders or medicine men were believed to have the ability to control or avert eclipses by magical means solidifying their role as provider and protector within the community we also show that many aboriginal groups understood the motions of the sunearthmoon system the connection between the lunar phases and tides and acknowledged that solar eclipses were caused by the moon blocking the sun,0
this essay offers a metalevel analysis in the sociology and history of physics in the context of the socalled arrow of time problem or two times problem which asserts that the empirically observed directionality of time is in conflict with physical theory i argue that there is actually no necessary conflict between physics and the arrow of time and that the observed directionality of time is perfectly consistent with physics unconstrained by certain optional metaphysical epistemological and methodological beliefs and practices characterizing the conventional or received view,0
it is shown that an aspect of the process of individuation may be thought of as a fuzzy set the process of individuation has been interpreted as a twovalued problem in the history of philosophy in this work i intend to show that such a process in its psychosocial aspect is better understood in terms of a fuzzy set characterized by a continuum membership function according to this perspective species and their members present different degrees of individuation such degrees are measured from the membership function of the psychosocial process of individuation thus a social analysis is suggested by using this approach in human societies,0
we present 25 accounts of comets from 40 australian aboriginal communities citing both supernatural perceptions of comets and historical accounts of bright comets historical and ethnographic descriptions include the great comets of 1843 1861 1901 1910 and 1927 we describe the perceptions of comets in aboriginal societies and show that they are typically associated with fear death omens malevolent spirits and evil magic consistent with many cultures around the world we also provide a list of words for comets in 16 different aboriginal languages,0
the origins of sociophysics are discussed from a personal testimony i trace back its history to the late seventies my twenty years of activities and research to establish and promote the field are reviewed in particular the conflicting nature of sociophysics with the physics community is revealed from my own experience recent presentations of a supposed natural growth from social sciences are criticized,0
mnay models situated in the current research landscape of modelling and simulating social processes have roots in physics this is visible in the name of specialties as econophysics or sociophysics this chapter describes the history of knowledge transfer from physics in particular physics of selforganization and evolution to the social sciences we discuss why physicists felt called to describe social processes across models and simulations the question how to explain the emergence of something new is the most intriguing one we present one model approach to this problem and introduce a game evolino inviting a larger audience to get acquainted with abstract evolutiontheory approaches to describe the quest for new ideas,0
nanotechnology as a social concept and investment focal point has drawn much attention here we consider the place of nanotechnology in the second great technological revolution of mankind that began some 200 years ago the socalled nanotechnology revolution represents both a continuation of prior science and technology trends and a reawakening to the benefits of significant investment in fundamental research we consider the role the military might play in the development of nanotechnology innovations nanotechnologys context in the history of technology and the global competition to lead the next technological revolution,0
the recently published book lise meitner and the dawn of the nuclear age by patricia rife boston birkhuser 1999 is reviewed in an essay for the lay audience meitner was a leading nuclear physicist at the time that the nucleus was the most exciting frontier of science to establish her career she had to overcome daunting prejudices against women in science and academia being of jewish origin in germany in the 1930s she narrowly escaped certain disaster meitner was a crucial participant in the discovery of nuclear fission yet did not share in the nobel prize that her collaborator otto hahn received in 1945 how these events came about how they were intertwined with contemporary history and how they fit into the evolution of meitners social conscience and her abhorrence of war are some of the fascinating subjects discussed in the book and reviewed in this essay,0
we analyse a coupled dataset collecting the mobile phone communications and bank transactions history of a large number of individuals living in a latin american country after mapping the social structure and introducing indicators of socioeconomic status demographic features and purchasing habits of individuals we show that typical consumption patterns are strongly correlated with identified socioeconomic classes leading to patterns of stratification in the social structure in addition we measure correlations between merchant categories and introduce a correlation network which emerges with a meaningful community structure we detect multivariate relations between merchant categories and show correlations in purchasing habits of individuals finally by analysing individual consumption histories we detect dynamical patterns in purchase behaviour and their correlations with the socioeconomic status demographic characters and the egocentric social network of individuals our work provides novel and detailed insight into the relations between social and consuming behaviour with potential applications in resource allocation marketing and recommendation system design,0
a formal theory of meaning the process of knowledge accumulation as multiplicative chaos is proposed the epistemological process is understood as the process of subjective extraction of some knowledge from the incoming information the concepts of nonsense are introduced as a meaning that has a minimum value equal to one and the level of intelligence as a geometric mean of the cumulative meaning the thesis of the multiplicativity of meaning its polymorphism is substantiated and numerous examples from world history are provided by analogy with classical thermodynamics three laws of thermodynamics of meaning are postulated estimates of the cumulative meaning when the comprehension of information multiplicative cascade is a random process with given statistical characteristics are carried out,0
we present evidence that the boorong aboriginal people of northwestern victoria observed the great eruption of eta  carinae in the nineteenth century and incorporated the event into their oral traditions we identify this star as well as others not specifically identified by name using descriptive material presented in the 1858 paper by william edward stanbridge in conjunction with early southern star catalogues this identification of a transient astronomical event supports the assertion that aboriginal oral traditions are dynamic and evolving and not static this is the only definitive indigenous record of  carinaes outburst identified in the literature to date,0
diversity equity and inclusion are the science leadership issues of our time as our nation and the field of astronomy grow more diverse we find ourselves in a position of enormous potential and opportunity a multitude of studies show how groups of diverse individuals with differing viewpoints outperform homogenous groups to find solutions that are more innovative creative and responsive to complex problems and promote higherorder thinking amongst the group research specifically into publications also shows that diverse author groups publish in higher quality journals and receive higher citation rates as we welcome more diverse individuals into astronomy we therefore find ourselves in a position of potential never before seen in the history of science with the best minds and most diverse perspectives our field has ever seen despite this enormous growing potential and the proven power of diversity the demographics of our field are not keeping pace with the changing demographics of the nation and astronomers of colour women lgbt individuals people with disabilities and those with more than one of these identities still face chilly or hostile work environments in the sciences if we are to fully support all astronomers and students in reaching their full scientific potential we must recognize that most of us tend to overestimate our ability to support our minoritized students and colleagues that our formal education system fails to prepare us for working in a multicultural environment and that most of us need some kind of training to help us know what we dont know and fill those gaps in our education to that end diversity and inclusion training for aas council and leadership heads of astronomy departments and faculty search committees should be a basic requirement throughout our field,0
for several decades a portrait of johannes kepler has been widely circulating among professional astronomers scientific and academic institutions and the general public despite its provenance and identification having been questioned in the early part of the last century this painting has reached iconic status we review its history from its first mention in the literature in the 1870s to a published but virtually unknown judgment of competent art experts of the 1920s that the work is in fact an early nineteenth century forgery we display the painting in context with other more secure portraits and suggest that if it is based on anything the painting may derive from the well known portrait from life of michael mstlin this correction takes on certain urgency since 2021 is the 450th anniversary of keplers birth,0
we first recall fundamentals of elementary climate physics solar constant radiative balance greenhouse effect astronomical parameters of the climate theory of milankovitch without disputing the analyzes of climatologists and the famous keeling curve revealing in an indisputable way the increase in co2 in the atmosphere since the industrial revolution we nevertheless insist on the main contributor to the greenhouse effect which is as we know water vapor faced with the difficulties that there will be in imposing zerocarbon policies everywhere in the world and especially in developing countries we show that it would perhaps be in our interest to act on soil drought which amounts in fact to being interested in the clouds the decrease in cloud cover due to a lack of water fixation in the soil in fact increases the general temperature and therefore the greenhouse effect acting on co2 will always have in this context much less effect than acting on water vapor even indirectly despite the difficulty of making this action sustainable due to the balance of atmospheric water vapor and the oceans it would be in our interest not to neglect this path and also possibly increase forest cover for this purpose given the problems of setting up zerocarbon policy on a global scale in desperation one can also consider protecting the earth with an artificial dust cloud,0
recent developments in cosmology indicate that every history having a nonzero probability is realized in infinitely many distinct regions of spacetime thus it appears that the universe contains infinitely many civilizations exactly like our own as well as infinitely many civilizations that differ from our own in any way permitted by physical laws we explore the implications of this conclusion for ethical theory and for the doomsday argument in the infinite universe we find that the doomsday argument applies only to effects which change the average lifetime of all civilizations and not those which affect our civilization alone,0
the municipal actors of the catalan capital wish to obtain a reversal of the negative representations concerning the lack of attractiveness of the district of poblenou considered as one of the poorest and most marginalized districts of the city while having as identity the expression of manchester southern europe recalling its role as a factory for spain and an emblem of the industrial revolution for the iberian peninsula conceptualization programming and construction of a new image of the city through this large urban wasteland are made concrete thanks to an urban marketing operation the specifications allow the construction of innovative buildings with resolutely contemporary architecture breaking with the regulations in force on the limitation of the height of skyscrapers in order to make this pericentral territory visible and attractive catalan society in the 21st century has produced a fragmented and dominating neocapitalist space through a thoughtless real estate frenzy and complex land games walking through the streets of poblenou one is forced to think of the work of henri lefebvre on the production of space and the three orders of understanding the design of an urban composition here the city has become a stake and the place of economic growth based on innovation finally neighborhood life is in complete mutation which seems for this territory to leave a chance for a long time on the sidelines we present an economic analysis grid of a cluster with graphs and statistics,0
the possibility that experiments at highenergy accelerators could create new forms of matter that would ultimately destroy the earth has been considered several times in the past quarter century one consequence of the earliest of these disaster scenarios was that the authors of a 1993 article in physics today who reviewed the experiments that had been carried out at the bevalac at lawrence berkeley laboratory were placed on the fbis unabomber watch list later concerns that experiments at the relativistic heavy ion collider at brookhaven national laboratory might create mini black holes or nuggets of stable strange quark matter resulted in a flurry of articles in the popular press i discuss this history as well as richard a posners provocative analysis and recommendations on how to deal with such scientific risks i conclude that better communication between scientists and nonscientists would serve to assuage unreasonable fears and focus attention on truly serious potential threats to humankind,0
philosophy has nurtured fundamental science by asking the right questions this scientific growth has fuelled research in various domains and introduced diverse disciplines nanotechnology is an interdisciplinary domain with numerous applications ranging from medical diagnostics and food technology to electronics and psychology exploring nanotechnologys philosophical and social perspective can better understand these domains and may open new doors for research this review addresses philosophical and other aspects of nanotechnology such as history definitions vision language laws politics and ethics this is an attempt to equip anyone in the field of nanotechnology with philosophical and social insights we expect this review to provide an introductory understanding of philosophy and other aspects to the nanotechnologists which are usually excluded from their degree curriculum,0
optimization and expansion are two modes of staged evolution of complex systems where macroscopic observables change at a decreasing respectively increasing rate a prime example of evolutionary expansion gross domestic product gdp time series gauge economic activities in changing societal structures and the accelerating trend of their growth probably reflects a manyfold increase of the human interactions that drive change we show how optimization and expansion can coexist by replacing wall clock time t as independent variable with a measure of human interactions intensity  our analysis of eight centuries of yearly gdp data from three regions of western europe is carried out in two steps first a monte carlo algorithm is used to fit the gdp data to a piecewise continuous function comprising a sequence of exponentials with different exponents in a second step gdp data are plotted vs  and shown to display two logarithmic regimes both decelerating that are joined by a powerlaw crossover period we connect the end of the first regime and the beginning of the second with the dawn of the industrial revolution and the societal impact of new transport communication and production technologies that became widely available after world war i we conclude that wealth evolution in terms of  is a decelerating process with the hallmarks of record dynamics optimization,0
we study the statistics of citations from all physical review journals for the 110year period 1893 until 2003 in addition to characterizing the citation distribution and identifying publications with the highest citation impact we investigate how citations evolve with time there is a positive correlation between the number of citations to a paper and the average age of citations citations from a publication have an exponentially decaying age distribution that is old papers tend to not get cited in contrast the citations to a publication are consistent with a powerlaw age distribution with an exponent close to 1 over a time range of 2 20 years we also identify a number of stronglycorrelated citation bursts and other dramatic features in the time history of citations to individual publications,0
the european organization for nuclear research cern in geneva is renowned for operating the worlds largest particle accelerator and is often regarded as a model of highprofile international collaboration less well known however is a key episode from the late 1950s when cern was confronted with the research priorities of similar organisations the issue centred on a cernsponsored study group on controlled thermonuclear fusion which brought together scientists from cern member states as well as representatives from the european atomic energy community euratom the european nuclear energy agency enea and the us atomic energy commission aec while the cern study group on fusion problems succeeded in creating an international network for exchanging reports and coordinating projects to avoid duplication it ultimately failed to establish joint fusion research programmes this article explores the reasons behind this outcome to provide insights into intergovernmental power dynamics underlying competition and how these factors favoured the creation of a new fusion research institution in the uk the culham laboratory in doing so the article contributes to a deeper understanding of the role of science in european integration while also highlighting that cerns involvement in applicationoriented research remains an underexplored aspect of its history,0
interstellar exploration will advance human knowledge and culture in multiple ways scientifically it will advance our understanding of the interstellar medium stellar astrophysics planetary science and astrobiology in addition significant societal and cultural benefits will result from a programme of interstellar exploration and colonisation most important will be the cultural stimuli resulting from expanding the horizons of human experience and increased opportunities for the spread and diversification of life and culture through the galaxy ultimately a programme of interstellar exploration may be the only way for human and posthuman societies to avoid the intellectual stagnation predicted for the end of history,0
this article shows the importance that has had the scientific research the technological development and the innovation processes in increasing the lethality of the available weapons during the last century a set of initiatives promoted by the scientific community to stop the nuclear arms race that threatened the continuation of life on the planet is described at this point a thorough survey of the texts and proposals of hippocratic oaths for scientists presented at different epochs is made it is observed that the interest in linking ethical aspects with science and technology issues shows an exponential growth behavior since the second world war it is shown how the several proposals of oaths and ethical commitments for scientists engineers and technologists are disseminated following a logistic growth behavior in the same manner as a disembodied technology in a particular niche the data analysis shows that there is a coincidence between the maximum rate of proposals and the historical moment at which the world had deployed the largest number of nuclear warheads 70586 as well as the largest world military expenditures in history usd 1485000000000 subsequently the origin of the hippocratic oath for scientists used for more than two decades in graduation ceremonies at the faculty of exact and natural sciences of the university of buenos aires is analyzed and linked with the historical circumstances of its birth,0
until this century the number of working female scientists has been indeterminate prevailing wisdom indicates that women historically have not excelled in the mathematics and sciences for various reasons these range from societal pressures to marry and bear children to a lack of systematic scientific education for females to the lack of opportunity in institutions and industry women until comparatively recently were not in control of their own financial lives and were therefore dependent upon the goodwill of a husband father or brother should they attempt to enter academic life many times their accomplishments and the accomplishments of these collaborating relatives become so merged as to become indistinguishable nevertheless with all these factors working against them history records an occasional bright star their diaries memoirs and correspondence detail both the similarities and the differences between their roles and the roles of modern female scientists in their own minds and in the estimation of those closest to them there exists in science a dichotomy between the desire to understand ones universe and the desire to change the understanding of others these two fundamental desires lead future scientists to the discipline historically the role of women has largely been interpretive rather than innovative by their own estimation but is interpretation the natural role of women scientists,0
the american scientific community is reeling from funding cuts and policy directives that will debilitate scientific research and education the underlying hostilities fueling these attacks have intensified in recent years as the covid19 pandemic increased suspicion of scientific experts and the institutional embrace of diversity equity and inclusion dei policies in 2020 prompted a backlash along longstanding political fault lines under the banner of antielitism opponents of science and dei have formed a coalition that sees attacks on higher education as a strategic means to achieve their political ends while some of their arguments contain legitimate criticisms academics must resist these attacks that seek to dismantle higher education altogether instead we should engage the public in our research process build a scientific practice representative of and accountable to the communities we serve and interrogate the aims of our work by critically studying the history of science,0
mathematical proofs are both paradigms of certainty and some of the most explicitlyjustified arguments that we have in the cultural record their very explicitness however leads to a paradox because the probability of error grows exponentially as the argument expands when a mathematician encounters a proof how does she come to believe it here we show that under a cognitivelyplausible belief formation mechanism combining deductive and abductive reasoning belief in mathematical arguments can undergo what we call an epistemic phase transition a dramatic and rapidlypropagating jump from uncertainty to nearcomplete confidence at reasonable levels of claimtoclaim error rates to show this we analyze an unusual dataset of fortyeight machineaided proofs from the formalized reasoning system coq including major theorems ranging from ancient to 21st century mathematics along with five handconstructed cases including euclid apollonius hernsteins topics in algebra and andrew wiless proof of fermats last theorem our results bear both on recent work in the history and philosophy of mathematics on how we understand proofs and on a question basic to cognitive science of how we justify complex beliefs,0
the technological discoveries and developments since dawn of civilization that resulted in the modern wristwatch are linked to the evolution of science itself a history of over 6000 years filled with amazing technical prowess since the emergence of the first cities in mesopotamia established by the umer civilization usage of gears for timekeeping has its origin in the islamic golden age about 1000 years ago although gears have been known for over 2000 years such as found in the antikythera mechanism only in the seventeenth century springs started to be used in clock making in the eighteenth century the amazing textittourbillon was designed and built to increase clock accuracy in the nineteenth century the tuning fork was used for the first time as timebase wristwatches started to become popular in the beginning of the twentieth century later in the second half of the twentieth century the first electronic wristwatch was designed and produced which brings us to the curious case of the bulova textitaccutron caliber 214 the first transistorized wristwatch another marvel of technological innovation and craftsmanship whose operation is frequently misunderstood in this paper the historical evolution of timekeeping is presented the goal is to show the early connection between science and engineering in the development of timekeeping devices this linked development only became common along the twentieth century and beyond,0
in this paper we present the results of the study on the development of social network analysis sna discipline and its evolution over time using the analysis of bibliographic networks the dataset consists of articles from the web of science clarivate analytics database and those published in the main journals in the field 70000 publications created by searching for the key word social network from the collected data we constructed several networks citation and twomode linking publications with authors keywords and journals analyzing the obtained networks we evaluated the trends in the fields growth noted the most cited works created a list of authors and journals with the largest amount of works and extracted the most often used keywords in the sna field next using the search path count approach we extracted the main path keyroute paths and link islands in the citation network based on the probabilistic flow node values we identified the most important articles our results show that authors from the social sciences who were most active through the whole history of the field development experienced the invasion of physicists from 2000s however starting from the 2010s a new very active group of animal social network analysis has emerged,0
the impact of machine learning ml algorithms in the age of big data and platform capitalism has not spared scientific research in academia in this work we will analyse the use of ml in fundamental physics and its relationship to other cases that directly affect society we will deal with different aspects of the issue from a bibliometric analysis of the publications to a detailed discussion of the literature to an overview on the productive and working context inside and outside academia the analysis will be conducted on the basis of three key elements the nonneutrality of science understood as its intrinsic relationship with history and society the nonneutrality of the algorithms in the sense of the presence of elements that depend on the choices of the programmer which cannot be eliminated whatever the technological progress is the problematic nature of a paradigm shift in favour of a datadriven science and society the deconstruction of the presumed universality of scientific thought from the inside becomes in this perspective a necessary first step also for any social and political discussion this is the subject of this work in the case study of ml,0
the dissertation brings together approaches across the fields of physics critical theory literary studies philosophy of physics sociology of science and history of science to synthesize a hybrid approach for instigating more rigorous and intense crossdisciplinary interrogations between the sciences and the humanities there are two levels of conversations going on in the dissertation at the first level the discussion is centered on a critical historiography and philosophical implications of the discovery higgs boson in relation to its position at the intersection of old current and the potential for new possibilities in quantum physics i then position my findings on the higgs boson in connection to the doubleslit experiment that represents foundational inquiries into quantum physics to demonstrate the bridge between fundamental physics and high energy particle physics the conceptualization of the variants of the doubleslit experiment informs the aforementioned critical comparisons at the second level of the conversation theories are produced from a close study of the physics objects as speculative engine for new knowledge generation that are then reconceptualized and rearticulated for extrapolation into the speculative ontology of hard science fiction particularly the hard science fiction written with the double intent of speaking to the science while producing imaginative and socially conscious science through the literary affordances of science fiction the works of science fiction examined here demonstrate the tension between the internal values of physics in the practice of theory and experiment and questions on ethics culture and morality,0
feyerabend frequently discussed physics he also referred to the history of the subject when motivating his philosophy of science alas as some examples show his understanding of physics remained superficial in this respect feyerabend is like popper the difference being his selfcriticism later on and the much more tolerant attitude toward the allowance of methods quite generally partly due to the complexity of the formalism and the new challenges of their findings which left philosophy proper at a loss physicists have attempted to developed their own meaning of their subject for instance in recent years the interpretation of quantum mechanics has stimulated a new type of experimental philosophy which seeks to operationalize emerging philosophical issues issues which are incomprehensible for most philosophers in this respect physics often appears to be a continuation of philosophy by other means yet feyerabend has also expressed profound insights into the possibilities for the progress of physics a legacy which remains to be implemented in the times to come the conquest of abundance the richness of reality the many worlds which still await discovery and the vast openness of the physical universe,0
we describe the wartime challenges associated with the rapid developments in plutonium chemistry and metallurgy that were necessary to produce the core of the trinity device beginning with microgram quantities of plutonium metal late in 1943 initial measurements showed a wide and confusing variance in density and other properties these confusing results were the first clues to the astounding complexity of plutonium as this complexity was revealed it introduced new challenges for the fabrication of kilogramscale parts in a remarkable period from january 1944 to june 1945 manhattan project scientists made rapid progress in understanding plutonium chemistry and metallurgy by early 1945 they had discovered five of the six ambientpressure phases of unalloyed plutonium and reported the density of these phases to within a value of 01 gcm3 of those accepted today they solved the stability problem introduced by these phases with a rapid alloy development program that ultimately identified gallium as the preferred element to stabilize the deltaphase producing a plutonium alloy still of scientific and technical interest today we conclude with a description of postwar developments in these areas including applications of wartime plutonium metallurgy to civilian applications in nuclear reactors we dedicate this paper to the memory of ed hammel the manhattan project plutonium metallurgist whose previous description and documentation of plutonium history during the war has been essential in our research,0
social media platforms often assume that users can selfcorrect against misinformation however social media users are not equally susceptible to all misinformation as their biases influence what types of misinformation might thrive and who might be at risk we call diverse misinformation the complex relationships between human biases and demographics represented in misinformation to investigate how users biases impact their susceptibility and their ability to correct each other we analyze classification of deepfakes as a type of diverse misinformation we chose deepfakes as a case study for three reasons 1 their classification as misinformation is more objective 2 we can control the demographics of the personas presented 3 deepfakes are a realworld concern with associated harms that must be better understood our paper presents an observational survey n2016 where participants are exposed to videos and asked questions about their attributes not knowing some might be deepfakes our analysis investigates the extent to which different users are duped and which perceived demographics of deepfake personas tend to mislead we find that accuracy varies by demographics and participants are generally better at classifying videos that match them we extrapolate from these results to understand the potential populationlevel impacts of these biases using a mathematical model of the interplay between diverse misinformation and crowd correction our model suggests that diverse contacts might provide herd correction where friends can protect each other altogether human biases and the attributes of misinformation matter greatly but having a diverse social group may help reduce susceptibility to misinformation,0
using current empirical data from 10000 bce to 2023 ce we reexamine a hyperbolic pattern of human population growth which was identified by von foerster et al in 1960 with a predicted singularity in 2026 we find that human population initially grew exponentially in time as ntpropto ett with t2080 years this growth then gradually evolved to be superexponential with a form similar to the bose function in statistical physics around 1700 population growth further accelerated entering the hyperbolic regime as ntproptotst1 with the extrapolated singularity year ts2030 which is close to the prediction by von foerster et al we attribute the switch from the superexponential to the hyperbolic regime to the onset of the industrial revolution and the transition to massive use of fossil fuels this claim is supported by a linear relation that we find between the increase in the atmospheric co2 level and population from 1700 to 2000 in the 21st century we observe that the inverse population curve 1nt deviates from a straight line and follows a pattern of avoided crossing described by the square root of the lorentzian function thus instead of a singularity we predict a peak in human population at ts2030 of the time width 32 years we also find that the increase in co2 level since 1700 is well fitted by rm arccot with f40 years which implies a peak in the annual co2 emissions at the same year ts2030,0
we consider the attitude of astronomers in argentina in connection with the new problems posed by relativity theory before and after gr was presented we begin considering the sequence of technical publications that appeared and use it to attempt to identify who were the relativity leaders and authors in the argentina scientific community of the 19101920s among them there are natives of argentina permanent resident scientists and occasional foreign visitors they are either academic scientists or high school teachers we leave aside the it philosophers and the it aficionados we discuss the scientific facts and publications they handled the modernity of their information and the language they use to transmit their ideas finally we consider astronomers proper first charles perrine an astronomer interested in astrophysics contracted by the government of argentina in the usa as director of its main observatory he became interested in testing the possible deflection of light rays by the sun towards 1912 his argentine expedition was the first to attempt that test perhaps perrine was not so much interested in relativity as in testing the particular astronomical effects it predicted in any case he attempted the test with the acquiescence and financial support of the argentine state and as a leading member of its official scientific elite we contrast his very specific and strictly scientific efforts with those of our second astronomer jos ubach sj a secondary school teacher of science at a leading buenos aires catholic school who reported in response to eddingtons expedition finally our third astronomer is flix aguilar who made an effort to contribute to the public understanding of einsteins theories in 1924 when einsteins visit to argentina had become a certainty,0
decisions regarding housing transportation and resource allocation would all benefit from accurate smallarea population forecasts while various triedandtested forecast methods exist at regional scales developing an accurate neighborhoodscale forecast remains a challenge partly due to complex drivers of residential choice ranging from housing policies to social preferences and economic status that cumulatively cause drastic neighborhoodscale segregation here we show how to forecast the dynamics of neighborhoodscale demographics by extending a novel statistical physics approach called densityfunctional fluctuation theory dfft to multicomponent timedependent systems in particular this technique observes the fluctuations in neighborhoodscale demographics to extract effective drivers of segregation as a demonstration we simulate a segregated city using a schellingtype segregation model and found that dfft accurately predicts how a cityscale demographic change trickles down to block scales should these results extend to actual human populations dfft could capitalize on the recent advances in demographic data collection and regionalscale forecasts to improve upon current smallarea population forecasts,0
motivated by recent successes in modelbased preelection polling we propose a kinetic model for opinion formation which includes voter demographics and socioeconomic factors like age sex ethnicity education level income and other measurable factors like behaviour in previous elections or referenda as a key driver in the opinion formation dynamics the model is based on toscanis kinetic opinion formation model and the leaderfollower model of dring et al and leads to a system of coupled boltzmanntype equations and associated approximate fokkerplancktype systems numerical examples using data from general elections in the united kingdom show the effect different demographics have on the opinion formation process and the outcome of elections,0
we argue that taxonomical concept development is vital for planetary science as in all branches of science but its importance has been obscured by unique historical developments the literature shows that the concept of planet developed by scientists during the copernican revolution was theoryladen and pragmatic for science it included both primaries and satellites as planets due to their common intrinsic geological characteristics about two centuries later the nonscientific public had just adopted heliocentrism and was motivated to preserve elements of geocentrism including teleology and the assumptions of astrology this motivated development of a folk concept of planet that contradicted the scientific view the folk taxonomy was based on what an object orbits making satellites out to be nonplanets and ignoring most asteroids astronomers continued to keep primaries and moons classed together as planets and continued teaching that taxonomy until the 1920s the astronomical community lost interest in planets ca 1910 to 1955 and during that period complacently accepted the folk concept enough time has now elapsed so that modern astronomers forgot this history and rewrote it to claim that the folk taxonomy is the one that was created by the copernican scientists starting ca 1960 when spacecraft missions were developed to send back detailed new data there was an explosion of publishing about planets including the satellites leading to revival of the copernican planet concept we present evidence that taxonomical alignment with geological complexity is the most useful scientific taxonomy for planets it is this complexity of both primary and secondary planets that is a key part of the chain of origins for life in the cosmos,0
the christy gadget is the informal name for the plutonium device detonated in the trinity test on july 16 1945 in september 1944 robert christy working in the theoretical implosion group proposed a novel concept that altered the design of the nuclear core in fat man while scientists originally intended to use a hollow sphere of plutonium this design entailed substantial risk due to the likelihood of asymmetries resulting from implosion christy proposed changing the design to a solid sphere of plutonium with a modulated neutron source and the design was eventually adopted tested at trinity and used in the attack on nagasaki while there is no question regarding the important role that christy played in demonstrating its feasibility as a reliable design there is a debate as to who initially proposed the idea though most sources have attributed this invention to christy some historical sources have attributed credit to christys group leader rudolf peierls or indeed other scientists this paper seeks to outline and resolve this dispute we present new unclassified evidence extracted from previously unavailable sources to unclassified audiences from the national security research center archives at los alamos national laboratory this evidence consists of 19451946 patent documentation oral history interview tapes of christy and peierls and monthly 1944 progress reports from the theoretical division though christy and peierls share joint credit on the patent both christys and peierls words and writings together with sources from hans bethe and edward teller support the traditional view that christy was indeed the originator of the idea while christy does deserve the majority of the credit for the invention and design we acknowledge the important role peierls and von neumann played in its development,0
this article is a commentary on the verdict of the laquila six the group of bureaucrats and scientists tried by an italian court as a result of their public statements in advance of the quake of 2009 apr 6 that left the city in ruins and cause more than 300 deaths it was not the worst such catastrophic event in recent italian history but it was one of if not the worst failures of risk assessment and preventive action the six were found guilty and condemned by a first level of the justice system to substantial prison terms the outcry provoked by the verdict in the world press and the international scientific community has fueled the already fiery debate over whether the six should have been tried at all they have been presented as martyrs to science being treated as scapegoats by a scientifically illiterate justice system and inflamed local population for not being able to perform the impossible predict the event petitions of support have been drafted and signed by thousands of working scientists and technical experts in many fields excoriating the court and the country for such an outrage against the scientific community often accompanied by ominous warnings about the chilling effect this will have on the availability of expert advice in times of need my purpose in this essay is to explain why this view of the events of the trial is misguided however well intentioned and misinformed,0
it is seldom acknowledged the tremendous burden that the nuclear age leaves on future generations and the environment for an extremely long time nuclear processes and products are activated at energies millions of times higher than the energies of chemical processes and consequently they cannot be eliminated by the natural environment on earth so it turns out that hundreds of nuclear tests performed in the atmosphere left a huge radioactive contamination rosalie bertell estimated 1300 millions victims of the nuclear age civil nuclear programs have produced enormous quantities of radioactive waste whose final disposal has not been solved by any country decommissioning of tens of shut down nuclear plants shall involve costs which were underestimated in the past spent nuclear fuel accumulates in decontamination pools or in dry cask storage but no final storage has been carried out yet radioactivity of spent fuel will last for tens of thousand years military nuclear programs leave besides almost 15000 nuclear warheads approximately 1300 metric tons of plutonium even mining of natural uranium was and is carried out mainly by poor and exploited populations which suffer serious health consequences paradoxically enough or maybe not french territory itself is widely contaminated all these facts have been downplayed during the whole history of the nuclear age future generations shall not be grateful,0
we analyze a coupled anonymized dataset collecting the mobile phone communication and bank transactions history of a large number of individuals after mapping the social structure and introducing indicators of socioeconomic status demographic features and purchasing habits of individuals we show that typical consumption patterns are strongly correlated with identified socioeconomic classes leading to patterns of stratification in the social structure in addition we measure correlations between merchant categories and introduce a correlation network which emerges with a meaningful community structure we detect multivariate relations between merchant categories and show correlations in purchasing habits of individuals our work provides novel and detailed insight into the relations between social and consuming behaviour with potential applications in recommendation system design,0
we use aggregated data from facebook to show that covid19 is more likely to spread between regions with stronger social network connections areas with more social ties to two early covid19 hotspots westchester county ny in the us and lodi province in italy generally had more confirmed covid19 cases by the end of march these relationships hold after controlling for geographic distance to the hotspots as well as the population density and demographics of the regions as the pandemic progressed in the us a countys social proximity to recent covid19 cases and deaths predicts future outbreaks over and above physical proximity and demographics in part due to its broad coverage social connectedness data provides additional predictive power to measures based on smartphone location or online search data these results suggest that data from online social networks can be useful to epidemiologists and others hoping to forecast the spread of communicable diseases such as covid19,0
the socialnetworking revolution of late eg with the advent of social media facebook and the like has been propelling the crusade to elucidate the embedded networks that underlie economic activity an unexampled synthesis of network science and economics uncovers how the web of human interactions spurred by familiarity and similarity could potentially induce the ups and downs ever so common to our economy zeroing in on the millionstrong global industry known as multilevel marketing this study finds that such a sociallypowered enterprise can only work stably through discrimination about who to make entrepreneurial connections with,0
since the industrial revolution accelerated urban growth has overflown administrative divisions merged cities into large built extensions and blurred the boundaries between urban and rural landuses these traits present in most of contemporary metropolis complicate the definition of cities a crucial issue considering that objective and comparable metrics are the basic inputs needed for the planning and design of sustainable urban environments in this context city definitions that respond to administrative or political criteria usually overlook human dynamics a key factor that could help to make cities comparable across the urban fabric of diverse social cultural and economic realities using a technique based on the spectral analysis of complex networks we rank places in 11 of the major chilean urban regions from a highresolution human mobility dataset official origindestination od surveys we propose a method for further distinguishing urban and rural landuses within these regions by means of a network centrality measure from which we construct a spectre of geographic places this spectre constructed from the ranking of locations as measured by their approximate number of embedded human flows allows us to probe several urban boundaries from the analysis of the urban scaling exponent of trips in relation to the population across these city delineations we identify two clearly distinct scaling regimes occurring in urban and rural areas the comparison of our results with land cover derived from remote sensing suggests that for the case of trips the scaling exponent in urban areas is close to linear we conclude with estimations for wellformed cities in the chilean urban system which according to our analysis could emerge from clusters composed by places that capture at least 138 trips over the expectation of the underlying mobility network,0
computermediated communication is driving fundamental changes in the nature of written language we investigate these changes by statistical analysis of a dataset comprising 107 million twitter messages authored by 27 million unique user accounts using a latent vector autoregressive model to aggregate across thousands of words we identify highlevel patterns in diffusion of linguistic change over the united states our model is robust to unpredictable changes in twitters sampling rate and provides a probabilistic characterization of the relationship of macroscale linguistic influence to a set of demographic and geographic predictors the results of this analysis offer support for prior arguments that focus on geographical proximity and population size however demographic similarity especially with regard to race plays an even more central role as cities with similar racial demographics are far more likely to share linguistic influence rather than moving towards a single unified netspeak dialect language evolution in computermediated communication reproduces existing fault lines in spoken american english,0
in 2018 in response to the proposed elimination of physics at a predominately hispanic and socioeconomically disadvantaged sed high school the northern californianevada chapter of the aapt investigated school demographics and their effect on physics offerings in public high schools in our region as access was a key issue the focus was on public noncharter high schools which are free to students and do not require winning a lottery for attendance as reported previously the data revealed that the percentage of hispanic students and the percentage of sed students at a high school are highly correlated r2060 additionally these factors could be used as predictors of a schools physics offerings to determine if the disparities in course offerings extended through other advanced placement ap stem classes the data was further analyzed revealing that as the popularity of an ap exam drops so do the relative odds of it being offered when comparing schools with different demographics a northern california public high school student is much more likely to get a strong selection of ap stem classes if their school serves an affluent nonhispanic student majority rather than mostly poor hispanic students,0
the recent history of respiratory pathogen epidemics including those caused by influenza and sarscov2 has highlighted the urgent need for advanced modeling approaches that can accurately capture heterogeneous disease dynamics and outcomes at the national scale thereby enhancing the effectiveness of resource allocation and decisionmaking in this paper we describe epicast 20 an agentbased model that utilizes a highly detailed synthetic population and highperformance computing techniques to simulate respiratory pathogen transmission across the entire united states this model replicates the contact patterns of over 320 million agents as they engage in daily activities at school work and within their communities epicast 20 supports vaccination and an array of nonpharmaceutical interventions that can be promoted or relaxed via highly granular user specified policies we illustrate the models capabilities using a wide range of outbreak scenarios highlighting the models varied dynamics as well as its extensive support for policy exploration this model provides a robust platform for conducting what if scenario analysis and providing insights into potential strategies for mitigating the impacts of infectious diseases,0
we propose a simple dynamical model of the formation of production networks among monopolistically competitive firms the model subsumes the standard general equilibrium approach  la arrowdebreu but displays a wide set of potential dynamic behaviors it robustly reproduces key stylized facts of firms demographics our main result is that competition between intermediate good producers generically leads to the emergence of scalefree production networks,0
the astronomy genealogy project astrogen a project of the historical astronomy division of the american astronomical society aas will soon appear on the aas website ultimately it will list the worlds astronomers with their highest degrees theses for those who wrote them academic advisors supervisors universities and links to the astronomers or their obituaries their theses when online and more at present the astrogen team is working on those who earned doctorates with astronomyrelated theses we show what can be earned already with just ten countries essentially completed,0
peer review is a process designed to produce a fair assessment of research quality before the publication of scholarly work in a journal demographics nepotism and seniority have been all shown to affect reviewer behavior suggesting the most common singleblind review method or the less common open review method might be biased a survey of current research indicates that doubleblind review offers a solution to many biases stemming from authors gender seniority or location without imposing any significant downsides,0
opinion polls mediated through a social network can give us in addition to usual demographics data like age gender and geographic location a friendship structure between voters and the temporal dynamics of their activity during the voting process using a facebook application we collected friendship relationships demographics and votes of over ten thousand users on the referendum on the definition of marriage in croatia held on 1st of december 2013 we also collected data on online news articles mentioning our application publication of these articles align closely with large peaks of voting activity indicating that these external events have a crucial influence in engaging the voters also existence of strongly connected friendship communities where majority of users vote during short time period and the fact that majority of users in general tend to friend users that voted the same suggest that peer influence also has its role in engaging the voters as we are not able to track activity of our users at all times and we do not know their motivations for expressing their votes through our application the question is whether we can infer peer and external influence using friendship network of users and the times of their voting we propose a new method for estimation of magnitude of peer and external influence in friendship network and demonstrate its validity on both simulated and actual data,0
the second quantum revolution has been producing groundbreaking scientific and technological outputs since the early 2000s however the scientific literature on the impact of this revolution on the industry specifically on startups is limited in this paper we present a landscaping study with a gathered dataset of 441 companies from 42 countries that we identify as quantum startups meaning that they mainly focus on quantum technologies qt as their primary priority business we answer the following questions 1 what are the temporal and geographical distributions of the quantum startups 2 how can we categorize them and how are these categories populated 3 are there any patterns that we can derive from empirical data on trends we found that more than 92 of these companies have been founded within the last 10 years and more than 50 of them are located in the us the uk and canada we categorized the qt startups into six fields i complementary technologies ii quantum computing hardware iii quantum computing softwareapplicationsimulation iv quantum cryptographycommunication v quantum sensing and metrology and vi supporting companies and analyzed the population of each field both for countries and temporally finally we argue that low levels of quantum startup activity in a country might be an indicator of a national initiative to be adopted afterwards which later sees both an increase in the number of startups and a diversification of activity in different qt fields,0
student belongingness is important for successful study paths and group work forms an important part of modern university physics education to study the group dynamics of introductory physics students at the university of helsinki we collected network data from seven laboratory course sections of approximately 20 students each for seven consecutive weeks the data was collected via the sociopatterns platform and supplemented with students major subject year of study and gender we also collected the mechanics baseline test to measure physics knowledge and the colorado learning attitudes about science survey to measure attitudes we developed metrics for studying the small networks of the laboratory sessions by using connections of the teaching assistant as a constant in the network we found both demographically homogeneous and heterogeneous groups that are stable while some students are consistently loosely connected to their networks we were not able to identify risk factors based on our results the physics laboratory course is equally successful in building strongly connected groups regardless of student demographics in the sections or the formed small groups sociopatterns supplemented with surveys thus provides an opportunity to look into the dynamics of students social networks,0
human papillomavirus infection is the most common sexually transmitted infection and causes serious complications such as cervical cancer in vulnerable female populations in regions such as east africa due to the scarcity of empirical data about sexual relationships in varying demographics computationally modelling the underlying sexual contact networks is important to understand human papillomavirus infection dynamics and prevention strategies in this work we present seconet a heterosexual contact network growth model for human papillomavirus disease simulation the growth model consists of three mechanisms that closely imitate realworld relationship forming and discontinuation processes in sexual contact networks we demonstrate that the networks grown from this model are scalefree as are the real world sexual contact networks and we demonstrate that the model can be calibrated to fit different demographic contexts by using a range of parameters we also undertake disease dynamics analysis of human papillomavirus infection using a compartmental epidemic model on the grown networks the presented seconet growth model is useful to computational epidemiologists who study sexually transmitted infections in general and human papillomavirus infection in particular,0
the time to the most recent common ancestor tmrca based on human mitochondrial dna mtdna is estimated to be twice that based on the nonrecombining part of the y chromosome nry these tmrcas have special demographic implications because mtdna is transmitted only from mother to child and nry from father to son therefore mtdna reflects female history and nry male history to investigate what caused the twotoone femalemale tmrca ratio in humans we develop a forwardlooking agentbased model abm with overlapping generations and individual life cycles we implement two main mating systems polygynandry and polygyny with different degrees in between in each mating system the male population can be either homogeneous or heterogeneous in the latter case some males are alphas and others are betas which reflects the extent to which they are favored by female mates a heterogeneous male population implies a competition among males with the purpose of signaling as alphas the introduction of a heterogeneous male population is found to reduce by a factor 2 the probability of finding equal female and male tmrcas and shifts the distribution of the tmrca ratio to higher values we find that high malemale competition is necessary to reproduce a tmrca ratio of 2 less than half the males can be alphas and betas can have at most half the fitness of alphas in addition in the modes that maximize the probability of having a tmrca ratio between 15 and 25 the present generation has 14 times as many female as male ancestors we also tested the effect of sexbiased migration and sexspecific death rates and found that these are unlikely to explain alone the sexbiased tmrca ratio observed in humans our results support the view that we are descended from males who were successful in a highly competitive context while females were facing a much smaller femalefemale competition,0
diverse nonpharmacological interventions npis serving as the primary approach for covid19 control prior to pharmaceutical interventions showed heterogeneous spatiotemporal effects on pandemic management investigating the dynamic compounding impacts of npis on pandemic spread is imperative however the challenges posed by data availability of highdimensional human behaviors and the complexity of modeling changing and interrelated factors are substantial to address these challenges this study analyzed social media data covid19 case rates apple mobility data and the stringency of stayathome policies in the united states throughout the year 2020 aiming to 1 uncover the spatiotemporal variations in npis during the covid19 pandemic utilizing geospatial big data 2 develop a statistical machine learning model that incorporates spatiotemporal dependencies and temporal lag effects for the detection of relationships 3 dissect the impacts of npis on the pandemic across space and time three indices were computed based on twitter currently known as x data the negative and positive sentiments adjusted by demographics nsad and psad and the ratio adjusted by demographics rad representing negative sentiment positive sentiment and public awareness of covid19 respectively the multivariate bayesian structural time series time lagged model mbststl was proposed to investigate the effects of npis accounting for spatial dependencies and temporal lag effects the developed mbststl model exhibited a high degree of accuracy determinants of covid19 health impacts transitioned from an emphasis on human mobility during the initial outbreak period to a combination of human mobility and stayathome policies during the rapid spread phase and ultimately to the compound of human mobility stayathome policies and public awareness of covid19,0
we use generating functional methods to solve the socalled inner product versions of the minority game mg with fake andor real market histories by generalizing the theory developed recently for lookup table mgs with real histories the phase diagrams of the lookup table and inner product mg versions are generally found to be identical with the exception of inner product mgs where histories are sampled linearly which are found to be structurally critical however we encounter interesting differences both in the theory where the role of the history frequency distribution in lookup table mgs is taken over by the eigenvalue spectrum of a history covariance matrix in inner product mgs and in the static and dynamic phenomenology of the models our theoretical predictions are supported by numerical simulations,0
the second quantum revolution facilitates the engineering of new classes of sensors communication technologies and computers with unprecedented capabilities supply chains for quantum technologies are emerging some focussed on commercially available components for enabling technologies andor quantumtechnologies research infrastructures others with already higher technologyreadiness levels near to the market in 2018 the european commission has launched its largescale and longterm quantum flagship research initiative to support and foster the creation and development of a competitive european quantum technologies industry as well as the consolidation and expansion of leadership and excellence in european quantum technology research one of the measures to achieve an accelerated development and uptake has been identified by the quantum flagship in its strategic research agenda the promotion of coordinated dedicated standardisation and certification efforts standardisation is indeed of paramount importance to facilitate the growth of new technologies and the development of efficient and effective supply chains the harmonisation of technologies methodologies and interfaces enables interoperable products innovation and competition all leading to structuring and hence growth of markets as quantum technologies are maturing time has come to start thinking about further standardisation needs this article presents insights on standardisation for quantum technologies from the perspective of the cencenelec focus group on quantum technologies fgqt which was established in june 2020 to coordinate and support the development of standards relevant for european industry and research,0
we surveyed 113 astronomers and 82 psychologists active in applying for federally funded research on their grantwriting history between january 2009 and november 2012 we collected demographic data effort levels success rates and perceived nonfinancial benefits from writing grant proposals we find that the average proposal takes 116 pi hours and 55 ci hours to write although time spent writing was not related to whether the grant was funded effort did translate into success however as academics who wrote more grants received more funding participants indicated modest nonmonetary benefits from grant writing with psychologists reporting a somewhat greater benefit overall than astronomers these perceptions of nonfinancial benefits were unrelated to how many grants investigators applied for the number of grants they received or the amount of time they devoted to writing their proposals we also explored the number of years an investigator can afford to apply unsuccessfully for research grants and our analyses suggest that funding rates below approximately 20 commensurate with current nih and nsf funding are likely to drive at least half of the active researchers away from federally funded research we conclude with recommendations and suggestions for individual investigators and for department heads,0
the inaugural edition of the mit quantum index report qir quantum technologies are evolving from theoretical concepts into tangible technologies with commercial promise their rapid progress is capturing global attention and suggests we stand on the cusp of a second quantum revolution unlocking the quantum opportunity is not simple one challenge is that quantum technologies can present a high barrier to understanding for nonexperts because they often rely on complex principles and concepts from a variety of specialist fields this can lead to confusion and intimidation for business leaders educators policymakers and others the quantum index report aims to reduce the complexity and make it possible for a wider audience to have a deeper understanding of the quantum landscape the quantum index report provides a comprehensive datadriven assessment of the state of quantum technologies for this inaugural edition we have focused on quantum computing and networking the report tracks measures and visualizes trends across research development education and public acceptance it aggregates data from academia industry and policy sources and aims to provide nonpartisan insights,0
this paper uses stoplevel passenger count data in four cities to understand the nationwide bus ridership decline between 2012 and 2018 the local characteristics associated with ridership change are evaluated in portland miami minneapolisstpaul and atlanta poisson models explain ridership as a crosssection and the change thereof as a panel while controlling for the change in frequency jobs and population the correlation with local sociodemographic characteristics are investigated using data from the american community survey the effect of changing neighborhood demographics on bus ridership are modeled using longitudinal employerhousehold dynamics data at a point in time neighborhoods with high proportions of nonwhite carless and most significantly highschooleducated residents are the most likely to have high ridership over time white neighborhoods are losing the most ridership across all four cities in miami and atlanta places with high concentrations of residents with college education and without access to a car also lose ridership at a faster rate in minneapolisstpaul the proportion of collegeeducated residents is linked to ridership gain the sign and significance of these results remain consistent even when controlling for intraurban migration although bus ridership is declining across neighborhood characteristics these results suggest that the underlying cause of bus ridership decline must be primarily affecting the travel behavior of white bus riders,0
ignoring the differences between countries human reproductive and dispersal behaviors can be described by some standardized models so whether there is a universal law of population growth hidden in the abundant and unstructured data from various countries remains unclear the agespecific population data constitute a threedimensional tensor containing more comprehensive information the existing literature often describes the characteristics of global or regional population evolution by subregion aggregation and statistical analysis which makes it challenging to identify the underlying rules by ignoring national or structural details statistical physics can be used to summarize the macro characteristics and evolution laws of complex systems based on the attributes and motions of masses of individuals by decomposing highdimensional tensors specifically it can be used to assess the evolution of age structure in various countries over the past approximately 70 years rather than simply focusing on the regions where aging has become apparent it provides a universal scheme for the growing elderly and working age populations indicating that the demographics on all continents are inevitably moving towards an aging population including the current young continents of africa and asia south america with a recent demographic dividend it is a force derived from the life cycle and most countries have been unable to avoid this universal evolutionary path in the foreseeable future,0
redlining is the discriminatory practice whereby institutions avoided investment in certain neighborhoods due to their demographics here we explore the lasting impacts of redlining on the spread of covid19 in new york city nyc using data available through the home mortgage disclosure act we construct a redlining index for each nyc census tract via a multilevel logistical model we compare this redlining index with the covid19 statistics for each nyc zip code tabulation area accurate mappings of the pandemic would aid the identification of the most vulnerable areas and permit the most effective allocation of medical resources while reducing ethnic health disparities,0
are big conflicts different from small or medium size conflicts to answer this question we leverage finegrained conflict data which we map to climate geography infrastructure economics raw demographics and demographic composition in africa with an unsupervised learning model we find three overarching conflict types representing major unrest local conflict and sporadic and spillover events major unrest predominantly propagates around densely populated areas with welldeveloped infrastructure and flat riparian geography local conflicts are in regions of median population density are diverse socioeconomically and geographically and are often confined within country borders finally sporadic and spillover conflicts remain small often in low population density areas with little infrastructure and poor economic conditions the three types stratify into a hierarchy of factors that highlights population infrastructure economics and geography respectively as the most discriminative indicators specifying conflict type negatively impacts the predictability of conflict intensity such as fatalities conflict duration and other measures of conflict size the competitive effect is a general consequence of weak statistical dependence hence we develop an empirical and bottomup methodology to identify conflict types knowledge of which can hurt predictability and cautions us about the limited utility of commonly available indicators,0
digital network failures stemming from instabilities in measurements of temporal order motivate attention to concurrent events a century of attempts to resolve the instabilities have never eliminated them do concurrent events occur at indeterminate times or are they better seen as events to which the very concept of temporal order cannot apply logical dependencies of messages propagating through digital networks can be represented by marked graphs on which tokens are moved in formal token games however available mathematical formulations of these token games invoke markings global snapshots of the locations of tokens on the graph the formulation in terms of global snapshots is misleading because distributed networks are never still they exhibit concurrent events inexpressible by global snapshots we reformulate token games used to represent digital networks so as to express concurrency the trick is to replace global snapshots with local snapshots detached from any central clock a local snapshot records an action at a node during a play of a token game assemblages of local records define acyclic directed graphs that we call history graphs we show how history graphs represent plays of token games with concurrent motions and importantly how history graphs can represent the history of a network operating while undergoing unpredictable changes,0
in the light of contemporary discussions of inter and transdisciplinarity this paper approaches econophysics and sociophysics to seek a response to the question whether these interdisciplinary fields could contribute to physics and economics drawing upon the literature on history and philosophy of science the paper argues that the two way traffic between physics and economics has a long history and this is likely to continue in the future,0
the ongoing fluid nature of the covid19 pandemic requires individuals to regularly seek information about best health practices local community spreading and public health guidelines in the absence of a unified response to the pandemic in the united states and clear consistent directives from federal and local officials people have used social media to collectively crowdsource covid19 elites a small set of trusted covid19 information sources we take a census of covid19 crowdsourced elites in the united states who have received sustained attention on twitter during the pandemic using a mixed methods approach with a panel of twitter users linked to public us voter registration records we find that journalists media outlets and political accounts have been consistently amplified around covid19 while epidemiologists public health officials and medical professionals make up only a small portion of all covid19 elites on twitter we show that covid19 elites vary considerably across demographic groups and that there are notable racial geographic and political similarities and disparities between various groups and the demographics of their elites with this variation in mind we discuss the potential for using the disproportionate online voice of crowdsourced covid19 elites to equitably promote timely public health information and mitigate rampant misinformation,0
food is an integral part of our lives cultures and wellbeing and is of major interest to public health the collection of daily nutritional data involves keeping detailed diaries or periodic surveys and is limited in scope and reach alternatively social media is infamous for allowing its users to update the world on the minutiae of their daily lives including their eating habits in this work we examine the potential of twitter to provide insight into uswide dietary choices by linking the tweeted dining experiences of 210k users to their interests demographics and social networks we validate our approach by relating the caloric values of the foods mentioned in the tweets to the statewide obesity rates achieving a pearson correlation of 077 across the 50 us states and the district of columbia we then build a model to predict countywide obesity and diabetes statistics based on a combination of demographic variables and food names mentioned on twitter our results show significant improvement over previous chi research culotta14 we further link this data to societal and economic factors such as education and income illustrating that for example areas with higher education levels tweet about food that is significantly less caloric finally we address the somewhat controversial issue of the social nature of obesity first raised by christakis fowler in 2007 by inducing two social networks using mentions and reciprocal following relationships,0
it is wellaccepted that the ability to go from one place to another or mobility contributes significantly to ones wellbeing the need for mobility is universal but the demand for mobility shows a great variation on a country basis this particular study looks at what are some of the most important factors on a global level that can help in predicting the passengerkilometerstravelled or passengermilestravelled pktpmt on a country by country basis this particular work tries to quantify the impact of some of the key variables like gross domestic product gdp population growth employment rate number of households age demographics within the population and macroeconomic variables on the total vehiclebased travel within each country a panelbased regression model is developed to identify the effect of some of the key macroeconomic variables on the countries pkt growth,0
zipfs law and power laws in general have attracted and continue to attract considerable attention in a wide variety of disciplines from astronomy to demographics to software structure to economics to linguistics to zoology and even warfare a recent model of random group formation attempts a general explanation of such phenomena based on jaynes notion of maximum entropy applied to a particular choice of cost function in the present article i argue that the cost function used in the rgf model is in fact unnecessarily complicated and that power laws can be obtained in a much simpler way by applying maximum entropy ideas directly to the shannon entropy subject only to a single constraint that the average of the logarithm of the observable quantity is specified,0
during the covid19 pandemic built environments in dense urban settings become major sources of infection this study tests the difference of demographics and surrounding built environments across high medium and lowinfection neighborhoods to inform the highrisk areas in the city we found that highinfection neighborhoods own a higher ratio of aged population than other neighborhoods on average however it shows no statistical difference in terms of population density additionally highinfection neighborhoods are closer to highrisk built environments than the others in a walking distance they also can access more of the highrisk built environments except for the wholesale markets and shopping malls these findings advise policymakers to deploy social distancing measures in precision regulating the access of highrisk facilities to mitigate the impacts of covid19,0
scientometrics is the field of quantitative studies of scholarly activity it has been used for systematic studies of the fundamentals of scholarly practice as well as for evaluation purposes although advocated from the very beginning the use of scientometrics as an additional method for science history is still under explored in this paper we show how a scientometric analysis can be used to shed light on the reception history of certain outstanding scholars as a case we look into citation patterns of a specific paper by the american sociologist robert k merton,0
we have conducted three empirical studies of the effects of friend recommendations and general ratings on how online users make choices these two components of social influence were investigated through user studies on mechanical turk we find that for a user deciding between two choices an additional rating star has a much larger effect than an additional friends recommendation on the probability of selecting an item equally important negative opinions from friends are more influential than positive opinions and people exhibit more random behavior in their choices when the decision involves less cost and risk our results can be generalized across different demographics implying that individuals trade off recommendations from friends and ratings in a similar fashion,0
the open availability of the entire history of the bitcoin transactions opens up the possibility to study this system at an unprecedented level of detail this contribution is devoted to the analysis of the mesoscale structural properties of the bitcoin user network bun across its entire history ie from 2009 to 2017 what emerges from our analysis is that the bun is characterized by a coreperiphery structure a deeper analysis of which reveals a certain degree of bowtieness ie the presence of a stronglyconnected component an in and an outcomponent together with some tendrils attached to the incomponent interestingly the evolution of the bun structural organization experiences fluctuations that seem to be correlated with the presence of bubbles ie periods of price surge and decline observed throughout the entire bitcoin history our results thus further confirm the interplay between structural quantities and price movements observed in previous analyses,0
recent advances on human dynamics have focused on the normal patterns of human activities with the quantitative understanding of human behavior under extreme events remaining a crucial missing chapter this has a wide array of potential applications ranging from emergency response and detection to traffic control and management previous studies have shown that human communications are both temporally and spatially localized following the onset of emergencies indicating that social propagation is a primary means to propagate situational awareness we study real anomalous events using countrywide mobile phone data finding that information flow during emergencies is dominated by repeated communications we further demonstrate that the observed communication patterns cannot be explained by inherent reciprocity in social networks and are universal across different demographics,0
this paper proposes a modification to minority game mg by adding some agents who play majority game into mg so it is referred to as mixgame the highlight of this model is that the two groups of agents in mixgame have different bounded abilities to deal with history information and to count their own performance through simulations this paper finds out that the local volatilities change a lot by adding some agents who play majority game into mg and the change of local volatilities largely depends on different combinations of history memories of the two groups furthermore this paper analyses the underlying mechanisms for this finding it also gives an example of applications of mixgame,0
we perform laboratory experiments to elucidate the role of historical information in games involving human coordination our approach follows prior work studying human network coordination using the task of graph coloring we first motivate this research by showing empirical evidence that the resolution of coloring conflicts is dependent upon the recent local history of that conflict we also conduct two tailored experiments to manipulate the game history that can be used by humans in order to determine i whether humans use historical information and ii whether they use it effectively in the first variant during the course of each coloring task the network positions of the subjects were periodically swapped while maintaining the global coloring state of the network in the second variant participants completed a series of 2coloring tasks some of which were restarts from checkpoints of previous tasks thus the participants restarted the coloring task from a point in the middle of a previous task without knowledge of the history that led to that point we report on the game dynamics and average completion times for the diverse graph topologies used in the swap and restart experiments,0
the identification of urban mobility patterns is very important for predicting and controlling spatial events in this study we analyzed millions of geographical checkins crawled from a leading chinese locationbased social networking service jiepangcom which contains demographic information that facilitates groupspecific studies we determined the distinct mobility patterns of natives and nonnatives in all five large cities that we considered we used a mixed method to assign different algorithms to natives and nonnatives which greatly improved the accuracy of location prediction compared with the basic algorithms we also propose socalled indigenization coefficients to quantify the extent to which an individual behaves like a native which depends only on their checkin behavior rather than requiring demographic information surprisingly the hybrid algorithm weighted using the indigenization coefficients outperformed a mixed algorithm that used additional demographic information suggesting the advantage of behavioral data in characterizing individual mobility compared with the demographic information the present location prediction algorithms can find applications in urban planning traffic forecasting mobile recommendation and so on,0
at the heart of technology transitions lie complex processes of social and industrial dynamics the quantitative study of sustainability transitions requires modelling work which necessitates a theory of technology substitution many if not most contemporary modelling approaches for future technology pathways overlook most aspects of transitions theory for instance dimensions of heterogenous investor choices dynamic rates of diffusion and the profile of transitions a significant body of literature however exists that demonstrates how transitions follow sshaped diffusion curves or lotkavolterra systems of equations this framework is used expost since timescales can only be reliably obtained in cases where the transitions have already occurred precluding its use for studying cases of interest where nascent innovations in protective niches await favourable conditions for their diffusion in principle scaling parameters of transitions can however be derived from knowledge of industrial dynamics technology turnover rates and technology characteristics in this context this paper presents a theory framework for evaluating the parameterisation of sshaped diffusion curves for use in simulation models of technology transitions without the involvement of historical data fitting making use of standard demography theory applied to technology at the unit level the classic lotkavolterra competition system emerges from first principles from demography theory its timescales explained in terms of technology lifetimes and industrial dynamics the theory is placed in the context of the multilevel perspective on technology transitions where innovation and the diffusion of new sociotechnical regimes take a prominent place as well as discrete choice theory the primary theoretical framework for introducing agent diversity,0
instant quality feedback in the form of online peer ratings is a prominent feature of modern massive online social networks mosns it allows network members to indicate their appreciation of a post comment photograph etc some mosns support both positive and negative signed ratings in this study we rated 11 thousand mosn member profiles and collected user responses to the ratings mosn users are very sensitive to peer ratings 33 of the subjects visited the researchers profile in response to rating 21 also rated the researchers profile picture and 5 left a text comment the grades left by the subjects are highly polarized out of the six available grades the most negative and the most positive are also the most popular the grades fall into three almost equally sized categories reciprocal generous and stingy we proposed quantitative measures for generosity reciprocity and benevolence and analyzed them with respect to the subjects demographics,0
in a world of hardening borders nations may deprive themselves of enjoying the benefits of cooperative immigrants here we analyze the effect of efficient cooperative immigrants on a population playing public goods games we considered a population structured on a square lattice with individuals playing public goods games with their neighbors the demographics are determined by stochastic birth death and migration the strategies spread through imitation dynamics our model shows that cooperation among natives can emerge due to social contagion of good rolemodel agents that can provide better quality public goods only a small fraction of efficient cooperators among immigrants is enough to trigger cooperation across the native population we see that native cooperation achieves its peak at moderate values of immigration rate such efficient immigrant cooperators act as nucleation centers for the growth of cooperative clusters that eventually dominate defection,0
the lack of diversity in physics remains a persistent worldwide problem despite being a quantitative discipline which relies on measurements to construct and validate hypotheses there remains a paucity of data on both demographics and experiences of marginalized groups in canada there has never been a nationwide assessment of those studying or working in physics here we present findings from canadian physics counts the first national survey of equity diversity and inclusion edi in the canadian physics community our intersectional approach allowed us to gather a wealth of information on gender identity sexual orientation race disability and more analyses revealed key findings including the first data on physicists who identify as nonbinary or gender diverse as well as the first data on black and indigenous scholars black physicists 12 and indigenous physicists 3 were found to be the most underrepresented while white men were overrepresented across all sectors among respondents with a disability 5 reported receiving full accommodations for their required needs at their place of work or study one in four respondents from bipoc gender diverse backgrounds identified as being disabled and the proportion of sexually diverse students who reported having a disability was more than three times higher than the proportion of heterosexual students with a disability the data also revealed that students represented more demographic diversity than working professionals highlighting the importance of acting today in order to retain the diverse physicists of tomorrow our analysis identifies areas for intervention and offers recommendations for building a diverse and inclusive physics community in canada that can be a global exemplar,0
we designed and ran an experiment to test how often peoples choices are reversed by others recommendations when facing different levels of confirmation and conformity pressures in our experiment participants were first asked to provide their preferences between pairs of items they were then asked to make second choices about the same pairs with knowledge of others preferences our results show that others peoples opinions significantly sway peoples own choices the influence is stronger when people are required to make their second decision sometime later 224 than immediately 141 moreover people are most likely to reverse their choices when facing a moderate number of opposing opinions finally the time people spend making the first decision significantly predicts whether they will reverse their decisions later on while demographics such as age and gender do not these results have implications for consumer behavior research as well as online marketing strategies,0
diverse and complex intervention policies deployed over the last years have shown varied effectiveness in controlling the covid19 pandemic however a systematic analysis and modelling of the combined effects of different viral lineages and complex intervention policies remains a challenge using largescale agentbased modelling and a highresolution computational simulation matching censusbased demographics of australia we carried out a systematic comparative analysis of several covid19 pandemic scenarios the scenarios covered two most recent australian census years 2016 and 2021 three variants of concern ancestral delta and omicron and five representative intervention policies in addition we introduced pandemic lorenz curves measuring an unequal distribution of the pandemic severity across local areas we quantified nonlinear effects of population heterogeneity on the pandemic severity highlighting that i the population growth amplifies pandemic peaks ii the changes in population size amplify the peak incidence more than the changes in density and iii the pandemic severity is distributed unequally across local areas we also examined and delineated the effects of urbanisation on the incidence bimodality distinguishing between urban and regional pandemic waves finally we quantified and examined the impact of school closures complemented by partial interventions and identified the conditions when inclusion of school closures may decisively control the transmission our results suggest that a public health response to longlasting pandemics must be frequently reviewed and adapted to demographic changes b in order to control recurrent waves massvaccination rollouts need to be complemented by partial npis and c healthcare and vaccination resources need to be prioritised towards the localities and regions with high population growth andor high density,0
the objective of this study was to investigate the importance of multiple countylevel features in the trajectory of covid19 we examined feature importance across 2787 counties in the united states using a datadriven machine learning model we trained random forest models using 23 features representing six key influencing factors affecting pandemic spread social demographics of counties population activities mobility within the counties movement across counties disease attributes and social network structure also we categorized counties into multiple groups according to their population densities and we divided the trajectory of covid19 into three stages the outbreak stage the social distancing stage and the reopening stage the study aims to answer two research questions 1 the extent to which the importance of heterogeneous features evolves in different stages 2 the extent to which the importance of heterogeneous features varies across counties with different characteristics we fitted a set of random forest models to determine weekly feature importance the results showed that 1 social demographic features such as gross domestic product population density and minority status maintained highimportance features throughout stages of covid19 across the 2787 studied counties 2 withincounty mobility features had the highest importance in county clusters with higher population densities 3 the feature reflecting the social network structure facebook social connectedness index had higher importance in the models for counties with higher population densities the results show that the datadriven machine learning models could provide important insights to inform policymakers regarding feature importance for counties with various population densities and in different stages of a pandemic life cycle,0
with the rapid accumulation of online information efficient web navigation has grown vital yet challenging to create an easily navigable cyberspace catering to diverse demographics understanding how people navigate differently is paramount while previous research has unveiled individual differences in spatial navigation such differences in knowledge space navigation remain sparse to bridge this gap we conducted an online experiment where participants played a navigation game on wikipedia and completed personal information questionnaires our analysis shows that age negatively affects knowledge space navigation performance while multilingualism enhances it under time pressure participants performance improves across trials and males outperform females an effect not observed in games without time pressure in our experiment successful routefinding is usually not related to abilities of innovative exploration of routes our results underline the importance of age multilingualism and time constraint in the knowledge space navigation,0
many modern areas have not learned their lessons and often hope for the wisdom of later generations resulting in them only possessing modern technology and difficult to iterate ancient civilizations at present there is no way to tell how we should learn from history and promote the gradual upgrading of civilization therefore we must tell the history of civilizations progress and the means of governance learn from experience to improve the comprehensive strength and survival ability of civilization and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts firstly we must follow the footsteps of history and explore the reasons for the longterm stability of each country in conflict including providing economic benefits to the people and means of suppressing them then use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage after analysis we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist the selection of rulers should consider multiple institutional aspects such as exams elections and drawing lots economic development follows a lognormal distribution and can be adjusted by expected value and variance using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap,0
how important are friendships in determining success by individuals and teams in complex collaborative environments by combining a novel data set containing the dynamics of millions of ad hoc teams from the popular multiplayer online first person shooter halo reach with survey data on player demographics play style psychometrics and friendships derived from an anonymous online survey we investigate the impact of friendship on collaborative and competitive performance in addition to finding significant differences in player behavior across these variables we find that friendships exert a strong influence leading to both improved individual and team performanceeven after controlling for the overall expertise of the teamand increased prosocial behaviors players also structure their ingame activities around social opportunities and as a result hidden friendship ties can be accurately inferred directly from behavioral time series virtual environments that enable such friendship effects will thus likely see improved collaboration and competition,0
homophily ranging from demographics to sentiments breeds connections in social networks either offline or online however with the prosperous growth of music streaming service whether homophily exists in online music listening remains unclear in this study two online social networks of a same group of active users are established respectively in netease music and weibo through presented multiple similarity measures it is evidently demonstrated that homophily does exist in music listening of both online social networks the unexpected music similarity in weibo also implies that knowledge from generic social networks can be confidently transfered to domainoriented networks for context enrichment and algorithm enhancement comprehensive factors that might function in formation of homophily are further probed and many interesting patterns are profoundly revealed it is found that female friends are more homogeneous in music listening and positive and energetic songs significantly pull users close our methodology and findings would shed lights on realistic applications in online music services,0
algorithmic historiography was proposed by eugene garfield in collaboration with irving sher in the 1960s but further developed only recently into histcitetm with alexander pudovkin as in history writing histcitetm reconstructs by drawing intellectual lineages in addition to cited references however documents can be attributed a multitude of other variables such as title words keywords journal names author names and even full texts new developments in multidimensional scaling mds enable us not only to visualize these patterns at each moment of time but also to animate them over time using title words coauthors and journal names in garfields oeuvre the method is demonstrated and further developed in this paper and in the animation at the variety and substantive content of the animation enables us to write visualize and animate the authors intellectual history,0
analysis of wars and conflicts between regions has been an important topic of interest throughout the history of humankind in the latter part of the 20th century in the aftermath of two world wars and the shadow of nuclear biological and chemical holocaust more was written on the subject than ever before wars have a negative impact on a countrys economy social order infrastructure and public health in this paper we study the wars fought in history and draw conclusions from that we explore the participation of countries in wars and the nature of relationships between various countries during different timelines a big part of todays wars is fought against terrorism therefore this study also attempts to shed light on different countries exposure to terrorist encounters and analyses the impact of wars on a countrys economy in terms of change in gdp,0
effective strategies of vaccine prioritization are essential to mitigate the impacts of severe infectious diseases we investigate the role of infection fatality ratio ifr and social contact matrices on vaccination prioritization using a compartmental epidemic model fueled by realworld data of different diseases and countries our study confirms that massive and early vaccination is extremely effective to reduce the disease fatality if the contagion is mitigated but the effectiveness is increasingly reduced as vaccination beginning delays in an uncontrolled epidemiological scenario the optimal and least effective prioritization strategies depend nonlinearly on epidemiological variables regions of the epidemiological parameter space in which prioritizing the most vulnerable population is more effective than the most contagious individuals depend strongly on the ifr age profile being for example substantially broader for covid19 in comparison with seasonal influenza demographics and social contact matrices deform the phase diagrams but do not alter their qualitative shapes,0
population dynamic of getting divorced depends on many global factors including social norms economy law or demographics as well as individual factors like the level of interpersonal or problemsolving skills of the spouses we sought to find such a relationship incorporating only quantitative variables and test theoretical model considering phase transition between coupling pairs and free single preferential states as a function of social and economic the analyzed data has been collected by un across almost all the countries since 1948 our first approach is followed by bouchauds model of social network of opinions which works well with dynamics of fertility rates in postwar europe unfortunately we postulate that this pure sociological and pure economic approach fail in general thus we did some observation about why it went wrong and where economy e g poland or law e g portugal has bigger impact on getting divorce than social pressure,0
can sustained openended technological progress preserve natural resources in a finite planet we address this question on the basis of a stylized model with genuine openended technological innovation where an innovation event corresponds to a random draw of a technology in the space of the parameters that define how it impacts the environment and how it interacts with the population technological innovation is endogenous because an innovation may invade if it satisfies constraints which depend on the state of the environment and of the population we find that openended innovation leads either to a sustainable future where global population saturates and the environment is preserved or to exploding population and a vanishing environment what drives the transition between these two phases is not the level of environmental impact of technologies but rather the demographic effects of technologies and labor productivity low demographic impact and high labor productivity as in several western countries today result in a schumpeterian dynamics where new greener technologies displace older ones thereby reducing the overall environmental impact in this scenario global population saturates to a finite value imposing strong selective pressure on technological innovation when technologies contribute significantly to demographic growth andor labor productivity is low technological innovation runs unrestrained population grows unbounded while the environment collapses as such our model captures subtle feedback effects between technological progress demography and sustainability that rationalize and align with empirical observations of a demographic transition and the environmental kuznets curve without deriving it from profit maximization based on individual incentives,0
with trends of urbanisation on the rise providing adequate housing to individuals remains a complex issue to be addressed often the slow output of relevant housing policies coupled with quickly increasing housing costs leaves individuals with the burden of finding housing that is affordable and safe in this paper we unveil how urban planning not just housing policies can prevent individuals from accessing better housing conditions we begin by proposing a clustering approach to characterising levels of housing insecurity in a city by considering multiple dimensions of housing then we define levels of transit efficiency in 20 us cities by comparing public transit journeys to carbased journeys finally we use geospatial autocorrelation to highlight how commuting to areas associated with better housing conditions results in transit commute times of over 30 minutes in most cities and commute times of over an hour in some cases ultimately we show the role that public transportation plays in locking vulnerable demographics into a cycle of poverty thus motivating a more holistic approach to addressing housing insecurity that extends beyond changing housing policies,0
one of the defining representations of women from medieval times is in the role of peaceweaver that is a woman was expected to weave peace between warring men the underlying assumption in scholarship on this topic is that female mediation lessens male violence this stance can however be questioned since it may be the result of genderbased peace and diplomacy models that relegate womens roles to that of conduits between men by analyzing the concept of communicability and relevance of certain nodes in complex networks we show how our sources afford women more complex and nuanced social roles as a case study we consider a historical narrative namely bedes ecclesiastical history of the english people which is a history of britain from the first to eighth centuries ad and was immensely popular all over europe in the middle ages,0
gas and vapour explosions have been involved in industrial accidents since the beginnings of industry a century ago at 1155 am on friday 24th september 1920 the petroleum barge warwick exploded in londons docklands and seven men were killed understanding what happened when it blew up as it was being refurbished and how to prevent similar explosions involves fluid mechanics and thermodynamics plus chemistry i recount the 1920 accident as an example together with the history of thermokinetic explosions prior to 1920 and up to the present day and i review the history and the actual state of the science of explosion and the roles of fluid mechanics thermodynamics and chemistry in that science the science of explosions has been aware of its societal implications from the beginning but despite advances in health and safety over the past century is there still work to do,0
music is a fundamental human construct and harmony provides the building blocks of musical language using the kunstderfuge corpus of classical music we analyze the historical evolution of the richness of harmonic vocabulary of 76 classical composers covering almost 6 centuries such corpus comprises about 9500 pieces resulting in more than 5 million tokens of music codewords the fulfilment of heaps law for the relation between the size of the harmonic vocabulary of a composer in codeword types and the total length of his works in codeword tokens with an exponent around 035 allows us to define a relative measure of vocabulary richness that has a transparent interpretation when coupled with the considered corpus this measure allows us to quantify harmony richness across centuries unveiling a clear increasing linear trend in this way we are able to rank the composers in terms of richness of vocabulary in the same way as for other related metrics such as entropy we find that the latter is particularly highly correlated with our measure of richness our approach is not specific for music and can be applied to other systems built by tokens of different types as for instance natural language,0
understanding demographic and migrational patterns constitutes a great challenge millions of individual decisions motivated by economic political demographic rational andor emotional reasons underlie the high complexity of demographic dynamics significant advances in quantitatively understanding such complexity have been registered in recent years as those involving the growth of cities but many fundamental issues still defy comprehension we present here compelling empirical evidence of a high level of regularity regarding time and spatial correlations in urban sprawl unraveling patterns about the inertia in the growth of cities and their interaction with each other by using one of the worlds most exhaustive extant demographic data basis that of the spanish governments institute ine with records covering 111 years and in 2011 45 million people distributed amongst more than 8000 population nuclei we show that the inertia of city growth has a characteristic time of 15 years and its interaction with the growth of other cities has a characteristic distance of 70 km distance is shown to be the main factor that entangles two cities a 60 of total correlations we present a mathematical model for population flows that i reproduces all these regularities and ii can be used to predict the populationevolution of cities the power of our current social theories is thereby enhanced,0
the belle ii collaboration comprises over 1000 international high energy physicists who investigate the properties of bquarks and other particles at the luminosity frontier in order to achieve our aim of a successful physics program it is essential that we emphasise contributions from a diverse community belle ii has thus far focused on diversity in gender and sexuality among other efforts within our collaboration these efforts are led by our two diversity officers elected to the newly created positions in 2018 their role has been to promote an inclusive atmosphere raising awareness of diversity and being a safe first point of call for issues of discrimination and harassment these proceedings accompany the short talk delivered during ichep 2020 marking the first conference the belle ii collaboration has presented in the diversity and inclusion stream it details the efforts described above as well as examining the evolving gender demographics of our community since membership began in 2011,0
this bibliometric analysis focuses on the general history of climate change research and more specifically on the discovery of the greenhouse effect first the reference publication year spectroscopy rpys is applied to a large publication set on climate change of 222060 papers published between 1980 and 2014 the references cited therein were extracted and analyzed with regard to publications which are cited most frequently second a new method for establishing a more subjectspecific publication set for applying rpys based on the cocitations of a marker reference is proposed rpysco the rpys of the climate change literature focuses on the history of climate change research in total we identified 35 highlycited publications across all disciplines which include fundamental early scientific works of the 19th century with a weak connection to climate change and some cornerstones of science with a stronger connection to climate change by using the arrhenius 1896 paper as a rpysco marker paper we selected only publications specifically discussing the discovery of the greenhouse effect and the role of carbon dioxide also we focused on the time period 18001850 to reveal the contributions of jbj fourier in terms of cited references using different rpys approaches in this study we were able to identify the complete range of works of the celebrated icons as well as many less known works relevant for the history of climate change research the analyses confirmed the potential of the rpys method for historical studies seminal papers are detected on the basis of the references cited by the overall community without any further assumptions,0
wikipedia wp as a collaborative dynamical system of humans is an appropriate subject of social studies each single action of the members of this society ie editors is well recorded and accessible using the cumulative data of 34 wikipedias in different languages we try to characterize and find the universalities and differences in temporal activity patterns of editors based on this data we estimate the geographical distribution of editors for each wp in the globe furthermore we also clarify the differences among different groups of wps which originate in the variance of cultural and social features of the communities of editors,0
infectious epidemics can be simulated by employing dynamical processes as interactions on network structures here we introduce techniques from the multiagent system mas domain in order to account for individual level characterization of societal dynamics for the sarscov2 pandemic we hypothesize that a mas model which considers rich spatial demographics hourly mobility data and daily contagion information from the metropolitan area of toronto can explain significant emerging behavior to investigate this hypothesis we designed with our modeling framework of choice gama an accurate environment which can be tuned to reproduce mobility and healthcare data in our case coming from tomtoms api and torontos open data we observed that some interesting contagion phenomena are directly influenced by mobility restrictions and curfew policies we conclude that while our model is able to reproduce nontrivial emerging properties largescale simulation are needed to further investigate the role of different parameters finally providing such an endtoend model can be critical for policymakers to compare their outcomes with past strategies in order to devise better plans for future measures,0
the analysis of longitudinal travel data enables investigating how mobility patterns vary across the population and identify the spatial properties thereof the objective of this study is to identify the extent to which users explore different parts of the network as well as identify distinctive user groups in terms of the spatial extent of their mobility patterns to this end we propose two means for representing spatial mobility profiles and clustering travellers accordingly we represent users patterns in terms of zonal visiting frequency profiles and gridcells spatial extent heatmaps we apply the proposed analysis to a largescale multimodal mobility data set from the public transport system in stockholm sweden we unravel three clusters locals commuters and explorers that best describe the zonal visiting frequency and show that their composition varies considerably across users place of residence and related demographics we also identify 18 clusters of visiting spatial extent which form four groups that follow similar shapes of travel extent yet oriented in different directions the approach proposed and applied in this study could be applied for any longitudinal individual travel demand data,0
previous research has shown a relationship between voter characteristics and voter support for tax bonds these findings however are difficult to interpret because of the high degree of collinearity across the measures from 13 demographic measures of voters in a library bond election seven independent principal components were extracted which accounted for 95 percent of the variance whereas the direct demographic measures showed inconsistent relationships with voting the principal components of low ses college experience female and service job were related to affirmative voting while high home value was related to negative voting,0
understanding how ambulance incidents are spatially distributed can shed light to the epidemiological dynamics of geographic areas and inform healthcare policy design here we analyze a longitudinal dataset of more than four million ambulance calls across a region of twelve million residents in the north west of england with the aim to explain geographic variations in ambulance call frequencies we employ a wide range of data layers including open government datasets describing population demographics and socioeconomic characteristics as well as geographic activity in online services such as foursquare working at a fine level of spatial granularity we demonstrate that daytime population levels and the deprivation status of an area are the most important variables when it comes to predicting the volume of ambulance calls at an area foursquare checkins on the other hand complement these government sourced indicators offering a novel view to population nightlife and commercial activity locally we demonstrate how checkin activity can provide an edge when predicting certain types of emergency incidents in a multivariate regression model,0
with people constantly migrating to different urban areas our mobility needs for work services and leisure are transforming rapidly the changing urban demographics pose several challenges for the efficient management of transit services to forecast transit demand planners often resort to sociological investigations or modelling that are either difficult to obtain inaccurate or outdated how can we then estimate the variegated demand for mobility we propose a simple method to identify the spatiotemporal demand for public transit in a city using a gaussian mixture model we decompose empirical ridership data into a set of temporal demand profiles representative of ridership over any given day a case of approximately 46 million daily transit traces from the greater london region reveals distinct demand profiles we find that a weighted mixture of these profiles can generate any station traffic remarkably well uncovering spatially concentric clusters of mobility needs our method of analysing the spatiotemporal geography of a city can be extended to other urban regions with different modes of public transit,0
in this article we make a case for a systematic application of complex network science to study art market history and more general collection dynamics we reveal social temporal spatial and conceptual network dimensions ie network node and link types previously implicit in the getty provenance index gpi as a pioneering art history database active since the 1980s the gpi provides online access to source material relevant for research in the history of collecting and art markets based on a subset of the gpi we characterize an aggregate of more than 267000 sales transactions connected to roughly 22000 actors in four countries over 20 years at daily resolution from 1801 to 1820 striving towards a deeper understanding on multiple levels we disambiguate social dynamics of buying brokering and selling while observing a general broadening of the market where large collections are split into smaller lots temporally we find annual market cycles that are shifted by country and obviously favor international exchange spatially we differentiate nearmonopolies from regions driven by competing subcenters while uncovering asymmetries of international market flux conceptually we track dynamics of artist attribution that clearly behave like product categories in a very slow supermarket taken together we introduce a number of meaningful network perspectives dealing with historical art auction data beyond the analysis of social networks within a single market region the results presented here have inspired a linked open data conversion of the gpi which is currently in process and will allow further analysis by a broad set of researchers,0
mutualistic interactions are vital constituents of ecological and socioeconomic systems empirical studies have found that the patterns of reciprocal relations among the participants often shows the salient features of being simultaneously nested and modular whether and how these two structural properties of mutualistic networks can emerge out of a common mechanism however remains unclear we propose a unified dynamic model based on the adaptation of niche relations that gives rise to both structural features we apply hutchinsons concept of niche interaction to networked cooperative species their niche relation evolves under the assumption of fitness maximization modularity and nestedness emerge concurrently through the accumulated local advantages in the structural and demographic distribution a rich ensemble of key dynamical behaviors are unveiled in the dynamical framework we demonstrate that mutualism can exhibit either a stabilizing or destabilizing effect on the evolved network which undergoes a drastic transition with the overall competition level most strikingly the adaptive network may exhibit a profound nature of historydependency in response to environmental changes allowing it to be found in alternative stable structures the adaptive nature of niche interactions as captured in our framework can underlie a broad class of ecological relations and also socioeconomic networks that engage in bipartite cooperation,0
the foundation of two very early usenet newsgroups in astrophysics still existent today and some milestones in their history have been tracked from the origins at princeton university in 1983 to 1994 they result to be pioneering experiences in this discipline and among the earliest ones of this kind in academic disciplines at large to the best of our knowledge an account of their birth and evolution is given here for the first time following key recommendations from the recent discipline of web history this research has combined multiple and differenttype sources building mainly on online archives of usenet newsgroups and on human contributions from the concerned scholarly community a final overview is proposed on how these early online communication tools have been perceived and used by the scholarly community involved this research reconstructs computer mediated communication experiences which were at risk of being forgotten provides a view of this environments uptake of new communication technology and contributes knowledge of some social dynamics of the astrophysics community in the last twenty years of the twentieth century,0
tracing the evolution of specific topics is a subject area which belongs to the general problem of mapping the structure of scientific knowledge often bibliometric data bases are used to study the history of scientific topic evolution from its appearance to its extinction or merger with other topics in this chapter the authors present an analysis of the academic response to the disaster that occurred in 1986 in chornobyl chernobyl ukraine considered as one of the most devastating nuclear power plant accidents in history using a bibliographic database the distributions of chornobylrelated papers in different scientific fields are analysed as are their growth rates and properties of coauthorship networks elements of descriptive statistics and tools of complexnetwork theory are used to highlight interdisciplinary as well as international effects in particular tools of complexnetwork science enable information visualization complemented by further quantitative analysis a further goal of the chapter is to provide a simple pedagogical introduction to the application of complexnetwork analysis for visual data representation and interdisciplinary communication,0
the production and consumption of information about bitcoin and other digital or crypto currencies have grown together with their market capitalisation however a systematic investigation of the relationship between online attention and market dynamics across multiple digital currencies is still lacking here we quantify the interplay between the attention towards digital currencies in wikipedia and their market performance we consider the entire edit history of currencyrelated pages and their view history from july 2015 first we quantify the evolution of the cryptocurrency presence in wikipedia by analysing the editorial activity and the network of coedited pages we find that a small community of tightly connected editors is responsible for most of the production of information about cryptocurrencies in wikipedia then we show that a simple trading strategy informed by wikipedia views performs better in terms of returns on investment than classic baseline strategies for most of the covered period our results contribute to the recent literature on the interplay between online information and investment markets and we anticipate it will be of interest for researchers as well as investors,0
mobile phone usage provides a wealth of information which can be used to better understand the demographic structure of a population in this paper we focus on the population of mexican mobile phone users our first contribution is an observational study of mobile phone usage according to gender and age groups we were able to detect significant differences in phone usage among different subgroups of the population our second contribution is to provide a novel methodology to predict demographic features namely age and gender of unlabeled users by leveraging individual calling patterns as well as the structure of the communication graph we provide details of the methodology and show experimental results on a real world dataset that involves millions of users,0
territorial subdivisions and geographic borders are essential for understanding phenomena in sociology political science history and economics they influence the interregional flow of information and crossborder trade and affect the diffusion of innovation and technology however most existing administrative borders were determined by a variety of historic and political circumstances along with some degree of arbitrariness societies have changed drastically and it is doubtful that currently existing borders reflect the most logical divisions fortunately at this point in history we are in a position to actually measure some aspects of the geographic structure of society through human mobility largescale transportation systems such as trains and airlines provide data about the number of people traveling between geographic locations and many promising human mobility proxies are being discovered such as cell phones bank notes and various online social networks in this chapter we apply two optimization techniques to a human mobility proxy bank note circulation to investigate the effective geographic borders that emerge from a direct analysis of human mobility,0
the purpose of this research is to identify correlates of bike station activity for nice ride minnesota a bike share system in minneapolis st paul metropolitan area in minnesota we obtained the number of trips to and from each of the 116 bike share stations operating in 2011 from nice ride minnesota data for independent variables included in models come from a variety of sources including the 2010 us census the metropolitan council a regional planning agency and the cities of minneapolis and st paul we use loglinear and negative binomial regression models to evaluate the marginal effects of these factors on average daily station trips our models have high goodness of fit and each of 13 independent variables is significant at the 10 level or higher the number of trips at nice ride stations is associated with neighborhood socio demographics ie age and race proximity to the central business district proximity to water accessibility to trails distance to other bike share stations and measures of economic activity analysts can use these results to optimize bike share operations locate new stations and evaluate the potential of new bike share programs,0
by fitting a compartment ode model for covid19 propagation to cumulative case and death data for us states and european countries we find that the case mortality rate seems to have decreased by at least 80 in most of the us and at least 90 in most of europe these are much larger and faster changes than reported in empirical studies such as the 18 decrease in mortality found for the new york city hospital system from march to august 2020 horwitz et al trends in covid19 riskadjusted mortality rates j hosp med 2020 our reported decreases surprisingly do not have strong correlations to other model parameters such as contact rate or other standard statenational metrics such as population density gdp and median age almost all the decreases occurred between midapril and midjune which unexpectedly corresponds to the time when many state and national lockdowns were released resulting in surges of new cases several plausible causes for this drop are examined such as improvements in treatment face mask wearing a new virus strain and potentially changing demographics of infected patients but none are overwhelmingly convincing given the currently available evidence,0
cascades of informationsharing are a primary mechanism by which content reaches its audience on social media and an active line of research has studied how such cascades which form as content is reshared from person to person develop and subside in this paper we perform a largescale analysis of cascades on facebook over significantly longer time scales and find that a more complex picture emerges in which many large cascades recur exhibiting multiple bursts of popularity with periods of quiescence in between we characterize recurrence by measuring the time elapsed between bursts their overlap and proximity in the social network and the diversity in the demographics of individuals participating in each peak we discover that content virality as revealed by its initial popularity is a main driver of recurrence with the availability of multiple copies of that content helping to spark new bursts still beyond a certain popularity of content the rate of recurrence drops as cascades start exhausting the population of interested individuals we reproduce these observed patterns in a simple model of content recurrence simulated on a real social network using only characteristics of a cascades initial burst we demonstrate strong performance in predicting whether it will recur in the future,0
as populations age the rise of multimorbidity poses a significant healthcare challenge however our ability to quantitatively forecast the progression of multimorbidity remains limited leveraging a nationwide dataset comprising approximately 45 million hospital stays spanning 17 years in austria we develop a new compartmental model for chronic disease trajectories across 132 distinct multimorbidity patterns compartments each compartment represents a distinct constellation of cooccurring chronic conditions with transitions modeled as age and sexdependent probabilities we use the compartmental disease trajectory model cdtm to simulate disease trajectories to 2030 estimating the frequency of all empirically observed cooccurrence patterns among more than 100 diagnosis groups we demonstrate the models utility in identifying highimpact prevention targets a 5 reduction in new cases of hypertensive disease i10i15 leads to a 057 sd 006 reduction in allcause mortality over a 15year period and a 057 sd 007 reduction in mortality for malignant neoplasms c00c97 we also evaluate longterm impacts of sarscov2 sequelae projecting earlier and more frequent hospitalizations across a range of diagnoses our fully datadriven modelling approach identifies leverage points for proactive preparation by physicians and policymakers to reduce the overall disease burden in the population emphasizing patientcentered healthcare planning in aging societies,0
urbanization and its problems require an indepth and comprehensive understanding of urban dynamics especially the complex and diversified lifestyles in modern cities digitally acquired data can accurately capture complex human activity but it lacks the interpretability of demographic data in this paper we study a privacyenhanced dataset of the mobility visitation patterns of 12 million people to 11 million places in 11 metro areas in the us to detect the latent mobility behaviors and lifestyles in the largest american cities despite the considerable complexity of mobility visitations we found that lifestyles can be automatically decomposed into only 12 latent interpretable activity behaviors on how people combine shopping eating working or using their free time rather than describing individuals with a single lifestyle we find that city dwellers behavior is a mixture of those behaviors those detected latent activity behaviors are equally present across cities and cannot be fully explained by main demographic features finally we find those latent behaviors are associated with dynamics like experienced income segregation transportation or healthy behaviors in cities even after controlling for demographic features our results signal the importance of complementing traditional census data with activity behaviors to understand urban dynamics,0
doing research is fighting what any other thing the human being could do fight against powers or to get powers that depends on us science can be a revolution or deadlocked idleness still waters without hitting the stones along their history trend to form bogs,0
i report the results of the test where the takers had to tell the prose of charles dickens from that of edward bulwerlytton who is considered by many to be the worst writer in history of letters the average score is about 50 which is on the level of random guessing this suggests that the quality of dickens prose is the same as of that of bulwerlytton,0
the complexities involved in modelling the transmission dynamics of covid19 has been a roadblock in achieving predictability in the spread and containment of the disease in addition to understanding the modes of transmission the effectiveness of the mitigation methods also needs to be built into any effective model for making such predictions we show that such complexities can be circumvented by appealing to scaling principles which lead to the emergence of universality in the transmission dynamics of the disease the ensuing data collapse renders the transmission dynamics largely independent of geopolitical variations the effectiveness of various mitigation strategies population demographics etc we propose a simple twoparameter model the blue sky model and show that one class of transmission dynamics can be explained by a solution that lives at the edge of a blue sky bifurcation in addition the data collapse leads to an enhanced degree of predictability in the disease spread for several geographical scales which can also be realized in a modelindependent manner as we show using a deep neural network the methodology adopted in this work can potentially be applied to the transmission of other infectious diseases and new universality classes may be found the predictability in transmission dynamics and the simplicity of our methodology can help in building policies for exit strategies and mitigation methods during a pandemic,0
all human societies present unique narratives that shape their customs and beliefs despite cultural differences some symbolic elements eg heroes and tricksters are common across many cultures here we reconcile these seemingly contradictory aspects by analyzing mythological themes and traditions at various scales our analysis revealed that global mythologies exhibit both geographic and thematic nesting across different scales manifesting in a layered structure the largest geographic clusters correspond to the new and old worlds which further divide into smaller bioregions this hierarchical manifestation closely aligns with historical human migration patterns at a large scale suggesting that narrative themes were carried through deep history at smaller scales the correspondence with bioregions indicates that these themes are locally adapted and diffused into variations across cultures over time our approach which treats myths and traditions as random variables without considering factors like geography history or story lineage suggests that the manifestation of mythology has been wellpreserved over time and thus opens exciting research avenues to reconstruct historical patterns and provide insight into human cultural narratives,0
we add to a growing literature suggesting that demographic grade gaps should be attributed to biases embedded in the courses themselves changes in the structure of two different introductory physics classes were made while leaving the topics covered and the level of coverage unchanged first a class where conceptual issues were studied before doing any complicated calculations had zero final exam grade gap between students from underrepresented racialethnic groups and their peers next four classes that offered students a retake exam each week between the regular biweekly exams during the term had zero gender gap in course grades our analysis indicates that demographic grade gaps can be attributed to the course structure a course deficit model rather than to student preparation a student deficit model,0
these proceedings accompany the belle ii talk in the science in society parallel session delivered during lepton photon 2021 in this talk we present updated membership statistics using 10 years of data with a diversity and inclusion lens and we present belle iis most recent activities to aid and improve diversity and inclusion this report has the intention to bring light to the social working environment and population representation within our collaboration and by extension within high energy physics belle ii is a particle physics collaboration that has over 1000 people from institutions in 26 countries who work together to achieve its physics goals belle ii is committed to fostering an open diverse and inclusive environment as part of this commitment it created a diversity office to raise awareness of diversity and inclusion issues promote an inclusive atmosphere within the collaboration provide a safe and confidential point to contact for collaborators to report any issues particularly those related to discrimination and harassment and ensure that persons from underrepresented groups are considered for positions of responsibility within the collaboration diversity and inclusion activities and initiatives at belle ii and analysis of the demographics of the collaboration will be presented,0
the origin of economic crises is a key problem for economics we present a model of longrun competitive markets to show that the multiplicity of behaviors in an economic system over a long time scale emerge as statistical regularities perfectly competitive markets obey boseeinstein statistics and purely monopolisticcompetitive markets obey boltzmann statistics and that how interaction among firms influences the evolutionary of competitive markets it has been widely accepted that perfect competition is most efficient our study shows that the perfectly competitive system as an extreme case of competitive markets is most efficient but not stable and gives rise to economic crises as society reaches full employment in the economic crisis revealed by our model many firms condense collapse into the lowest supply level zero supply namely bankruptcy status in analogy to boseeinstein condensation this curious phenomenon arises because perfect competition homogeneous competitions equals symmetric indistinguishable investment direction a fact abhorred by nature therefore we urge the promotion of monopolistic competition heterogeneous competitions rather than perfect competition to provide early warning of economic crises we introduce a resolving index of investment which approaches zero in the runup to an economic crisis on the other hand our model discloses as a profound conclusion that the technological level for a longrun social or economic system is proportional to the freedom disorder of this system in other words technology equals the entropy of system as an application of this new concept we give a possible answer to the needham question why was it that despite the immense achievements of traditional china it had been in europe and not in china that the scientific and industrial revolutions occurred,0
disparity in spatial accessibility is strongly associated with growing inequalities among urban communities since improving levels of accessibility for certain communities can provide them with upward social mobility and address social exclusion and inequalities in cities it is important to understand the nature and distribution of spatial accessibility among urban communities to support decisionmakers in achieving inclusion and fairness in policy interventions in cities we present an opensource and datadriven framework to understand the spatial nature of accessibility to infrastructure among the different demographics we find that accessibility to a wide range of infrastructure in any city 54 cities converges to a zipfs law suggesting that inequalities also appear proportional to growth processes in these cities then assessing spatial inequalities among the socioeconomically clustered urban profiles for 10 of those cities we find urban communities are distinctly segregated along social and spatial lines we find low accessibility scores for populations who have a larger share of minorities earn less and have a relatively lower number of individuals with a university degree these findings suggest that the reproducible framework we propose may be instrumental in understanding processes leading to spatial inequalities and in supporting cities to devise targeted measures for addressing inequalities for certain underprivileged communities,0
we understand the dynamics of the world around us as by associating pairs of events where one event has some influence on the other these pairs of events can be aggregated into a web of memories representing our understanding of an episode of history the events and the associations between them need not be directly experiencedthey can also be acquired by communication in this paper we take a network approach to study the dynamics of memories of history first we investigate the network structure of a data set consisting of reported events by several individuals and how associations connect them we focus our measurement on degree distributions degree correlations cycles which represent inconsistencies as they would break the time ordering and community structure we proceed to model effects of communication using an agentbased model we investigate the conditions for the memory webs of different individuals to converge to collective memories how groups where the individuals have similar memories but different from other groups can form our work outlines how the cognitive representation of memories and social structure can coevolve as a contagious process we generate some testable hypotheses including that the number of groups is limited as a function of the total population size,0
in the face of serious infectious diseases governments endeavour to implement containment measures such as public vaccination at a macroscopic level meanwhile individuals tend to protect themselves by avoiding contacts with infections at a microscopic level however a comprehensive understanding of how such combined strategy influences epidemic dynamics is still lacking we study a susceptibleinfectedsusceptible epidemic model with imperfect vaccination on dynamic contact networks where the macroscopic intervention is represented by random vaccination of the population and the microscopic protection is characterised by susceptible individuals rewiring contacts from infective neighbours in particular the model is formulated both in populations without and then with demographic effects using the pairwise approximation and the probability generating function approach we investigate both dynamics of the epidemic and the underlying network for populations without demography the emerging degree correlations bistable states and oscillations demonstrate the combined effects of the public vaccination program and individual protective behavior compared to either strategy in isolation the combination of public vaccination and individual protection is more effective in preventing and controlling the spread of infectious diseases by increasing both the invasion threshold and the persistence threshold for populations with additional demographic factors the integration between vaccination intervention and individual rewiring may promote epidemic spreading due to the birth effect moreover the degree distributions of both networks in the steady state is closely related to the degree distribution of newborns which leads to uncorrelated connectivity all the results demonstrate the importance of both local protection and global intervention as well as the demographic effects,0
recreational fishing is an important economic driver and provides multiple social benefits to predict fishing activity identifying variables related to variation such as gender or covid19 is helpful we conducted a canadawide email survey of users of an online fishing platform and analyzed responses focusing on gender the impact of covid19 and variables directly related to fishing effort genders 90 men and 10 women significantly differed in demographics socioeconomic status and fishing skills but showed similar fishing preferences fishing effort in terms of trip frequency and travel distance covid19 altered trip frequency for almost half of fishers with changes varying by gender and activity level a bayesian network revealed travel distance as the main determinant of trip frequency negatively impacting fishing activity for 61 of fishers with fishing expertise also playing a role the results suggest that among active fishers socioeconomic differences between genders do not drive fishing effort but responses to covid19 were genderspecific recognizing these patterns is critical for equitable policymaking and accurate socioecological models thereby improving resource management and sustainability,0
we analyze mathematical models of the global human population growth and compare them to actual dynamics of the world population and of the world surplus product we consider a possibility that the socalled worlds demographic transition is not a dynamic crossover but a phase transition that affects all aspects of our life,0
hate and extremism cannot be controlled globally without understanding how they operate at scale both have escalated dramatically during the israelhamas and ukrainerussia wars here we show how the online hateextremism system is now operating at unprecedented scale across 26 social media platforms of all sizes audience demographics and geographic locations and we analyze individuals journeys through it this new picture contradicts notions of rabbithole activity at the fringe of the internet instead it shows that hateextremism support now enjoys a direct link to more than a billion of the general global population and that newcomers now enjoy a rich variety of online journey experiences during which they get to mingle with experienced violent actors discuss topics from diverse news sources and learn to collectively adapt in order to bypass platform shutdowns our results mean that law enforcement must expect future mass shooters to have increasingly hardtounderstand online journeys that new eu laws will fall short because the combined impact of many smaller lesserknown platforms outstrips larger ones like twitter and that the current global hateextremism infrastructure will become increasingly robust in 2024 and beyond fortunately it also reveals a new opportunity for systemwide control akin to adaptive vs extinction treatments for cancer,0
one of the central difficulties of addressing the covid19 pandemic has been accurately measuring and predicting the spread of infections in particular official covid19 case counts in the united states are under counts of actual caseloads due to the absence of universal testing policies researchers have proposed a variety of methods for recovering true caseloads often through the estimation of statistical models on more reliable measures such as death and hospitalization counts positivity rates and demographics however given the disproportionate impact of covid19 on marginalized racial ethnic and socioeconomic groups it is important to consider potential unintended effects of case correction methods on these groups thus we investigate two of these correction methods for their impact on a downstream covid19 case prediction task for that purpose we tailor an auditing approach and evaluation protocol to analyze the fairness of the covid19 prediction task by measuring the difference in model performance between majoritywhite counties and majorityminority counties we find that one of the correction methods improves fairness decreasing differences in performance between majoritywhite and majorityminority counties while the other method increases differences introducing bias while these results are mixed it is evident that correction methods have the potential to exacerbate existing biases in covid19 case data and in downstream prediction tasks researchers planning to develop or use case correction methods must be careful to consider negative effects on marginalized groups,0
can we predict topperforming products services or businesses by only monitoring the behavior of a small set of individuals although most previous studies focused on the predictive power of hub individuals with many social contacts which sources of customer behavioral data are needed to address this question remains unclear mostly due to the scarcity of available datasets that simultaneously capture individuals purchasing patterns and social interactions here we address this question in a unique largescale dataset that combines individuals creditcard purchasing history with their social and mobility traits across an entire nation surprisingly we find that the purchasing history alone enables the detection of small sets of discoverers whose early purchases offer reliable success predictions for the brickandmortar stores they visit in contrast with the assumptions by most existing studies on wordofmouth processes the hubs selected by social network centrality are not consistently predictive of success our findings show that companies and organizations with access to largescale purchasing data can detect the discoverers and leverage their behavior to anticipate market trends without the need for social network data,0
the network characteristics based on the phonological similarities in the lexicons of several languages were examined these languages differed widely in their history and linguistic structure but commonalities in the network characteristics were observed these networks were also found to be different from other networks studied in the literature the properties of these networks suggest explanations for various aspects of linguistic processing and hint at deeper organization within human language,0
cities around the world face significant barriers to grow urban cycling including competing budgetary priorities and carcentric streets thus when making decisions regarding the installation of bicycle infrastructure it is crucial to understand if and to what extent different bicyclelane types increase bicycle ridership however associations between bicycle infrastructure and bicycle ridership have primarily been studied in the context of individual lanes and corridors or when analyzed at the scale of entire cities generalized across different bikelane types drawing upon 72 million bikeshare trips from citi bike in new york we demonstrate that there is an approximately 18 increase in bikeshare trips at adjacent stations in the 12 months following the installation of protected bike lanes those with a physical barrier between cyclists and automobile traffic and a 14 increase associated with painted bike lanes where a line of pavement marking is present and sharrows where a normal traffic lane is marked with a bike stencil however using a differenceindifferences analysis we detect a causal effect on bikeshare ridership only following the installation of protected bike lanes with an average monthly increase of 379 rides per station p0001 despite this causal effect being pronounced among census block groups with higher percentages of older adults 688 rides per month per station p0001 the causal effect of protected bike lanes on bikeshare ridership is absent in census block groups where the percentage of black residents is medium to high taken together these findings indicate that planners must emphasize protected bike lanes to spur ridership and simultaneously target policies and programming to communities of color to ensure that such infrastructure makes urban cycling a viable option for all residents,0
scientists often reinvent things that were long known here we review these activities as related to the mechanism of producing power law distributions originally proposed in 1922 by yule to explain experimental data on the sizes of biological genera collected by willis we also review the history of reinvention of closely related branching processes random graphs and coagulation models,0
culinary practices are influenced by climate culture history and geography molecular composition of recipes in a cuisine reveals patterns in food preferences indian cuisine encompasses a number of diverse subcuisines separated by geographies climates and cultures its culinary system has a long history of healthcentric dietary practices focused on disease prevention and promotion of health we study food pairing in recipes of indian cuisine to show that in contrast to positive food pairing reported in some western cuisines indian cuisine has a strong signature of negative food pairing more the extent of flavor sharing between any two ingredients lesser their cooccurrence this feature is independent of recipe size and is not explained by ingredient categorybased recipe constitution alone ingredient frequency emerged as the dominant factor specifying the characteristic flavor sharing pattern of the cuisine spices individually and as a category form the basis of ingredient composition in indian cuisine we also present a culinary evolution model which reproduces ingredient use distribution as well as negative food pairing of the cuisine our study provides a basis for designing novel signature recipes healthy recipe alterations and recipe recommender systems,0
the multifaceted nature of disaster impact shows that densely populated areas contribute more to aggregate burden while sparsely populated but heavily affected regions suffer disproportionately at the individual level this study introduces a framework for quantifying the societal impacts of power outages by translating customer weighted outage exposure into deprivation measures integrating welfare metrics with three recovery indicators average outage days per customer restoration duration and relative restoration rate computed from sequential eagle i observations and linked to zip code tabulation area demographics applied to four united states hurricanes beryl 2024 texas helene 2024 florida milton 2024 florida and ida 2021 louisiana this standardized pipeline provides the first cross event fine scale evaluation of outage impacts and their drivers results demonstrate regressive patterns with greater burdens in lower income areas mechanistic analysis shows deprivation increases with longer restoration durations and decreases with faster restoration rates explainable modeling identifies restoration duration as the dominant driver and clustering reveals distinct recovery typologies not captured by conventional reliability metrics this framework delivers a transferable method for assessing outage impacts and equity comparative cross event evidence linking restoration dynamics to social outcomes and actionable spatial analyses that support equity informed restoration planning and resilience investment,0
urban systems are at the core of current sustainability concerns and their study from a complexity perspective has a long history in several disciplines we survey this literature and discuss future research directions relevant to sustainable planning in particular the construction of integrative approaches we finally illustrate this research program with the coupling of urban simulation models to explore tradeoffs between sustainable development goals in systems of cities,0
quantifying regularities in behavioral dynamics is of crucial interest for understanding collective social events such as panics or political revolutions with the widespread use of digital communication media it has become possible to study massive data streams of usercreated content in which individuals express their sentiments often towards a specific topic here we investigate messages from various online media created in response to major collectively followed events such as sport tournaments presidential elections or a large snow storm we relate content length and message rate and find a systematic correlation during events which can be described by a power law relation the higher the excitation the shorter the messages we show that on the one hand this effect can be observed in the behavior of most regular users and on the other hand is accentuated by the engagement of additional user demographics who only post during phases of high collective activity further we identify the distributions of content lengths as lognormals in line with statistical linguistics and suggest a phenomenological law for the systematic dependence of the message rate to the lognormal mean parameter our measurements have practical implications for the design of microblogging and messaging services in the case of the existing service twitter we show that the imposed limit of 140 characters per message currently leads to a substantial fraction of possibly dissatisfying to compose tweets that need to be truncated by their users,0
in this information era commuters prefer to know a reliable travel time to plan ahead of their journey using both public and private modes in this direction reliability analysis using the location data of the buses is conducted in two folds in the current work i reliability analysis of a public transit service at route level and ii travel time reliability analysis of a route utilizing the location data of the buses the reliability parameters assessed for public transit service are headway passenger waiting time travel speed and travel time as per the service level benchmarks for urban transport by the national urban transport policy government of india and travel time reliability parameters such as buffer time index travel time index and planning time index are assessed as per federal highway administration department of transportation u s the study is conducted in tumakuru city india for a significant bus route in a limited data sources scenario the results suggest that i the level of service of the public transit service needs improvement iiaround 30 excess of average travel time is needed as buffer time iii more than double the amount of free flow travel time must be planned during peak hours and in the worst case in the future the analysis conducted for the route can be extended for citywide performance analysis in both folds also the same method can be applied to cities with similar demographics and trafficrelated infrastructure,0
the city of honolulu hawaii is currently planning and developing a new rail transit system while honolulu has supportive density and topography for rail transit questions remain about its ability to effectively integrate urban design and accessibility across the system every transit trip begins and ends with a walking trip from origins and to destinations transportation planning must account for pedestrian safety comfort and access ildefons cerdas 19th century utopian plan for barcelonas eixample district produced a renowned livable urban form the eixample with its wellintegrated rail transit serves as a model of urban design land use transportation planning and pedestrianscaled streets working in synergy to produce accessibility this study discusses the urban form of honolulu and the history and planning of its new rail transit system then it reviews the history of cerdas plan for the eixample and discusses its urban form and performance today finally it draws several lessons from barcelonas urban design accessibility and rail transit planning and critically discusses their applicability to policy and design in honolulu this discussion is situated within wider debates around livable cities and social justice as it contributes several form and design lessons to the livability and accessibility literature while identifying potential concerns with privatization and displacement,0
we introduce an agentbased model of interaction drawing on the contingency approach from luhmanns theory of social systems the agent interactions are defined by the exchange of distinct messages message selection is based on the history of the interaction and developed within the confines of the problem of double contingency we examine interaction strategies in the light of the messageexchange description using analytical and computational methods,0
quantifying individual performance in the game of cricket is critical for team selection in international matches the number runs scored by batsmen and wickets taken by bowlers serves as a natural way of quantifying the performance of a cricketer traditionally the batsmen and bowlers are rated on their batting or bowling average respectively however in a game like cricket it is always important the manner in which one scores the runs or claims a wicket scoring runs against a strong bowling lineup or delivering a brilliant performance against a team with strong batting lineup deserves more credit a players average is not able to capture this aspect of the game in this paper we present a refined method to quantify the quality of runs scored by a batsman or wickets taken by a bowler we explore the application of social network analysis sna to rate the players in a team performance we generate directed and weighted network of batsmenbowlers using the playervsplayer information available for test cricket and odi cricket additionally we generate network of batsmen and bowlers based on the dismissal record of batsmen in the history of cricket test 18772011 and odi 19712011 our results show that it m muralitharan is the most successful bowler in history of cricket our approach could potentially be applied in domestic matches to judge a players performance which in turn pave the way for a balanced team selection for international matches,0
data from software repositories have become an important foundation for the empirical study of software engineering processes a recurring theme in the repository mining literature is the inference of developer networks capturing eg collaboration coordination or communication from the commit history of projects most of the studied networks are based on the coauthorship of software artefacts because this neglects detailed information on code changes and code ownership we introduce git2net a scalable python software that facilitates the extraction of finegrained coediting networks in large git repositories it uses text mining techniques to analyse the detailed history of textual modifications within files we apply our tool in two case studies using github repositories of multiple open source as well as a commercial software project specifically we use data on more than 12 million commits and more than 25000 developers to test a hypothesis on the relation between developer productivity and coediting patterns in software teams we argue that git2net opens up a massive new source of highresolution data on human collaboration patterns that can be used to advance theory in empirical software engineering computational social science and organisational studies,0
we study the effect of globalization on the korean market one of the emerging markets some characteristics of the korean market are different from those of the mature market according to the latest market data and this is due to the influence of foreign markets or investors we concentrate on the market network structures over the past two decades with knowledge of the history of the market and determine the globalization effect and market integration as a function of time,0
the inspection game is the canonical model for the strategic conflict between law enforcement inspectors and citizens potential criminals its classical mixedstrategy nash equilibrium msne is afflicted by a paradox the equilibrium crime rate is independent of both the penalty size p and the crime gain g undermining the efficacy of deterrence policy we reexamine this challenge using evolutionary game theory focusing on the longterm fixation probabilities of strategies in finite asymmetric population sizes subject to demographic noise the deterministic limit of our model exhibits stable limit cycles around the msne which coincides with the neutral fixed point of the equilibrium analysis crucially in finite populations demographic noise drives the system away from this cycle and toward absorbing states our results demonstrate that high absolute penalties p are highly effective at suppressing crime by influencing the geometry of the deterministic dynamics which in turn biases the fixation probability toward the criminal extinction absorbing state thereby restoring the intuitive role of p furthermore we reveal a ushaped policy landscape where both high penalties and light penalties where p approx g are successful suppressors maximizing criminal risk at intermediate penalty levels most critically we analyze the realistic asymptotic limit of extreme population sizes asymmetry where inspectors are exceedingly rare in this limit the systems dynamic outcome is entirely decoupled from the citizen payoff parameters p and g and is instead determined by the initial frequency of crime relative to the deterrence threshold the ratio of inspection cost to reward for catching a criminal this highlights that effective crime suppression requires managing the interaction between deterministic dynamics demographic noise and initial conditions,0
this essay argues that a new form of democracy an emergent democracy will develop as a result of the use of internet communication tools and platforms such as blogs the essay explores a variety of tools available and explores the history of democracy modern experiments with democracy and how these tools might support democracy the essay also explores concerns as these new tools emerge these issues include concerns such as privacy and the societally negative use of these tools by corporations totalitarian regimes and terrorists,0
knowledge amount is an integral indicator of the development of society humanity produces knowledge in response to challenges from nature and society knowledge production depends on population size and human productivity productivity is a function of knowledge amount the purpose of this study is to find this function and verify it on empirical material including global demographic and information data the productivity function is a basic element of the theory that results in the dynamic equations of knowledge production and population growth a separate problem is the quantitative assessment of knowledge to solve it we consider knowledge representations in the form of patents articles and books knowledge is stored in various types of devices which together form a global informational storage storage capacity is increasing rapidly as digital technology advances we compare storage capacity with the memory occupied by the forms of knowledge representation the results obtained in this study contribute to the theory of knowledge production and related demographic dynamics and allow us to deepen our understanding of civilization development,0
we study the effect of noisy infection contact and recovery rates on the distribution of outbreak sizes in the stochastic sir model the rates are modeled as ornsteinuhlenbeck processes with finite correlation time and variance which we illustrate using outbreak data from the rsv 20192020 season in the us in the limit of large populations we find analytical solutions for the outbreaksize distribution in the longcorrelated adiabatic and shortcorrelated white noise regimes and demonstrate that the distribution can be highly skewed with significant probabilities for large fluctuations away from meanfield theory furthermore we assess the relative contribution of demographic and reactionrate noise on the outbreaksize variance and show that demographic noise becomes irrelevant in the presence of slowly varying reactionrate noise but persists for large system sizes if the noise is fast finally we show that the crossover to the whitenoise regime typically occurs for correlation times that are on the same order as the characteristic recovery time in the model,0
we introduce a simple multiplicative model to describe the temporal behavior and the ultimate outcome of an epidemic our model accounts in a minimalist way for the competing influences of imposing publichealth restrictions when the epidemic is severe and relaxing restrictions when the epidemic is waning our primary results are that different instances of an epidemic with identical starting points have disparate outcomes and each epidemic temporal history is strongly fluctuating,0
multiplayer online battle arena has become a popular game genre it also received increasing attention from our research community because they provide a wealth of information about human interactions and behaviors a major problem is extracting meaningful patterns of activity from this type of data in a way that is also easy to interpret here we propose to exploit tensor decomposition techniques and in particular nonnegative tensor factorization to discover hidden correlated behavioral patterns of play in a popular game league of legends we first collect the entire gaming history of a group of about one thousand players totaling roughly 100k matches by applying our methodological framework we then separate players into groups that exhibit similar features and playing strategies as well as similar temporal trajectories ie behavioral progressions over the course of their gaming history this will allow us to investigate how players learn and improve their skills,0
the problem of identifying the optimal location for a new retail store has been the focus of past research especially in the field of land economy due to its importance in the success of a business traditional approaches to the problem have factored in demographics revenue and aggregated human flow statistics from nearby or remote areas however the acquisition of relevant data is usually expensive with the growth of locationbased social networks fine grained data describing user mobility and popularity of places has recently become attainable in this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from foursquare in new york the features we mine are based on two general signals geographic where features are formulated according to the types and density of nearby places and user mobility which includes transitions between venues or the incoming flow of mobile users from distant areas our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis although variations may exist too as explained by heterogeneities in the way retail facilities attract users we also show that performance improves significantly when combining multiple features in supervised learning algorithms suggesting that the retail success of a business may depend on multiple factors,0
over the last decade proposal success rates in the fundamental sciences have dropped significantly astronomy and related fields funded by nasa and nsf are no exception data across agencies show that this is not principally the result of a decline in proposal merit the proportion of proposals receiving high rankings is largely unchanged nor of a shift in proposer demographics seniority gender and institutional affiliation have all remained unchanged nor of an increase beyond inflation in the average requested funding per proposal nor of an increase in the number of proposals per investigator in any one year rather the statistics are consistent with a scenario in which agency budgets for competed research are flat or decreasing in inflationadjusted dollars the overall population of investigators has grown and a larger proportion of these investigators are resubmitting meritorious but unfunded proposals this white paper presents statistics which support this conclusion as well as recent research on the time cost of proposal writing versus that of producing publishable results we conclude that an aspirational proposal success rate of 3035 would still provide a healthily competitive environment for researchers would more fully utilize the scientific capacity of the communitys facilities and missions and provide relief to the funding agencies who face the logistics of everincreasing volumes of proposals,0
the world is changing at an everincreasing pace and it has changed in a much more fundamental way than one would think primarily because it has become more connected and interdependent than in our entire history every new product every new invention can be combined with those that existed before thereby creating an explosion of complexity structural complexity dynamic complexity functional complexity and algorithmic complexity how to respond to this challenge and what are the costs,0
data from software repositories have become an important foundation for the empirical study of software engineering processes a recurring theme in the repository mining literature is the inference of developer networks capturing eg collaboration coordination or communication from the commit history of projects most of the studied networks are based on the coauthorship of software artefacts defined at the level of files modules or packages while this approach has led to insights into the social aspects of software development it neglects detailed information on code changes and code ownership eg which exact lines of code have been authored by which developers that is contained in the commit log of software projects addressing this issue we introduce git2net a scalable python software that facilitates the extraction of finegrained coediting networks in large git repositories it uses text mining techniques to analyse the detailed history of textual modifications within files this information allows us to construct directed weighted and timestamped networks where a link signifies that one developer has edited a block of source code originally written by another developer our tool is applied in case studies of an open source and a commercial software project we argue that it opens up a massive new source of highresolution data on human collaboration patterns,0
our recent minimal model of cooperation p gawronski et al physica a 388 2009 3581 is modified as to allow for timedependent altruism this evolution is based on reputation of other agents which in turn depends on history we show that this modification leads to two absorbing states of the whole system where the cooperation flourishes in one state and is absent in another one the effect is compared with the results obtained with the model of indirect reciprocity where the altruism of agents is constant,0
hysteresis is treated as a history dependent branching and the use of the classical preisach model for the analysis of macroeconomic hysteresis is first discussed then a new preisachtype model is introduced as a macroeconomic aggregation of more realistic microeconomic hysteresis than in the case of the classical preisach model it is demonstrated that this model is endowed with a more general mechanism of branching and may account for the continuous evolution of the economy and its effect on hysteresis furthermore it is shown that the sluggishness of economic recovery is an intrinsic manifestation of hysteresis branching,0
network science is the field dedicated to the investigation and analysis of complex systems via their representations as networks we normally model such networks as graphs sets of nodes connected by sets of edges and a number of node and edge attributes this deceptively simple object is the starting point of neverending complexity due to its ability to represent almost every facet of reality chemical interactions protein pathways inside cells neural connections inside the brain scientific collaborations financial relations citations in art history just to name a few examples if we hope to make sense of complex networks we need to master a large analytic toolbox graph and probability theory linear algebra statistical physics machine learning combinatorics and more this book aims at providing the first access to all these tools it is intended as an atlas because its interest is not in making you a specialist in using any of these techniques rather after reading this book you will have a general understanding about the existence and the mechanics of all these approaches you can use such an understanding as the starting point of your own career in the field of network science this has been so far an interdisciplinary endeavor the founding fathers of this field come from many different backgrounds mathematics sociology computer science physics history digital humanities and more this atlas is charting your path to be something different from all of that a pure network scientist,0
higherorder network analysis uses the ideas of hypergraphs simplicial complexes multilinear and tensor algebra and more to study complex systems these are by now well established mathematical abstractions whats new is that the ideas can be tested and refined on the type of largescale data arising in todays digital world this research area therefore is making an impact across many applications here we provide a brief history guide and survey,0
consumption practices are determined by a combination of economic social and cultural forces we posit that lower economic constraints leave more room to diversify consumption along cultural and social aspects in the form of omnivorous or lifestylebased niche consumption we provide empirical evidence for this diversity hypothesis by analysing millions of mobiletracked visits from thousands of census block groups to thousands of stores in new york state the results show that high income is significantly associated with diverse consumption across brands and price levels the associations between diversity and income persist but are less prominent for necessitybased consumption and for the densely populated and demographically diverse new york city the associations replicate for education as an alternative measure of socioeconomic status and for the state of texas we further illustrate that the associations cannot be explained by simple geographic constraints including the neighbourhoods demographic diversity the residents geographic mobility and the stores local availability so deeper social and cultural factors must be at play,0
social evolutionary theory seeks to explain increases in the scale and complexity of human societies from origins to present over the course of the twentieth century social evolutionary theory largely fell out of favor as a way of investigating human history just when advances in complex systems science and computer science saw the emergence of powerful new conceptions of complex systems and in particular new methods of measuring complexity we propose that these advances in our understanding of complex systems and computer science should be brought to bear on our investigations into human history to that end we present a new framework for modeling how human societies coevolve with their biotic environments recognizing that both a society and its environment are computers this leads us to model the dynamics of each of those two systems using the same new kind of computational machine which we define here for simplicity we construe a society as a set of interacting occupations and technologies similarly under such a model a biotic environment is a set of interacting distinct ecological and environmental processes this provides novel ways to characterize social complexity which we hope will cast new light on the archaeological and historical records our framework also provides a natural way to formalize both the energetic thermodynamic costs required by a society as it runs and the ways it can extract thermodynamic resources from the environment in order to pay for those costs and perhaps to grow with any leftover resources,0
flash flooding events with their intense and sudden nature present unique challenges for disaster researchers and emergency planners to quantify the extent to which hotspots of flash flooding share similar social and physical features the research uses community scale crowdsourced data and k means clustering crowdsourced data offers the potential to allocate limited resources to improve spatial understanding and to minimize the future effects of natural hazards the research evaluates the impacts of tropical storm imelda on houston metropolitan and hurricane ida on new york city it develops a combined flash flood impact index based on fema claims 311 calls and waze traffic reports which is able to capture a combination of crowdsourced data for the societal impact of flash flooding in addition k means clustering offers an essential tool for evaluating attributes associated with flash flooding events by grouping data into k number clusters of features thus k means clustering evaluates the significance of a communitys socio demographic social capital and physical features to the combined flood impact index to ensure accessibility and replicability to different types of communities our research uses publicly available datasets to understand how socio demographic data social capital and physical connectivity and development affect flash flood resilience the findings reveal the intricate relationships associated with flash flooding impact as a combination of socio demographic and physical features for instance the cluster with the highest scores of the flash flood impact index from tropical storm imelda had the highest percentage of minority population and lower income while the cluster with the second highest score flash flood impact index had the highest percentage of impervious surface and number of pois,0
the manhattan project was one of the largest scientific collaborations ever undertaken it operated thanks to a complex social network of extraordinary minds and it became undoubtedly one of the most remarkable intellectual efforts of human history it also had devastating consequences during and after the atomic bombings of hiroshima and nagasaki despite the loss of hundreds of thousands of human lives during the bombing and the subsequent events the scientific journey itself stands as a testament to human achievement as highlighted in christopher nolans film portrayal of oppenheimer,0
i give a brief overview of arxiv history and describe the current state of arxiv practice both technical and sociological this commentary originally appeared in the embo journal 19 oct 2016 it was intended as an update on comments from the late 1990s regarding use of preprints by biologists or lack thereof but may be of interest to practitioners of other disciplines it is based largely on a keynote presentation i gave to the asapbio inaugural meeting in feb 2016 and responds as well to some followup questions,0
the characteristics of social partners have long been hypothesized as influential in guiding group interactions understanding how demographic cues impact networks of creative collaborators is critical for elevating creative performances therein we conducted a randomized experiment to investigate how the knowledge of peers gender and racial identities distorts peoples connection patterns and the resulting creative outcomes in a dynamic social network consistent with prior work we found that creative inspiration links are primarily formed with top ideagenerators however when gender and racial identities are known not only is there 1 an increase of 8203 in the odds of samegender connections but not for samerace connections but 2 the semantic similarity of ideasets stimulated by these connections also increase significantly compared to demographyagnostic networks negatively impacting the outcomes of divergent creativity we found that ideas tend to be more homogeneous within demographic groups than between taking away diversitybonuses from similaritybased links and partly explaining the results these insights can inform intelligent interventions to enhance networkwide creative performances,0
using largescale call detail records of anonymised mobile phone service subscribers with demographic and location information we investigate how a longdistance residential move within the country affects the mobile communication patterns between an ego who moved and a frequently called alter who did not move by using clustering methods in analysing the call frequency time series we find that such egoalter pairs are grouped into two clusters those with the call frequency increasing and those with the call frequency decreasing after the move of the ego this indicates that such residential moves are correlated with a change in the communication pattern soon after moving we find that the premove calling behaviour is a relevant predictor for the postmove calling behaviour while demographic and location information can help in predicting whether the call frequency will rise or decay they are not relevant in predicting the actual call frequency volume we also note that at four months after the move most of these close pairs maintain contact even if the call frequency is decreased,0
massive vaccination against pandemics such as coronavirus sarscov2 presents several complexities the criteria to assess public health policies are fundamental to distribute vaccines in an effective way in order to avoid as many infections and deaths as possible usually these policies are focused on determining sociodemographic groups of people and establishing a vaccination order among these groups this work focuses on optimizing the way of distributing vaccines among the different populations of a region for a period of time once established the priority sociodemographic groups for this aim we use a seir model which takes into account vaccination also for this model we prove theoretical results concerning the convergence of solutions on the longterm and the stability of fixed points and analyze the impact of an hypothetical vaccination during the covid19 pandemics in spain after that we introduce a heuristic approach in order to minimize the covid19 spreading by planning effective vaccine distributions among the populations of a region over a period of time as an application the impact of distributing vaccines in the valencian community spain according to this method is computed in terms of the number of saved infected individuals,0
social contagion is the process in which people adopt a belief idea or practice from a neighbor and pass it along to someone else for over 100 years scholars of social contagion have almost exclusively made the same implicit assumption that only one belief idea or practice spreads through the population at a time it is a default assumption that we dont bother to state let alone justify the assumption is so ingrained that our literature doesnt even have a word for whatever is to be diffused because we have never needed to discuss more than one of them but this assumption is obviously false millions of beliefs ideas and practices lets call them diffusants spread through social contagion every day to assume that diffusants spread one at a time or more generously that they spread independently of one another is to assume that interactions between diffusants have no influence on adoption patterns this could be true or it could be wildly off the mark weve never stopped to find out this paper makes a direct comparison between the spread of independent and interdependent beliefs using simulations observational data and a 2400subject laboratory experiment i find that in assuming independence between diffusants scholars have overlooked social processes that fundamentally change the outcomes of social contagion interdependence between beliefs generates polarization irrespective of social network structure homophily demographics politics or any other commonly cited cause it also coordinates structures of beliefs that can have both internal justification and social support without any grounding in external truth,0
the functioning of the cryptocurrency bitcoin relies on the open availability of the entire history of its transactions this makes it a particularly interesting socioeconomic system to analyse from the point of view of network science here we analyse the evolution of the network of bitcoin transactions between users we achieve this by using the complete transaction history from december 5th 2011 to december 23rd 2013 this period includes three bubbles experienced by the bitcoin price in particular we focus on the global and local structural properties of the user network and their variation in relation to the different period of price surge and decline by analysing the temporal variation of the heterogeneity of the connectivity patterns we gain insights on the different mechanisms that take place during bubbles and find that hubs ie the most connected nodes had a fundamental role in triggering the burst of the second bubble finally we examine the local topological structures of interactions between users we discover that the relative frequency of triadic interactions experiences a strong change before during and after a bubble and suggest that the importance of the hubs grows during the bubble these results provide further evidence that the behaviour of the hubs during bubbles significantly increases the systemic risk of the bitcoin network and discuss the implications on public policy interventions,0
the present research is about the analysis of the wave function of the universe applied to cosmological and quantum states also the wave function includes the history of the universe itself in several states starting from the big bang and going to the actual time regarding the entropic consideration of hawking for a black hole the wave function contains all the information of the matter that can collapse into a black hole and the information is transmitted as radiation as a consequence the information is analyzed as a quantum information from qubits idea,0
the murder of george floyd by police in may 2020 sparked international protests and renewed attention in the black lives matter movement here we characterize ways in which the online activity following george floyds death was unparalleled in its volume and intensity including setting records for activity on twitter prompting the saddest day in the platforms history and causing george floyds name to appear among the ten most frequently used phrases in a day where he is the only individual to have ever received that level of attention who was not known to the public earlier that same week further we find this attention extended beyond george floyd and that more black victims of fatal police violence received attention following his death than during other past moments in black lives matters history we place that attention within the context of prior online racial justice activism by showing how the names of black victims of police violence have been lifted and memorialized over the last 12 years on twitter our results suggest that the 2020 wave of attention to the black lives matter movement centered past instances of police violence in an unprecedented way demonstrating the impact of the movements rhetorical strategy to say their names,0
us funding agencies alone distribute a yearly total of roughly 65b dollars largely through the process of proposal peer review scientists compete for project funding by submitting grant proposals which are evaluated by selected panels of peer reviewers similar funding systems are in place in most advanced democracies however in spite of its venerable history proposal peer review is increasingly struggling to deal with the increasing mismatch between demand and supply of research funding,0
by means of laboratory experiment i examine the relation between fairness judgments made behind the veil of ignorance and actual behavior in a model situation of income inequality as the evidence shows when material selfinterest is at stake vast majority of subjects tends to abandon the fairness norm rather small regard for efficiency is present in the data furthermore as low income players go through a sequence of games against high earners and experience changes in income disparity the history effect proves to override structural characteristics of the redistribution game,0
metrics derived from twitter and other social mediaoften referred to as altmetricsare increasingly used to estimate the broader social impacts of scholarship such efforts however may produce highly misleading results as the entities that participate in conversations about science on these platforms are largely unknown for instance if altmetric activities are generated mainly by scientists does it really capture broader social impacts of science here we present a systematic approach to identifying and analyzing scientists on twitter our method can identify scientists across many disciplines without relying on external bibliographic data and be easily adapted to identify other stakeholder groups in science we investigate the demographics sharing behaviors and interconnectivity of the identified scientists we find that twitter has been employed by scholars across the disciplinary spectrum with an overrepresentation of social and computer and information scientists underrepresentation of mathematical physical and life scientists and a better representation of women compared to scholarly publishing analysis of the sharing of urls reveals a distinct imprint of scholarly sites yet only a small fraction of shared urls are sciencerelated we find an assortative mixing with respect to disciplines in the networks between scientists suggesting the maintenance of disciplinary walls in social media our work contributes to the literature both methodologically and conceptuallywe provide new methods for disambiguating and identifying particular actors on social media and describing the behaviors of scientists thus providing foundational information for the construction and use of indicators on the basis of social media metrics,0
in this short article i leverage the national crime victimization survey from 1992 to 2022 to examine how income education employment and key demographic factors shape the type of crime victims experience violent vs property using balanced classification splits and logistic regression models evaluated by f1score there is an isolation of the socioeconomic drivers of victimization group a models and then an introduction of demographic factors such as age gender race and marital status controls called group b models the results consistently proves that higher income and education lower the odds of violent relative to property crime while men younger individuals and racial minorities face disproportionately higher violentcrime risks on the geographic spectrum the suburban models achieve the strongest predictive performance with an accuracy of 0607 and f1 of 0590 urban areas benefit from adding education and employment predictors and crime in rural areas are still unpredictable using these current factors the patterns found in this study shows the need for specific interventions like educational investments in metropolitan settings economic support in rural communities and demographicaware prevention strategies,0
this paper explores the relationships between migration and trade using a complexnetwork approach we show that i both weighted and binary versions of the networks of international migration and trade are strongly correlated ii such correlations can be mostly explained by country economicdemographic size and geographical distance iii pairs of countries that are more central in the internationalmigration network trade more,0
the kumbh is a religious hindu festival that has been celebrated for centuries the 2013 kumbh mela a grander form of the annual kumbh was purportedly the largest gathering of people in human history many of the participants carried cell phones making it possible for us to use a datadriven approach to document this magnificent festival we used call detail records cdrs from participants attending the event a total of 390 million records to investigate its population dynamics we report here on some of our preliminary findings,0
massive changes in many aspects related to social groups of different socioeconomic backgrounds were caused by the covid19 pandemic and as a result the overall state of mental health was severely affected globally this study examined how the pandemic affected sri lankan citizens representing a range of socioeconomic backgrounds in terms of their mental health the data used in this research was gathered from 3020 households using a nationwide facetoface survey from which a processed dataset of 921 responses was considered for the final analysis four distinct factors were identified by factor analysis fa that was conducted and subsequently the population was clustered using unsupervised clustering to determine which population subgroups were affected similarly two such subgroups were identified where the respective relationships to the retrieved principal factors and their demographics were thoroughly examined and interpreted this resulted in the identification of contrasting perspectives between the two groups toward the maintenance and the state of social relationships during the pandemic which revealed that one group was more socially connected in nature resulting in their mental state being comparatively better in coping with the pandemic the other group was seen to be more socially reserved showing an opposite reaction toward social connections while their mental wellbeing declined showing symptoms such as loneliness and emptiness in response to the pandemic the study examined the role of social media and it was observed that social media was perceived as a substitute for the lack of social connections or primarily used as a coping mechanism in response to the challenges of the pandemic,0
cooperation as a selforganized collective behavior plays a significant role in the evolution of ecosystems and human society reinforcement learning rl offers a new perspective distinct from imitation learning in evolutionary games for exploring the mechanisms underlying its emergence however most existing studies with the public good game pgg employ a selfregarding setup or are on pairwise interaction networks players in the real world however optimize their policies based not only on their histories but also on the histories of their coplayers and the game is played in a group manner in the work we investigate the evolution of cooperation in the pgg under the otherregarding reinforcement learning evolutionary game orrleg on hypergraph by combining the qlearning algorithm and evolutionary game framework where other players action history is incorporated and the game is played on hypergraphs our results show that as the synergy factor increases the parameter interval is divided into three distinct regions the absence of cooperation ac medium cooperation mc and high cooperation hc accompanied by two abrupt transitions in the cooperation level near two transition points respectively interestingly we identify regular and anticoordinated chessboard structures in the spatial pattern that positively contribute to the first cooperation transition but adversely affect the second furthermore we provide a theoretical treatment for the first transition with an approximated first transition point and reveal that players with a longsighted perspective and low exploration rate are more likely to reciprocate kindness with each other thus facilitating the emergence of cooperation our findings contribute to understanding the evolution of human cooperation where otherregarding information and group interactions are commonplace,0
some of the most pivotal moments in intellectual history occur when a new ideology sweeps through a society supplanting an established system of beliefs in a rapid revolution of thought yet in many cases the new ideology is as extreme as the old why is it then that moderate positions so rarely prevail here in the context of a simple model of opinion spreading we test seven plausible strategies for deradicalizing a society and find that only one of them significantly expands the moderate subpopulation without risking its extinction in the process,0
the metaphor of cities as organisms has a long history in urban planning and a few urban modeling approaches have explicitly been linked to artificial life we propose in that paper to explore the extent of artificial life and artificial intelligence application to urban issues by constructing and exploring a citation network of around 225000 papers it shows that most of the literature is indeed application of methodologies and a rather strong modularity of approaches we finally develop alife concepts which have a strong potential for the development of new urban theories,0
we introduce and study a general model of social network formation and evolution based on the concept of preferential link formation between similar nodes and increased similarity between connected nodes the model is studied numerically and analytically for three definitions of similarity in common with realworld social networks we find coexistence of high and low connectivity phases and history dependence we suggest that the positive feedback between linking and similarity which is responsible for the models behaviour is also an important mechanism in real social networks,0
enhancing urban biodiversity is increasingly advanced as a naturebased solution that can help align public health and biodiversity conservation agendas yet research on the relationship between biodiversity and psychological wellbeing provides inconsistent results the goal of this interdisciplinary research was to understand how components of psychological wellbeing of green space users relate to species richness and abundance additionally we investigated how key characteristics that shape the way people interact with nature affinity towards nature and ecological knowledge moderate the wellbeing biodiversity relationship we sampled bird butterfly and plant in 24 urban gardens in israel and distributed 600 closeended questionnaires insitu to measure psychological wellbeing naturerelatedness ecological knowledge perceived species richness and demographics components of psychological wellbeing were mostly associated with perceived species richness and to lesser extent with actual species richness and abundance for all taxa naturerelatedness moderated these relationships respondents with high naturerelatedness demonstrated positive wellbeingrichness relationships while those with intermediate or low naturerelatedness showed no or even negative relationships respectively opposite relationships were recorded for bird abundance ie negative versus positive wellbeingabundance relationship for individuals with high or low naturerelatedness respectively overall individuals demonstrated poor ecological knowledge of species and this variable moderated the relationships between wellbeing components and perceived butterfly richness and bird abundance our results demonstrate that onesizedoesnotfitall when considering the relationship between psychological wellbeing and biodiversity and that affinity to nature is a key moderator for this relationship,0
in the months leading up to political elections in the united states forecasts are widespread and take on multiple forms including projections of what party will win the popular vote state ratings and predictions of vote margins at the state level it can be challenging to evaluate how accuracy changes in the lead up to election day or to put probabilistic forecasts into historical context moreover forecasts differ between analysts highlighting the many choices in the forecasting process with this as motivation here we take a more comprehensive view and begin to unpack some of the choices involved in election forecasting building on a prior compartmental model of election dynamics we present the forecasts of this model across months years and types of race by gathering together monthly forecasts of presidential senatorial and gubernatorial races from 20042022 we provide a largerscale perspective and discuss how treating polling data in different ways affects forecast accuracy we conclude with our 2024 election forecasts upcoming at the time of writing,0
in this introduction the editors showcase the papers by way of a structured project and seek to clarify the two key concepts cited in the title we consider the history of the idea that knowledge is an economic factor and discuss the question of whether regions provide the relevant system of reference for knowledgebased economic development current transformations in universityindustrygovernment relations at various levels can be considered as a metamorphosis in industry organization the concept of constructed advantage will be elaborated the various papers arising from a conference on this subject hosted by memorial university newfoundland canada are approached from this perspective,0
in the framework of complexity theory which provides a unified framework for natural and social sciences we study the complex and interesting problem of the internal structure similarities and differences between the mazatec dialects an endangered otomanguean language spoken in southeast mexico the analysis is based on some databases which are used to compute linguistic distances between the dialects the results are interpreted in the light of linguistics as well as statistical considerations and used to infer the history of the development of the observed pattern of diversity,0
throughout history a relatively small number of individuals have made a profound and lasting impact on science and society despite longstanding multidisciplinary interests in understanding careers of elite scientists there have been limited attempts for a quantitative careerlevel analysis here we leverage a comprehensive dataset we assembled allowing us to trace the entire career histories of nearly all nobel laureates in physics chemistry and physiology or medicine over the past century we find that although nobel laureates were energetic producers from the outset producing works that garner unusually high impact their careers before winning the prize follow relatively similar patterns as ordinary scientists being characterized by hot streaks and increasing reliance on collaborations we also uncovered notable variations along their careers often associated with the nobel prize including shifting coauthorship structure in the prizewinning work and a significant but temporary dip in the impact of work they produce after winning the nobel together these results document quantitative patterns governing the careers of scientific elites offering an empirical basis for a deeper understanding of the hallmarks of exceptional careers in science,0
we study an evolutionary spatial prisoners dilemma game where the fitness of the players is determined by both the payoffs from the current interaction and their history we consider the situation where the selection timescale is slower than the interaction timescale this is done by implementing probabilistic reproduction on an individual level we observe that both too fast and too slow reproduction rates hamper the emergence of cooperation in other words there exists an intermediate selection timescale that maximizes cooperation another factor we find to promote cooperation is a diversity of reproduction timescales,0
nuclear archaeology research provides scientific methods to reconstruct the operating histories of fissile material production facilities to account for past fissile material production while it has typically focused on analyzing material in permanent reactor structures spent fuel or highlevel waste also hold information about the reactor operation in this computational study we explore a bayesian inference framework for reconstructing the operational history from measurements of isotope ratios from a sample of nuclear waste we investigate two different inference models the first model discriminates between three potential reactors of origin magnox pwr and phwr while simultaneously reconstructing the fuel burnup time since irradiation initial enrichment and average power density the second model reconstructs the fuel burnup and time since irradiation of two batches of waste in a mixed sample each of the models is applied to a set of simulated test data and the performance is evaluated by comparing the highest posterior density regions to the corresponding parameter values of the test dataset both models perform well on the simulated test cases which highlights the potential of the bayesian inference framework and opens up avenues for further investigation,0
as the coronavirus disease 2019 covid19 continues to be a global pandemic policy makers have enacted and reversed nonpharmaceutical interventions with various levels of restrictions to limit its spread data driven approaches that analyze temporal characteristics of the pandemic and its dependence on regional conditions might supply information to support the implementation of mitigation and suppression strategies to facilitate research in this direction on the example of the united states we present a machinereadable dataset that aggregates relevant data from governmental journalistic and academic sources on the us county level in addition to countylevel timeseries data from the jhu csse covid19 dashboard our dataset contains more than 300 variables that summarize population estimates demographics ethnicity housing education employment and income climate transit scores and healthcare systemrelated metrics furthermore we present aggregated outofhome activity information for various points of interest for each county including grocery stores and hospitals summarizing data from safegraph and google mobility reports we compile information from ihme state and countylevel government and newspapers for dates of the enactment and reversal of nonpharmaceutical interventions by collecting these data as well as providing tools to read them we hope to accelerate research that investigates how the disease spreads and why spread may be different across regions our dataset and associated code are available at githubcomjieyingwucovid19uscountylevelsummaries,0
we present a toy model of opinion spreading in a society which combines a selfreinforcing mechanism with diffusion the relative strength of these two mechanisms called the affectability of the system is a free parameter of the model the model is run on a scalefree network and its asymptotic behaviour is investigated a surprising emergent effect is observed if every individual becomes more attentive to the opinions of others the society as a whole switches from a plurality of nuanced opinions into the state of an absolute consensus on an extreme opinion this counterintuitive emergent behaviour may help to explain certain paradoxes in human history,0
in the same sense as classical logic is a formal theory of truth the recently initiated approach called computability logic is a formal theory of computability it understands interactive computational problems as games played by a machine against the environment their computability as existence of a machine that always wins the game logical operators as operations on computational problems and validity of a logical formula as being a scheme of always computable problems the present contribution gives a detailed exposition of a soundness and completeness proof for an axiomatization of one of the most basic fragments of computability logic the logical vocabulary of this fragment contains operators for the so called parallel and choice operations and its atoms represent elementary problems ie predicates in the standard sense this article is selfcontained as it explains all relevant concepts while not technically necessary however familiarity with the foundational paper introduction to computability logic would greatly help the reader in understanding the philosophy underlying motivations potential and utility of computability logic the context that determines the value of the present results online introduction to the subject is available at and,0
in this paper we show several similarities among logic systems that deal simultaneously with deductive and quantitative inference we claim it is appropriate to call the tasks those systems perform as quantitative logic reasoning analogous properties hold throughout that class for whose members there exists a set of linear algebraic techniques applicable in the study of satisfiability decision problems in this presentation we consider as quantitative logic reasoning the tasks performed by propositional probabilistic logic firstorder logic with counting quantifiers over a fragment containing unary and limited binary predicates and propositional lukasiewicz infinitelyvalued probabilistic logic,0
logic programming has long being advocated for legal reasoning and several approaches have been put forward relying upon explicit representation of the law in logic programming terms in this position paper we focus on the proleg logicprogrammingbased framework for formalizing and reasoning with japanese presupposed ultimate fact theory specifically we examine challenges and opportunities in leveraging deep learning techniques for improving legal reasoning using proleg identifying four distinct options ranging from enhancing fact extraction using deep learning to endtoend solutions for reasoning with textual legal descriptions we assess advantages and limitations of each option considering their technical feasibility interpretability and alignment with the needs of legal practitioners and decisionmakers we believe that our analysis can serve as a guideline for developers aiming to build effective decisionsupport systems for the legal domain while fostering a deeper understanding of challenges and potential advancements by neurosymbolic approaches in legal applications,0
modal logics for reasoning about the power of coalitions capture the notion of effectivity functions associated with game forms the main goal of coalition logics is to provide formal tools for modeling the dynamics of a game frame whose states may correspond to different game forms the two classes of effectivity functions studied are the families of playable and truly playable effectivity functions respectively in this paper we generalize the concept of effectivity function beyond the yesno truth scale this enables us to describe the situations in which the coalitions assess their effectivity in degrees based on functions over the outcomes taking values in a finite ukasiewicz chain then we introduce two modal extensions of ukasiewicz finitevalued logic together with manyvalued neighborhood semantics in order to encode the properties of manyvalued effectivity functions associated with game forms as our main results we prove completeness theorems for the two newly introduced modal logics,0
computability logic cl see is a recently launched program for redeveloping logic as a formal theory of computability as opposed to the formal theory of truth that logic has more traditionally been formulas in it represent computational problems truth means existence of an algorithmic solution and proofs encode such solutions within the line of research devoted to finding axiomatizations for ever more expressive fragments of cl the present paper introduces a new deductive system cl12 and proves its soundness and completeness with respect to the semantics of cl conservatively extending classical predicate calculus and offering considerable additional expressive and deductive power cl12 presents a reasonable computationally meaningful constructive alternative to classical logic as a basis for applied theories to obtain a model example of such theories this paper rebuilds the traditional classicallogicbased peano arithmetic into a computabilitylogicbased counterpart among the purposes of the present contribution is to provide a starting point for what as the author wishes to hope might become a new line of research with a potential of interesting findings an exploration of the presumably quite unusual metatheory of clbased arithmetic and other clbased applied systems,0
in this work we answer a long standing request for temporal embeddings of deontic stit logics by introducing the multiagent stit logic tds the logic is based upon atemporal utilitarian stit logic yet the logic presented here will be neutral instead of committing ourselves to utilitarian theories we prove the logic tds sound and complete with respect to relational frames not employing any utilitarian function we demonstrate how these neutral frames can be transformed into utilitarian temporal frames while preserving validity last we discuss problems that arise from employing binary utility functions in a temporal setting,0
we consider the question of extending propositional logic to a logic of plausible reasoning and posit four requirements that any such extension should satisfy each is a requirement that some property of classical propositional logic be preserved in the extended logic as such the requirements are simpler and less problematic than those used in coxs theorem and its variants as with coxs theorem our requirements imply that the extended logic must be isomorphic to finiteset probability theory we also obtain specific numerical values for the probabilities recovering the classical definition of probability as a theorem with truth assignments that satisfy the premise playing the role of the possible cases,0
we present trichotomy results characterizing the complexity of reasoning with disjunctive logic programs to this end we introduce a certain definition schema for classes of programs based on a set of allowed arities of rules we show that each such class of programs has a finite representation and for each of the classes definable in the schema we characterize the complexity of the existence of an answer set problem next we derive similar characterizations of the complexity of skeptical and credulous reasoning with disjunctive logic programs such results are of potential interest on the one hand they reveal some reasons responsible for the hardness of computing answer sets on the other hand they identify classes of problem instances for which the problem is easy in p or easier than in general in np we obtain similar results for the complexity of reasoning with disjunctive programs under the supportedmodel semantics to appear in theory and practice of logic programming tplp,0
we consider propositional modal logic with two modal operators box and d in topological semantics box is interpreted as an interior operator and d as difference we show that some important topological properties are expressible in this language in addition we present a few logics and proofs of fmp and of completeness theorems,0
we investigate the computational complexity of admissibility of inference rules in infinitevalued ukasiewicz propositional logic  it was shown in that admissibility in  is checkable in pspace we establish that this result is optimal ie admissible rules of  are pspacecomplete in contrast derivable rules of  are known to be conpcomplete,0
defeasible reasoning is a simple but efficient approach to nonmonotonic reasoning that has recently attracted considerable interest and that has found various applications defeasible logic and its variants are an important family of defeasible reasoning methods so far no relationship has been established between defeasible logic and mainstream nonmonotonic reasoning approaches in this paper we establish close links to known semantics of logic programs in particular we give a translation of a defeasible theory d into a metaprogram pd we show that under a condition of decisiveness the defeasible consequences of d correspond exactly to the sceptical conclusions of pd under the stable model semantics without decisiveness the result holds only in one direction all defeasible consequences of d are included in all stable models of pd if we wish a complete embedding for the general case we need to use the kunen semantics of pd instead,0
we introduce versions of gametheoretic semantics gts for alternatingtime temporal logic atl in gts truth is defined in terms of existence of a winning strategy in a semantic evaluation game and thus the gametheoretic perspective appears in the framework of atl on two semantic levels on the object level in the standard semantics of the strategic operators and on the metalevel where gametheoretic logical semantics is applied to atl we unify these two perspectives into semantic evaluation games specially designed for atl the gametheoretic perspective enables us to identify new variants of the semantics of atl based on limiting the time resources available to the verifier and falsifier in the semantic evaluation game we introduce and analyse an unbounded and ordinal bounded gts and prove these to be equivalent to the standard tarskistyle compositional semantics we show that in these both versions of gts truth of atl formulae can always be determined in finite time ie without constructing infinite paths we also introduce a nonequivalent finitely bounded semantics and argue that it is natural from both logical and gametheoretic perspectives,0
intuitionistic propositional logic is proved to be an infinitely many valued logic by kurt gdel 1932 and it is proved by stanisaw jakowski 1936 to be a countably many valued logic in this paper we provide alternative proofs for these theorems by using models of saul kripke 1959 gdels proof gave rise to an intermediate propositional logic between intuitionistic and classical that is known nowadays as gdel or the gdeldummet logic and is studied by fuzzy logicians as well we also provide some results on the interdefinablility of propositional connectives in this logic,0
the paper proposes a new knowledge representation language called dlp which extends disjunctive logic programming with strong negation by inheritance the addition of inheritance enhances the knowledge modeling features of the language providing a natural representation of default reasoning with exceptions a declarative modeltheoretic semantics of dlp is provided which is shown to generalize the answer set semantics of disjunctive logic programs the knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in dlp the complexity of dlp is analyzed proving that inheritance does not cause any computational overhead as reasoning in dlp has exactly the same complexity as reasoning in disjunctive logic programming this is confirmed by the existence of an efficient translation from dlp to plain disjunctive logic programming using this translation an advanced kr system supporting the dlp language has been implemented on top of the dlv system and has subsequently been integrated into dlv,0
we consider systems of rational agents who act and interact in pursuit of their individual and collective objectives we study and formalise the reasoning of an agent or of an external observer about the expected choices of action of the other agents based on their objectives in order to assess the reasoners ability or expectation to achieve their own objective to formalize such reasoning we extend paulys coalition logic with three new modal operators of conditional strategic reasoning thus introducing the logic for local conditional strategic reasoning constr we provide formal semantics for the new conditional strategic operators in concurrent game models introduce the matching notion of bisimulation for each of them prove bisimulation invariance and hennessymilner property for each of them and discuss and compare briefly their expressiveness finally we also propose systems of axioms for each of the basic operators of constr and for the full logic,0
the four authors present their speculations about the future developments of mathematical logic in the twentyfirst century the areas of recursion theory proof theory and logic for computer science model theory and set theory are discussed independently,0
justification theory is a unifying semantic framework while it has its roots in nonmonotonic logics it can be applied to various areas in computer science especially in explainable reasoning its most central concept is a justification an explanation why a property holds or does not hold in a model in this paper we continue the study of justification theory by means of three major contributions the first is studying the relation between justification theory and game theory we show that justification frameworks can be seen as a special type of games the established connection provides the theoretical foundations for our next two contributions the second contribution is studying under which condition two different dialects of justification theory graphs as explanations vs trees as explanations coincide the third contribution is establishing a precise criterion of when a semantics induced by justification theory yields consistent results in the past proving that such semantics were consistent took cumbersome and elaborate proofs we show that these criteria are indeed satisfied for all common semantics of logic programming this paper is under consideration for acceptance in theory and practice of logic programming tplp,0
similaritybased logic programming briefly slp has been proposed to enhance the lp paradigm with a kind of approximate reasoning which supports flexible information retrieval applications this approach uses a fuzzy similarity relation r between symbols in the programs signature while keeping the syntax for program clauses as in classical lp another recent proposal is the qlpd scheme for qualified logic programming an extension of the lp paradigm which supports approximate reasoning and more this approach uses annotated program clauses and a parametrically given domain d whose elements qualify logical assertions by measuring their closeness to various users expectations in this paper we propose a more expressive scheme sqlprd which subsumes both slp and qlpd as particular cases we also show that sqlprd programs can be transformed into semantically equivalent qlpd programs as a consequence existing qlpd implementations can be used to give efficient support for similaritybased reasoning,0
defeasible logics provide several linguistic features to support the expression of defeasible knowledge there is also a wide variety of such logics expressing different intuitions about defeasible reasoning however the logics can only combine in trivial ways this limits their usefulness in contexts where different intuitions are at play in different aspects of a problem in particular in some legal settings different actors have different burdens of proof which might be expressed as reasoning in different defeasible logics in this paper we introduce annotated defeasible logic as a flexible formalism permitting multiple forms of defeasibility and establish some properties of the formalism this paper is under consideration for acceptance in theory and practice of logic programming,0
the logics of knowledge are modal logics that have been shown to be effective in representing and reasoning about knowledge in multiagent domains relatively few computational frameworks for dealing with computation of models and useful transformations in logics of knowledge eg to support multiagent planning with knowledge actions and degrees of visibility have been proposed this paper explores the use of logic programming lp to encode interesting forms of logics of knowledge and compute kripke models the lp modeling is expanded with useful operators on kripke structures to support multiagent planning in the presence of both worldaltering and knowledge actions this results in the first ever implementation of a planner for this type of complex multiagent domains,0
the importance of intuitionistic temporal logics in computer science and artificial intelligence has become increasingly clear in the last few years from the prooftheory point of view intuitionistic temporal logics have made it possible to extend functional languages with new features via type theory while from its semantical perspective several logics for reasoning about dynamical systems and several semantics for logic programming have their roots in this framework in this paper we consider several axiomatic systems for intuitionistic linear temporal logic and show that each of these systems is sound for a class of structures based either on kripke frames or on dynamic topological systems our topological semantics features a new interpretation for the henceforth modality that is a natural intuitionistic variant of the classical one using the soundness results we show that the seven logics obtained from the axiomatic systems are distinct,0
sequenttype proof systems constitute an important and widelyused class of calculi wellsuited for analysing proof search in my masters thesis i introduce sequenttype calculi for a variant of default logic employing lukasiewiczs threevalued logic as the underlying base logic this version of default logic has been introduced by radzikowska addressing some representational shortcomings of standard default logic more specifically the calculi discussed in my thesis axiomatise brave and skeptical reasoning for this version of default logic respectively following the sequent method first introduced in the context of nonmonotonic reasoning by bonatti and olivetti which employ a complementary calculus for axiomatising invalid formulas taking care of expressing the consistency condition of defaults,0
in this paper we present an alternative interpretation of propositional inquisitive logic as an epistemic logic of knowing how in our setting an inquisitive logic formula  being supported by a state is formalized as knowing how to resolve  more colloquially knowing how  is true holds on the s5 epistemic model corresponding to the state based on this epistemic interpretation we use a dynamic epistemic logic with both knowhow and knowthat operators to capture the epistemic information behind the innocentlooking connectives in inquisitive logic we show that the set of valid knowhow formulas corresponds precisely to the inquisitive logic the main result is a complete axiomatization with intuitive axioms using the full dynamic epistemic language moreover we show that the knowhow operator and the dynamic operator can both be eliminated without changing the expressivity over models which is consistent with the modal translation of inquisitive logic existing in the literature we hope our framework can give an intuitive alternative interpretation of various concepts and technical results in inquisitive logic and also provide a powerful and flexible tool to do inquisitive reasoning in an epistemic context,0
we present a general logical framework for reasoning about agents cognitive attitudes of both epistemic type and motivational type we show that it allows us to express a variety of relevant concepts for qualitative decision theory including the concepts of knowledge belief strong belief conditional belief desire conditional desire strong desire and preference we also present two extensions of the logic one by the notion of choice and the other by dynamic operators for belief change and desire change and we apply the former to the analysis of singlestage games under incomplete information we provide sound and complete axiomatizations for the basic logic and for its two extensions the paper is under consideration in theory and practice of logic programming tplp,0
we seize the opportunity of the publication of selected papers from the emphlogic categories semantics workshop in the emphjournal of applied logic to survey some current trends in logic namely intuitionistic and linear type theories that interweave categorical geometrical and computational considerations we thereafter present how these rich logical frameworks can model the way language conveys meaning,0
this is the list of the full papers accepted for presentation at the 32nd international conference on logic programming new york city usa october 1821 2016 in addition to the main conference itself iclp hosted four preconference workshops the autumn school on logic programing and a doctoral consortium the final versions of the full papers will be published in a special issue of the journal theory and practice of logic programming tplp we received eighty eight abstract submissions of which twenty seven papers were accepted for publication as tplp rapid communications papers deemed of sufficiently high quality to be presented as the conference but not enough to be appear in tplp will be published as technical communications in the oasics series fifteen papers fell into this category,0
probabilistic logic programming is an effective formalism for encoding problems characterized by uncertainty some of these problems may require the optimization of probability values subject to constraints among probability distributions of random variables here we introduce a new class of probabilistic logic programs namely probabilistic optimizable logic programs and we provide an effective algorithm to find the best assignment to probabilities of random variables such that a set of constraints is satisfied and an objective function is optimized this paper is under consideration for acceptance in theory and practice of logic programming,0
epistemic logic programs constitute an extension of the stable models semantics to deal with new constructs called subjective literals informally speaking a subjective literal allows checking whether some regular literal is true in all stable models or in some stable model as it can be imagined the associated semantics has proved to be nontrivial as the truth of the subjective literal may interfere with the set of stable models it is supposed to query as a consequence no clear agreement has been reached and different semantic proposals have been made in the literature unfortunately comparison among these proposals has been limited to a study of their effect on individual examples rather than identifying general properties to be checked in this paper we propose an extension of the wellknown splitting property for logic programs to the epistemic case to this aim we formally define when an arbitrary semantics satisfies the epistemic splitting property and examine some of the consequences that can be derived from that including its relation to conformant planning and to epistemic constraints interestingly we prove through counterexamples that most of the existing proposals fail to fulfill the epistemic splitting property except the original semantics proposed by gelfond in 1991,0
probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities this paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs several such tasks such as computing the marginals given evidence and learning from partial interpretations have not really been addressed for probabilistic logic programs before the first contribution of this paper is a suite of efficient algorithms for various inference tasks it is based on a conversion of the program and the queries and evidence to a weighted boolean formula this allows us to reduce the inference tasks to wellstudied tasks such as weighted model counting which can be solved using stateoftheart methods known from the graphical model and knowledge compilation literature the second contribution is an algorithm for parameter estimation in the learning from interpretations setting the algorithm employs expectation maximization and is built on top of the developed inference algorithms the proposed approach is experimentally evaluated the results show that the inference algorithms improve upon the stateoftheart in probabilistic logic programming and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations,0
this paper tries to justify the relevance of an introductory course in mathematical logic in the philosophy curriculum for analyzing philosophical arguments in natural language it is argued that the representation of the structure of natural language arguments in freemans diagramming system can provide an intuitive foundation for the inferential processes involved in the use of first order logic natural deduction rules,0
relational descriptions have been used in formalizing diverse computational notions including for example operational semantics typing and acceptance by nondeterministic machines we therefore propose a restricted logical theory over relations as a language for specifying such notions our specification logic is further characterized by an ability to explicitly treat binding in object languages once such a logic is fixed a natural next question is how we might prove theorems about specifications written in it we propose to use a second logic called a reasoning logic for this purpose a satisfactory reasoning logic should be able to completely encode the specification logic associated with the specification logic are various notions of binding for quantifiers within formulas for eigenvariables within sequents and for abstractions within terms to provide a natural treatment of these aspects the reasoning logic must encode binding structures as well as their associated notions of scope free and bound variables and captureavoiding substitution further to support arguments about provability the reasoning logic should possess strong mechanisms for constructing proofs by induction and coinduction we provide these capabilities here by using a logic called g which represents relations over lambdaterms via definitions of atomic judgments contains inference rules for induction and coinduction and includes a special generic quantifier we show how provability in the specification logic can be transparently encoded in g we also describe an interactive theorem prover called abella that implements g and this twolevel logic approach and we present several examples that demonstrate the efficacy of abella in reasoning about computations,0
this paper concerns an expansion of firstorder belnapdunn logic whose connectives and quantifiers all have a counterpart in classical logic the language and logical consequence relation of this logic are defined a proof system for this logic is presented and the soundness and completeness of this proof system is established the minor differences between the presented proof system for the defined logic and a sound and complete proof system for the version of classical logic with the same language illustrates the close relationship between the logical consequence relations of these logics a clear characterization of the classical nature of the connectives and quantifiers of the defined logic is given by means of classical laws of logical equivalence moreover a simple embedding of this logic in classical logic is presented and the potential of the logic for dealing with inconsistencies and incompletenesses in inductive machine learning is discussed,0
we begin by discussing the history of quantum logic dividing it into three eras or lives the first life has to do with birkhoff and von neumanns algebraic approach in the 1930s the second life has to do with attempt to understand quantum logic as logic that began in the late 1950s and blossomed in the 1970s and the third life has to do with recent developments in quantum logic coming from its connections to quantum computation we discuss our own work connecting quantum logic to quantum computation viewing quantum logic as the logic of quantum registers storing qubits and make some speculations about mathematics based on quantum principles,0
in this paper a conditional logic is defined and studied this conditional logic dmbl is constructed as a deterministic counterpart to the bayesian conditional the logic is unrestricted so that any logical operations are allowed a notion of logical independence is also defined within the logic itself this logic is shown to be nontrivial and is not reduced to classical propositions a model is constructed for the logic completeness results are proved it is shown that any unconditioned probability can be extended to the whole logic dmbl the bayesian conditional is then recovered from the probabilistic dmbl at last it is shown why dmbl is compliant with lewis triviality,0
today many different probabilistic programming languages exist and even more inference mechanisms for these languages still most logic programming based languages use backward reasoning based on sld resolution for inference while these methods are typically computationally efficient they often can neither handle infinite andor continuous distributions nor evidence to overcome these limitations we introduce distributional clauses a variation and extension of satos distribution semantics we also contribute a novel approximate inference method that integrates forward reasoning with importance sampling a wellknown technique for probabilistic inference to achieve efficiency we integrate two logic programming techniques to direct forward sampling magic sets are used to focus on relevant parts of the program while the integration of backward reasoning allows one to identify and avoid regions of the sample space that are inconsistent with the evidence,0
this note is about the relationship between two theories of negation as failure one based on program completion the other based on stable models or answer sets francois fages showed that if a logic program satisfies a certain syntactic condition which is now called tightness then its stable models can be characterized as the models of its completion we extend the definition of tightness and fages theorem to programs with nested expressions in the bodies of rules and study tight logic programs containing the definition of the transitive closure of a predicate,0
this paper presents a simple decidable logic of functional dependence lfd based on an extension of classical propositional logic with dependence atoms plus dependence quantifiers treated as modalities within the setting of generalized assignment semantics for first order logic the expressive strength complete proof calculus and metaproperties of lfd are explored various language extensions are presented as well up to undecidable modalstyle logics for independence and dynamic logics of changing dependence models finally more concrete settings for dependence are discussed continuous dependence in topological models linear dependence in vector spaces and temporal dependence in dynamical systems and games,0
the past few years have seen a surge of interest in the field of probabilistic logic learning and statistical relational learning in this endeavor many probabilistic logics have been developed problog is a recent probabilistic extension of prolog motivated by the mining of large biological networks in problog facts can be labeled with probabilities these facts are treated as mutually independent random variables that indicate whether these facts belong to a randomly sampled program different kinds of queries can be posed to problog programs we introduce algorithms that allow the efficient execution of these queries discuss their implementation on top of the yapprolog system and evaluate their performance in the context of large networks of biological entities,0
this paper specifies an extensive form as a 5ary relation that is as a set of quintuples which satisfies eight abstract axioms each quintuple is understood to list a player a situation that is a name for an information set a decision node an action and a successor node accordingly the axioms are understood to specify abstract relationships between players situations nodes and actions such an extensive form is called a pentaform finally a pentaform game is defined to be a pentaform together with utility functions to ground this new specification in the literature the paper defines the concept of a traditional game to represent the literatures many specifications of finitehorizon and infinitehorizon games the papers main result is to construct an intuitive bijection between pentaform games and traditional games secondary results concern disaggregating pentaforms by subsets constructing pentaforms by unions and initial pentaform applications to selten subgames and perfectrecall an extensive application to dynamic programming is in streufert 2023 arxiv230203855,0
this paper presents a logic framework for modeling the interaction among deductive databases in a p2p peer to peer environment each peer joining a p2p system provides or imports data from its neighbors by using a set of mapping rules ie a set of semantic correspondences to a set of peers belonging to the same environment two different types of mapping rules are defined mapping rules allowing to import a maximal set of atoms not leading to inconsistency called maximal mapping rules and mapping rules allowing to import a minimal set of atoms needed to restore consistency called minimal mapping rules implicitly the use of maximal mapping rules states it is preferable to import as long as no inconsistencies arise whereas the use of minimal mapping rules states that it is preferable not to import unless a inconsistency exists the paper presents three different declarative semantics of a p2p system i the max weak model semantics in which mapping rules are used to import as much knowledge as possible from a peers neighborhood without violating local integrity constraints ii the min weak model semantics in which the p2p system can be locally inconsistent and the information provided by the neighbors is used to restore consistency that is to only integrate the missing portion of a correct but incomplete database iii the maxmin weak model semantics that unifies the previous two different perspectives captured by the max weak model semantics and min weak model semantics this last semantics allows to characterize each peer in the neighborhood as a resource used either to enrich integrate or to fix repair the knowledge so as to define a kind of integraterepair strategy for each peer under consideration in theory and practice of logic programming tplp,0
we study how large language models llms think through their representation space we propose a novel geometric framework that models an llms reasoning as flows embedding trajectories evolving where logic goes we disentangle logical structure from semantics by employing the same natural deduction propositions with varied semantic carriers allowing us to test whether llms internalize logic beyond surface form this perspective connects reasoning with geometric quantities such as position velocity and curvature enabling formal analysis in representation and concept spaces our theory establishes 1 llm reasoning corresponds to smooth flows in representation space and 2 logical statements act as local controllers of these flows velocities using learned representation proxies we design controlled experiments to visualize and quantify reasoning flows providing empirical validation of our theoretical framework our work serves as both a conceptual foundation and practical tools for studying reasoning phenomenon offering a new lens for interpretability and formal analysis of llms behavior,0
contextuality is a key signature of quantum nonclassicality which has been shown to play a central role in enabling quantum advantage for a wide range of informationprocessing and computational tasks we study the logic of contextuality from a structural point of view in the setting of partial boolean algebras introduced by kochen and specker in their seminal work these contrast with traditional quantum logic  la birkhoff and von neumann in that operations such as conjunction and disjunction are partial only being defined in the domain where they are physically meaningful we study how this setting relates to current work on contextuality such as the sheaftheoretic and graphtheoretic approaches we introduce a general free construction extending the commeasurability relation on a partial boolean algebra ie the domain of definition of the binary logical operations this construction has a surprisingly broad range of uses we apply it in the study of a number of issues including establishing the connection between the abstract measurement scenarios studied in the contextuality literature and the setting of partial boolean algebras formulating various contextuality properties in this setting including probabilistic contextuality as well as the strong stateindependent notion of contextuality given by kochenspecker paradoxes which are logically contradictory statements validated by partial boolean algebras specifically those arising from quantum mechanics investigating a logical exclusivity principle and its relation to the probabilistic exclusivity principle widely studied in recent work on contextuality as a step towards closing in on the set of quantumrealisable correlations developing some work towards a logical presentation of the hilbert space tensor product using logical exclusivity to capture some of its salient quantum features,0
graded modal logic is the formal language obtained from ordinary propositional modal logic by endowing its modal operators with cardinality constraints under the familiar possibleworlds semantics these augmented modal operators receive interpretations such as it is true at no fewer than 15 accessible worlds that or it is true at no more than 2 accessible worlds that we investigate the complexity of satisfiability for this language over some familiar classes of frames this problem is more challenging than its ordinary modal logic counterpartespecially in the case of transitive frames where graded modal logic lacks the treemodel property we obtain tight complexity bounds for the problem of determining the satisfiability of a given graded modal logic formula over the classes of frames characterized by any combination of reflexivity seriality symmetry transitivity and the euclidean property,0
this paper provides a gentle introduction to problem solving with the idp3 system the core of idp3 is a finite model generator that supports first order logic enriched with types inductive definitions aggregates and partial functions it offers its users a modeling language that is a slight extension of predicate logic and allows them to solve a wide range of search problems apart from a small introductory example applications are selected from problems that arose within machine learning and data mining research these research areas have recently shown a strong interest in declarative modeling and constraint solving as opposed to algorithmic approaches the paper illustrates that the idp3 system can be a valuable tool for researchers with such an interest the first problem is in the domain of stemmatology a domain of philology concerned with the relationship between surviving variant versions of text the second problem is about a somewhat related problem within biology where phylogenetic trees are used to represent the evolution of species the third and final problem concerns the classical problem of learning a minimal automaton consistent with a given set of strings for this last problem we show that the performance of our solution comes very close to that of a stateofthe art solution for each of these applications we analyze the problem illustrate the development of a logicbased model and explore how alternatives can affect the performance,0
a new syntactic characterization of problems complete via turing reductions is presented general canonical forms are developed in order to define such problems one of these forms allows us to define complete problems on ordered structures and another form to define them on unordered nonaristotelian structures using the canonical forms logics are developed for complete problems in various complexity classes evidence is shown that there cannot be any complete problem on aristotelian structures for several complexity classes our approach is extended beyond complete problems using a similar form a logic is developed to capture the complexity class npcap conp which very likely contains no complete problem,0
we present a logic programming framework that orchestrates multiple variants of an optimization problem and reasons about their results to support highstakes medical decisionmaking the logic programming layer coordinates the construction and evaluation of multiple optimization formulations translating solutions into logical facts that support further symbolic reasoning and ensure efficient resource allocationspecifically targeting the right patient right platform right escort right time right destination principle this capability is integrated into guardiantwin a decision support system for forward medical evacuation medevac where rapid and explainable resource allocation is critical through a series of experiments our framework demonstrates an average reduction in casualties by 3575 compared to standard baselines additionally we explore how users engage with the system via an intuitive interface that delivers explainable insights ultimately enhancing decisionmaking in critical situations this work demonstrates how logic programming can serve as a foundation for modular interpretable and operationally effective optimization in missioncritical domains,0
nonclassical generalizations of classical modal logic have been developed in the contexts of constructive mathematics and natural language semantics in this paper we discuss a general approach to the semantics of nonclassical modal logics via algebraic representation theorems we begin with complete lattices l equipped with an antitone operation neg sending 1 to 0 a completely multiplicative operation box and a completely additive operation diamond such lattice expansions can be represented by means of a set x together with binary relations vartriangleleft r and q satisfying some firstorder conditions used to represent lneg box and diamond respectively indeed any lattice l equipped with such a neg a multiplicative box and an additive diamond embeds into the lattice of propositions of a frame xvartriangleleftrq building on our recent study of fundamental logic we focus on the case where neg is dually selfadjoint aleq neg b implies bleqneg a and diamond neg aleqnegbox a in this case the representations can be constrained so that rq ie we need only add a single relation to xvartriangleleft to represent both box and diamond using these results we prove that a system of fundamental modal logic is sound and complete with respect to an elementary class of birelational structures xvartriangleleft r,0
in a recent line of research two familiar concepts from logic programming semantics unfounded sets and splitting were extrapolated to the case of epistemic logic programs the property of epistemic splitting provides a natural and modular way to understand programs without epistemic cycles but surprisingly was only fulfilled by gelfonds original semantics g91 among the many proposals in the literature on the other hand g91 may suffer from a kind of selfsupported unfounded derivations when epistemic cycles come into play recently the absence of these derivations was also formalised as a property of epistemic semantics called foundedness moreover a first semantics proved to satisfy foundedness was also proposed the socalled founded autoepistemic equilibrium logic faeel in this paper we prove that faeel also satisfies the epistemic splitting property something that together with foundedness was not fulfilled by any other approach up to date to prove this result we provide an alternative characterisation of faeel as a combination of g91 with a simpler logic we called founded epistemic equilibrium logic feel which is somehow an extrapolation of the stable model semantics to the modal logic s5 under consideration for acceptance in tplp,0
lewis theory of counterfactuals is the foundation of many contemporary notions of causality in this paper we extend this theory in the temporal direction to enable symbolic counterfactual reasoning on infinite sequences such as counterexamples found by a model checker and trajectories produced by a reinforcement learning agent in particular our extension considers a more relaxed notion of similarity between worlds and proposes two additional counterfactual operators that close a semantic gap between the previous two in this more general setting further we consider versions of counterfactuals that minimize the distance to the witnessing counterfactual worlds a common requirement in causal analysis to automate counterfactual reasoning in the temporal domain we introduce a logic that combines temporal and counterfactual operators and outline decision procedures for the satisfiability and tracechecking problems of this logic,0
we examine how well the stateoftheart sota models used in legal reasoning support abductive reasoning tasks abductive reasoning is a form of logical inference in which a hypothesis is formulated from a set of observations and that hypothesis is used to explain the observations the ability to formulate such hypotheses is important for lawyers and legal scholars as it helps them articulate logical arguments interpret laws and develop legal theories our motivation is to consider the belief that deep learning models especially large language models llms will soon replace lawyers because they perform well on tasks related to legal text processing but to do so we believe requires some form of abductive hypothesis formation in other words while llms become more popular and powerful we want to investigate their capacity for abductive reasoning to pursue this goal we start by building a logicaugmented dataset for abductive reasoning with 498697 samples and then use it to evaluate the performance of a sota model in the legal field our experimental results show that although these models can perform well on tasks related to some aspects of legal text processing they still fall short in supporting abductive reasoning tasks,0
topological semantics for modal logic based on the cantor derivative operator gives rise to derivative logics also referred to as dlogics unlike logics based on the topological closure operator dlogics have not previously been studied in the framework of dynamical systems which are pairs xf consisting of a topological space x equipped with a continuous function fcolon xto x we introduce the logics bfwk4c bfk4c and bfglc and show that they all have the finite kripke model property and are sound and complete with respect to the dsemantics in this dynamical setting in particular we prove that bfwk4c is the dlogic of all dynamic topological systems bfk4c is the dlogic of all td dynamic topological systems and bfglc is the dlogic of all dynamic topological systems based on a scattered space we also prove a general result for the case where f is a homeomorphism which in particular yields soundness and completeness for the corresponding systems bfwk4h bfk4h and bfglh the main contribution of this work is the foundation of a general proof method for finite model property and completeness of dynamic topological dlogics furthermore our result for bfglc constitutes the first step towards a proof of completeness for the trimodal topotemporal language with respect to a finite axiomatisation something known to be impossible over the class of all spaces,0
in this paper a conditional logic is defined and studied this conditional logic dmbl is constructed as close as possible to the bayesian and is unrestricted that is one is able to use any operator without restriction a notion of logical independence is also defined within the logic itself this logic is shown to be non trivial and is not reduced to classical propositions a model is constructed for the logic completeness results are proved it is shown that any unconditioned probability can be extended to the whole logic dmbl the bayesian is then recovered from the probabilistic dmbl at last it is shown why dmbl is compliant with lewis triviality,0
we provide a direct method for proving craig interpolation for a range of modal and intuitionistic logics including those containing a converse modality we demonstrate this method for classical tense logic its extensions with path axioms and for biintuitionistic logic these logics do not have straightforward formalisations in the traditional gentzenstyle sequent calculus but have all been shown to have cutfree nested sequent calculi the proof of the interpolation theorem uses these calculi and is purely syntactic without resorting to embeddings semantic arguments or interpreted connectives external to the underlying logical language a novel feature of our proof includes an orthogonality condition for defining duality between interpolants,0
in prooftheoretic semantics meaning is based on inference it may seen as the mathematical expression of the inferentialist interpretation of logic much recent work has focused on baseextension semantics in which the validity of formulas is given by an inductive definition generated by provability in a base of atomic rules baseextension semantics for classical and intuitionistic propositional logic have been explored by several authors in this paper we develop baseextension semantics for the classical propositional modal systems k kt k4 and s4 with square as the primary modal operator we establish appropriate soundness and completeness theorems and establish the duality between square and a natural presentation of lozenge we also show that our semantics is in its current form not complete with respect to euclidean modal logics our formulation makes essential use of relational structures on bases,0
this paper describes a resolution based description logic reasoning system called dlog dlog transforms description logic axioms into a prolog program and uses the standard prolog execution for efficiently answering instance retrieval queries from the description logic point of view dlog is an abox reasoning engine for the full shiq language the dlog approach makes it possible to store the individuals in a database instead of memory which results in better scalability and helps using description logic ontologies directly on top of existing information sources to appear in theory and practice of logic programming tplp,0
we present a syntactic abstraction method to reason about firstorder modal logics by using theorem provers for standard firstorder logic and for propositional modal logic,0
we obtain for the first time a modular manyvalued semantics for combined logics which is built directly from manyvalued semantics for the logics being combined by means of suitable universal operations over partial nondeterministic logical matrices our constructions preserve finitevaluedness in the context of multipleconclusion logics whereas unsurprisingly it may be lost in the context of singleconclusion logics besides illustrating our constructions over a wide range of examples we also develop concrete applications of our semantic characterizations namely regarding the semantics of strengthening a given manyvalued logic with additional axioms the study of conditions under which a given logic may be seen as a combination of simpler syntactically defined fragments whose calculi can be obtained independently and put together to form a calculus for the whole logic and also general conditions for decidability to be preserved by the combination mechanism,0
we propose a new version of formula size game for modal logic the game characterizes the equivalence of pointed kripkemodels up to formulas of given numbers of modal operators and binary connectives our game is similar to the wellknown adlerimmerman game however due to a crucial difference in the definition of positions of the game its winning condition is simpler and the second player does not have a trivial optimal strategy thus unlike the adlerimmerman game our game is a genuine twoperson game we illustrate the use of the game by proving a nonelementary succinctness gap between bisimulation invariant firstorder logic mathrmfo and basic modal logic mathrmml we also present a version of the game for the modal calculus mathrml and show that mathrmfo is also nonelementarily more succinct than mathrml,0
we present a proof system for a multimodal logic based on our previous work on a multimodal martinloef type theory the specification of modes modalities and implications between them is given as a mode theory ie a small 2category the logic is extended to a lambda calculus establishing a curryhoward correspondence,0
with help of a compact prologbased theorem prover for intuitionistic propositional logic we synthesize minimal assumptions under which a given formula formula becomes a theorem after applying our synthesis algorithm to cover basic abductive reasoning mechanisms we synthesize conjunctions of literals that mimic rows of truth tables in classical or intermediate logics and we abduce conditional hypotheses that turn the theorems of classical or intermediate logics into theorems in intuitionistic logic one step further we generalize our abductive reasoning mechanism to synthesize more expressive sequent premises using a minimal set of canonical formulas to which arbitrary formulas in the calculus can be reduced while preserving their provability organized as a selfcontained literate prolog program the paper supports interactive exploration of its content and ensures full replicability of our results,0
description logics are knowledge representation languages that have been designed to strike a balance between expressivity and computational tractability many different description logics have been developed and numerous computational problems for these logics have been studied for their computational complexity however essentially all complexity analyses of reasoning problems for description logics use the onedimensional framework of classical complexity theory the multidimensional framework of parameterized complexity theory is able to provide a much more detailed image of the complexity of reasoning problems in this paper we argue that the framework of parameterized complexity has a lot to offer for the complexity analysis of description logic reasoning problemswhen one takes a progressive and forwardlooking view on parameterized complexity tools we substantiate our argument by means of three case studies the first case study is about the problem of concept satisfiability for the logic alc with respect to nearly acyclic tboxes the second case study concerns concept satisfiability for alc concepts parameterized by the number of occurrences of union operators and the number of occurrences of full existential quantification the third case study offers a critical look at data complexity results from a parameterized complexity point of view these three case studies are representative for the wide range of uses for parameterized complexity methods for description logic problems,0
reasoning over knowledge graphs kgs is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations current approaches rely on learning geometries to embed entities in vector space for logical query operations but they suffer from subpar performance on complex queries and datasetspecific representations in this paper we propose a novel decoupled approach languageguided abstract reasoning over knowledge graphs lark that formulates complex kg reasoning as a combination of contextual kg search and logical query reasoning to leverage the strengths of graph extraction algorithms and large language models llm respectively our experiments demonstrate that the proposed approach outperforms stateoftheart kg reasoning methods on standard benchmark datasets across several logical query constructs with significant performance gain for queries of higher complexity furthermore we show that the performance of our approach improves proportionally to the increase in size of the underlying llm enabling the integration of the latest advancements in llms for logical reasoning over kgs our work presents a new direction for addressing the challenges of complex kg reasoning and paves the way for future research in this area,0
we generalize intuitionistic tense logics to the multimodal case by placing grammar logics on an intuitionistic footing we provide axiomatizations for a class of base intuitionistic grammar logics as well as provide axiomatizations for extensions with combinations of seriality axioms and what we call intuitionistic path axioms we show that each axiomatization is sound and complete with completeness being shown via a typical canonical model construction,0
plausible reasoning concerns situations whose inherent lack of precision is not quantified that is there are no degrees or levels of precision and hence no use of numbers like probabilities a hopefully comprehensive set of principles that clarifies what it means for a formal logic to do plausible reasoning is presented a new propositional logic called propositional plausible logic ppl is defined and applied to some important examples ppl is the only nonnumeric nonmonotonic logic we know of that satisfies all the principles and correctly reasons with all the examples some important results about ppl are proved,0
this paper presents a uniform substitution calculus for differential game logic dgl churchs uniform substitutions substitute a term or formula for a function or predicate symbol everywhere after generalizing them to differential game logic and allowing for the substitution of hybrid games for game symbols uniform substitutions make it possible to only use axioms instead of axiom schemata thereby substantially simplifying implementations instead of subtle schema variables and soundnesscritical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones the resulting axiomatization adopts only a finite number of ordinary dgl formulas as axioms which uniform substitutions instantiate soundly this paper proves soundness and completeness of uniform substitutions for the monotone modal logic dgl the resulting axiomatization admits a straightforward modular implementation of dgl in theorem provers,0
probabilistic logic programming is a major part of statistical relational artificial intelligence where approaches from logic and probability are brought together to reason about and learn from relational domains in a setting of uncertainty however the behaviour of statistical relational representations across variable domain sizes is complex and scaling inference and learning to large domains remains a significant challenge in recent years connections have emerged between domain size dependence lifted inference and learning from sampled subpopulations the asymptotic behaviour of statistical relational representations has come under scrutiny and projectivity was investigated as the strongest form of domainsize dependence in which query marginals are completely independent of the domain size in this contribution we show that every probabilistic logic program under the distribution semantics is asymptotically equivalent to an acyclic probabilistic logic program consisting only of determinate clauses over probabilistic facts we conclude that every probabilistic logic program inducing a projective family of distributions is in fact everywhere equivalent to a program from this fragment and we investigate the consequences for the projective families of distributions expressible by probabilistic logic programs to facilitate the application of classical results from finite model theory we introduce the abstract distribution semantics defined as an arbitrary logical theory over probabilistic facts this bridges the gap to the distribution semantics underlying probabilistic logic programming in this representation determinate logic programs correspond to quantifierfree theories making asymptotic quantifier elimination results available for the setting of probabilistic logic programming this paper is under consideration for acceptance in tplp,0
game logic with sabotage mathsfgls is introduced as a simple and natural extension of parikhs game logic with a single additional primitive which allows players to lay traps for the opponent mathsfgls can be used to model infinite sabotage games in which players can change the rules during game play in contrast to game logic which is strictly less expressive mathsfgls is exactly as expressive as the modal calculus this reveals a close connection between the entangled nested recursion inherent in modal fixpoint logics and adversarial dynamic rule changes characteristic for sabotage games a natural hilbertstyle proof calculus for mathsfgls is presented and proved complete using syntactic equiexpressiveness reductions the completeness of a simple extension of parikhs calculus for game logic follows,0
abstract reasoning ie inferring complicated patterns from given observations is a central building block of artificial general intelligence while humans find the answer by either eliminating wrong candidates or first constructing the answer prior deep neural network dnnbased methods focus on the former discriminative approach this paper aims to design a framework for the latter approach and bridge the gap between artificial and human intelligence to this end we propose logicguided generation loge a novel generative dnn framework that reduces abstract reasoning as an optimization problem in propositional logic loge is composed of three steps extract propositional variables from images reason the answer variables with a logic layer and reconstruct the answer image from the variables we demonstrate that loge outperforms the black box dnn frameworks for generative abstract reasoning under the raven benchmark ie reconstructing answers based on capturing correct rules of various attributes from observations,0
answer set programming is a prominent declarative programming paradigm used in formulating combinatorial search problems and implementing different knowledge representation formalisms frequently several related and yet substantially different answer set programs exist for a given problem sometimes these encodings may display significantly different performance uncovering precise formal links between these programs is often important and yet far from trivial this paper presents formal results carefully relating a number of interesting program rewritings it also provides the proof of correctness of system projector concerned with automatic program rewritings for the sake of efficiency under consideration in theory and practice of logic programming tplp,0
quantitative logic reasons about the degree to which formulas are satisfied this paper studies the fundamental reasoning principles of higherorder quantitative logic and their application to reasoning about probabilistic programs and processes we construct an affine calculus for 1bounded complete metric spaces and the monad for probability measures equipped with the kantorovic distance the calculus includes a form of guarded recursion interpreted via banachs fixed point theorem useful eg for recursive programming with processes we then define an affine higherorder quantitative logic for reasoning about terms of our calculus the logic includes novel principles for guarded recursion and induction over probability measures and natural numbers we illustrate the expressivity of the logic by a sequence of case studies proving upper limits on bisimilarity distances of markov processes showing convergence of a temporal learning algorithm and of a random walk using a coupling argument finally we show how to encode a probabilistic hoare logic in our logic,0
computability logic is a formal theory of computational tasks and resources its formulas represent interactive computational problems logical operators stand for operations on computational problems and validity of a formula is understood as being a scheme of problems that always have algorithmic solutions a comprehensive online source on the subject is available at the earlier article propositional computability logic i proved soundness and completeness for the in a sense minimal nontrivial fragment cl1 of computability logic the present paper extends that result to the significantly more expressive propositional system cl2 what makes cl2 more expressive than cl1 is the presence of two sorts of atoms in its language elementary atoms representing elementary computational problems ie predicates and general atoms representing arbitrary computational problems cl2 conservatively extends cl1 with the latter being nothing but the generalatomfree fragment of the former,0
in this paper we study the expressive power of kary exclusion logic exc that is obtained by extending first order logic with kary exclusion atoms it is known that without arity bounds exclusion logic is equivalent with dependence logic by observing the translations we see that the expressive power of exc lies in between kary and k1ary dependence logics we will show that at least in the case of k1 the both of these inclusions are proper in a recent work by the author it was shown that kary inclusionexclusion logic is equivalent with kary existential second order logic eso we will show that on the level of sentences it is possible to simulate inclusion atoms with exclusion atoms and this way express esosentences by using only kary exclusion atoms for this translation we also need to introduce a novel method for unifying the values of certain variables in a team as a consequence exc captures eso on the level of sentences and we get a strict arity hierarchy for exclusion logic it also follows that kary inclusion logic is strictly weaker than exc finally we will use similar techniques to formulate a translation from eso to kary inclusion logic with strict semantics consequently for any arity fragment of inclusion logic strict semantics is more expressive than lax semantics,0
epistemic logic has become a major field of philosophical logic ever since the groundbreaking work by hintikka 1962 despite its various successful applications in theoretical computer science ai and game theory the technical development of the field has been mainly focusing on the propositional part ie the propositional modal logics of knowing that however knowledge is expressed in everyday life by using various other locutions such as knowing whether knowing what knowing how and so on knowingwh hereafter such knowledge expressions are better captured in quantified epistemic logic as was already discussed by hintikka 1962 and his sequel works at length this paper aims to draw the attention back again to such a fascinating but largely neglected topic we first survey what hintikka and others did in the literature of quantified epistemic logic and then advocate a new quantifierfree approach to study the epistemic logics of knowingwh which we believe can balance expressivity and complexity and capture the essential reasoning patterns about knowingwh we survey our recent line of work on the epistemic logics of knowing whether knowing what and knowing how to demonstrate the use of this new approach,0
extending the popular answer set programming asp paradigm by introspective reasoning capacities has received increasing interest within the last years particular attention is given to the formalism of epistemic logic programs elps where standard rules are equipped with modal operators which allow to express conditions on literals for being known or possible ie contained in all or some answer sets respectively elps thus deliver multiple collections of answer sets known as world views employing elps for reasoning problems so far has mainly been restricted to standard decision problems complexity analysis and enumeration development of systems of world views in this paper we take a next step and contribute to epistemic logic programming in two ways first we establish quantitative reasoning for elps where the acceptance of a certain set of literals depends on the number proportion of world views that are compatible with the set second we present a novel system that is capable of efficiently solving the underlying counting problems required to answer such quantitative reasoning problems our system exploits the graphbased measure treewidth and works by iteratively finding and refining graph abstractions of an elp program on top of these abstractions we apply dynamic programming that is combined with utilizing existing searchbased solvers like eclingo for hard combinatorial subproblems that appear during solving it turns out that our approach is competitive with existing systems that were introduced recently this work is under consideration for acceptance in tplp,0
logical frameworks based on intuitionistic or linear logics with highertype quantification have been successfully used to give highlevel modular and formal specifications of many important judgments in the area of programming languages and inference systems given such specifications it is natural to consider proving properties about the specified systems in the framework for example given the specification of evaluation for a functional programming language prove that the language is deterministic or that evaluation preserves types one challenge in developing a framework for such reasoning is that higherorder abstract syntax hoas an elegant and declarative treatment of objectlevel abstraction and substitution is difficult to treat in proofs involving induction in this paper we present a metalogic that can be used to reason about judgments coded using hoas this metalogic is an extension of a simple intuitionistic logic that admits higherorder quantification over simply typed lambdaterms key ingredients for hoas as well as induction and a notion of definition we explore the difficulties of formal metatheoretic analysis of hoas encodings by considering encodings of intuitionistic and linear logics and formally derive the admissibility of cut for important subsets of these logics we then propose an approach to avoid the apparent tradeoff between the benefits of higherorder abstract syntax and the ability to analyze the resulting encodings we illustrate this approach through examples involving the simple functional and imperative programming languages pcf and pcf we formally derive such properties as unicity of typing subject reduction determinacy of evaluation and the equivalence of transition semantics and natural semantics presentations of evaluation,0
two results are presented concerning the entailment problem in separation logic with inductively defined predicate symbols and theory reasoning first we show that the entailment problem is undecidable for rules with bounded treewidth if theory reasoning is considered the result holds for a wide class of theories even with a very low expressive power for instance it applies to the natural numbers with the successor function or with the usual order second we show that every entailment problem can be reduced to an entailment problem containing no equality neither in the formulas nor in the recursive rules defining the semantics of the predicate symbols,0
we continue work of our earlier paper lewitzka and brunner minimally generated abstract logics logica universalis 32 2009 where abstract logics and particularly intuitionistic abstract logics are studied abstract logics can be topologized in a direct and natural way this facilitates a topological study of classes of concrete logics whenever they are given in abstract form moreover such a direct topological approach avoids the often complex algebraic and latticetheoretic machinery usually applied to represent logics motivated by that point of view we define in this paper the category of intuitionistic abstract logics with stable logic maps as morphisms and the category of implicative spectral spaces with spectral maps as morphisms we show the equivalence of these categories and conclude that the larger categories of distributive abstract logics and distributive sober spaces are equivalent too,0
in probabilistic logic programming plp the most commonly studied inference task is to compute the marginal probability of a query given a program in this paper we consider two other important tasks in the plp setting the maximumaposteriori map inference task which determines the most likely values for a subset of the random variables given evidence on other variables and the most probable explanation mpe task the instance of map where the query variables are the complement of the evidence variables we present a novel algorithm included in the pita reasoner which tackles these tasks by representing each problem as a binary decision diagram and applying a dynamic programming procedure on it we compare our algorithm with the version of problog that admits annotated disjunctions and can perform map and mpe inference experiments on several synthetic datasets show that pita outperforms problog in many cases,0
epistemic logic programs elps extend answer set programming asp with epistemic negation and have received renewed interest in recent years this led to the development of new research and efficient solving systems for elps in practice elps are often written in a modular way where each module interacts with other modules by accepting sets of facts as input and passing on sets of facts as output an interesting question then presents itself under which conditions can such a module be replaced by another one without changing the outcome for any set of input facts this problem is known as uniform equivalence and has been studied extensively for asp for elps however such an investigation is as of yet missing in this paper we therefore propose a characterization of uniform equivalence that can be directly applied to the language of stateoftheart elp solvers we also investigate the computational complexity of deciding uniform equivalence for two elps and show that it is on the third level of the polynomial hierarchy,0
we define a family of propositional constructive modal logics corresponding each to a different classical modal system the logics are defined in the style of wijesekeras constructive modal logic and are both prooftheoretically and semantically motivated on the one hand they correspond to the singlesuccedent restriction of standard sequent calculi for classical modal logics on the other hand they are obtained by incorporating the hereditariness of intuitionistic kripke models into the classical satisfaction clauses for modal formulas we show that for the considered classical logics the prooftheoretical and the semantical approach return the same constructive systems,0
we study the complexity of predicate logics based on team semantics we show that the satisfiability problems of twovariable independence logic and inclusion logic are both nexptimecomplete furthermore we show that the validity problem of twovariable dependence logic is undecidable thereby solving an open problem from the team semantics literature we also briefly analyse the complexity of the bernaysschnfinkelramsey prefix classes of dependence logic,0
in this paper we analyze kary inclusionexclusion logic inex which is obtained by extending first order logic with kary inclusion and exclusion atoms we show that every formula of inex can be expressed with a formula of kary existential second order logic eso conversely every formula of eso with at most kary free relation variables can be expressed with a formula of inex from this it follows that on the level of sentences inex captures the expressive power of eso we also introduce several useful operators that can be expressed in inex in particular we define inclusion and exclusion quantifiers and socalled term value preserving disjunction which is essential for the proofs of the main results in this paper furthermore we present a novel method of relativization for team semantics and analyze the duality of inclusion and exclusion atoms,0
problems in two axiomatizations of jakowskis discussive or discursive logic d2 are considered a recent axiomatization of d2 and completeness proof relative to d2s intended semantics seems to be mistaken because some formulas valid according to the intended semantics turn out to be unprovable although no new axiomatization is offered nor a repaired completeness proof given the shortcomings identified here may be a step toward an improved axiomatization,0
topological semantics for modal logic based on the cantor derivative operator gives rise to derivative logics also referred to as dlogics unlike logics based on the topological closure operator dlogics have not previously been studied in the framework of dynamic topological systems dtss which are pairs xf consisting of a topological space x equipped with a continuous function f x x we introduce the logics wk4c k4c and glc and show that they all have the finite kripke model property and are sound and complete with respect to the dsemantics in this dynamical setting we also prove a general result for the case where f is a homeomorphism which yields soundness and completeness for the corresponding systems wk4h k4h and glh of special interest is glc which is the dlogic of all dtss based on a scattered space we use the completeness of glc and the properties of scattered spaces to demonstrate the first sound and complete dynamic topological logic in the original trimodal language in particular we show that the version of dtl based on the class of scattered spaces is finitely axiomatisable over the original language and that the natural axiomatisation is sound and complete,0
recent technological advances have led to unprecedented amounts of generated data that originate from the web sensor networks and social media analytics in terms of defeasible reasoning for example for decision making could provide richer knowledge of the underlying domain traditionally defeasible reasoning has focused on complex knowledge structures over small to medium amounts of data but recent research efforts have attempted to parallelize the reasoning process over theories with large numbers of facts such work has shown that traditional defeasible logics come with overheads that limit scalability in this work we design a new logic for defeasible reasoning thus ensuring scalability by design we establish several properties of the logic including its relation to existing defeasible logics our experimental results indicate that our approach is indeed scalable and defeasible reasoning can be applied to billions of facts,0
recursive definitions of predicates are usually interpreted either inductively or coinductively recently a more powerful approach has been proposed called flexible coinduction to express a variety of intermediate interpretations necessary in some cases to get the correct meaning we provide a detailed formal account of an extension of logic programming supporting flexible coinduction syntactically programs are enriched by coclauses clauses with a special meaning used to tune the interpretation of predicates as usual the declarative semantics can be expressed as a fixed point which however is not necessarily the least nor the greatest one but is determined by the coclauses correspondingly the operational semantics is a combination of standard sld resolution and cosld resolution we prove that the operational semantics is sound and complete with respect to declarative semantics restricted to finite comodels this paper is under consideration for acceptance in tplp,0
reasoning with quantifier expressions in natural language combines logical and arithmetical features transcending strict divides between qualitative and quantitative our topic is this cooperation of styles as it occurs in common linguistic usage and its extension into the broader practice of natural language plus grassroots mathematics we begin with a brief review of firstorder logic with counting operators and cardinality comparisons this system is known to be of high complexity and drowns out finer aspects of the combination of logic and counting we move to a small fragment that can represent numerical syllogisms and basic reasoning about comparative size monadic firstorder logic with counting we provide normal forms that allow for axiomatization determine which arithmetical notions can be defined on finite and on infinite models and conversely we discuss which logical notions can be defined out of purely arithmetical ones and what sort of nonclassical logics can be induced next we investigate a series of strengthenings again using normal form methods the monadic secondorder version is close in a precise sense to additive presburger arithmetic while versions with the natural device of tuple counting take us to diophantine equations making the logic undecidable we also define a system that combines basic modal logic over binary accessibility relations with counting needed to formulate ubiquitous reasoning patterns such as the pigeonhole principle we return to our starting point in natural language confronting the architecture of our formal systems with linguistic quantifier vocabulary and syntax we conclude with some general thoughts on yet further entanglements of logic and counting in formal systems on rethinking the qualitativequantitative divide and on connecting our analysis to empirical findings in cognitive science,0
this paper investigates firstorder game logic and firstorder modal mucalculus which extend their propositional modal logic counterparts with firstorder modalities of interpreted effects such as variable assignments unlike in the propositional case both logics are shown to have the same expressive power and their proof calculi to have the same deductive power both calculi are also mutually relatively complete in the presence of differential equations corollaries obtain usable and complete translations between differential game logic a logic for the deductive verification of hybrid games and the differential mucalculus the modal mucalculus for hybrid systems the differential mucalculus is complete with respect to firstorder fixpoint logic and differential game logic is complete with respect to its odefree fragment,0
this paper explores relational syllogistic logics a family of logical systems related to reasoning about relations in extensions of the classical syllogistic these are all decidable logical systems we prove completeness theorems and complexity results for a natural subfamily of relational syllogistic logics parametrized by constructors for terms and for sentences,0
lpsupsetmathsff is a threevalued paraconsistent propositional logic which is essentially the same as j3 it has most properties that have been proposed as desirable properties of a reasonable paraconsistent propositional logic however it follows easily from already published results that there are exactly 8192 different threevalued paraconsistent propositional logics that have the properties concerned in this paper properties concerning the logical equivalence relation of a logic are used to distinguish lpsupsetmathsff from the others as one of the bonuses of focussing on the logical equivalence relation it is found that only 32 of the 8192 logics have a logical equivalence relation that satisfies the identity annihilation idempotent and commutative laws for conjunction and disjunction for most properties of lpsupsetmathsff that have been proposed as desirable properties of a reasonable paraconsistent propositional logic its paracomplete analogue has a comparable property in this paper properties concerning the logical equivalence relation of a logic are also used to distinguish the paracomplete analogue of lpsupsetmathsff from the other threevalued paracomplete propositional logics with those comparable properties,0
this paper continues the line of research aimed at investigating the relationship between logic programs and firstorder theories we extend the definition of program completion to programs with input and output in a subset of the input language of the asp grounder gringo study the relationship between stable models and completion in this context and describe preliminary experiments with the use of two software tools anthem and vampire for verifying the correctness of programs with input and output proofs of theorems are based on a lemma that relates the semantics of programs studied in this paper to stable models of firstorder formulas under consideration for acceptance in tplp,0
proof nets provide abstract counterparts to sequent proofs modulo rule permutations the idea being that if two proofs have the same underlying proofnet they are in essence the same proof providing a convincing proofnet counterpart to proofs in the classical sequent calculus is thus an important step in understanding classical sequent calculus proofs by convincing we mean that a there should be a canonical function from sequent proofs to proof nets b it should be possible to check the correctness of a net in polynomial time c every correct net should be obtainable from a sequent calculus proof and d there should be a cutelimination procedure which preserves correctness previous attempts to give proofnetlike objects for propositional classical logic have failed at least one of the above conditions in the author presented a calculus of proof nets expansion nets satisfying a and b the paper defined a sequent calculus corresponding to expansion nets but gave no explicit demonstration of c that sequent calculus called lkast in this paper is a novel onesided sequent calculus with both additively and multiplicatively formulated disjunction rules in this paper a selfcontained extended version of we give a full proof of c for expansion nets with respect to lkast and in addition give a cutelimination procedure internal to expansion nets this makes expansion nets the first notion of proofnet for classical logic satisfying all four criteria,0
two traditional paradigms are often used to describe the behavior of agents in multiagent complex systems in the first one agents are considered to be fully rational and systems are seen as multiplayer games in the second one agents are considered to be fully stochastic processes and the system itself is seen as a large stochastic process from the standpoint of a particular agent having to choose a strategy the choice of the paradigm is crucial the most adequate strategy depends on the assumptions made on the other agents in this paper we focus on twoplayer games and their application to the automated synthesis of reliable controllers for reactive systems a field at the crossroads between computer science and mathematics in this setting the reactive system to control is a player and its environment is its opponent usually assumed to be fully antagonistic or fully stochastic we illustrate several recent developments aiming to breach this narrow taxonomy by providing formal concepts and mathematical frameworks to reason about richer behavioral models the interest of such models is not limited to reactive system synthesis but extends to other application fields of game theory the goal of our contribution is to give a highlevel presentation of key concepts and applications aimed at a broad audience to achieve this goal we illustrate those rich behavioral models on a classical challenge of the everyday life planning a journey in an uncertain environment,0
we present new descriptive complexity characterisations of classes reg regular languages lcfl linear contextfree languages and cfl contextfree languages as restrictions on inference rules size of formulae and permitted connectives in the lambek calculus fragments of the intuitionistic noncommutative linear logic with directionsensitive implication connectives our identification of the lambek calculus fragments with proof complexity reg and lcfl is the first result of its kind we further show the cfl complexity of one of the strictly weakest possible variants of the logic admitting only a single inference rule the proof thereof moreover is based on a direct translation between typelogical and formal grammar and structural induction on provable sequents a simpler and more intuitive method than those employed in prior works we thereby establish a clear conceptual utility of the cutelimination theorem for comparing formal grammar and sequent calculus and identify the exact analogue of the greibach normal form in lambek grammar we believe the result presented herein constitutes a first step toward a more extensive and richer characterisation of the interaction between computation and logic as well as a finergrained complexity separation of various sequent calculi,0
a logicenriched type theory ltt is a type theory extended with a primitive mechanism for forming and proving propositions we construct two ltts named ltto and ltto which we claim correspond closely to the classical predicative systems of second order arithmetic acao and aca we justify this claim by translating each secondorder system into the corresponding ltt and proving that these translations are conservative this is part of an ongoing research project to investigate how ltts may be used to formalise different approaches to the foundations of mathematics the two ltts we construct are subsystems of the logicenriched type theory lttw which is intended to formalise the classical predicative foundation presented by herman weyl in his monograph das kontinuum the system acao has also been claimed to correspond to weyls foundation by casting acao and aca as ltts we are able to compare them with lttw it is a consequence of the work in this paper that lttw is strictly stronger than acao the conservativity proof makes use of a novel technique for proving one ltt conservative over another involving defining an interpretation of the stronger system out of the expressions of the weaker this technique should be applicable in a wide variety of different cases outside the present work,0
this paper introduces a refinement of the sequent calculus approach called cirquent calculus while in gentzenstyle proof trees sibling or cousin etc sequents are disjoint sequences of formulas in cirquent calculus they are permitted to share elements explicitly allowing or disallowing shared resources and thus taking to a more subtle level the resourceawareness intuitions underlying substructural logics cirquent calculus offers much greater flexibility and power than sequent calculus does a need for substantially new deductive tools came with the birth of computability logic see the semantically constructed formal theory of computational resources which has stubbornly resisted any axiomatization attempts within the framework of traditional syntactic approaches cirquent calculus breaks the ice removing contraction from the full collection of its rules yields a sound and complete system for the basic fragment cl5 of computability logic doing the same in sequent calculus on the other hand throws out the baby with the bath water resulting in the strictly weaker affine logic an implied claim of computability logic is that it is cl5 rather than affine logic that adequately materializes the resource philosophy traditionally associated with the latter to strengthen this claim the paper further introduces an abstract resource semantics and shows the soundness and completeness of cl5 with respect to it,0
we study monadic secondorder logic mso over finite words extended with nonuniform arbitrary monadic predicates we show that it defines a class of languages that has algebraic automatatheoretic and machineindependent characterizations we consider the regularity question given a language in this class when is it regular to answer this we show a substitution property and the existence of a syntactical predicate we give three applications the first two are to give very simple proofs that the straubing conjecture holds for all fragments of mso with monadic predicates and that the crane beach conjecture holds for mso with monadic predicates the third is to show that it is decidable whether a language defined by an mso formula with morphic predicates is regular,0
we propose a categorial grammar based on classical multiplicative linear logic this can be seen as an extension of abstract categorial grammars acg and is at least as expressive however constituents of it linear logic grammars llg are not abstract terms but simply tuples of words with labeled endpoints we call them it multiwords at least this gives a concrete and intuitive representation of acg a key observation is that the class of multiwords has a fundamental algebraic structure namely multiwords can be organized in a category very similar to the category of topological cobordisms this category is symmetric monoidal closed and compact closed and thus is a model of linear calculus and classical linear logic we think that this category is interesting on its own right in particular it might provide categorical representation for other formalisms on the other hand many models of language semantics are based on commutative logic or more generally on symmetric monoidal closed categories but the category of it word cobordisms is a category of language elements which is itself symmetric monoidal closed and independent of any grammar thus it might prove useful in understanding language semantics as well,0
in this paper we present simple example of propositional logic which has one modal operator and is based on intuitionistic core this system is very weak in modal sense eg rules of regularity or monotonicity do not hold it has complete semantics composed of possible worlds equipped with neighborhoods and preorder relation we discuss certain restrictions imposed on those structures also we present characterization of axiom 4 known from logic s4,0
this volume contains the proceedings of the fourth international symposium on games automata logic and formal verification gandalf 2013 the symposium took place in borca di cadore italy from 29th to 31st of august 2013 the proceedings of the symposium contain the abstracts of three invited talks and 17 papers that were accepted after a careful evaluation for presentation at the conference the topics of the accepted papers range over a wide spectrum including algorithmic and behavioral game theory game semantics formal languages and automata theory modal and temporal logics software verification hybrid systems,0
although conventional logical systems based on logical calculi have been successfully used in mathematics and beyond they have definite limitations that restrict their application in many cases for instance the principal condition for any logical calculus is its consistency at the same time knowledge about large object domains in science or in practice is essentially inconsistent logical prevarieties and varieties were introduced to eliminate these limitations in a logically correct way in this paper the logic of reasonable inferences is described this logic has been applied successfully to model legal reasoning with inconsistent knowledge it is demonstrated that this logic is a logical variety and properties of logical varieties related to legal reasoning are developed,0
in this paper we consider epistemic logic programs which extend answer set programming asp with epistemic operators and epistemic negation and a recent approach to the semantics of such programs in terms of world views we propose some observations on the existence and number of world views we show how to exploit an extended asp semantics in order to i provide a characterization of world views different from existing ones ii query world views and query the whole set of world views,0
we consider an extension of logic programs called programs that can be used to define predicates over infinite lists programs allow us to specify properties of the infinite behavior of reactive systems and in general properties of infinite sequences of events the semantics of programs is an extension of the perfect model semantics we present variants of the familiar unfoldfold rules which can be used for transforming programs we show that these new rules are correct that is their application preserves the perfect model semantics then we outline a general methodology based on program transformation for verifying properties of programs we demonstrate the power of our transformationbased verification methodology by proving some properties of buechi automata and regular languages,0
the paper presents a software tool for analysis and interactive engagement in various logical reasoning tasks a first feature of the program consists in providing an interface for working with logicspecific repositories of formal knowledge a second feature provides the means to intuitively visualize and interactively generate the underlying logical structure that propels customary logical reasoning tasks starting from this we argue that both aspects have didactic potential and can be integrated in teaching activities to provide an engaging learning experience,0
we present a game semantics for intuitionistic type theory specifically we propose categories with families of a new variant of games and strategies for both extensional and intensional variants of the type theory with dependent function dependent pair and identity types as well as universes our games and strategies generalize the existing notion of games and strategies and achieve an interpretation of dependent types and the hierarchy of universes in an intuitive manner we believe that it is a significant step towards a computational and intensional interpretation of the type theory,0
in a previous paper a tableau calculus has been presented which constitute a decision procedure for hybrid logic with the converse and global modalities and a restricted use of the binder this work extends such a calculus to multimodal logic with transitive relations and relation inclusion assertions the separate addition of either transitive relations or relation hierarchies to the considered decidable fragment of multimodal hybrid logic can easily be shown to stay decidable by resorting to results already proved in the literature however such results do not directly allow for concluding whether the logic including both features is still decidable the existence of a terminating sound and complete calculus for the considered logic proves that the addition of transitive relations and relation hierarchies to such an expressive decidable fragment of hybrid logic yields a decidable logic,0
we extend the inflationary fixedpoint logic ifp with a new kind of secondorder quantifiers which have polylogarithmic bounds we prove that on ordered structures the new logic existslogtextifp captures the limited nondeterminism class textp in order to study its expressive power we also design a new version of ehrenfeuchtfrass game for this logic and show that our capturing result will not hold on the general case ie on all the finite structures,0
this volume contains the proceedings of the fifth international symposium on games automata logic and formal verification gandalf 2014 the symposium took place in verona italy from 10th to 12th of september 2014 the proceedings of the symposium contain the abstracts of three invited talks and 19 papers that were accepted after a careful evaluation for presentation at the conference the topics of the accepted papers range over a wide spectrum including algorithmic and behavioral game theory game semantics formal languages and automata theory modal and temporal logics software verification hybrid systems,0
we consider dynamic versions of epistemic logic as formulated in baltag and moss logics for epistemic programs 2004 that paper proposed a logical language actually families of languages parameterized by action signatures for dynamic epistemic logic it had been shown that validity in the language is pi11complete so there are no recursively axiomatized complete logical systems for it in contrast this paper proves a weak completeness result for the fragment without action iteration and a strong completeness result for the fragment without action iteration and common knowledge our work involves a detour into term rewriting theory the argument uses modal filtration and thus we obtain the finite model property and hence decidability we also give a translation of our largest language into pdl thereby obtaining a second proof of decidability the paper closes with some results on expressive power these are mostly concerned with comparing the actioniterationfree language with modal logic augmented by transitive closure operators we answer a natural question about the languages we obtain by varying the action signature we prove that a logical language with operators for private announcements is more expressive than one for public announcements,0
we study various formulations of the completeness of firstorder logic phrased in constructive type theory and mechanised in the coq proof assistant specifically we examine the completeness of variants of classical and intuitionistic natural deduction and sequent calculi with respect to modeltheoretic algebraic and gametheoretic semantics as completeness with respect to the standard modeltheoretic semantics  la tarski and kripke is not readily constructive we analyse connections of completeness theorems to markovs principle and weak knigs lemma and discuss nonstandard semantics admitting assumptionfree completeness we contribute a reusable coq library for firstorder logic containing all results covered in this paper,0
complexity and decidability of logics is a major research area involving a huge range of different logical systems this calls for a unified and systematic approach for the field we introduce a research program based on an algebraic approach to complexity classifications of fragments of firstorder logic fo and beyond our base system gra or general relation algebra is equiexpressive with fo it resembles cylindric algebra but employs a finite signature with only seven different operators we provide a comprehensive classification of the decidability and complexity of the systems obtained by limiting the allowed sets of operators we also give algebraic characterizations of the best known decidable fragments of fo furthermore to move beyond fo we introduce the notion of a generalized operator and briefly study related systems,0
this paper presents an example of formal reasoning about the semantics of a prolog program of practical importance the sat solver of howe and king the program is treated as a definite clause logic program with added control the logic program is constructed by means of stepwise refinement hand in hand with its correctness and completeness proofs the proofs are declarative they do not refer to any operational semantics each step of the logic program construction follows a systematic approach to constructing programs which are provably correct and complete we also prove that correctness and completeness of the logic program is preserved in the final prolog program additionally we prove termination occurcheck freedom and nonfloundering our example shows how dealing with logic and with control can be separated most of the proofs can be done at the logic level abstracting from any operational semantics the example employs approximate specifications they are crucial in simplifying reasoning about logic programs it also shows that the paradigm of semanticspreserving program transformations may be not sufficient we suggest considering transformations which preserve correctness and completeness with respect to an approximate specification,0
inconsistency robustness is information system performance in the face of continually pervasive inconsistencies a fundamental principle of inconsistency robustness is to make contradictions explicit so that arguments for and against propositions can be formalized this paper explores the role of inconsistency robustness in the history and theory of logic programs robert kowalski put forward a bold thesis looking back on our early discoveries i value most the discovery that computation could be subsumed by deduction however mathematical logic cannot always infer computational steps because computational systems make use of arbitration for determining which message is processed next by a recipient that is sent multiple messages concurrently since reception orders are in general indeterminate they cannot be inferred from prior information by mathematical logic alone therefore mathematical logic cannot in general implement computation over the course of history the term functional program has grown more precise and technical as the field has matured logic program should be on a similar trajectory accordingly logic program should have a general precise characterization in the fall of 1972 different characterizations of logic programs that have continued to this day a logic program uses hornclause syntax for forward and backward chaining each computational step according to actor model of a logic program is deductively inferred eg in direct logic the above examples are illustrative of how issues of inconsistency robustness have repeatedly arisen in logic programs,0
we present a comprehensive programme analysing the decomposition of proof systems for nonclassical logics into proof systems for other logics especially classical logic using an algebra of constraints that is one recovers a proof system for a target logic by enriching a proof system for another typically simpler logic with an algebra of constraints that act as correctness conditions on the latter to capture the former for example one may use boolean algebra to give constraints in a sequent calculus for classical propositional logic to produce a sequent calculus for intuitionistic propositional logic the idea behind such forms of reduction is to obtain a tool for uniform and modular treatment of proof theory and provide a bridge between semantics logics and their proof theory the article discusses the theoretical background of the project and provides several illustrations of its work in the field of intuitionistic and modal logics the results include the following a uniform treatment of modular and cutfree proof systems for a large class of propositional logics a general criterion for a novel approach to soundness and completeness of a logic with respect to a modeltheoretic semantics and a case study deriving a modeltheoretic semantics from a prooftheoretic specification of a logic,0
we give an introduction to logic tailored for algebraists explaining how proofs in linear logic can be viewed as algorithms for constructing morphisms in symmetric closed monoidal categories with additional structure this is made explicit by showing how to represent proofs in linear logic as linear maps between vector spaces the interesting part of this vector space semantics is based on the cofree cocommutative coalgebra of sweedler,0
this article presents an overview of computability logic the gamesemantically constructed logic of interactive computational tasks and resources there is only one nonoverview technical section in it devoted to a proof of the soundness of affine logic with respect to the semantics of computability logic a comprehensive online source on the subject can be found at,0
we present a probabilistic extension of the description logic mathcalalc for reasoning about statistical knowledge we consider conditional statements over proportions of the domain and are interested in the probabilisticlogical consequences of these proportions after introducing some general reasoning problems and analyzing their properties we present first algorithms and complexity results for reasoning in some fragments of statistical mathcalalc,0
we introduce a restricted secondorder logic mathrmsomathitplog for finite structures where secondorder quantification ranges over relations of size at most polylogarithmic in the size of the structure we demonstrate the relevance of this logic and complexity class by several problems in database theory we then prove a fagins style theorem showing that the boolean queries which can be expressed in the existential fragment of mathrmsomathitplog corresponds exactly to the class of decision problems that can be computed by a nondeterministic turing machine with random access to the input in time olog nk for some k ge 0 ie to the class of problems computable in nondeterministic polylogarithmic time it should be noted that unlike fagins theorem which proves that the existential fragment of secondorder logic captures np over arbitrary finite structures our result only holds over ordered finite structures since mathrmsomathitplog is too weak as to define a total order of the domain nevertheless mathrmsomathitplog provides natural levels of expressibility within polylogarithmic space in a way which is closely related to how secondorder logic provides natural levels of expressibility within polynomial space indeed we show an exact correspondence between the quantifier prefix classes of mathrmsomathitplog and the levels of the nondeterministic polylogarithmic time hierarchy analogous to the correspondence between the quantifier prefix classes of secondorder logic and the polynomialtime hierarchy our work closely relates to the constant depth quasipolynomial size andor circuits and corresponding restricted secondorder logic defined by david a mix barrington in 1992 we explore this relationship in detail,0
sandqvist gave a prooftheoretic semantics pts for classical logic cl that explicates the meaning of the connectives without assuming bivalance later he gave a semantics for intuitionistic propositional logic ipl while soundness in both cases is proved through standard techniques the proof completeness for cl is complex and somewhat obscure but clear and simple for ipl makinson gave a simplified proof of completeness for classical propositional logic cpl by directly relating the the pts to the logics extant truthfunctional semantics in this paper we give an elementary constructive and native in the sense that it does not presuppose the modeltheoretic interpretation of classical logic proof of completeness the pts of cl using the techniques applies for ipl simultaneously we give a proof of soundness and completeness for firstorder intuitionistic logic il,0
a modal logic based on quantum logic is formalized in its simplest possible form specifically a relational semantics and a sequent calculus are provided and the soundness and the completeness theorems connecting both notions are demonstrated this framework is intended to serve as a basis for formalizing various modal logics over quantum logic such as quantum alethic logic quantum temporal logic quantum epistemic logic and quantum dynamic logic,0
this paper proposes the use of constraint logic programming clp to model sql queries in a dataindependent abstract layer by focusing on some semantic properties for signalling possible errors in such queries first we define a translation from sql to datalog and from datalog to clp so that solving this clp program will give information about inconsistency tautology and possible simplifications we use different constraint domains which are mapped to sql types and propose them to cooperate for improving accuracy our approach leverages a deductive system that includes sql and datalog and we present an implementation in this system which is currently being tested in classroom showing its advantages and differences with respect to other approaches as well as some performance data this paper is under consideration for acceptance in tplp,0
the nonclassical nonmonotonic inference relation associated with the answer set semantics for logic programs gives rise to a relationship of strong equivalence between logical programs that can be verified in 3valued goedel logic g3 the strongest nonclassical intermediate propositional logic lifschitz pearce and valverde 2001 in this paper we will show that kc the logic obtained by adding axiom a v a to intuitionistic logic is the weakest intermediate logic for which strongly equivalent logic programs in a language allowing negations are logically equivalent,0
this volume contains the proceedings of the first symposium on games automata logic and formal verification gandalf held in minori amalfi coast italy 1718 june 2010 the symposium has been promoted by a number of italian computer scientists interested in game theory mathematical logic automata theory and their applications to the specification design and verification of complex systems it covers a large spectrum of research topics ranging from theoretical aspects to concrete applications its aim is to provide a forum where people from different areas and possibly with a different background can successfully interact the highlevel international profile of the event is witnessed by the composition of the program committee and by the final program,0
this paper proposes the meeting of fuzzy logic with paraconsistency in a very precise and foundational way specifically in this paper we introduce expansions of the fuzzy logic mtl by means of primitive operators for consistency and inconsistency in the style of the socalled logics of formal inconsistency lfis the main novelty of the present approach is the definition of postulates for this type of operators over mtlalgebras leading to the definition and axiomatization of a family of logics expansions of mtl whose degreepreserving counterpart are paraconsistent and moreover lfis,0
in recent years the logic of questions and dependencies has been investigated in the closely related frameworks of inquisitive logic and dependence logic these investigations have assumed classical logic as the background logic of statements and added formulas expressing questions and dependencies to this classical core in this paper we broaden the scope of these investigations by studying questions and dependency in the context of intuitionistic logic we propose an intuitionistic team semantics where teams are embedded within intuitionistic kripke models the associated logic is a conservative extension of intuitionistic logic with questions and dependence formulas we establish a number of results about this logic including a normal form result a completeness result and translations to classical inquisitive logic and modal dependence logic,0
in this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of firstorder logic clauses into kernel machines in particular we consider a multitask learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples enforcing a set of fol constraints on the admissible configurations of their values the predicates are defined on the feature spaces in which the input objects are represented and can be either known a priori or approximated by an appropriate kernelbased learner a general approach is presented to convert the fol clauses into a continuous implementation that can deal with the outputs computed by the kernelbased predicates the learning problem is formulated as a semisupervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples a regularization term and a penalty term that enforces the constraints on both the supervised and unsupervised examples unfortunately the penalty term is not convex and it can hinder the optimization process however it is possible to avoid poor solutions by using a two stage learning schema in which the supervised examples are learned first and then the constraints are enforced,0
we present a firstorder probabilistic epistemic logic which allows combining operators of knowledge and probability within a group of possibly infinitely many agents the proposed framework is the first order extension of the logic of fagin and halpern from jacm 413403671994 we define its syntax and semantics and prove the strong completeness property of the corresponding axiomatic system,0
we study model and frame definability of various modal logics let mla denote the fragment of modal logic extended with the universal modality in which the universal modality occurs only positively we show that a class of kripke models is definable in mla if and only if the class is elementary and closed under disjoint unions and surjective bisimulations we also characterise the definability of mla in the spirit of the wellknown goldblattthomason theorem we show that an elementary class f of kripke frames is definable in mla if and only if f is closed under taking generated subframes and bounded morphic images and reflects ultrafilter extensions and finitely generated subframes in addition we study frame definability relative to finite transitive frames and give an analogous characterisation of mladefinability relative to finite transitive frames finally we initiate the study of model and frame definability in teambased logics we study extended modal dependence logic extended modal inclusion logic and modal team logic we establish strict linear hierarchies with respect to model definability and frame definability respectively we show that with respect to model and frame definability the before mentioned teambased logics except modal dependence logic either coincide with mla or plain modal logic ml thus as a corollary we obtain model theoretic characterisation of model and frame definability for the teambased logics,0
this volume contains the proceedings of the 13th international symposium on games automata logic and formal verification gandalf 2022 the aim of gandalf 2022 symposium is to bring together researchers from academia and industry which are actively working in the fields of games automata logics and formal verification the idea is to cover an ample spectrum of themes ranging from theory to applications and stimulate crossfertilization,0
this volume contains the proceedings of the 12th international symposium on games automata logic and formal verification gandalf 2021 the aim of gandalf 2021 symposium is to bring together researchers from academia and industry which are actively working in the fields of games automata logics and formal verification the idea is to cover an ample spectrum of themes ranging from theory to applications and stimulate crossfertilization,0
the ordinal folding index ofi is a new fully computable yardstick that measures how many rounds of selfreference a statement protocol or position must unfold before its truth or outcome stabilises by turning this abstract foldback depth into a single ordinal number ofi forges a direct link between areas that are usually studied in isolation the closure stages of fixedpoint logics the timetowin values of infinite parity games and the ordinal progressions that calibrate the strength of formal theories we prove that ofi refines all classical gametheoretic and logical metrics while remaining algorithmically enumerable supply a polynomialtime approximation scheme on finite arenas and show how the index coincides exactly with the length of the shortest winning strategy in the associated evaluation game alongside the theory we outline five open problems from the completeness of the computableordinal spectrum to the possibility of compressing deep selfreference that chart a research programme at the intersection of computeraided logic algorithmic game theory and ordinal analysis ofi thus invites game theorists and logicians alike to view infinite play transfinite induction and reflective reasoning through a single intuitive lens opening common ground for techniques,0
this volume contains the proceedings of the tenth international symposium on games automata logic and formal verification gandalf 2019 the symposium took place in bordeaux france from the 2nd to the 3rd of september 2010 the gandalf symposium was established by a group of italian computer scientists interested in mathematical logic automata theory game theory and their applications to the specification design and verification of complex systems its aim is to provide a forum where people from different areas and possibly with different backgrounds can fruitfully interact gandalf has a truly international spirit as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers,0
we propose a categorial grammar based on classical multiplicative linear logic this can be seen as an extension of abstract categorial grammars acg and is at least as expressive however constituents of it linear logic grammars llg are not abstract terms but simply tuples of words with labeled endpoints and supplied with specific it plugging instructions the sets of endpoints are subdivided into the it incoming and the it outgoing parts we call such objects it word cobordisms a key observation is that word cobordisms can be organized in a category very similar to the familiar category of topological cobordisms this category is symmetric monoidal closed and compact closed and thus is a model of linear calculus and classical as well as intuitionistic linear logic this allows us using linear logic as a typing system for word cobordisms at least this gives a concrete and intuitive representation of acg we think however that the category of word cobordisms which has a rich structure and is independent of any grammar might be interesting on its own right,0
this volume contains the proceedings of the eighth international symposium on games automata logic and formal verification gandalf 2017 the symposium took place in roma italy from the 20th to the 22nd of september 2017 the gandalf symposium was established by a group of italian computer scientists interested in mathematical logic automata theory game theory and their applications to the specification design and verification of complex systems its aim is to provide a forum where people from different areas and possibly with different backgrounds can fruitfully interact gandalf has a truly international spirit as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers,0
hybrid logic is one of the extensions of modal logic the manydimensional product of hybrid logic is called hybrid product logic hpl we construct a sound and complete tableau calculus for twodimensional hpl also we made a tableau calculus for hybrid dependent product logic hdpl where one dimension depends on the other in addition we add a special rule to the tableau calculus for hdpl and show that it is still sound and complete all of them lack termination however,0
in this paper we introduce a foundation for computable model theory of rational pavelka logic an extension of ukasiewicz logic and continuous logic and prove effective versions of some theorems in model theory we show how to reduce continuous logic to rational pavelka logic we also define notions of computability and decidability of a model for logics with computable but uncountable set of truth values show that provability degree of a formula wrt a linear theory is computable and use this to carry out an effective henkin construction therefore for any effectively given consistent linear theory in continuous logic we effectively produce its decidable model this is the best possible since we show that the computable model theory of continuous logic is an extension of computable model theory of classical logic we conclude with noting that the unique separable model of a separably categorical and computably axiomatizable theory such as that of a probability space or an lp banach lattice is decidable,0
the objective of this book is to give a comprehensive presentation of the research field concerned with infinite duration games on graphs historically these game models appeared in the study of automata and logic and they later became important for program verification and synthesis they have many more applications in particular some of the models investigated in this book were introduced and studied in neighbouring research communities such as optimisation reinforcement learning model theory and set theory,0
logicbased approaches to ai have the advantage that their behavior can in principle be explained with the help of proofs of the computed consequences for ontologies based on description logic dl we have put this advantage into practice by showing how proofs for consequences derived by dl reasoners can be computed and displayed in a userfriendly way however these methods are insufficient in applications where also numerical reasoning is relevant the present paper considers proofs for dls extended with concrete domains cds based on the rational numbers which leave reasoning tractable if integrated into the lightweight dl mathcalehspace01emmathcallbot since no implemented dl reasoner supports these cds we first develop reasoning procedures for them and show how they can be combined with reasoning approaches for pure dls both for mathcalehspace01emmathcallbot and the more expressive dl mathcalalc these procedures are designed such that it is easy to extract proofs from them we show how the extracted cd proofs can be combined with proofs on the dl side into integrated proofs that explain both the dl and the cd reasoning,0
this paper employs the linear nested sequent framework to design a new cutfree calculus lnif for intuitionistic fuzzy logicthe firstorder gdel logic characterized by linear relational frames with constant domains linear nested sequentswhich are nested sequents restricted to linear structuresprove to be a wellsuited prooftheoretic formalism for intuitionistic fuzzy logic we show that the calculus lnif possesses highly desirable prooftheoretic properties such as invertibility of all rules admissibility of structural rules and syntactic cutelimination,0
in this paper we study logics of dependence on the propositional level we prove that several interesting propositional logics of dependence including propositional dependence logic propositional intuitionistic dependence logic as well as propositional inquisitive logic are expressively complete and have disjunctive or conjunctive normal forms we provide deduction systems and prove the completeness theorems for these logics,0
the paper introduces fuzzy linguistic logic programming which is a combination of fuzzy logic programming introduced by p vojtas and hedge algebras in order to facilitate the representation and reasoning on human knowledge expressed in natural languages in fuzzy linguistic logic programming truth values are linguistic ones eg verytrue veryprobablytrue and littlefalse taken from a hedge algebra of a linguistic truth variable and linguistic hedges modifiers can be used as unary connectives in formulae this is motivated by the fact that humans reason mostly in terms of linguistic terms rather than in terms of numbers and linguistic hedges are often used in natural languages to express different levels of emphasis the paper presents i the language of fuzzy linguistic logic programming ii a declarative semantics in terms of herbrand interpretations and models iii a procedural semantics which directly manipulates linguistic terms to compute a lower bound to the truth value of a query and proves its soundness iv a fixpoint semantics of logic programs and based on it proves the completeness of the procedural semantics v several applications of fuzzy linguistic logic programming and vi an idea of implementing a system to execute fuzzy linguistic logic programs,0
we analyze the computational complexity of admissibility and unifiability with parameters in transitive modal logics the class of clusterextensible clx logics was introduced in the first part of this series of papers we completely classify the complexity of unifiability or inadmissibility in any clx logic as being complete for one of exp2 nexp conexp pspace or p2 in addition to the main case where arbitrary parameters are allowed we consider restricted problems with the number of parameters bounded by a constant and the parameterfree case our upper bounds are specific to clx logics but we also include similar results for logics of bounded depth and width in contrast our lower bounds are very general they apply each to a class of all transitive logics whose frames allow occurrence of certain finite subframes we also discuss the baseline problem of complexity of derivability it is conpcomplete or pspacecomplete for each clx logic in particular we prove pspacehardness of derivability for a broad class of transitive logics that includes all logics with the disjunction property,0
we develop a framework for epistemic logic that combines relevant modal logic with classical propositional logic in our framework the agent is modeled as reasoning in accordance with a relevant modal logic while the propositional fragment of our logics is classical in order to achieve this feature we modify the relational semantics for relevant modal logics so that validity in a model is defined as satisfaction throughout a set of designated states that as far as propositional connectives are concerned behave like classical possible worlds the main technical result of the paper is a modular completeness theorem parametrized by the relevant modal logic formalizing the agents reasoning,0
our central observation is that unbounded additive recurrence establishes a homomorphism between mathbbn and modus ponens in a constructive sense by finding sums of nonconsecutive fibonacci indices each inference step corresponds to a geometric constraint whose verification requires omlog n bitoperations logical entailment can be interpreted constructively as arcclosures under scaling offering a bridge between additive combinatorics proof theory and symbolic computation,0
given a conditional sentence pq if p then q and respective facts four different types of inferences are observed in human reasoning affirming the antecedent aa or modus ponens reasons q from p affirming the consequent ac reasons p from q denying the antecedent da reasons q from p and denying the consequent dc or modus tollens reasons p from q among them aa and dc are logically valid while ac and da are logically invalid and often called logical fallacies nevertheless humans often perform ac or da as pragmatic inference in daily life in this paper we realize ac da and dc inferences in answer set programming eight different types of completion are introduced and their semantics are given by answer sets we investigate formal properties and characterize human reasoning tasks in cognitive psychology those completions are also applied to commonsense reasoning in ai,0
differential game logic dgl is a logic for specifying and verifying properties of hybrid games ie games that combine discrete continuous and adversarial dynamics unlike hybrid systems hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives the logic dgl can be used to study the existence of winning strategies for such hybrid games ie ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent hybrid games are determined ie from each state one player has a winning strategy yet computing their winning regions may take transfinitely many steps the logic dgl nevertheless has a sound and complete axiomatization relative to any expressive logic separating axioms are identified that distinguish hybrid games from hybrid systems finally dgl is proved to be strictly more expressive than the corresponding logic of hybrid systems by characterizing the expressiveness of both,0
inquisitive modal logic inqml in its epistemic incarnation extends standard epistemic logic to capture not just the information that agents have but also the questions that they are interested in we use the natural notion of bisimulation equivalence in the setting of inqml as introduced in to characterise the expressiveness of inqml as the bisimulation invariant fragment of firstorder logic over natural classes of twosorted firstorder structures that arise as relational encodings of inquisitive epistemic s5like models the nonelementary nature of these classes crucially requires nonclassical modeltheoretic methods for the analysis of firstorder expressiveness irrespective of whether we aim for characterisations in the sense of classical or of finite model theory,0
we study the notion of conservative translation between logics introduced by feitosa and dottaviano we show that classical propositional logic cpc is universal in the sense that every finitary consequence relation over a countable set of formulas can be conservatively translated into cpc the translation is computable if the consequence relation is decidable more generally we show that one can take instead of cpc a broad class of logics extensions of a certain fragment of full lambek calculus fl including most nonclassical logics studied in the literature hence in a sense almost any two reasonable deductive systems can be conservatively translated into each other we also provide some counterexamples in particular the paraconsistent logic lp is not universal,0
i aim to promote an alternative agenda for teaching modal logic chiefly inspired by the relationships between modal logic and philosophy the guiding idea for this proposal is a reappraisal of the interest of modal logic in philosophy which do not stem mainly from mathematical issues but which is motivated by central problems of philosophy and language i will point out some themes to start elaborating a guide for a more comprehensive approach to teach modal logic and consider the contributions of dualprocess theories in cognitive science in order to explore a pedagogical framework for the proposed point of view,0
we design temporal description logics suitable for reasoning about temporal conceptual data models and investigate their computational complexity our formalisms are based on dllite logics with three types of concept inclusions ranging from atomic concept inclusions and disjointness to the full booleans as well as cardinality constraints and role inclusions in the temporal dimension they capture future and past temporal operators on concepts flexible and rigid roles the operators always and some time on roles data assertions for particular moments of time and global concept inclusions the logics are interpreted over the cartesian products of object domains and the flow of time z satisfying the constant domain assumption we prove that the most expressive of our temporal description logics which can capture lifespan cardinalities and either qualitative or quantitative evolution constraints turn out to be undecidable however by omitting some of the temporal operators on conceptsroles or by restricting the form of concept inclusions we obtain logics whose complexity ranges between pspace and nlogspace these positive results were obtained by reduction to various clausal fragments of propositional temporal logic which opens a way to employ propositional or firstorder temporal provers for reasoning about temporal data models,0
this paper develops the model theory of normal modal logics based on partial possibilities instead of total worlds following humberstone 1981 instead of kripke 1963 possibility semantics can be seen as extending to modal logic the semantics for classical logic used in weak forcing in set theory or as semanticizing a negative translation of classical modal logic into intuitionistic modal logic thus possibility frames are based on posets with accessibility relations like intuitionistic modal frames but with the constraint that the interpretation of every formula is a regular open set in the alexandrov topology on the poset the standard world frames for modal logic are the special case of possibility frames wherein the poset is discrete we develop the beginnings of duality theory definabilitycorrespondence theory and completeness theory for possibility frames,0
a shallow semantical embedding for public announcement logic with relativized common knowledge is presented this embedding enables the firsttime automation of this logic with offtheshelf theorem provers for classical higherorder logic it is demonstrated i how metatheoretical studies can be automated this way and ii how nontrivial reasoning in the target logic public announcement logic required eg to obtain a convincing encoding and automation of the wise men puzzle can be realized key to the presented semantical embedding in contrast eg to related work on the semantical embedding of normal modal logics is that evaluation domains are modeled explicitly and treated as additional parameter in the encodings of the constituents of the embedded target logic while they were previously implicitly shared between meta logic and target logic,0
justification logics are modallike logics that provide a framework for reasoning about justifications this paper introduces labeled sequent calculi for justification logics as well as for hybrid modaljustification logics using the method due to sara negri we internalize the kripkestyle semantics of justification logics known as fitting models within the syntax of the sequent calculus to produce labeled sequent calculus we show that our labeled sequent calculi enjoy a weak subformula property all of the rules are invertible and the structural rules weakening and contraction and cut are admissible finally soundness and completeness are established and termination of proof search for some of the labeled systems are shown we describe a procedure for some of the labeled systems which produces a derivation for valid sequents and a countermodel for nonvalid sequents we also show a model correspondence for justification logics in the context of labeled sequent calculus,0
in game theory mechanism design is concerned with the design of incentives so that a desired outcome of the game can be achieved in this paper we explore the concept of equilibrium design where incentives are designed to obtain a desirable equilibrium that satisfies a specific temporal logic property our study is based on a framework where system specifications are represented as temporal logic formulae games as quantitative concurrent game structures and players goals as meanpayoff objectives we consider system specifications given by ltl and gr1 formulae and show that designing incentives to ensure that a given temporal logic property is satisfied on someevery nash equilibrium of the game can be achieved in pspace for ltl properties and in npp 2 for gr1 specifications we also examine the complexity of related decision and optimisation problems such as optimality and uniqueness of solutions as well as considering social welfare and show that the complexities of these problems lie within the polynomial hierarchy equilibrium design can be used as an alternative solution to rational synthesis and verification problems for concurrent games with meanpayoff objectives when no solution exists or as a technique to repair concurrent games with undesirable nash equilibria in an optimal way,0
the implication relationship between subsystems in reverse mathematics has an underlying logic which can be used to deduce certain new reverse mathematics results from existing ones in a routine way we use techniques of modal logic to formalize the logic of reverse mathematics into a system that we name slogic we argue that slogic captures precisely the logical content of the implication and nonimplication relations between subsystems in reverse mathematics we present a sound complete decidable and compact tableaustyle deductive system for slogic and explore in detail two fragments that are particularly relevant to reverse mathematics practice and automated theorem proving of reverse mathematics results,0
we study admissibility of inference rules and unification with parameters in transitive modal logics extensions of k4 in particular we generalize various results on parameterfree admissibility and unification to the setting with parameters specifically we give a characterization of projective formulas generalizing ghilardis characterization in the parameterfree case leading to new proofs of rybakovs results that admissibility with parameters is decidable and unification is finitary for logics satisfying suitable frame extension properties called clusterextensible logics in this paper we construct explicit bases of admissible rules with parameters for clusterextensible logics and give their semantic description we show that in the case of finitely many parameters these logics have independent bases of admissible rules and determine which logics have finite bases as a sideline we show that clusterextensible logics have various nice properties in particular they are finitely axiomatizable and have an exponentialsize model property we also give a rather general characterization of logics with directed filtering unification in the sequel we will use the same machinery to investigate the computational complexity of admissibility and unification with parameters in clusterextensible logics and we will adapt the results to logics with unique top cluster eg s42 and superintuitionistic logics,0
the paper gives a soundness and completeness proof for the implicative fragment of intuitionistic calculus with respect to the semantics of computability logic which understands intuitionistic implication as interactive algorithmic reduction this concept more precisely the associated concept of reducibility is a generalization of turing reducibility from the traditional inputoutput sorts of problems to computational tasks of arbitrary degrees of interactivity see for a comprehensive online source on computability logic,0
we develop a method for showing that various modal logics that are valid in their countably generated canonical kripke frames must also be valid in their uncountably generated ones this is applied to many systems including the logics of finite width and a broader class of multimodal logics of finite achronal width that are introduced here,0
we study the mathematical properties of bilateral statebased modal logic bsml a modal logic employing statebased semantics also known as team semantics which has been used to account for free choice inferences and related linguistic phenomena this logic extends classical modal logic with a nonemptiness atom which is true in a state if and only if the state is nonempty we introduce two extensions of bsml and show that the extensions are expressively complete and develop natural deduction axiomatizations for the three logics,0
we prove that the determinacy of galestewart games whose winning sets are accepted by realtime 1counter bchi automata is equivalent to the determinacy of effective analytic galestewart games which is known to be a large cardinal assumption we show also that the determinacy of wadge games between two players in charge of omegalanguages accepted by 1counter bchi automata is equivalent to the effective analytic wadge determinacy using some results of set theory we prove that one can effectively construct a 1counter bchi automaton a and a bchi automaton b such that 1 there exists a model of zfc in which player 2 has a winning strategy in the wadge game wla lb 2 there exists a model of zfc in which the wadge game wla lb is not determined moreover these are the only two possibilities ie there are no models of zfc in which player 1 has a winning strategy in the wadge game wla lb,0
recent years witness a growing interest in nonstandard epistemic logics of knowing whether knowing what knowing how and so on these logics are usually not normal ie the standard axioms and reasoning rules for modal logic may be invalid in this paper we show that the conditional knowing value logic proposed by wang and fan citewf13 can be viewed as a disguised normal modal logic by treating the negation of the kv operator as a special diamond under this perspective it turns out that the original firstorder kripke semantics can be greatly simplified by introducing a ternary relation ric in standard kripke models which associates one world with two iaccessible worlds that do not agree on the value of constant c under intuitive constraints the modal logic based on such kripke models is exactly the one studied by wang and fan 20132014 moreover there is a very natural binary generalization of the knowing value diamond which surprisingly does not increase the expressive power of the logic the resulting logic with the binary diamond has a transparent normal modal system which sharpens our understanding of the knowing value logic and simplifies some previously hard problems,0
we provide a sound and complete proof system for an extension of kleenes ternary logic to predicates the concept of theory is extended with for each function symbol a formula that specifies when the function is defined the notion of is defined is extended to terms and formulas via a straightforward recursive algorithm the is defined formulas are constructed so that they themselves are always defined the completeness proof relies on the henkin construction for each formula precisely one of the formula its negation and the negation of its is defined formula is true on the constructed model many other ternary logics in the literature can be reduced to ours partial functions are ubiquitous in computer science and even in inequation solving at schools our work was motivated by an attempt to explain precisely in terms of logic typical informal methods of reasoning in such applications,0
we introduce a notion of kripke model for classical logic for which we constructively prove soundness and cutfree completeness we discuss the novelty of the notion and its potential applications,0
we introduce a modal logic for describing statistical knowledge which we call statistical epistemic logic we propose a kripke model dealing with probability distributions and stochastic assignments and show a stochastic semantics for the logic to our knowledge this is the first semantics for modal logic that can express the statistical knowledge dependent on nondeterministic inputs and the statistical significance of observed results by using statistical epistemic logic we express a notion of statistical secrecy with a confidence level we also show that this logic is useful to formalize statistical hypothesis testing and differential privacy in a simple and abstract manner,0
the work is devoted to computability logic col the philosophicalmathematical platform and longterm project for redeveloping classical logic after replacing truth by computability in its underlying semantics see this article elaborates some basic complexity theory for the col framework then it proves soundness and completeness for the deductive system cl12 with respect to the semantics of col including the version of the latter based on polynomial time computability instead of computabilityinprinciple cl12 is a sequent calculus system where the meaning of a sequent intuitively can be characterized as the succedent is algorithmically reducible to the antecedent and where formulas are built from predicate letters function letters variables constants identity negation parallel and choice connectives and blind and choice quantifiers a case is made that cl12 is an adequate logical basis for constructive applied theories including complexityoriented ones,0
in database theory the term textitdatabase transformation was used to refer to a unifying treatment for computable queries and updates recently it was shown that nondeterministic database transformations can be captured exactly by a variant of asms the socalled database abstract state machines dbasms in this article we present a logic for dbasms extending the logic of nanchen and strk for asms in particular we develop a rigorous proof system for the logic for dbasms which is proven to be sound and complete the most difficult challenge to be handled by the extension is a proper formalisation capturing nondeterminism of database transformations and all its related features such as consistency update sets or multisets associated with dbasm rules as the database part of a state of database transformations is a finite structure and dbasms are restricted by allowing quantifiers only over the database part of a state we resolve this problem by taking update sets explicitly into the logic ie by using an additional modal operator where x is interpreted as an update set  generated by a dbasm rule the dbasm logic provides a powerful verification tool to study properties of database transformations,0
temporal reasoning with conditionals is more complex than both classical temporal reasoning and reasoning with timeless conditionals and can lead to some rather counterintuitive conclusions for instance aristotles famous sea battle tomorrow puzzle leads to a fatalistic conclusion whether there will be a sea battle tomorrow or not but that is necessarily the case now we propose a branchingtime logic ltc to formalise reasoning about temporal conditionals and provide that logic with adequate formal semantics the logic ltc extends the nexttime fragment of ctl with operators for model updates restricting the domain to only future moments where antecedent is still possible to satisfy we provide formal semantics for these operators that implements the restrictor interpretation of antecedents of temporalized conditionals by suitably restricting the domain of discourse as a motivating example we demonstrate that a naturally formalised in our logic version of the sea battle argument renders it unsound thereby providing a solution to the problem with fatalist conclusion that it entails because its underlying reasoning per cases argument no longer applies when these cases are treated not as material implications but as temporal conditionals on the technical side we analyze the semantics of ltc and provide a series of reductions of ltcformulae first recursively eliminating the dynamic update operators and then the path quantifiers in such formulae using these reductions we obtain a sound and complete axiomatization for ltc and reduce its decision problem to that of the modal logic kd,0
in this paper we present a constructive proof of cut elimination for a system of full second order logic with the structural rules absorbed and using sets instead of sequences the standard problem of the cutrank growth is avoided by using a new parameter for the induction the cutweight this technique can also be applied to first order logic,0
we give a procedure for counting the number of different proofs of a formula in various sorts of propositional logic this number is either an integer that may be 0 if the formula is not provable or infinite,0
using the latticetheoretic version of the euler characteristic introduced by v klee and gc rota in the sixties we define the euler characteristic of a formula in gdel logic over finitely or infinitely many truthvalues we then prove that the information encoded by the euler characteristic is classical ie coincides with the analogous notion defined over boolean logic building on this we define manyvalued versions of the euler characteristic of a formula varphi and prove that they indeed provide information about the logical status of varphi in gdel logic specifically our first main result shows that the manyvalued euler characteristics are invariants that separate manyvalued tautologies from nontautologies further we offer an initial investigation of the linear structure of these generalised characteristics our second main result is that the collection of manyvalued characteristics forms a linearly independent set in the real vector space of all valuations of gdel logic over finitely many propositional variables,0
probabilistic justification logic is a modal logic with two kind of modalities probability measures and explicit justification terms we present a tableau procedure that can be used to decide the satisfiability problem for this logic in polynomial space we show that this upper complexity bound is tight,0
several formal systems such as resolution and minimal model semantics provide a framework for logic programming in this paper we will survey the use of structural proof theory as an alternative foundation researchers have been using this foundation for the past 35 years to elevate logic programming from its roots in firstorder classical logic into higherorder versions of intuitionistic and linear logic these more expressive logic programming languages allow for capturing stateful computations and rich forms of abstractions including higherorder programming modularity and abstract data types termlevel bindings are another kind of abstraction and these are given an elegant and direct treatment within both proof theory and these extended logic programming languages logic programming has also inspired new results in proof theory such as those involving polarity and focused proofs these recent results provide a highlevel means for presenting the differences between forwardchaining and backwardchaining style inferences anchoring logic programming in proof theory has also helped identify its connections and differences with functional programming deductive databases and model checking,0
in prior work we showed that logic programming compilation can be given a prooftheoretic justification for generic abstract logic programming languages and demonstrated this technique in the case of hereditary harrop formulas and their linear variant compiled clauses were themselves logic formulas except for the presence of a secondorder abstraction over the atomic goals matching their head in this paper we revisit our previous results into a more detailed and fully logical justification that does away with this spurious abstraction we then refine the resulting technique to support wellmoded programs efficiently,0
this volume contains the proceedings of the third international symposium on games automata logic and formal verification gandalf held in naples italy from september 6th to 8th 2012 gandalf was founded by a number of italian computer scientists interested in mathematical logic automata theory game theory and their applications to the specification design and verification of complex systems its aim is to provide a forum where people from different areas and possibly with different backgrounds can fruitfully interact even though the idea of the symposium emerged within the italian research community the event has a truly international nature as witnessed by the composition of the conference committees and the programme,0
we provide a method of translating theories of nutes defeasible logic into logic programs and a corresponding translation in the opposite direction under certain natural restrictions the conclusions of defeasible theories under the ambiguity propagating defeasible logic adl correspond to those of the wellfounded semantics for normal logic programs and so it turns out that the two formalisms are closely related using the same translation of logic programs into defeasible theories the semantics for the ambiguity blocking defeasible logic ndl can be seen as indirectly providing an ambiguity blocking semantics for logic programs we also provide antimonotone operators for both adl and ndl each based on the gelfondlifschitz gl operator for logic programs for defeasible theories without defeaters or priorities on rules the operator for adl corresponds to the gl operator and so can be seen as partially capturing the consequences according to adl similarly the operator for ndl captures the consequences according to ndl though in this case no restrictions on theories apply both operators can be used to define stable model semantics for defeasible theories,0
eventdriven reactive functionalities are an urgent need in nowadays distributed serviceoriented applications and semantic webbased environments an important problem to be addressed is how to correctly and efficiently capture and process the eventbased behavioral reactive logic represented as eca rules in combination with other conditional decision logic which is represented as derivation rules in this paper we elaborate on a homogeneous integration approach which combines derivation rules reaction rules eca rules and other rule types such as integrity constraint into the general framework of logic programming the developed ecalp language provides expressive features such as idbased updates with support for external and selfupdates of the intensional and extensional knowledge transactions including integrity testing and an event algebra to define and process complex events and actions based on a novel intervalbased event calculus variant,0
we characterise the sentences in monadic secondorder logic mso that are over finite structures equivalent to a datalog program in terms of an existential pebble game we also show that for every class c of finite structures that can be expressed in mso and is closed under homomorphisms and for all integers lk there exists a canonical datalog program pi of width lk in the sense of feder and verdi the same characterisations also hold for guarded secondorder logic gso which properly extends mso to prove our results we show that every class c in gso whose complement is closed under homomorphisms is a finite union of constraint satisfaction problems csps of countably categorical structures the intersection of mso and datalog is known to contain the class of nested monadically defined queries nemodeq likewise we show that the intersection of gso and datalog contains all problems that can be expressed by the more expressive language of nested guarded queries yet by exploiting our results we can show that neither of the two query languages can serve as a characterization as we exhibit a query in the intersection of mso and datalog that is not expressible in nested guarded queries,0
in a seminal work k segerberg introduced a deontic logic called dal to investigate normative reasoning over actions dal marked the beginning of a new area of research in deontic logic by shifting the focus from deontic operators on propositions to deontic operators on actions in this work we revisit dal and provide a complete algebraization for it in our algebraization we introduce deontic action algebras algebraic structures consisting of a boolean algebra for interpreting actions a boolean algebra for interpreting formulas and two mappings from one boolean algebra to the other interpreting the deontic concepts of permission and prohibition we elaborate on how the framework underpinning deontic action algebras enables the derivation of different deontic action logics by removing or imposing additional conditions over either of the boolean algebras we leverage this flexibility to demonstrate how we can capture in this framework several logics in the dal family furthermore we introduce four variations of dal by a enriching the algebra of formulas with propositions on states b adopting a heyting algebra for state propositions c adopting a heyting algebra for actions and d adopting heyting algebras for both we illustrate these new deontic action logics with examples and establish their algebraic completeness,0
the euler characteristic can be defined as a special kind of valuation on finite distributive lattices this work begins with some brief consideration on the role of the euler characteristic on nm algebras the algebraic counterpart of nilpotent minimum logic then we introduce a new valuation a modified version of the euler characteristic we call idempotent euler characteristic we show that the new valuation encodes information about the formul in nm propositional logic,0
this work deals with the problem of combining reactive features such as the ability to respond to events and define complex events with the execution of transactions over general knowledge bases kbs with this as goal we build on transaction logic tr a logic precisely designed to model and execute transactions in kbs defined by arbitrary logic theories in it transactions are written in a logicprogramming style by combining primitive update operations over a general kb with the usual logic programming connectives and some additional connectives eg to express sequence of actions while tr is a natural choice to deal with transactions it remains the question whether tr can be used to express complex events but also to deal simultaneously with the detection of complex events and the execution of transactions in this paper we show that the former is possible while the latter is not for that we start by illustrating how tr can express complex events and in particular how snoop event expressions can be translated in the logic afterwards we show why tr fails to deal with the two issues together and to solve the intended problem propose transaction logic with events its syntax model theory and executional semantics the achieved solution is a nonmonotonic extension of tr which guarantees that every complex event detected in a transaction is necessarily responded,0
we investigate the proof complexity of extended frege ef systems for basic transitive modal logics k4 s4 gl augmented with the bounded branching axioms mathbfbbk first we study feasibility of the disjunction property and more general extension rules in ef systems for these logics we show that the corresponding decision problems reduce to total conp search problems or equivalently disjoint np pairs in the binary case more precisely the decision problem for extension rules is equivalent to a certain special case of interpolation for the classical ef system next we use this characterization to prove superpolynomial or even exponential with stronger hypotheses separations between ef and substitution frege sf systems for all transitive logics contained in mathbfs42grzbb2 or mathbfgl2bb2 under some assumptions weaker than mathrmpspace ne np we also prove analogous results for superintuitionistic logics we characterize the decision complexity of multiconclusion vissers rules in ef systems for gabbayde jongh logics mathbf tk and we show conditional separations between ef and sf for all intermediate logics contained in mathbft2 kc,0
description logics dls are wellknown knowledge representation formalisms focused on the representation of terminological knowledge due to their firstorder semantics these languages in their classical form are not suitable for representing and handling uncertainty a probabilistic extension of a lightweight dl was recently proposed for dealing with certain knowledge occurring in uncertain contexts in this paper we continue that line of research by introducing the bayesian extension balc of the propositionally closed dl alc we present a tableaubased procedure for deciding consistency and adapt it to solve other probabilistic contextual and general inferences in this logic we also show that all these problems remain exptimecomplete the same as reasoning in the underlying classical alc,0
simple type theory is suited as framework for combining classical and nonclassical logics this claim is based on the observation that various prominent logics including quantified multimodal logics and intuitionistic logics can be elegantly embedded in simple type theory furthermore simple type theory is sufficiently expressive to model combinations of embedded logics and it has a well understood semantics offtheshelf reasoning systems for simple type theory exist that can be uniformly employed for reasoning within and about combinations of logics,0
this paper presents rules of inference for a binary quantifier i for the formalisation of sentences containing definite descriptions within intuitionist positive free logic i binds one variable and forms a formula from two formulas ix means the f is g the system is shown to have desirable prooftheoretic properties it is proved that deductions in it can be brought into normal form the discussion is rounded up by comparisons between the approach to the formalisation of definite descriptions recommended here and the more usual approach that uses a termforming operator  where xf means the f,0
we revisit the crucial issue of natural game equivalences and semantics of game logics based on these we present reasons for investigating finer concepts of game equivalence than equality of standard powers though staying short of modal bisimulation concretely we propose a more finegrained notion of equality of basic powers which record what players can force plus what they leave to others to do a crucial feature of interaction this notion is closer to gametheoretic strategic form as we explain in detail while remaining amenable to logical analysis we determine the properties of basic powers via a new representation theorem find a matching instantial neighborhood game logic and show how our analysis can be extended to a new game algebra and dynamic game logic,0
this volume contains the proceedings of the seventh international symposium on games automata logic and formal verification gandalf 2016 the symposium took place in catania italy from the 14th to the 16th of september 2016 the proceedings of the symposium contain abstracts of the 3 invited talks and 21 full papers that were accepted after a careful evaluation for presentation at the conference the topics of the accepted papers cover algorithmic game theory automata theory synthesis formal verification and dynamic modal and temporal logics,0
topological fixpoint logics are a family of logics that admits topological models and where the fixpoint operators are defined with respect to the topological interpretations here we consider a topological fixpoint logic for relational structures based on stone spaces where the fixpoint operators are interpreted via clopen sets we develop a gametheoretic semantics for this logic first we introduce games characterising clopen fixpoints of monotone operators on stone spaces these fixpoint games allow us to characterise the semantics for our topological fixpoint logic using a twoplayer graph game adequacy of this game is the main result of our paper finally we define bisimulations for the topological structures under consideration and use our game semantics to prove that the truth of a formula of our topological fixpoint logic is bisimulationinvariant,0
this paper considers a formalisation of classical logic using general introduction rules and general elimination rules it proposes a definition of maximal formula segment and maximal segment suitable to the system and gives reduction procedures for them it is then shown that deductions in the system convert into normal form ie deductions that contain neither maximal formulas nor maximal segments and that deductions in normal form satisfy the subformula property tarskis rule is treated as a general introduction rule for implication the general introduction rule for negation has a similar form maximal formulas with implication or negation as main operator require reduction procedures of a more intricate kind not present in normalisation for intuitionist logic the correction added to the end of the paper corrects an error theorem 2 is mistaken and so is a corollary drawn from it as well as a corollary that was concluded by the same mistake luckily this does not affect the main result of the paper,0
priest has provided a simple tableau calculus for chellass conditional logic ck we provide rules which when added to priests system result in tableau calculi for chellass ck and lewiss vc completeness of these tableaux however relies on the cut rule,0
logic programming has developed as a rich field built over a logical substratum whose main constituent is a nonclassical form of negation sometimes coexisting with classical negation the field has seen the advent of a number of alternative semantics with kripkekleene semantics the wellfounded semantics the stable model semantics and the answerset semantics standing out as the most successful we show that all aforementioned semantics are particular cases of a generic semantics in a framework where classical negation is the unique form of negation and where the literals in the bodies of the rules can be marked to indicate that they can be the targets of hypotheses a particular semantics then amounts to choosing a particular marking scheme and choosing a particular set of hypotheses when a literal belongs to the chosen set of hypotheses all marked occurrences of that literal in the body of a rule are assumed to be true whereas the occurrences of that literal that have not been marked in the body of the rule are to be derived in order to contribute to the firing of the rule hence the notion of hypothetical reasoning that is presented in this framework is not based on making global assumptions but more subtly on making local contextual assumptions taking effect as indicated by the chosen marking scheme on the basis of the chosen set of hypotheses our approach offers a unified view on the various semantics proposed in logic programming classical in that only classical negation is used and links the semantics of logic programs to mechanisms that endow rulebased systems with the power to harness hypothetical reasoning,0
game semantics aim at describing the interactive behaviour of proofs by interpreting formulas as games on which proofs induce strategies in this article we introduce a game semantics for a fragment of first order propositional logic one of the main difficulties that has to be faced when constructing such semantics is to make them precise by characterizing definable strategies that is strategies which actually behave like a proof this characterization is usually done by restricting to the model to strategies satisfying subtle combinatory conditions such as innocence whose preservation under composition is often difficult to show here we present an original methodology to achieve this task which requires to combine tools from game semantics rewriting theory and categorical algebra we introduce a diagrammatic presentation of definable strategies by the means of generators and relations those strategies can be generated from a finite set of atomic strategies and that the equality between strategies generated in such a way admits a finite axiomatization these generators satisfy laws which are a variation of bialgebras laws thus bridging algebra and denotational semantics in a clean and unexpected way,0
the firstorder theory of mall multiplicative additive linear logic over only equalities is an interesting but weak logic since it cannot capture unbounded infinite behavior instead of accounting for unbounded behavior via the addition of the exponentials and we add least and greatest fixed point operators the resulting logic which we call mumall satisfies two fundamental proof theoretic properties we establish weak normalization for it and we design a focused proof system that we prove complete that second result provides a strong normal form for cutfree proof structures that can be used for example to help automate proof search we show how these foundations can be applied to intuitionistic logic,0
we introduce bpdl a combination of propositional dynamic logic pdl with the basic fourvalued modal logic bk studied by odintsov and wansing modal logics with belnapian truth values j appl nonclass log 20 279301 2010 we modify the standard arguments based on canonical models and filtration to suit the fourvalued context and prove weak completeness and decidability of bpdl,0
this papers develops a logical language for representing probabilistic causal laws our interest in such a language is twofold first it can be motivated as a fundamental study of the representation of causal knowledge causality has an inherent dynamic aspect which has been studied at the semantical level by shafer in his framework of probability trees in such a dynamic context where the evolution of a domain over time is considered the idea of a causal law as something which guides this evolution is quite natural in our formalization a set of probabilistic causal laws can be used to represent a class of probability trees in a concise flexible and modular way in this way our work extends shafers by offering a convenient logical representation for his semantical objects second this language also has relevance for the area of probabilistic logic programming in particular we prove that the formal semantics of a theory in our language can be equivalently defined as a probability distribution over the wellfounded models of certain logic programs rendering it formally quite similar to existing languages such as icl or prism because we can motivate and explain our language in a completely selfcontained way as a representation of probabilistic causal laws this provides a new way of explaining the intuitions behind such probabilistic logic programs we can say precisely which knowledge such a program expresses in terms that are equally understandable by a nonlogician moreover we also obtain an additional piece of knowledge representation methodology for probabilistic logic programs by showing how they can express probabilistic causal laws,0
we present a proof system for the provability logic glp in the formalism of nested sequents and prove the cut elimination theorem for it as an application we obtain the reduction of glp to its important fragment called j syntactically,0
this paper presents a soundness and completeness proof for propositional intuitionistic calculus with respect to the semantics of computability logic the latter interprets formulas as interactive computational problems formalized as games between a machine and its environment intuitionistic implication is understood as algorithmic reduction in the weakest possible and hence most natural sense disjunction and conjunction as deterministicchoice combinations of problems disjunction machines choice conjunction environments choice and absurd as a computational problem of universal strength see for a comprehensive online source on computability logic,0
we show a model construction for a system of higherorder illative combinatory logic mathcali thus establishing its strong consistency we also use a variant of this construction to provide a complete embedding of firstorder intuitionistic predicate logic with secondorder propositional quantifiers into the system mathcali0 of barendregt bunder and dekkers which gives a partial answer to a question posed by these authors,0
in the refinement calculus monotonic predicate transformers are used to model specifications for imperative programs together with a natural notion of simulation they form a category enjoying many algebraic properties we build on this structure to make predicate transformers into a de notational model of full linear logic all the logical constructions have a natural interpretation in terms of predicate transformers ie in terms of specifications we then interpret proofs of a formula by a safety property for the corresponding specification,0
coalgebraic logic programming coalp is a dialect of logic programming designed to bring a more precise compiletime and runtime analysis of termination and productivity for recursive and corecursive functions in logic programming its second goal is to introduce guarded lazy corecursion akin to functional theorem provers into logic programming in this paper we explain lazy features of coalp and compare them with the loopanalysis and eager execution in coinductive logic programming colp we conclude by outlining the future directions in developing the guarded corecursion in logic programming,0
large language models llms are increasingly explored as generalpurpose reasoners particularly in agentic contexts however their outputs remain prone to mathematical and logical errors this is especially challenging in openended tasks where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies to address this issue we propose logicenhanced language model agents lelma a framework that integrates llms with formal logic to enable validation and refinement of natural language reasoning lelma comprises three components an llmreasoner an llmtranslator and a solver and employs autoformalization to translate reasoning into logic representations which are then used to assess logical validity using gametheoretic scenarios such as the prisoners dilemma as testbeds we highlight the limitations of both less capable gemini 10 pro and advanced gpt4o models in generating logically sound reasoning lelma achieves high accuracy in error detection and improves reasoning correctness via selfrefinement particularly in gpt4o the study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous openended reasoning tasks,0
in his autobiographic essay written in 1999 from logic to computer science and back martin david davis 381928112023 indicated that he viewed himself as a logician emphand a computer scientist he expanded the essay in 2016 and expressed a new perspective through a changed title my life as a logician he points out that logic was the unifying theme underlying his scientific career our paper attempts to provide a consistent vision that illuminates davis successive contributions leading to his landmark writings on computability unsolvable problems automated reasoning as well as the history and philosophy of computing,0
we propose a semantic foundation for logics for reasoning in settings that possess a distinction between equality of variables a coarser equivalence of variables and a notion of conditional independence between variables we show that such relations can be modelled naturally in atomic sheaf toposes,0
this paper treats logic programming with three kinds of negation default weak and strict negations a 3valued logic model theory is discussed for logic programs with three kinds of negation the procedure is constructed for negations so that a soundness of the procedure is guaranteed in terms of 3valued logic model theory,0
equilibrium logic is an approach to nonmonotonic reasoning that extends the stablemodel and answerset semantics for logic programs in particular it includes the general case of nested logic programs where arbitrary boolean combinations are permitted in heads and bodies of rules as special kinds of theories in this paper we present polynomial reductions of the main reasoning tasks associated with equilibrium logic and nested logic programs into quantified propositional logic an extension of classical propositional logic where quantifications over atomic formulas are permitted we provide reductions not only for decision problems but also for the central semantical concepts of equilibrium logic and nested logic programs in particular our encodings map a given decision problem into some formula such that the latter is valid precisely in case the former holds the basic tasks we deal with here are the consistency problem brave reasoning and skeptical reasoning additionally we also provide encodings for testing equivalence of theories or programs under different notions of equivalence viz ordinary strong and uniform equivalence for all considered reasoning tasks we analyse their computational complexity and give strict complexity bounds,0
we investigate the complexity of satisfiability for finitevariable fragments of propositional dynamic logics we consider three formalisms belonging to three representative complexity classes broadly understoodregular pdl which is exptimecomplete pdl with intersection which is 2exptimecomplete and pdl with parallel composition which is undecidable we show that for each of these logics the complexity of satisfiability remains unchanged even if we only allow as inputs formulas built solely out of propositional constants ie without propositional variables moreover we show that this is a consequence of the richness of the expressive power of variablefree fragments for all the logics we consider such fragments are as semantically expressive as entire logics we conjecture that this is representative of pdlstyle as well as closely related logics,0
this paper shows that even at the most basic level the parallel countable branching and uncountable branching recurrences of computability logic see validate different principles,0
probabilistic logic programming plp under the distribution semantics is a leading approach to practical reasoning under uncertainty an advantage of the distribution semantics is its suitability for implementation as a prolog or python library available through two wellmaintained implementations namely problog and cplintpita however current formulations of the distribution semantics use pointprobabilities making it difficult to express epistemic uncertainty such as arises from for example hierarchical classifications from computer vision models belief functions generalize probability measures as nonadditive capacities and address epistemic uncertainty via interval probabilities this paper introduces intervalbased capacity logic programs based on an extension of the distribution semantics to include belief functions and describes properties of the new framework that make it amenable to practical applications,0
in this note we consider the problem of introducing variables in temporal logic programs under the formalism of temporal equilibrium logic tel an extension of answer set programming asp for dealing with lineartime modal operators to this aim we provide a definition of a firstorder version of tel that shares the syntax of firstorder lineartime temporal logic ltl but has a different semantics selecting some ltl models we call temporal stable models then we consider a subclass of theories called splittable temporal logic programs that are close to usual logic programs but allowing a restricted use of temporal operators in this setting we provide a syntactic definition of safe variables that suffices to show the property of domain independence that is addition of arbitrary elements in the universe does not vary the set of temporal stable models finally we present a method for computing the derivable facts by constructing a nontemporal logic program with variables that is fed to a standard asp grounder the information provided by the grounder is then used to generate a subset of ground temporal rules which is equivalent to and generally smaller than the full program instantiation,0
logic programs with ordered disjunction lpods extend classical logic programs with the capability of expressing preferential disjunctions in the heads of program rules the initial semantics of lpods although simple and quite intuitive is not purely modeltheoretic a consequence of this is that certain properties of programs appear nontrivial to formalize in purely logical terms an example of this state of affairs is the characterization of the notion of strong equivalence for lpods although the results of faber et al 2008 are accurately developed they fall short of characterizing strong equivalence of lpods as logical equivalence in some specific logic this comes in sharp contrast with the wellknown characterization of strong equivalence for classical logic programs which as proved by lifschitz et al 2001 coincides with logical equivalence in the logic of hereandthere in this paper we obtain a purely logical characterization of strong equivalence of lpods as logical equivalence in a fourvalued logic moreover we provide a new proof of the conpcompleteness of strong equivalence for lpods which has an interest in its own right since it relies on the special structure of such programs our results are based on the recent logical semantics of lpods introduced by charalambidis et al 2021 a fact which we believe indicates that this new semantics may prove to be a useful tool in the further study of lpods,0
part of the theory of logic programming and nonmonotonic reasoning concerns the study of fixedpoint semantics for these paradigms several different semantics have been proposed during the last two decades and some have been more successful and acknowledged than others the rationales behind those various semantics have been manifold depending on ones point of view which may be that of a programmer or inspired by commonsense reasoning and consequently the constructions which lead to these semantics are technically very diverse and the exact relationships between them have not yet been fully understood in this paper we present a conceptually new method based on level mappings which allows to provide uniform characterizations of different semantics for logic programs we will display our approach by giving new and uniform characterizations of some of the major semantics more particular of the least model semantics for definite programs of the fitting semantics and of the wellfounded semantics a novel characterization of the weakly perfect model semantics will also be provided,0
this volume contains the proceedings of the 11th international symposium on games automata logic and formal verification gandalf 2020 the symposium took place as a fully online event on september 2122 2020 the gandalf symposium was established by a group of italian computer scientists interested in mathematical logic automata theory game theory and their applications to the specification design and verification of complex systems its aim is to provide a forum where people from different areas and possibly with different backgrounds can fruitfully interact gandalf has a truly international spirit as witnessed by the composition of the program and steering committee and by the country distribution of the submitted papers,0
a class of models is presented in the form of continuation monads polymorphic for firstorder individuals that is sound and complete for minimal intuitionistic predicate logic the proofs of soundness and completeness are constructive and the computational content of their composition is in particular a normalisationbyevaluation program for simply typed lambda calculus with sum types although the inspiration comes from danvys typedirected partial evaluator for the same lambda calculus the there essential use of delimited control operators ie computational effects is avoided the role of polymorphism is crucial dropping it allows one to obtain a notion of model complete for classical predicate logic the connection between ours and kripke models is made through a strengthening of the doublenegation shift schema,0
this paper constructs a cirquent calculus system and proves its soundness and completeness with respect to the semantics of computability logic see the logical vocabulary of the system consists of negation parallel conjunction parallel disjunction branching recurrence and branching corecurrence the article is published in two parts with the present part i containing preliminaries and a soundness proof and the forthcoming part ii containing a completeness proof,0
markov logic networks mlns are wellsuited for expressing statistics such as with high probability a smoker knows another smoker but not for expressing statements such as there is a smoker who knows most other smokers which is necessary for modeling eg influencers in social networks to overcome this shortcoming we study quantified mlns which generalize mlns by introducing statistical universal quantifiers allowing to express also the latter type of statistics in a principled way our main technical contribution is to show that the standard reasoning tasks in quantified mlns maximum a posteriori and marginal inference can be reduced to their respective mln counterparts in polynomial time,0
there are various interesting semantics extensions designed for argumentation frameworks they enable to assign a meaning eg to oddlength cycles our main motivation is to transfer semantics proposed by baroni giacomin and guida for argumetation frameworks with oddlength cycles to logic programs with oddlength cycles through default negation the developed construction is even stronger for a given logic program an argumentation framework is defined the construction enables to transfer each semantics of the resulting argumentation framework to a semantics of the given logic program weak points of the construction are discussed and some future continuations of this approach are outlined,0
this paper constructs a cirquent calculus system and proves its soundness and completeness with respect to the semantics of computability logic see the logical vocabulary of the system consists of negation parallel conjunction parallel disjunction branching recurrence and branching corecurrence the article is published in two parts with the previous part i containing preliminaries and a soundness proof and the present part ii containing a completeness proof,0
abstract separation logics are a family of extensions of hoare logic for reasoning about programs that manipulate resources such as memory locations these logics are abstract because they are independent of any particular concrete resource model their assertion languages called propositional abstract separation logics pasls extend the logic of boolean bunched implications bbi in various ways in particular these logics contain the connectives and denoting the composition and extension of resources respectively this added expressive power comes at a price since the resulting logics are all undecidable given their wide applicability even a semidecision procedure for these logics is desirable although several pasls and their relationships with bbi are discussed in the literature the proof theory and automated reasoning for these logics were open problems solved by the conference version of this paper which developed a modular proof theory for various pasls using cutfree labelled sequent calculi this paper nontrivially improves upon this previous work by giving a general framework of calculi on which any new axiom in the logic satisfying a certain form corresponds to an inference rule in our framework and the completeness proof is generalised to consider such axioms our base calculus handles calcagno et als original logic of separation algebras by adding sound rules for partialdeterminism and cancellativity while preserving cutelimination we then show that many important properties in separation logic such as indivisible unit disjointness splittability and crosssplit can be expressed in our general axiom form thus our framework offers inference rules and completeness for these properties for free finally we show how our calculi reduce to calculi with global label substitutions enabling more efficient implementation,0
logic can be made useful for programming and for databases independently of logic programming to be useful in this way logic has to provide a mechanism for the definition of new functions and new relations on the basis of those given in the interpretation of a logical theory we provide this mechanism by creating a compositional semantics on top of the classical semantics in this approach verification of computational results relies on a correspondence between logic interpretations and a class definition in languages like java or c the advantage of this approach is the combination of an expressive medium for the programmer with in the case of c optimal use of computer resources,0
in the literature of game theory the information sets of extensive form games have different interpretations which may lead to confusions and paradoxical cases we argue that the problem lies in the mixup of two interpretations of the extensive form game structures game rules or game runs which do not always coincide in this paper we try to separate and connect these two views by proposing a dynamic epistemic framework in which we can compute the runs step by step from the game rules plus the given assumptions of the players we propose a modal logic to describe players knowledge and its change during the plays and provide a complete axiomatization we also show that under certain conditions the mixup of the rules and the runs is not harmful due to the structural similarity of the two,0
this paper presents a monoidal category whose morphisms are games in the sense of game theory not game semantics and an associated diagrammatic language the two basic operations of a monoidal category namely categorical composition and tensor product correspond roughly to sequential and simultaneous composition of games this leads to a compositional theory in which we can reason about properties of games in terms of corresponding properties of the component parts in particular we give a definition of nash equilibrium which is recursive on the causal structure of the game the key technical idea in this paper is the use of continuation passing style for reasoning about the future consequences of players choices closely based on applications of selection functions in game theory additionally the clean categorical foundation gives many opportunities for generalisation for example to learning agents,0
a modal logic is emphnoniterative if it can be defined by axioms that do not nest modal operators and emphrank1 if additionally all propositional variables in axioms are in scope of a modal operator it is known that every syntactically defined rank1 modal logic can be equipped with a canonical coalgebraic semantics ensuring soundness and strong completeness in the present work we extend this result to noniterative modal logics showing that every noniterative modal logic can be equipped with a canonical coalgebraic semantics defined in terms of a copointed functor again ensuring soundness and strong completeness via a canonical model construction like in the rank1 case the canonical coalgebraic semantics is equivalent to a neighbourhood semantics with suitable frame conditions so the known strong completeness of noniterative modal logics over neighbourhood semantics is implied as an illustration of these results we discuss deontic logics with factual detachment which is captured by axioms that are noniterative but not rank1,0
nonmonotonic causal logic introduced by norman mccain and hudson turner became a basis for the semantics of several expressive action languages mccains embedding of definite propositional causal theories into logic programming paved the way to the use of answer set solvers for answering queries about actions described in such languages in this paper we extend this embedding to nondefinite theories and to firstorder causal logic,0
while finitevariable fragments of the propositional modal logic s5complete with respect to reflexive symmetric and transitive framesare polynomialtime decidable the restriction to finitevariable formulas for logics of reflexive and transitive frames yields fragments that remain intractable the role of the symmetry condition in this context has not been investigated we show that symmetry either by itself or in combination with reflexivity produces logics that behave just like logics of reflexive and transitive frames ie their finitevariable fragments remain intractable namely pspacehard this raises the question of where exactly the borderline lies between modal logics whose finitevariable fragments are tractable and the rest,0
we develop a conceptually clear intuitive and feasible decision procedure for testing satisfiability in the full multiagent epistemic logic cmaelcd with operators for common and distributed knowledge for all coalitions of agents mentioned in the language to that end we introduce hintikka structures for cmaelcd and prove that satisfiability in such structures is equivalent to satisfiability in standard models using that result we design an incremental tableaubuilding procedure that eventually constructs a satisfying hintikka structure for every satisfiable input set of formulae of cmaelcd and closes for every unsatisfiable input set of formulae,0
we investigate properties of the formula p to box p in the basic modal logic k we show that k satisfies an infinitary weaker variant of the rule of margins to box  neg and as a consequence we obtain various negative results about admissibility and unification in k we describe a complete set of unifiers ie substitutions making the formula provable of p to box p and use it to establish that k has the worst possible unification type nullary in wellbehaved transitive modal logics admissibility and unification can be analyzed in terms of projective formulas introduced by ghilardi in particular projective formulas coincide for these logics with formulas that are admissibly saturated ie derive all their multipleconclusion admissible consequences or exact ie axiomatize a theory of a substitution in contrast we show that in k the formula p to box p is admissibly saturated but neither projective nor exact all our results for k also apply to the basic description logic alc,0
we present some applications of intermediate logics in the field of answer set programming asp a brief but comprehensive introduction to the answer set semantics intuitionistic and other intermediate logics is given some equivalence notions and their applications are discussed some results on intermediate logics are shown and applied later to prove properties of answer sets a characterization of answer sets for logic programs with nested expressions is provided in terms of intuitionistic provability generalizing a recent result given by pearce it is known that the answer set semantics for logic programs with nested expressions may select nonminimal models minimal models can be very important in some applications therefore we studied them in particular we obtain a characterization in terms of intuitionistic logic of answer sets which are also minimal models we show that the logic g3 characterizes the notion of strong equivalence between programs under the semantic induced by these models finally we discuss possible applications and consequences of our results they clearly state interesting links between asp and intermediate logics which might bring research in these two areas together,0
a cyclic proof system is a proof system whose proof figure is a tree with cycles the cutelimination in a proof system is fundamental it is conjectured that the cutelimination in the cyclic proof system for firstorder logic with inductive definitions does not hold this paper shows that the conjecture is correct by giving a sequent not provable without the cut rule but provable in the cyclic proof system,0
the main contribution of this paper is the introduction of a dynamic logic formalism for reasoning about information flow in composite quantum systems this builds on our previous work on a complete quantum dynamic logic for single systems here we extend that work to a sound but not necessarily complete logic for composite systems which brings together ideas from the quantum logic tradition with concepts from dynamic modal logic and from quantum computation this logic of quantum programs lqp is capable of expressing important features of quantum measurements and unitary evolutions of multipartite states as well as giving logical characterisations to various forms of entanglement for example the bell states the ghz states etc we present a finitary syntax a relational semantics and a sound proof system for this logic as applications we use our system to give formal correctness proofs for the teleportation protocol and for a standard quantum secret sharing protocol a whole range of other quantum circuits and programs including other wellknown protocols for example superdense coding entanglement swapping logicgate teleportation etc can be similarly verified using our logic,0
constraint propagation is one of the basic forms of inference in many logicbased reasoning systems in this paper we investigate constraint propagation for firstorder logic fo a suitable language to express a wide variety of constraints we present an algorithm with polynomialtime data complexity for constraint propagation in the context of an fo theory and a finite structure we show that constraint propagation in this manner can be represented by a datalog program and that the algorithm can be executed symbolically ie independently of a structure next we extend the algorithm to foid the extension of fo with inductive definitions finally we discuss several applications,0
while most approaches in formal methods address system correctness ensuring robustness has remained a challenge in this paper we present and study the logic rltl which provides a means to formally reason about both correctness and robustness in system design furthermore we identify a large fragment of rltl for which the verification problem can be efficiently solved ie verification can be done by using an automaton recognizing the behaviors described by the rltl formula varphi of size at most mathcalo left 3 varphi right where varphi is the length of varphi this result improves upon the previously known bound of mathcaloleft5varphi right for rltl verification and is closer to the ltl bound of mathcaloleft 2varphi right the usefulness of this fragment is demonstrated by a number of case studies showing its practical significance in terms of expressiveness the ability to describe robustness and the finegrained information that rltl brings to the process of system verification moreover these advantages come at a low computational overhead with respect to ltl verification,0
in this work we show that both logic programming and abstract argumentation frameworks can be interpreted in terms of nelsons constructive logic n4 we do so by formalizing in this logic two principles that we call noncontradictory inference and strengthened closed world assumption the first states that no belief can be held based on contradictory evidence while the latter forces both unknown and contradictory evidence to be regarded as false using these principles both logic programming and abstract argumentation frameworks are translated into constructive logic in a modular way and using the object language logic programming implication and abstract argumentation supports become in the translation a new implication connective following the noncontradictory inference principle attacks are then represented by combining this new implication with strong negation under consideration in theory and practice of logic programming tplp,0
the present work is devoted to computability logic col the young and volcanic researchproject developed by giorgi japaridze our main goal is to provide the reader with a clear panoramic view of this vast new land starting from its core knots and making our way towards the outer threads in a somewhat threedimensional spacial gait furthermore through the present work we provide a tentative proof for the decidability of one of cols numerous axiomatisations namely cl15 thus our expedition initially takes off for an aerial perusal overview of this fertile steppe the first chapter introduces col in a philosophical fashion exposing and arguing its main key points we then move over to unfold its semantics and syntax profiles allowing the reader to become increasingly more familiar with this new environment landing on to the second chapter we thoroughly introduce cirquent calculus the new deductive system japaridze has developed in order to axiomatise computability logic indeed this new proofsystem can also be a useful tool for many other logics we then review each of the 17 axiomatisations found so far the third chapter zoomsin on cl15 in order to come up with a possible solution to its open problem we outline its soundness and completeness proofs then provide some few deductive examples and finally build a tentative proof of its decidability lastly the fourth chapter focuses on the potential and actual applications of computability logic both in arithmetic clarithmetic and in artificial intelligence systems meaning knowledgebase and planningandaction ones we close our journey with some final remarks on the richness of this framework and hence the researchworthiness it entails,0
the pebbling comonad introduced by abramsky dawar and wang provides a categorical interpretation for the kpebble games from finite model theory the cokleisli category of the pebbling comonad specifies equivalences under different fragments and extensions of infinitary kvariable logic moreover the coalgebras over this pebbling comonad characterise treewidth and correspond to tree decompositions in this paper we introduce the pebblerelation comonad which characterises pathwidth and whose coalgebras correspond to path decompositions we further show that the existence of a cokleisli morphism in this comonad is equivalent to truth preservation in the restricted conjunction fragment of kvariable infinitary logic we do this using dalmaus pebblerelation game and an equivalent allinone pebble game we then provide a similar treatment to the corresponding cokleisli isomorphisms via a bijective version of the allinone pebble game finally we show as a consequence a new lovsztype theorem relating pathwidth to the restricted conjunction fragment of kvariable infinitary logic with counting quantifiers,0
meseguers rewriting logic and the rewriting logic crwl are two wellknown approaches to rewriting as logical deduction that despite some clear similarities were designed with different objectives here we study the relationships between them both at a syntactic and at a semantic level even though it is not possible to establish an entailment system map between them both can be naturally simulated in each other semantically there is no embedding between the corresponding institutions along the way the notions of entailment and satisfaction in meseguers rewriting logic are generalized we also use the syntactic results to prove reflective properties of crwl,0
tabled logic programming is receiving increasing attention in the logic programming community it avoids many of the shortcomings of sld execution and provides a more flexible and often extremely efficient execution mechanism for logic programs in particular tabled execution of logic programs terminates more often than execution based on sldresolution in this article we introduce two notions of universal termination of logic programming with tabling quasitermination and the stronger notion of lgtermination we present sufficient conditions for these two notions of termination namely quasiacceptability and lgacceptability and we show that these conditions are also necessary in case the tabling is wellchosen starting from these conditions we give modular termination proofs ie proofs capable of combining termination proofs of separate programs to obtain termination proofs of combined programs finally in the presence of mode information we state sufficient conditions which form the basis for automatically proving termination in a constraintbased way,0
there are two known general results on the finite model property fmp of commutators bimodal logics with commuting and confluent modalities if l is finitely axiomatisable by modal formulas having universal horn firstorder correspondents then both and are determined by classes of frames that admit filtration and so have the fmp on the negative side if both l and l are determined by transitive frames and have frames of arbitrarily large depth then does not have the fmp in this paper we show that commutators with a weakly connected component often lack the fmp our results imply that the above positive result does not generalise to universally axiomatisable component logics and even commutators without transitive components such as can lack the fmp we also generalise the above negative result to cases where one of the component logics has frames of depth one only such as and the decidable product logic s43xs5 we also show cases when already half of commutativity is enough to force infinite frames,0
in previous work we presented a hierarchy of classical modal systems along with algebraic semantics for the reasoning about intuitionistic truth belief and knowledge deviating from gdels interpretation of ipc in s4 our modal systems contain ipc in the way established in the modal operator can be viewed as a predicate for intuitionistic truth ie proof epistemic principles are partially adopted from intuitionistic epistemic logic iel in the present paper we show that the s5style systems of our hierarchy correspond to an extended brouwerheytingkolmogorov interpretation and are complete wrt a relational semantics based on intuitionistic general frames in this sense our s5style logics are adequate and complete systems for the reasoning about proof combined with belief or knowledge the proposed relational semantics is a uniform framework in which also iel can be modeled verificationbased intuitionistic knowledge formalized in iel turns out to be a special case of the kind of knowledge described by our s5style systems,0
graded modal logics generalise standard modal logics via families of modalities indexed by an algebraic structure whose operations mediate between the different modalities the graded ofcourse modality r captures how many times a proposition is used and has an analogous interpretation to the ofcourse modality from linear logic the ofcourse modality from linear logic can be modelled by a linear exponential comonad and graded ofcourse can be modelled by a graded linear exponential comonad benton showed in his seminal paper on linearnonlinear logic that the ofcourse modality can be split into two modalities connecting intuitionistic logic with linear logic forming a symmetric monoidal adjunction later fujii et al demonstrated that every graded comonad can be decomposed into an adjunction and a strict action we give a similar result to benton leveraging fujii et als decomposition showing that graded modalities can be split into two modalities connecting a graded logic with a graded linear logic we propose a sequent calculus its proof theory and categorical model and a natural deduction system which we show is isomorphic to the sequent calculus system interestingly our system can also be understood as linearnonlinear logic composed with an action that adds the grading further illuminating the shared principles between linear logic and a class of graded modal logics,0
it is standard to regard the intuitionistic restriction of a classical logic as increasing the expressivity of the logic because the classical logic can be adequately represented in the intuitionistic logic by doublenegation while the other direction has no truthpreserving propositional encodings we show here that subexponential logic which is a family of substructural refinements of classical logic each parametric over a preorder over the subexponential connectives does not suffer from this asymmetry if the preorder is systematically modified as part of the encoding precisely we show a bijection between synthetic ie focused partial sequent derivations modulo a given encoding particular instances of our encoding for particular subexponential preorders give rise to both known and novel adequacy theorems for substructural logics,0
we study implicational formulas in the context of proof complexity of intuitionistic propositional logic ipc on the one hand we give an efficient transformation of tautologies to implicational tautologies that preserves the lengths of intuitionistic extended frege ef or substitution frege sf proofs up to a polynomial on the other hand ef proofs in the implicational fragment of ipc polynomially simulate full intuitionistic logic for implicational tautologies the results also apply to other fragments of other superintuitionistic logics under certain conditions in particular the exponential lower bounds on the length of intuitionistic ef proofs by hrube citehrulbint generalized to exponential separation between ef and sf systems in superintuitionistic logics of unbounded branching by jebek citeejsfef can be realized by implicational tautologies,0
proof search has been used to specify a wide range of computation systems in order to build a framework for reasoning about such specifications we make use of a sequent calculus involving induction and coinduction these proof principles are based on a proof theoretic rather than settheoretic notion of definition definitions are akin to logic programs where the left and right rules for defined atoms allow one to view theories as closed or defining fixed points the use of definitions and free equality makes it possible to reason intentionally about syntax we add in a consistent way rules for pre and post fixed points thus allowing the user to reason inductively and coinductively about properties of computational system making full use of higherorder abstract syntax consistency is guaranteed via cutelimination where we give the first to our knowledge cutelimination procedure in the presence of general inductive and coinductive definitions,0
epistemic logic programs elps are an extension of answer set programming asp with epistemic operators that allow for a form of metareasoning that is reasoning over multiple possible worlds existing elp solving approaches generally rely on making multiple calls to an asp solver in order to evaluate the elp however in this paper we show that there also exists a direct translation from elps into nonground asp with bounded arity the resulting asp program can thus be solved in a single shot we then implement this encoding method using recently proposed techniques to handle large nonground asp rules into the prototype elp solving system selp which we present in this paper this solver exhibits competitive performance on a set of elp benchmark instances under consideration in theory and practice of logic programming tplp,0
a model checking computation checks whether a given logical sentence is true in a given finite structure provenance analysis abstracts from such a computation mathematical information on how the result depends on the atomic data that describe the structure in database theory provenance analysis by interpretations in commutative semirings has been rather succesful for positive query languages such a unions of conjunctive queries positive relational algebra or datalog however it did not really offer an adequate treatment of negation or missing information here we propose a new approach for the provenance analysis of logics with negation such as firstorder logic and fixedpoint logics it is closely related to a provenance analysis of the associated modelchecking games and based on new semirings of dualindeterminate polynomials or dualindeterminate formal power series these are obtained by taking quotients of traditional provenance semirings by congruences that are generated by products of positive and negative provenance tokens beyond the use for modelchecking problems in logics provenance analysis of games is of independent interest provenance values in games provide detailed information about the number and properties of the strategies of the players far beyond the question whether or not a player has a winning strategy from a given position,0
we prove strong completeness results for some modal logics with the universal modality with respect to their topological semantics over 0dimensional denseinthemselves metric spaces we also use failure of compactness to show that for some languages and spaces no standard modal deductive system is strongly complete,0
we apply to logic programming some recently emerging ideas from the field of reductionbased communicating systems with the aim of giving evidence of the hidden interactions and the coordination mechanisms that rule the operational machinery of such a programming paradigm the semantic framework we have chosen for presenting our results is tile logic which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results as main contributions we mention the finitary presentation of abstract unification and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming moreover the compositionality of the tile semantics is guaranteed by standard results as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property an extension of the approach for handling constraint systems is also discussed,0
sentences containing definite descriptions expressions of the form the f can be formalised using a binary quantifier  that forms a formula out of two predicates where x is read as the f is g this is an innovation over the usual formalisation of definite descriptions with a term forming operator the present paper compares the two approaches after a brief overview of the system mathbfinf of intuitionist negative free logic extended by such a quantifier which was presented in citepkurbisiotai mathbfinf is first compared to a system of tennants and an axiomatic treatment of a term forming  operator within intuitionist negative free logic both systems are shown to be equivalent to the subsystem of mathbfinf in which the g of x is restricted to identity mathbfinf is then compared to an intuitionist version of a system of lamberts which in addition to the term forming operator has an operator for predicate abstraction for indicating scope distinctions the two systems will be shown to be equivalent through a translation between their respective languages advantages of the present approach over the alternatives are indicated in the discussion,0
we study logical reduction factorization of relations into relations of lower arity by boolean or relative products that come from applying conjunctions and existential quantifiers to predicates ie by primitive positive formulas of predicate calculus our algebraic framework unifies natural joins and data dependencies of database theory and relational algebra of clone theory with the bond algebra of cs peirce we also offer new constructions of reductions systematically study irreducible relations and reductions to them and introduce a new characteristic of relations ternarity that measures their complexity of relating and allows to refine reduction results in particular we refine peirces controversial reduction thesis and show that reducibility behavior is dramatically different on finite and infinite domains,0
we explore the problem of explaining observations starting from a classically inconsistent theory by adopting a paraconsistent framework we consider two expansions of the wellknown belnapdunn paraconsistent fourvalued logic mathsfbd mathsfbdcirc introduces formulas of the form circ the information on  is reliable while mathsfbdtriangle augments the language with triangles there is information that  is true we define and motivate the notions of abduction problems and explanations in mathsfbdcirc and mathsfbdtriangle and show that they are not reducible to one another we analyse the complexity of standard abductive reasoning tasks solution recognition solution existence and relevance necessity of hypotheses in both logics finally we show how to reduce abduction in mathsfbdcirc and mathsfbdtriangle to abduction in classical propositional logic thereby enabling the reuse of existing abductive reasoning procedures,0
the article introduces a ceteris paribus modal logic interpreted on the equivalence classes induced by sets of propositional atoms this logic is used to embed two logics of agency and games namely atemporal stit and the coalition logic of propositional control clpc the embeddings highlight a common ceteris paribus structure underpinning the key modal operators of both logics they clarify the relationship between stit and clpc and enable the transfer of complexity results to the ceteris paribus logic,0
logics of nonsense allow a third truth value to express propositions that are emphnonsense these logics are ideal formalisms to understand how errors are handled in programs and how they propagate throughout the programs once they appear in this paper we give a hintikkan game semantics for logics of nonsense and prove its correctness we also discuss how a known solution method in game theory the iterated elimination of strictly dominated strategies relates to semantic games for logics of nonsense finally we extend the logics of nonsense only by means of semantic games developing a new logic of nonsense and propose a new game semantics for priests logic of paradox,0
we propose a modular method for proving termination of general logic programs ie logic programs with negation it is based on the notion of acceptable programs but it allows us to prove termination in a truly modular way we consider programs consisting of a hierarchy of modules and supply a general result for proving termination by dealing with each module separately for programs which are in a certain sense wellbehaved namely wellmoded or welltyped programs we derive both a simple verification technique and an iterative proof method some examples show how our system allows for greatly simplified proofs,0
we extend answer set semantics to deal with inconsistent programs containing classical negation by finding a best answer set within the context of inconsistent programs it is natural to have a partial order on rules representing a preference for satisfying certain rules possibly at the cost of violating less important ones we show that such a rule order induces a natural order on extended answer sets the minimal elements of which we call preferred answer sets we characterize the expressiveness of the resulting semantics and show that it can simulate negation as failure disjunction and some other formalisms such as logic programs with ordered disjunction the approach is shown to be useful in several application areas eg repairing database where minimal repairs correspond to preferred answer sets to appear in theory and practice of logic programming tplp,0
this paper introduces presto a symbolic partial evaluator for maudes rewriting logic theories that can improve system analysis and verification in presto the automated optimization of a conditional rewrite theory r whose rules define the concurrent transitions of a system is achieved by partially evaluating with respect to the rules of r an underlying companion equational logic theory e that specifies the algebraic structure of the system states of r this can be particularly useful for specializing an overly general equational theory e whose operators may obey complex combinations of associativity commutativity andor identity axioms when being plugged into a host rewrite theory r as happens for instance in protocol analysis where sophisticated equational theories for cryptography are used presto implements different unfolding operators that are based on folding variant narrowing the symbolic engine of maudes equational theories when combined with an appropriate abstraction algorithm they allow the specialization to be adapted to the theory termination behavior and bring significant improvement while ensuring strong correctness and termination of the specialization we demonstrate the effectiveness of presto in several examples of protocol analysis where it achieves a significant speedup actually the transformation provided by presto may cut down an infinite folding variant narrowing space to a finite one and moreover some of the costly algebraic axioms and rule conditions may be eliminated as well as far as we know this is the first partial evaluator for maude that respects the semantics of functional logic concurrent and objectoriented computations under consideration in theory and practice of logic programming tplp,0
in this paper we present tableau proof systems for various justification logics we show that the tableau systems are sound and complete with respect to mkrtychev models in order to prove the completeness of the tableaux we give a syntactic proof of cut elimination we also show the subformula property for our tableaux,0
firstorder multiplicative intuitionistic linear logic mill1 can be seen as an extension of the lambek calculus in addition to the fragment of mill1 which corresponds to the lambek calculus of moot piazza 2001 i will show fragments of mill1 which generate the multiple contextfree languages and which correspond to the displacement calculus of morrilll ea,0
formal reasoning about finite sets and cardinality is an important tool for many applications including software verification where very often one needs to reason about the size of a given data structure and not only about what its elements are the constraint logic programming tool log provides a decision procedure for deciding the satisfiability of formulas involving very general forms of finite sets without cardinality in this paper we adapt and integrate a decision procedure for a theory of finite sets with cardinality into log the proposed solver is proved to be a decision procedure for its formulas besides the new clp instance is implemented as part of the log tool in turn the implementation uses howe and kings prolog sat solver and prologs clpq library as an integer linear programming solver the empirical evaluation of this implementation based on 250 real verification conditions shows that it can be useful in practice,0
it follows from the famous fagins theorem that all problems in np are expressible in existential secondorder logic eso and vice versa indeed there are wellknown eso characterizations of npcomplete problems such as 3colorability hamiltonicity and clique furthermore the eso sentences that characterize those problems are simple and elegant however there are also np problems that do not seem to possess equally simple and elegant eso characterizations in this work we are mainly interested in this latter class of problems in particular we characterize in secondorder logic the class of hypercube graphs and the classes satqbfk of satisfiable quantified boolean formulae with k alternations of quantifiers we also provide detailed descriptions of the strategies followed to obtain the corresponding nontrivial secondorder sentences finally we sketch a thirdorder logic sentence that defines the class satqbf bigcupk geq 1 satqbfk the subformulae used in the construction of these complex second and thirdorder logic sentences are good candidates to form part of a library of formulae same as libraries of frequently used functions simplify the writing of complex computer programs a library of formulae could potentially simplify the writing of complex second and thirdorder queries minimizing the probability of error,0
the paper analyzes dynamic epistemic logic from a topological perspective the main contribution consists of a framework in which dynamic epistemic logic satisfies the requirements for being a topological dynamical system thus interfacing discrete dynamic logics with continuous mappings of dynamical systems the setting is based on a notion of logical convergence demonstratively equivalent with convergence in stone topology presented is a flexible parametrized family of metrics inducing the latter used as an analytical aid we show maps induced by action model transformations continuous with respect to the stone topology and present results on the recurrent behavior of said maps,0
a prototype system is described whose core functionality is based on propositional logic the elimination of secondorder operators such as boolean quantifiers and operators for projection forgetting and circumscription this approach allows to express many representational and computational tasks in knowledge representation for example computation of abductive explanations and models with respect to logic programming semantics in a uniform operational system backed by a uniform classical semantic framework,0
reasoning on defeasible knowledge is a topic of interest in the area of description logics as it is related to the need of representing exceptional instances in knowledge bases in this direction in our previous works we presented a framework for representing contextualized owl rl knowledge bases with a notion of justified exceptions on defeasible axioms reasoning in such framework is realized by a translation into asp programs the resulting reasoning process for owl rl however introduces a complex encoding in order to capture reasoning on the negative information needed for reasoning on exceptions in this paper we apply the justified exception approach to knowledge bases in textitdllitecal r ie the language underlying owl ql we provide a definition for textitdllitecal r knowledge bases with defeasible axioms and study their semantic and computational properties in particular we study the effects of exceptions over unnamed individuals the limited form of textitdllitecal r axioms allows us to formulate a simpler asp encoding where reasoning on negative information is managed by direct rules the resulting materialization method gives rise to a complete reasoning procedure for instance checking in textitdllitecal r with defeasible axioms under consideration in theory and practice of logic programming tplp,0
in this paper we delve into notation3 logic n3 an extension of rdf which empowers users to craft rules introducing fresh blank nodes to rdf graphs this capability is pivotal in various applications such as ontology mapping given the ubiquitous presence of blank nodes directly or in auxiliary constructs across the web however the availability of fast n3 reasoners fully supporting blank node introduction remains limited conversely engines like vlog or nemo though not explicitly designed for semantic web rule formats cater to analogous constructs namely existential rules we investigate the correlation between n3 rules featuring blank nodes in their heads and existential rules we pinpoint a subset of n3 that seamlessly translates to existential rules and establish a mapping preserving the equivalence of n3 formulae to showcase the potential benefits of this translation in n3 reasoning we implement this mapping and compare the performance of n3 reasoners like eye and cwm against vlog and nemo both on native n3 rules and their translated counterparts our findings reveal that existential rule reasoners excel in scenarios with abundant facts while the eye reasoner demonstrates exceptional speed in managing a high volume of dependent rules additionally to the original conference version of this paper we include all proofs of the theorems and introduce a new section dedicated to n3 lists featuring builtin functions and how they are implemented in existential rules adding lists to our translationframework gives interesting insights on related design decisions influencing the standardization of n3,0
we investigate a recent proposal for modal hypersequent calculi the interpretation of relational hypersequents incorporates an accessibility relation along the hypersequent these systems give the same interpretation of hypersequents as lellmans linear nested sequents but were developed independently by restall for s5 and extended to other normal modal logics by parisi the resulting systems obey dosens principle the modal rules are the same across different modal logics different modal systems only differ in the presence or absence of external structural rules with the exception of s5 the systems are modular in the sense that different structural rules capture different properties of the accessibility relation we provide the first direct semantical cutfree completeness proofs for k t and d and show how this method fails in the case of b and s4,0
this paper makes a first step towards a logic of learning from experiments for this we investigate formal frameworks for modeling the interaction of causal and qualitative epistemic reasoning crucial for our approach is the idea that the notion of an intervention can be used as a formal expression of a real or hypothetical experiment in a first step we extend the wellknown causal models with a simple hintikkastyle representation of the epistemic state of an agent in the resulting setting one can talk not only about the knowledge of an agent about the values of variables and how interventions affect them but also about knowledge update the resulting logic can model reasoning about thought experiments however it is unable to account for learning from experiments which is clearly brought out by the fact that it validates the no learning principle for interventions therefore in a second step we implement a more complex notion of knowledge that allows an agent to observe measure certain variables when an experiment is carried out this extended system does allow for learning from experiments for all the proposed logical systems we provide a sound and complete axiomatization,0
we investigate the extension of monadic second order logic interpreted over infinite words and trees with generalized for almost all quantifiers interpreted using the notions of baire category and lebesgue measure,0
we consider the lambek calculus or noncommutative multiplicative intuitionistic linear logic extended with iteration or kleene star axiomatised by means of an rule and prove that the derivability problem in this calculus is 10hard this solves a problem left open by buszkowski 2007 who obtained the same complexity bound for infinitary action logic which additionally includes additive conjunction and disjunction as a byproduct we prove that any contextfree language without the empty word can be generated by a lambek grammar with unique type assignment without lambeks nonemptiness restriction imposed cf safiullin 2007,0
typepreserving or typed compilation uses typing derivations to certify correctness properties of compilation we have designed and implemented a typepreserving compiler for a simplytyped dialect of prolog we call tprolog the crux of our approach is a new certifying abstract machine which we call the typed warren abstract machine twam the twam has a dependent type system strong enough to specify the semantics of a logic program in the logical framework lf we present a soundness metatheorem which constitutes a partial correctness guarantee welltyped programs implement the logic program specified by their type this metatheorem justifies our design and implementation of a certifying compiler from tprolog to twam,0
suitable extensions of the monadic secondorder theory of k successors have been proposed in the literature to capture the notion of time granularity in this paper we provide the monadic secondorder theories of downward unbounded layered structures which are infinitely refinable structures consisting of a coarsest domain and an infinite number of finer and finer domains and of upward unbounded layered structures which consist of a finest domain and an infinite number of coarser and coarser domains with expressively complete and elementarily decidable temporal logic counterparts we obtain such a result in two steps first we define a new class of combined automata called temporalized automata which can be proved to be the automatatheoretic counterpart of temporalized logics and show that relevant properties such as closure under boolean operations decidability and expressive equivalence with respect to temporal logics transfer from component automata to temporalized ones then we exploit the correspondence between temporalized logics and automata to reduce the task of finding the temporal logic counterparts of the given theories of time granularity to the easier one of finding temporalized automata counterparts of them,0
this paper studies the stable model semantics of logic programs with abstract constraint atoms and their properties we introduce a succinct abstract representation of these constraint atoms in which a constraint atom is represented compactly we show two applications first under this representation of constraint atoms we generalize the gelfondlifschitz transformation and apply it to define stable models also called answer sets for logic programs with arbitrary constraint atoms the resulting semantics turns out to coincide with the one defined by son et al which is based on a fixpoint approach one advantage of our approach is that it can be applied in a natural way to define stable models for disjunctive logic programs with constraint atoms which may appear in the disjunctive head as well as in the body of a rule as a result our approach to the stable model semantics for logic programs with constraint atoms generalizes a number of previous approaches second we show that our abstract representation of constraint atoms provides a means to characterize dependencies of atoms in a program with constraint atoms so that some standard characterizations and properties relying on these dependencies in the past for logic programs with ordinary atoms can be extended to logic programs with constraint atoms,0
a propositional logic program p may be identified with a pfpfcoalgebra on the set of atomic propositions in the program the corresponding cpfpfcoalgebra where cpfpf is the cofree comonad on pfpf describes derivations by resolution that correspondence has been developed to model firstorder programs in two ways with lax semantics and saturated semantics based on locally ordered categories and right kan extensions respectively we unify the two approaches exhibiting them as complementary rather than competing reflecting the theoremproving and proofsearch aspects of logic programming while maintaining that unity we further refine lax semantics to give finitary models of logic programs with existential variables and to develop a precise semantic relationship between variables in logic programming and worlds in local state,0
we discuss four common mistakes in the teaching and textbooks of modal logic the first one is missing the axiom diamondvarphileftrightarrownegboxnegvarphi when choosing diamond as the primitive modal operator misunderstanding that box and diamond are symmetric the second one is forgetting to make the set of formulas for filtration closed under subformulas when proving the finite model property through filtration neglecting that boxvarphi and diamondvarphi may be abbreviations of formulas the third one is giving wrong definitions of canonical relations in minimal canonical models that are unmatched with the primitive modal operators the final one is misunderstanding the rule of necessitation without knowing its distinction from the rule of modus ponens to better understand the rule of necessitation we summarize six ways of defining deductive consequence in modal logic omitted definition classical definition ternary definition reduced definition bounded definition and deflationary definition and show that the last three definitions are equivalent to each other,0
this paper presents a way of formalising definite descriptions with a binary quantifier  where x is read as the f is g introduction and elimination rules for  in a system of intuitionist negative free logic are formulated procedures for removing maximal formulas of the form x are given and it is shown that deductions in the system can be brought into normal form,0
ehrenfeuchtfrass ef games are a basic tool in finite model theory for proving definability lower bounds with many applications in complexity theory and related areas they have been applied to study various logics giving insights on quantifier rank and other logical complexity measures in this paper we present an ef game to capture formula size in counting logic with a bounded number of variables the game combines games introduced previously for counting logic quantifier rank due to immerman and lander and for firstorder formula size due to adler and immerman and hella and vnnen the game is used to prove the main result of the paper an extension of a formula size lower bound of grohe and schweikardt for distinguishing linear orders from 3variable firstorder logic to 3variable counting logic as far as we know this is the first formula size lower bound for counting logic,0
standpoint logic is a recently proposed formalism in the context of knowledge integration which advocates a multiperspective approach permitting reasoning with a selection of diverse and possibly conflicting standpoints rather than forcing their unification in this paper we introduce nested sequent calculi for propositional standpoint logicsproof systems that manipulate trees whose nodes are multisets of formulaeand show how to automate standpoint reasoning by means of nondeterministic proofsearch algorithms to obtain worstcase complexityoptimal proofsearch we introduce a novel technique in the context of nested sequents referred to as coloring which consists of taking a formula as input guessing a certain coloring of its subformulae and then running proofsearch in a nested sequent calculus on the colored input our technique lets us decide the validity of standpoint formulae in conp since proofsearch only produces a partial proof relative to each permitted coloring of the input we show how all partial proofs can be fused together to construct a complete proof when the input is valid and how certain partial proofs can be transformed into a countermodel when the input is invalid these certificates ie proofs and countermodels serve as explanations of the invalidity of the input,0
propositional dynamic logic or pdl was invented as a logic for reasoning about regular programming constructs we propose a new perspective on pdl as a multiagent strategic logic masl this logic for strategic reasoning has group strategies as first class citizens and brings game logic closer to standard modal logic we demonstrate that masl can express key notions of game theory social choice theory and voting theory in a natural way we give a sound and complete proof system for masl and we show that masl encodes coalition logic next we extend the language to epistemic multiagent strategic logic emasl we give examples of what it can express we propose to use it for posing new questions in epistemic social choice theory and we give a calculus for reasoning about a natural class of epistemic game models we end by listing avenues for future research and by tracing connections to a number of other logics for reasoning about strategies,0
we present a theory of parameterized dynamic logic namely dlp for specifying and reasoning about a rich set of program models based on their transitional behaviours different from most dynamic logics that deal with regular expressions or a particular type of formalisms dlp introduces a type of labels called program configurations as explicit program status for symbolic executions allowing programs and formulas to be of arbitrary forms according to interested domains this characteristic empowers dynamic logical formulas with a direct support of symbolicexecutionbased reasoning while still maintaining reasoning based on syntactic structures in traditional dynamic logics through a rulelifting process we propose a proof system and build a cyclic preproof structure special for dlp which guarantees the soundness of infinite proof trees induced by symbolically executing programs with explicitimplicit loop structures the soundness of dlp is formally analyzed and proved dlp provides a flexible verification framework based on the theories of dynamic logics it helps reduce the burden of developing different dynamiclogic theories for different programs and save the additional transformations in the derivations of noncompositional programs we give some examples of instantiations of dlp in particular domains showing the potential and advantages of using dlp in practical usage,0
in this chapter we present an approach using formal methods to synthesize reactive defense strategy in a cyber network equipped with a set of decoy systems we first generalize formal graphical security modelsattack graphsto incorporate defenders countermeasures in a gametheoretic model called an attackdefend game on graph this game captures the dynamic interactions between the defender and the attacker and their defenseattack objectives in formal logic then we introduce a class of hypergames to model asymmetric information created by decoys in the attackerdefender interactions given qualitative security specifications in formal logic we show that the solution concepts from hypergames and reactive synthesis in formal methods can be extended to synthesize effective dynamic defense strategy using cyber deception the strategy takes the advantages of the misperception of the attacker to ensure security specification is satisfied which may not be satisfiable when the information is symmetric,0
log read setlog was born as a constraint logic programming clp language where sets and binary relations are firstclass citizens thus fostering set programming internally log is a constraint satisfiability solver implementing decision procedures for several fragments of set theory hence log can be used as a declarative set logic programming language and as an automated theorem prover for set theory over time log has been extended with some components integrated to the satisfiability solver thus providing a formal verification environment in this paper we make a comprehensive presentation of this environment which includes a language for the description of state machines based on set theory an interactive environment for the execution of functional scenarios over state machines a generator of verification conditions for state machines automated verification of state machines and test case generation state machines are both programs and specifications exactly the same code works as a program and as its specification in this way with a few additions a clp language turned into a seamlessly integrated programming and automated proof system,0
dealing with context dependent knowledge has led to different formalizations of the notion of context among them is the contextualized knowledge repository ckr framework which is rooted in description logics but links on the reasoning side strongly to logic programs and answer set programming asp in particular the ckr framework caters for reasoning with defeasible axioms and exceptions in contexts which was extended to knowledge inheritance across contexts in a coverage specificity hierarchy however the approach supports only this single type of contextual relation and the reasoning procedures work only for restricted hierarchies due to nontrivial issues with model preference under exceptions in this paper we overcome these limitations and present a generalization of ckr hierarchies to multiple contextual relations along with their interpretation of defeasible axioms and preference to support reasoning we use asp with algebraic measures which is a recent extension of asp with weighted formulas over semirings that allows one to associate quantities with interpretations depending on the truth values of propositional atoms notably we show that for a relevant fragment of ckr hierarchies with multiple contextual relations query answering can be realized with the popular asprin framework the algebraic measures approach is more powerful and enables eg reasoning with epistemic queries over ckrs which opens interesting perspectives for the use of quantitative asp extensions in other applications,0
parikhs game logic is a pdllike fixpoint logic interpreted on monotone neighbourhood frames that represent the strategic power of players in determined twoplayer games game logic translates into a fragment of the monotone calculus which in turn is expressively equivalent to monotone modal automata parity games and automata are important tools for dealing with the combinatorial complexity of nested fixpoints in modal fixpoint logics such as the modal calculus in this paper we 1 discuss the semantics a of game logic over neighbourhood structures in terms of parity games and 2 use these games to obtain an automatatheoretic characterisation of the fragment of the monotone calculus that corresponds to game logic our proof makes extensive use of structures that we call syntax graphs that combine the easeofuse of syntax trees of formulas with the flexibility and succinctness of automata they are essentially a graphbased view of the alternating tree automata that were introduced by wilke in the study of modal calculus,0
this paper presents a logic language for expressing np search and optimization problems specifically first a language obtained by extending positive datalog with intuitive and efficient constructs namely stratified negation constraints and exclusive disjunction is introduced next a further restricted language only using a restricted form of disjunction to define nondeterministically subsets or partitions of relations is investigated this language called np datalog captures the power of datalog with unstratified negation in expressing search and optimization problems a system prototype implementing np datalog is presented the system translates np datalog queries into opl programs which are executed by the ilog opl development studio our proposal combines easy formulation of problems expressed by means of a declarative logic language with the efficiency of the ilog system several experiments show the effectiveness of this approach,0
this is a reflection on the authors experience in teaching logic at the graduate level in a computer science department the main lesson is that model building and the process of modelling must be placed at the centre stage of logic teaching furthermore effective use must be supported with adequate tools finally logic is the methodology underlying many applications it is hence paramount to pass on its principles methods and concepts to computer science audiences,0
we extend firstorder logic with counting by a new operator that allows it to formalise a limited form of recursion which can be evaluated in logarithmic space the resulting logic lrec has a data complexity in logspace and it defines logspacecomplete problems like deterministic reachability and boolean formula evaluation we prove that lrec is strictly more expressive than deterministic transitive closure logic with counting and incomparable in expressive power with symmetric transitive closure logic stc and transitive closure logic with or without counting lrec is strictly contained in fixedpoint logic with counting fpc we also study an extension lrec of lrec that has nicer closure properties and is more expressive than both lrec and stc but is still contained in fpc and has a data complexity in logspace our main results are that lrec captures logspace on the class of directed trees and that lrec captures logspace on the class of interval graphs,0
we address the problem of compiling defeasible theories to datalogneg programs we prove the correctness of this compilation for the defeasible logic dlpartial but the techniques we use apply to many other defeasible logics structural properties of dlpartial are identified that support efficient implementation andor approximation of the conclusions of defeasible theories in the logic compared with other defeasible logics we also use previously wellstudied structural properties of logic programs to adapt to incomplete datalogneg implementations,0
we present a heuristic framework for attacking the undecidable termination problem of logic programs as an alternative to current terminationnontermination proof approaches we introduce an idea of termination prediction which predicts termination of a logic program in case that neither a termination nor a nontermination proof is applicable we establish a necessary and sufficient characterization of infinite generalized sldnfderivations with arbitrary concrete or moded queries and develop an algorithm that predicts termination of general logic programs with arbitrary nonfloundering queries we have implemented a termination prediction tool and obtained quite satisfactory experimental results except for five programs which break the experiment time limit our prediction is 100 correct for all 296 benchmark programs of the termination competition 2007 of which eighteen programs cannot be proved by any of the existing stateoftheart analyzers like aprove07 nti polytool and talp,0
we present a family of minimal modal logics namely modal logics based on minimal propositional logic corresponding each to a different classical modal logic the minimal modal logics are defined based on their classical counterparts in two distinct ways 1 via embedding into fusions of classical modal logics through a natural extension of the gdeljohansson translation of minimal logic into modal logic s4 2 via extension to modal logics of the multi vs singlesuccedent correspondence of sequent calculi for classical and minimal logic we show that despite being mutually independent the two methods turn out to be equivalent for a wide class of modal systems moreover we compare the resulting minimal version of k with the constructive modal logic ck studied in the literature displaying tight relations among the two systems based on these relations we also define a constructive correspondent for each minimal system thus obtaining a family of constructive modal logics which includes ck as well as other constructive modal logics studied in the literature,0
we work primarily with the kripke frame consisting of twodimensional minkowski spacetime with the irreflexive accessibility relation can reach with a slowerthanlight signal we show that in the basic temporal language the set of validities over this frame is decidable we then refine this to pspacecomplete in both cases the same result for the corresponding reflexive frame follows immediately with a little more work we obtain pspacecompleteness for the validities of the halpernshoham logic of intervals on the real line with two different combinations of modalities,0
constructorbased conditional rewriting logic is a general framework for integrating firstorder functional and logic programming which gives an algebraic semantics for nondeterministic functionallogic programs in the context of this formalism we introduce a simple notion of program module as an open program which can be extended together with several mechanisms to combine them these mechanisms are based on a reduced set of operations however the high expressiveness of these operations enable us to model typical constructs for program modularization like hiding exportimport genericityinstantiation and inheritance in a simple way we also deal with the semantic aspects of the proposal by introducing an immediate consequence operator and studying several alternative semantics for a program module based on this operator in the line of logic programming the operator itself its least fixpoint the least model of the module the set of its prefixpoints term models of the module and some other variations in order to find a compositional and fully abstract semantics wrt the set of operations and a natural notion of observability,0
fixedpoint logic with rank fpr is an extension of fixedpoint logic with counting fpc with operators for computing the rank of a matrix over a finite field the expressive power of fpr properly extends that of fpc and is contained in ptime but not known to be properly contained we give a circuit characterization for fpr in terms of families of symmetric circuits with rank gates along the lines of that for fpc given by this requires the development of a broad framework of circuits in which the individual gates compute functions that are not symmetric ie invariant under all permutations of their inputs in the case of fpc the proof of equivalence of circuits and logic rests heavily on the assumption that individual gates compute such symmetric functions and so novel techniques are required to make this work for fpr,0
partial correctness of imperative or functional programming divides in logic programming into two notions correctness means that all answers of the program are compatible with the specification completeness means that the program produces all the answers required by the specifications we also consider semicompleteness completeness for those queries for which the program does not diverge this paper presents an approach to systematically construct provably correct and semicomplete logic programs for a given specification normal programs are considered under kunens 3valued completion semantics of negation as finite failure and the wellfounded semantics of negation as possibly infinite failure the approach is declarative it abstracts from details of operational semantics like eg the form of the selected literals procedure calls during the computation the proposed method is simple and can be used maybe informally in actual everyday programming,0
we present a collection of modular open source c libraries for the development of logic synthesis applications these libraries can be used to develop applications for the design of classical and emerging technologies as well as for the implementation of quantum compilers all libraries are well documented and well tested furthermore being headeronly the libraries can be readily used as core components in complex logic synthesis systems,0
we present a pspace algorithm that decides satisfiability of the graded modal logic grkra natural extension of propositional modal logic kr by counting expressionswhich plays an important role in the area of knowledge representation the algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem thus we exactly fix the complexity of the problem and refute an exptimehardness conjecture we extend the results to the logic grkr cap i which augments grkr with inverse relations and intersection of accessibility relations this establishes a kind of theoretical benchmark that all algorithmic approaches can be measured against,0
this article introduces differential hybrid games which combine differential games with hybrid games in both kinds of games two players interact with continuous dynamics the difference is that hybrid games also provide all the features of hybrid systems and discrete games but only deterministic differential equations differential games instead provide differential equations with continuoustime game input by both players but not the luxury of hybrid games such as mode switches and discretetime or alternating adversarial interaction this article augments differential game logic with modalities for the combined dynamics of differential hybrid games it shows how hybrid games subsume differential games and introduces differential game invariants and differential game variants for proving properties of differential games inductively,0
clph is an instantiation of the general constraint logic programming scheme with the constraint domain of hedges hedges are finite sequences of unranked terms built over variadic function symbols and three kinds of variables for terms for hedges and for function symbols constraints involve equations between unranked terms and atoms for regular hedge language membership we study algebraic semantics of clph programs define a sound terminating and incomplete constraint solver investigate two fragments of constraints for which the solver returns a complete set of solutions and describe classes of programs that generate such constraints,0
logic programs with ordered disjunction lpods extend classical logic programs with the capability of expressing alternatives with decreasing degrees of preference in the heads of program rules despite the fact that the operational meaning of ordered disjunction is clear there exists an important open issue regarding its semantics in particular there does not exist a purely modeltheoretic approach for determining the most preferred models of an lpod at present the selection of the most preferred models is performed using a technique that is not based exclusively on the models of the program and in certain cases produces counterintuitive results we provide a novel modeltheoretic semantics for lpods which uses an additional truth value in order to identify the most preferred models of a program we demonstrate that the proposed approach overcomes the shortcomings of the traditional semantics of lpods moreover the new approach can be used to define the semantics of a natural class of logic programs that can have both ordered and classical disjunctions in the heads of clauses this allows programs that can express not only strict levels of preferences but also alternatives that are equally preferred this work is under consideration for acceptance in tplp,0
recent years have witnessed the success of deep neural networks in many research areas the fundamental idea behind the design of most neural networks is to learn similarity patterns from data for prediction and inference which lacks the ability of cognitive reasoning however the concrete ability of reasoning is critical to many theoretical and practical problems on the other hand traditional symbolic reasoning methods do well in making logical inference but they are mostly hard rulebased reasoning which limits their generalization ability to different tasks since difference tasks may require different rules both reasoning and generalization ability are important for prediction tasks such as recommender systems where reasoning provides strong connection between user history and target items for accurate prediction and generalization helps the model to draw a robust user portrait over noisy inputs in this paper we propose logicintegrated neural network linn to integrate the power of deep learning and logic reasoning linn is a dynamic neural architecture that builds the computational graph according to input logical expressions it learns basic logical operations such as and or not as neural modules and conducts propositional logical reasoning through the network for inference experiments on theoretical task show that linn achieves significant performance on solving logical equations and variables furthermore we test our approach on the practical task of recommendation by formulating the task into a logical inference problem experiments show that linn significantly outperforms stateoftheart recommendation models in topk recommendation which verifies the potential of linn in practice,0
the importance of transformations and normal forms in logic programming and generally in computer science is well documented this paper investigates transformations and normal forms in the context of defeasible logic a simple but efficient formalism for nonmonotonic reasoning based on rules and priorities the transformations described in this paper have two main benefits on one hand they can be used as a theoretical tool that leads to a deeper understanding of the formalism and on the other hand they have been used in the development of an efficient implementation of defeasible logic,0
linear logic programming uses provability as the basis for computation in the operational semantics based on provability executing the additiveconjunctive goal g1 g2 from a program p simply terminates with a success if both g1 and g2 are solvable from p this is an unsatisfactory situation as a central action of the action of choosing either g1 or g2 by the user is missing in this semantics we propose to modify the operational semantics above to allow for more active participation from the user we illustrate our idea via muprolog an extension of prolog with additive goals,0
we study the international standard xacml 30 for describing security access control policy in a compositional way our main contribution is to derive a logic that precisely captures the idea behind the standard and to formally define the semantics of the policy combining algorithms of xacml to guard against modelling artefacts we provide an alternative way of characterizing the policy combining algorithms and we formally prove the equivalence of these approaches this allows us to pinpoint the shortcoming of previous approaches to formalization based either on belnap logic or on dalgebra,0
existing refinement calculi provide frameworks for the stepwise development of imperative programs from specifications this paper presents a refinement calculus for deriving logic programs the calculus contains a widespectrum logic programming language including executable constructs such as sequential conjunction disjunction and existential quantification as well as specification constructs such as general predicates assumptions and universal quantification a declarative semantics is defined for this widespectrum language based on executions executions are partial functions from states to states where a state is represented as a set of bindings the semantics is used to define the meaning of programs and specifications including parameters and recursion to complete the calculus a notion of correctnesspreserving refinement over programs in the widespectrum language is defined and refinement laws for developing programs are introduced the refinement calculus is illustrated using example derivations and prototype tool support is discussed,0
logic programming is a turing complete language as a consequence designing algorithms that decide termination and nontermination of programs or decide inductivecoinductive soundness of formulae is a challenging task for example the existing stateoftheart algorithms can only semidecide coinductive soundness of queries in logic programming for regular formulae another less famous but equally fundamental and important undecidable property is productivity if a derivation is infinite and coinductively sound we may ask whether the computed answer it determines actually computes an infinite formula if it does the infinite computation is productive this intuition was first expressed under the name of computations at infinity in the 80s in modern days of the internet and stream processing its importance lies in connection to infinite data structure processing recently an algorithm was presented that semidecides a weaker property of productivity of logic programs a logic program is productive if it can give rise to productive derivations in this paper we strengthen these recent results we propose a method that semidecides productivity of individual derivations for regular formulae thus we at last give an algorithmic counterpart to the notion of productivity of derivations in logic programming this is the first algorithmic solution to the problem since it was raised more than 30 years ago we also present an implementation of this algorithm,0
the need for diverse chromosomal modifications in biotechnology synthetic biology and basic research requires the development of new technologies with crispr swapndrop we extend the limits of genome editing to largescale invivo dna transfer between bacterial species its modular platform approach facilitates species specific adaptation to confer genome editing in various species in this study we show the implementation of the crispr swapndrop concept for the model organism escherichia coli and the currently fastest growing and biotechnologically relevant organism vibrio natriegens we demonstrate the excision transfer and integration of 151kb chromosomal dna between e coli strains and from e coli to v natriegens without sizelimiting intermediate dna extraction with the transfer of the e coli mg1655 wild type lac operon we establish a functional lactose and galactose degradation pathway in v natriegens to extend its biotechnological spectrum we also transfer the e coli dh5alpha lac operon and make v natriegens capable of alphacomplementation a step towards an ultrafast cloning strain furthermore crispr swapndrop is designed to be the swiss army knife of genome engineering its spectrum of application comprises scarless markerfree iterative and parallel insertions and deletions genome rearrangements as well as gene transfer between strains and across species the modular character facilitates dna library applications and the recycling of standardized parts its novel multicolor scarless coselection system significantly improves editing efficiency to 92 for single edits and 83 for quadruple edits and provides visual quality controls throughout the assembly and editing process,0
gene and rna editing methods technologies and applications are emerging as innovative forms of therapy and medicine offering more efficient implementation compared to traditional pharmaceutical treatments current trends emphasize the urgent need for advanced methods and technologies to detect public health threats including diseases and viral agents gene and rna editing techniques enhance the ability to identify modify and ameliorate the effects of genetic diseases disorders and disabilities viral detection and identification methods present numerous opportunities for enabling technologies such as crispr applicable to both rna and gene editing through the use of specific cas proteins this article explores the distinctions and benefits of rna and gene editing processes emphasizing their contributions to the future of medical treatment crispr technology particularly its adaptation via the cas13 protein for rna editing is a significant advancement in gene editing the article will delve into rna and gene editing methodologies focusing on techniques that alter and modify genetic coding atoi and ctou editing are currently the most predominant methods of rna modification crispr stands out as the most costeffective and customizable technology for both rna and gene editing unlike permanent changes induced by cutting an individuals dna genetic code rna editing offers temporary modifications by altering nucleoside bases in rna strands which can then attach to dna strands as temporary modifiers,0
type i crisprcas systems are the most common among six types of crisprcas systems however nonselftargeting genome editing based on a single cas3 of type i crisprcas systems has not been reported here we present the subtype ibsvi crisprcas system with three confirmed crisprs and a cas gene cluster and genome editing based on this system found in streptomyces virginiae ibl14 importantly like the animalderived bacterial protein spcas9 1368 aminoacids the single compact nonanimalderived bacterial protein svicas3 771 aminoacids can also direct templatebased microbial genome editing through the target cells own homologydirected repair system which breaks the view that the genome editing based on type i crisprcas systems requires a full cascade notably no offtarget changes or indelformation were detected in the analysis of potential offtarget sites this discovery broadens our understanding of the diversity of type i crisprcas systems and will facilitate new developments in genome editing tools,0
crisprcas systems are an adaptive immunity that protects prokaryotes against foreign genetic elements genetic templates acquired during past infection events enable dnainteracting enzymes to recognize foreign dna for destruction due to the programmability and specificity of these genetic templates crisprcas systems are potential alternative antibiotics that can be engineered to selftarget antimicrobial resistance genes on the chromosome or plasmid however several fundamental questions remain to repurpose these tools against drugresistant bacteria for endogenous crisprcas selftargeting antimicrobial resistance genes and functional crisprcas systems have to cooccur in the target cell furthermore these tools have to outplay dna repair pathways that respond to the nuclease activities of cas proteins even for exogenous crisprcas delivery here we conduct a comprehensive survey of crisprcas genomes first we address the cooccurrence of crisprcas systems and antimicrobial resistance genes in the crisprcas genomes we show that the average number of these genes varies greatly by the crisprcas type and some crisprcas types ie and iiia have over 20 genes per genome next we investigate the dna repair pathways of these crisprcas genomes revealing that the diversity and frequency of these pathways differ by the crisprcas type the interplay between crisprcas systems and dna repair pathways is essential for the acquisition of new spacers in crispr arrays we conduct simulation studies to demonstrate that the efficiency of these dna repair pathways may be inferred from the timeseries patterns in the rna structure of crispr repeats this bioinformatic survey of crisprcas genomes elucidates the necessity to consider multifaceted interactions between different genes and systems to design effective crisprbased antimicrobials,0
clustered regularly interspaced short palindromic repeats crispr is a gene editing technology that has revolutionized the fields of biology and medicine however one of the challenges of using crispr is predicting the ontarget efficacy and offtarget sensitivity of singleguide rnas sgrnas this is because most existing methods are trained on separate datasets with different genes and cells which limits their generalizability in this paper we propose a novel ensemble learning method for sgrna design that is accurate and generalizable our method combines the predictions of multiple machine learning models to produce a single more robust prediction this approach allows us to learn from a wider range of data which improves the generalizability of our model we evaluated our method on a benchmark dataset of sgrna designs and found that it outperformed existing methods in terms of both accuracy and generalizability our results suggest that our method can be used to design sgrnas with high sensitivity and specificity even for new genes or cells this could have important implications for the clinical use of crispr as it would allow researchers to design more effective and safer treatments for a variety of diseases,0
rnaguided gene editing based on the crisprcas system is currently the most effective genome editing technique here we report that the svicas3 from the subtype ibsvi cas system in streptomyces virginiae ibl14 is an rnaguided and dnaguided dna endonuclease suitable for the hdrdirected gene andor base editing of eukaryotic cell genomes the genome editing efficiency of svicas3 guided by dna is no less than that of svicas3 guided by rna in particular tdna as a template and a guide does not require a protospaceradjacent motif demonstrating that crispr as the basis for crrna design is not required for the svicas3mediated gene and base editing this discovery will broaden our understanding of enzyme diversity in crisprcas systems will provide important tools for the creation and modification of living things and the treatment of human genetic diseases and will usher in a new era of dnaguided gene editing and base editing,0
we developed pgmap an analysis pipeline to map grna sequencing reads from dualtargeting crispr screens pgmap output includes a dual grna read counts table and quality control metrics including the proportion of correctlypaired reads and crispr library sequencing coverage across all time points and samples pgmap is implemented using snakemake and is available opensource under the mit license at,0
multiplex and multidirectional control of metabolic pathways is crucial for metabolic engineering to improve product yield of fuels chemicals and pharmaceuticals to achieve this goal artificial transcriptional regulators such as crisprbased transcription regulators have been developed to specifically activate or repress genes of interest here we found that by deploying guide rnas to target on dna sites at different locations of genetic cassettes we could use just one synthetic crisprbased transcriptional regulator to simultaneously activate and repress gene expressions by using the pairwise datasets of guide rnas and gene expressions we developed a datadriven predictive model to rationally design this system for finetuning expression of target genes we demonstrated that this system could achieve programmable control of metabolic fluxes when using yeast to produce versatile chemicals we anticipate that this master crisprbased transcription regulator will be a valuable addition to the synthetic biology toolkit for metabolic engineering speeding up the designbuildtest cycle in industrial biomanufacturing as well as generating new biological insights on the fates of eukaryotic cells,0
highdimensional singlecell data poses significant challenges in identifying underlying biological patterns due to the complexity and heterogeneity of cellular states we propose a comprehensive genecell dependency visualization via unsupervised clustering growing hierarchical selforganizing map ghsom specifically designed for analyzing highdimensional singlecell data like singlecell sequencing and crispr screens ghsom is applied to cluster samples in a hierarchical structure such that the selfgrowth structure of clusters satisfies the required variations between and within we propose a novel significant attributes identification algorithm to identify features that distinguish clusters this algorithm pinpoints attributes with minimal variation within a cluster but substantial variation between clusters these key attributes can then be used for targeted data retrieval and downstream analysis furthermore we present two innovative visualization tools cluster feature map and cluster distribution map the cluster feature map highlights the distribution of specific features across the hierarchical structure of ghsom clusters this allows for rapid visual assessment of cluster uniqueness based on chosen features the cluster distribution map depicts leaf clusters as circles on the ghsom grid with circle size reflecting cluster data size and color customizable to visualize features like cell type or other attributes we apply our analysis to three singlecell datasets and one crispr dataset cellgene database and evaluate clustering methods with internal and external ch and ari scores ghsom performs well being the best performer in internal evaluation ch42 in external evaluation ghsom has the thirdbest performance of all methods,0
multiplexed assays of variant effect maves perform simultaneous characterization of many variants prime editing has been recently adopted for introducing many variants in their native genomic contexts however robust protocols and standards are limited preventing widespread uptake herein we describe curated loci prime editing clipe which is an accessible lowcost experimental pipeline to perform maves using prime editing of a target gene as well as a companion shiny app pegrna designer to rapidly and easily design userspecific mave libraries,0
we demonstrate that endonuclease deficient clustered regularly interspaced short palindromic repeats crisprassociated cas9 protein dcas9 fused to the photoconvertible fluorescence protein monomeric meos31 dcas9meos3 can be used to resolve subdiffraction limited features of repetitive gene elements thus providing a new route to investigate highorder chromatin organization at these sites,0
genetic screens mediated via crisprcas9 combined with highcontent readouts have emerged as powerful tools for biological discovery however computational analyses of these screens come with additional challenges beyond those found with standard scrnaseq analyses for example perturbationinduced variations of interest may be subtle and masked by other dominant source of variation shared with controls and variable guide efficiency results in some cells not undergoing genetic perturbation despite expressing a guide rna while a number of methods have been developed to address the former problem by explicitly disentangling perturbationinduced variations from those shared with controls less attention has been paid to the latter problem of noisy perturbation labels to address this issue here we propose contrastivevi a generative modeling framework that both disentangles perturbationinduced from nonperturbationrelated variations while also inferring whether cells truly underwent genomic edits applied to three largescale perturbseq datasets we find that contrastivevi better recovers known perturbationinduced variations compared to previous methods while successfully identifying cells that escaped the functional consequences of guide rna expression an opensource implementation of our model is available at url,0
predicting guide rna grna activity is critical for effective crisprcas12 genome editing but remains challenging due to limited data variation across protospacer adjacent motifs pamsshort sequence requirements for cas binding and reliance on largescale training we investigate whether pretrained biological foundation model originally trained on transcriptomic data can improve grna activity estimation even without domainspecific pretraining using embeddings from existing rna foundation model as input to lightweight regressor we show substantial gains over traditional baselines we also integrate chromatin accessibility data to capture regulatory context improving performance further our results highlight the effectiveness of pretrained foundation models and chromatin accessibility data for grna activity prediction,0
predicting phenotypes with complex genetic bases based on a small interpretable set of variant features remains a challenging task conventionally datadriven approaches are utilized for this task yet the high dimensional nature of genotype data makes the analysis and prediction difficult motivated by the extensive knowledge encoded in pretrained llms and their success in processing complex biomedical concepts we set to examine the ability of llms in feature selection and engineering for tabular genotype data with a novel knowledgedriven framework we develop freeform freeflow reasoning and ensembling for enhanced feature output and robust modeling designed with chainofthought and ensembling principles to select and engineer features with the intrinsic knowledge of llms evaluated on two distinct genotypephenotype datasets genetic ancestry and hereditary hearing loss we find this framework outperforms several datadriven methods particularly on lowshot regimes freeform is available as opensource framework at github,0
rna editing by members of the doublestranded rnaspecific adar family leads to sitespecific conversion of adenosine to inosine atoi in precursor messenger rnas editing by adars is believed to occur in all metazoa and is essential for mammalian development currently only a limited number of human adar substrates are known while indirect evidence suggests a substantial fraction of all premrnas being affected here we describe a computational search for adar editing sites in the human transcriptome using millions of available expressed sequences 12723 atoi editing sites were mapped in 1637 different genes with an estimated accuracy of 95 raising the number of known editing sites by two orders of magnitude we experimentally validated our method by verifying the occurrence of editing in 26 novel substrates atoi editing in humans primarily occurs in noncoding regions of the rna typically in alu repeats analysis of the large set of editing sites indicates the role of editing in controlling dsrna stability,0
information theoretic analysis of genetic languages indicates that the naturally occurring 20 amino acids and the triplet genetic code arose by duplication of 10 amino acids of classii and a doublet genetic code having codons nny and anticodons overleftarrowrm gnn evidence for this scenario is presented based on the properties of aminoacyltrna synthetases amino acids and nucleotide bases,0
the integration of bioinformatics predictions and experimental validation plays a pivotal role in advancing biological research from understanding molecular mechanisms to developing therapeutic strategies bioinformatics tools and methods offer powerful means for predicting gene functions protein interactions and regulatory networks but these predictions must be validated through experimental approaches to ensure their biological relevance this review explores the various methods and technologies used for experimental validation including gene expression analysis proteinprotein interaction verification and pathway validation we also discuss the challenges involved in translating computational predictions to experimental settings and highlight the importance of collaboration between bioinformatics and experimental research finally emerging technologies such as crispr gene editing nextgeneration sequencing and artificial intelligence are shaping the future of bioinformatics validation and driving more accurate and efficient biological discoveries,0
it is widely recognized nowadays that complex diseases are caused by amongst the others multiple genetic factors the recent advent of genomewide association study gwa has triggered a wave of research aimed at discovering genetic factors underlying common complex diseases while the number of reported susceptible genetic variants is increasing steadily the application of such findings into diseases prognosis for the general population is still unclear and there are doubts about whether the size of the contribution by such factors is significant in this respect some recent simulationbased studies have shed more light to the prospect of genetic tests in this report we discuss several aspects of simulationbased studies their parameters their assumptions and the information they provide,0
finding tumour genetic markers is essential to biomedicine due to their relevance for cancer detection and therapy development in this paper we explore a recently released dataset of chromosome rearrangements in 2586 cancer patients where different sorts of alterations have been detected using a random forest classifier we evaluate the relevance of several features some directly available in the original data some engineered by us related to chromosome rearrangements this evaluation results in a set of potential tumour genetic markers some of which are validated in the bibliography while others are potentially novel,0
the explanation is that in aortic tissue both diseased and nondiseased a bak1 pseudogene is expressed while in the matching blood samples the actual bak1 gene is expressed this explanation was reached after we realized that bak1 has two edited copies in human genome these copies are probably bak1 pseudogenes one copy belongs to chromosome 11 ng0055993 and the other to chromosome 20 nc0008505 the first copy has frameshifts which means that probably it does not express any functional protein by other hand the chromosome 20 copy has no frameshifts and what is more important contains all the reported polymorphisms,0
algebraic properties of the genetic code are analyzed the investigations of the genetic code on the basis of matrix approaches matrix genetics are described the degeneracy of the vertebrate mitochondria genetic code is reflected in the blackandwhite mosaic of the 88matrix of 64 triplets 20 amino acids and stopsignals this mosaic genetic matrix is connected with the matrix form of presentation of the special 8dimensional yinyangalgebra and of its particular 4dimensional case the special algorithm which is based on features of genetic molecules exists to transform the mosaic genomatrix into the matrices of these algebras two new numeric systems are defined by these 8dimensional and 4dimensional algebras genetic yinyangoctaves and genetic tetrions their comparison with quaternions by hamilton is presented elements of new genovector calculation and ideas of genetic mechanics are discussed these algebras are considered as models of the genetic code and as its possible precode basis they are related with binary oppositions of the yinyang type and they give new opportunities to investigate evolution of the genetic code the revealed fact of the relation between the genetic code and these genetic algebras is discussed in connection with the idea by pythagoras all things are numbers simultaneously these genetic algebras can be utilized as the algebras of genetic operators in biological organisms the described results are related with the problem of algebraization of bioinformatics they take attention to the question what is life from the viewpoint of algebra,0
a major challenge in imaging genetics and similar fields is to link highdimensional data in one domain eg genetic data to high dimensional data in a second domain eg brain imaging data the standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes that entails executing one genomewide association study gwas for each predefined imaging measure although this approach has been tremendously successful one shortcoming is that phenotypes must be predefined consequently effects that are not confined to preselected regions of interest or that reflect larger brainwide patterns can easily be missed in this work we introduce a partial least squares plsbased framework which we term clusterbootstrap pls clubpls that can work with large input dimensions in both domains as well as with large sample sizes one key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in both domains we applied clubpls to investigating the genetic basis of surface area and cortical thickness in a sample of 33000 subjects from the uk biobank we found 107 genomewide significant locusphenotype pairs that are linked to 386 different genes we found that a vast majority of these loci could be technically validated at a high rate using classic gwas or genomewide inferred statistics gwis we found that 85 locusphenotype pairs exceeded the genomewide suggestive p1e05 threshold,0
we consider a model for substratedepletion oscillations in genetic systems based on a stochastic differential equation with a slowly evolving external signal we show the existence of critical transitions in the system we apply two methods to numerically test the synthetic time series generated by the system for early indicators of critical transitions a detrended fluctuation analysis method and a novel method based on topological data analysis persistence diagrams,0
motivation the consistent amount of different types of omics data requires novel methods of analysis and data integration in this work we describe regression2net a computational approach to analyse gene expression and methylation profiles via regression analysis and networkbased techniques results we identified 284 and 447 unique candidate genes potentially associated to the glioblastoma pathology from two networks inferred from mixed genetic datasets indepth biological analysis of these networks reveals genes that are related to energy metabolism cell cycle control aatf immune system response and several types of cancer importantly we observed significant over representation of cancer related pathways including glioma especially in the methylation network this confirms the strong link between methylation and glioblastomas potential glioma suppressor genes accn3 and accn4 linked to nbpf1 neuroblastoma breakpoint family have been identified in our expression network numerous abc transporter genes abca1 abcb1 present in the expression network suggest drug resistance of glioblastoma tumors,0
the search for similar genetic sequences is one of the main bioinformatics tasks the genetic sequences data banks are growing exponentially and the searching techniques that use linear time are not capable to do the search in the required time anymore another problem is that the clock speed of the modern processors are not growing as it did before instead the processing capacity is growing with the addiction of more processing cores and the techniques which does not use parallel computing does not have benefits from these extra cores this work aims to use data indexing techniques to reduce the searching process computation cost united with the parallelization of the searching techniques to use the computational capacity of the multi core processors to verify the viability of using these two techniques simultaneously a software which uses parallelization techniques with inverted indexes was developed experiments were executed to analyze the performance gain when parallelism is utilized the search time gain and also the quality of the results when it compared with others searching tools the results of these experiments were promising the parallelism gain overcame the expected speedup the searching time was 20 times faster than the parallelized ncbi blast and the searching results showed a good quality when compared with this tool the software source code is available at,0
the advancement of novel combinatorial crispr screening technologies enables the identification of synergistic gene combinations on a large scale this is crucial for developing novel and effective combination therapies but the combinatorial space makes exhaustive experimentation infeasible we introduce naiad an active learning framework that efficiently discovers optimal gene pairs capable of driving cells toward desired cellular phenotypes naiad leverages singlegene perturbation effects and adaptive gene embeddings that scale with the training data size mitigating overfitting in smallsample learning while capturing complex gene interactions as more data is collected evaluated on four crispr combinatorial perturbation datasets totaling over 350000 genetic interactions naiad trained on small datasets outperforms existing models by up to 40 relative to the secondbest naiads recommendation system prioritizes gene pairs with the maximum predicted effects resulting in the highest marginal gain in each aiexperiment round and accelerating discovery with fewer crispr experimental iterations our naiad framework improves the identification of novel effective gene combinations enabling more efficient crispr library design and offering promising applications in genomics research and therapeutic development,0
the aetiology of polygenic obesity is multifactorial which indicates that lifestyle and environmental factors may influence multiples genes to aggravate this disorder several lowrisk single nucleotide polymorphisms snps have been associated with bmi however identified loci only explain a small proportion of the variation observed for this phenotype the linear nature of genome wide association studies gwas used to identify associations between genetic variants and the phenotype have had limited success in explaining the heritability variation of bmi and shown low predictive capacity in classification studies gwas ignores the epistatic interactions that less significant variants have on the phenotypic outcome in this paper we utilise a novel deep learningbased methodology to reduce the high dimensional space in gwas and find epistatic interactions between snps for classification purposes snps were filtered based on the effects associations have with bmi since bonferroni adjustment for multiple testing is highly conservative an important proportion of snps involved in snpsnp interactions are ignored therefore only snps with pvalues 1x102 were considered for subsequent epistasis analysis using stacked auto encoders sae this allows the nonlinearity present in snpsnp interactions to be discovered through progressively smaller hidden layer units and to initialise a multilayer feedforward artificial neural network ann classifier the classifier is finetuned to classify extremely obese and nonobese individuals the best results were obtained with 2000 compressed units se0949153 sp0933014 gini0949936 logloss01956 auc097497 and mse0054057 using 50 compressed units it was possible to achieve se0785311 sp0799043 gini0703566 logloss0476864 auc085178 and mse0156315,0
dna language models have revolutionized our ability to understand and design dna sequencesthe fundamental language of lifewith unprecedented precision enabling transformative applications in therapeutics synthetic biology and gene editing however this capability also poses substantial dualuse risks including the potential for creating pathogens viruses and even bioweapons to address these biosecurity challenges we introduce two innovative watermarking techniques to reliably track the designed dna dnamark and centralmark dnamark employs synonymous codon substitutions to embed watermarks in dna sequences while preserving the original function centralmark further advances this by creating inheritable watermarks that transfer from dna to translated proteins leveraging protein embeddings to ensure detection across the central dogma both methods utilize semantic embeddings to generate watermark logits enhancing robustness against natural mutations synthesis errors and adversarial attacks evaluated on our therapeutic dna benchmark dnamark and centralmark achieve f1 detection scores above 085 under various conditions while maintaining over 60 sequence similarity to ground truth and degeneracy scores below 15 a case study on the crisprcas9 system underscores centralmarks utility in realworld settings this work establishes a vital framework for securing dna language models balancing innovation with accountability to mitigate biosecurity risks,0
we describe here the new concept of homomorphisms of probabilistic regulatory gene networksprn the homomorphisms are special mappings between two probabilistic networks that consider the algebraic action of the iteration of functions and the probabilistic dynamic of the two networks it is proved here that the class of prn together with the homomorphisms form a category with products and coproducts projections are special homomorphisms induced by invariant subnetworks here it is proved that an homomorphism for 0  1 produces simultaneous markov chains in both networks that permit to introduce the concepts of isomorphism of markov chains and similar networks,0
interventions play a pivotal role in the study of complex biological systems in drug discovery genetic interventions such as crispr base editing have become central to both identifying potential therapeutic targets and understanding a drugs mechanism of action with the advancement of crispr and the proliferation of genomescale analyses such as transcriptomics a new challenge is to navigate the vast combinatorial space of concurrent genetic interventions addressing this our work concentrates on estimating the effects of pairwise genetic combinations on the cellular transcriptome we introduce two novel contributions salt a biologicallyinspired baseline that posits the mostly additive nature of combination effects and peper a deep learning model that extends salts additive assumption to achieve unprecedented accuracy our comprehensive comparison against existing stateoftheart methods grounded in diverse metrics and our outofdistribution analysis highlight the limitations of current models in realistic settings this analysis underscores the necessity for improved modelling techniques and data acquisition strategies paving the way for more effective exploration of genetic intervention effects,0
reproducibility in genomewide association studies gwas is crucial for ensuring reliable genomic research outcomes however limited access to original genomic datasets mainly due to privacy concerns prevents researchers from reproducing experiments to validate results in this paper we propose a novel method for gwas reproducibility validation that detects unintentional errors without the need for dataset sharing our approach leverages pvalues from gwas outcome reports to estimate contingency tables for each single nucleotide polymorphism snp and calculates the hamming distance between the minor allele frequencies mafs derived from these contingency tables and publicly available phenotypespecific maf data by comparing the average hamming distance we validate results that fall within a trusted threshold as reliable while flagging those that exceed the threshold for further inspection this approach not only allows researchers to validate the correctness of gwas findings of other researchers but it also provides a selfcheck step for the researchers before they publish their findings we evaluate our approach using three reallife snp datasets from opensnp showing its ability to detect unintentional errors effectively even when small errors occur such as 1 of snps being reported incorrectly this novel validation technique offers a promising solution to the gwas reproducibility challenge balancing the need for rigorous validation with the imperative of protecting sensitive genomic data thereby enhancing trust and accuracy in genetic research,0
decoding the genome confers the capability to predict characteristics of the organismphenotype from dna genotype we describe the present status and future prospects of genomic prediction of complex traits in humans some highly heritable complex phenotypes such as height and other quantitative traits can already be predicted with reasonable accuracy from dna alone for many diseases including important common conditions such as coronary artery disease breast cancer type i and ii diabetes individuals with outlier polygenic scores eg top few percent have been shown to have 5 or even 10 times higher risk than average several psychiatric conditions such as schizophrenia and autism also fall into this category we discuss related topics such as the genetic architecture of complex traits sibling validation of polygenic scores and applications to adult health in vitro fertilization embryo selection and genetic engineering,0
postgenomic research deals with challenging problems in screening genomes of organisms for particular functions or potential for being the targets of genetic engineering for desirable biological features phenotyping of wild type and mutants is a timeconsuming and costly effort by many individuals this article is a preliminary progress report in research on largescale automation of phenotyping steps imaging informatics and data analysis needed to study plant geneproteins networks that influence growth and development of plants our results undermine the significance of phenotypic traits that are implicit in patterns of dynamics in plant root response to sudden changes of its environmental conditions such as sudden reorientation of the root tip against the gravity vector including dynamic features besides the common morphological ones has paid off in design of robust and accurate machine learning methods to automate a typical phenotyping scenario ie to distinguish the wild type from the mutants,0
the mainstream of research in genetics epigenetics and imaging data analysis focuses on statistical association or exploring statistical dependence between variables despite their significant progresses in genetic research understanding the etiology and mechanism of complex phenotypes remains elusive using association analysis as a major analytical platform for the complex data analysis is a key issue that hampers the theoretic development of genomic science and its application in practice causal inference is an essential component for the discovery of mechanical relationships among complex phenotypes many researchers suggest making the transition from association to causation despite its fundamental role in science engineering and biomedicine the traditional methods for causal inference require at least three variables however quantitative genetic analysis such as qtl eqtl mqtl and genomicimaging data analysis requires exploring the causal relationships between two variables this paper will focus on bivariate causal discovery we will introduce independence of cause and mechanism icm as a basic principle for causal inference algorithmic information theory and additive noise model anm as major tools for bivariate causal discovery largescale simulations will be performed to evaluate the feasibility of the anm for bivariate causal discovery to further evaluate their performance for causal inference the anm will be applied to the construction of gene regulatory networks also the anm will be applied to traitimaging data analysis to illustrate three scenarios presence of both causation and association presence of association while absence of causation and presence of causation while lack of association between two variables,0
insertion of transposed elements within mammalian genes is thought to be an important contributor to mammalian evolution and speciation insertion of transposed elements into introns can lead to their activation as alternatively spliced cassette exons an event called exonization elucidation of the evolutionary constraints that have shaped fixation of transposed elements within human and mouse protein coding genes and subsequent exonization is important for understanding of how the exonization process has affected transcriptome and proteome complexities here we show that exonization of transposed elements is biased towards the beginning of the coding sequence in both human and mouse genes analysis of single nucleotide polymorphisms snps revealed that exonization of transposed elements can be populationspecific implying that exonizations may enhance divergence and lead to speciation snp density analysis revealed differences between alu and other transposed elements finally we identified cases of primatespecific alu elements that depend on rna editing for their exonization these results shed light on te fixation and the exonization process within human and mouse genes,0
the transmission and evolution of severe acute respiratory syndrome coronavirus 2 sarscov2 are of paramount importance to the controlling and combating of coronavirus disease 2019 covid19 pandemic currently near 15000 sarscov2 single mutations have been recorded having a great ramification to the development of diagnostics vaccines antibody therapies and drugs however little is known about sarscov2 evolutionary characteristics and general trend in this work we present a comprehensive genotyping analysis of existing sarscov2 mutations we reveal that host immune response via apobec and adar gene editing gives rise to near 65 of recorded mutations additionally we show that children under age five and the elderly may be at high risk from covid19 because of their overreacting to the viral infection moreover we uncover that populations of oceania and africa react significantly more intensively to sarscov2 infection than those of europe and asia which may explain why african americans were shown to be at increased risk of dying from covid19 in addition to their high risk of getting sick from covid19 caused by systemic health and social inequities finally our study indicates that for two viral genome sequences of the same origin their evolution order may be determined from the ratio of mutation type ct over tc,0
the efficient recognition of pathogens by the adaptive immune system relies on the diversity of receptors displayed at the surface of immune cells tcell receptor diversity results from an initial random dna editing process called vdj recombination followed by functional selection of cells according to the interaction of their surface receptors with self and foreign antigenic peptides to quantify the effect of selection on the highly variable elements of the receptor we apply a probabilistic maximum likelihood approach to the analysis of highthroughput sequence data from the chain of human tcell receptors we quantify selection factors for v and j gene choice and for the length and aminoacid composition of the variable region our approach is necessary to disentangle the effects of selection from biases inherent in the recombination process inferred selection factors differ little between donors or between naive and memory repertoires the number of sequences shared between donors is wellpredicted by the model indicating a purely stochastic origin of such public sequences we find a significant correlation between biases induced by vdj recombination and our inferred selection factors together with a reduction of diversity during selection both effects suggest that natural selection acting on the recombination process has anticipated the selection pressures experienced during somatic evolution,0
detecting the interactions of genetic compounds like genes snps proteins metabolites etc can potentially unravel the mechanisms behind complex traits and common genetic disorders several methods have been taken into consideration for the analysis of different types of genetic data regression being one of the most widely adopted without any doubt a common data type is represented by gene expression profiles from which gene regulatory networks have been inferred with different approaches in this work we review nine penalised regression methods applied to microarray data to infer the topology of the network of interactions we evaluate each method with respect to the complexity of biological data we analyse the limitations of each of them in order to suggest a number of precautions that should be considered to make their predictions more significant and reliable,0
data integration methods aim to extract lowdimensional embeddings from highdimensional outcomes to remove unwanted variations such as batch effects and unmeasured covariates across heterogeneous datasets however multiple hypothesis testing after integration can be biased due to datadependent processes we introduce a robust postintegrated inference pii method that adjusts for latent heterogeneity using control outcomes leveraging causal interpretations we derive nonparametric identifiability of the direct effects using negative control outcomes by utilizing surrogate control outcomes as an extension of negative control outcomes we develop semiparametric inference on projected direct effect estimands accounting for hidden mediators confounders and moderators these estimands remain statistically meaningful under model misspecifications and with errorprone embeddings we provide bias quantifications and finitesample linear expansions with uniform concentration bounds the proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification facilitating dataadaptive estimation with machine learning algorithms our proposal is evaluated with random forests through simulations and analysis of singlecell crispr perturbed datasets with potential unmeasured confounders,0
the hox gene collinearity enigma has often been approached using models based on biomolecular mechanisms the biophysical model is an alternative approach speculating that collinearity is caused by physical forces pulling the hox clusters from a territory where they are inactive to a distinct spatial domain where they are activated in a step by step manner hox gene translocations have recently been observed in support of the biophysical model furthermore genetic engineering experiments performed in embryonic mice gave rise to some unexpected mutant expressions that biomolecular models could not predict in several cases when anterior hoxd genes are deleted the expression of the genes whose expression is probed in the mutants are impossible to anticipate on the contrary the biophysical model offers convincing explanation all these experimental results support the idea of physical forces being responsible for hox gene collinearity in order to test the validity of the various models further certain experiment involving gene deletions are proposed the biophysical and biomolecular models predict different results for these experiments hence the expected outcome will confirm or question the validity of these models,0
the study of genome rearrangement has many flavours but they all are somehow tied to edit distances on variations of a multigraph called the breakpoint graph we study a weighted 2break distance on eulerian 2edgecolored multigraphs which generalizes weighted versions of several double cut and join problems including those on genomes with unequal gene content we affirm the connection between cycle decompositions and edit scenarios first discovered with the sorting by reversals problem using this we show that the problem of finding a parsimonious scenario of minimum cost on an eulerian 2edgecolored multigraph with a general cost function for 2breaks can be solved by decomposing the problem into independent instances on simple alternating cycles for breakpoint graphs and a more constrained cost function based on coloring the vertices we give a polynomialtime algorithm for finding a parsimonious 2break scenario of minimum cost while showing that finding a nonparsimonious 2break scenario of minimum cost is nphard,0
background the increasing availability of databases containing both magnetic resonance imaging mri and genetic data allows researchers to utilize multimodal data to better understand the characteristics of dementia of alzheimers type dat objective the goal of this study was to develop and analyze novel biomarkers that can help predict the development and progression of dat methods we used feature selection and ensemble learning classifier to develop an imagegenotypebased dat score that represents a subjects likelihood of developing dat in the future three feature types were used mri only genetic only and combined multimodal data we used a novel data stratification method to better represent different stages of dat using a predefined 05 threshold on dat scores we predicted whether or not a subject would develop dat in the future results our results on alzheimers disease neuroimaging initiative adni database showed that dementia scores using genetic data could better predict future dat progression for currently normal control subjects accuracy0857 compared to mri accuracy0143 while mri can better characterize subjects with stable mild cognitive impairment accuracy0614 compared to genetics accuracy0356 combining mri and genetic data showed improved classification performance in the remaining stratified groups conclusion mri and genetic data can contribute to dat prediction in different ways mri data reflects anatomical changes in the brain while genetic data can detect the risk of dat progression prior to the symptomatic onset combining information from multimodal data in the right way can improve prediction performance,0
deep learning architectures such as convolutional neural networks and transformers have revolutionized biological sequence modeling with recent advances driven by scaling up foundation and taskspecific models the computational resources and large datasets required however limit their applicability in biological contexts we introduce lyra a subquadratic architecture for sequence modeling grounded in the biological framework of epistasis for understanding sequencetofunction relationships mathematically we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships we demonstrate that lyra is performant across over 100 wideranging biological tasks achieving stateoftheart sota performance in many key areas including protein fitness landscape prediction biophysical property prediction eg disordered protein region functions peptide engineering applications eg antibody binding cellpenetrating peptide prediction rna structure analysis rna function prediction and crispr guide design it achieves this with ordersofmagnitude improvements in inference speed and reduction in parameters up to 120000fold in our tests compared to recent biology foundation models using lyra we were able to train and run every task in this study on two or fewer gpus in under two hours democratizing access to biological sequence modeling at sota performance with potential applications to many fields,0
since genetic algorithm was proposed by john holland holland j h 1975 in the early 1970s the study of evolutionary algorithm has emerged as a popular research field civicioglu besdok 2013 researchers from various scientific and engineering disciplines have been digging into this field exploring the unique power of evolutionary algorithms hadka reed 2013 many applications have been successfully proposed in the past twenty years for example mechanical design lampinen zelinka 1999 electromagnetic optimization rahmatsamii michielssen 1999 environmental protection bertini felice moretti pizzuti 2010 finance larkin ryan 2010 musical orchestration esling carpentier agon 2010 pipe routing furuholmen glette hovin torresen 2010 and nuclear reactor core design sacco henderson rioscoelho ali pereira 2009 in particular its function optimization capability was highlighted goldberg richardson 1987 because of its high adaptability to different function landscapes to which we cannot apply traditional optimization techniques wong leung wong 2009 here we review the applications of evolutionary algorithms in bioinformatics,0
in this paper we aim to give a tutorial for undergraduate students studying statistical methods andor bioinformatics the students will learn how data visualization can help in genomic sequence analysis students start with a fragment of genetic text of a bacterial genome and analyze its structure by means of principal component analysis they discover that the information in the genome is encoded by nonoverlapping triplets next they learn how to find gene positions this exercise on pca and kmeans clustering enables active study of the basic bioinformatics notions appendix 1 contains program listings that go along with this exercise appendix 2 includes 2d pca plots of triplet usage in moving frame for a series of bacterial genomes from gcpoor to gcrich ones animated 3d pca plots are attached as separate gif files topology cluster structure and geometry mutual positions of clusters of these plots depends clearly on gccontent,0
agerelated macular degeneration amd is a major cause of blindness in older adults severely affecting vision and quality of life despite advances in understanding amd the molecular factors driving the severity of subretinal scarring fibrosis remain elusive hampering the development of effective therapies this study introduces a machine learningbased framework to predict key genes that are strongly correlated with lesion severity and to identify potential therapeutic targets to prevent subretinal fibrosis in amd using an original rna sequencing rnaseq dataset from the diseased retinas of jr5558 mice we developed a novel and specific feature engineering technique including pathwaybased dimensionality reduction and genebased feature expansion to enhance prediction accuracy two iterative experiments were conducted by leveraging ridge and elasticnet regression models to assess biological relevance and gene impact the results highlight the biological significance of several key genes and demonstrate the frameworks effectiveness in identifying novel therapeutic targets the key findings provide valuable insights for advancing drug discovery efforts and improving treatment strategies for amd with the potential to enhance patient outcomes by targeting the underlying genetic mechanisms of subretinal lesion development,0
genomescale screening experiments in cancer produce long lists of candidate genes that require extensive interpretation for biological insight and prioritization for followup studies interrogation of gene lists frequently represents a significant and timeconsuming undertaking in which experimental biologists typically combine results from a variety of bioinformatics resources in an attempt to portray and understand cancer relevance as a means to simplify and strengthen the support for this endeavor we have developed oncoenrichr a flexible bioinformatics tool that allows cancer researchers to comprehensively interrogate a given gene list along multiple facets of cancer relevance oncoenrichr differs from general gene set analysis frameworks through the integration of an extensive set of prior knowledge specifically relevant for cancer including ranked genetumor type associations literaturesupported protooncogene and tumor suppressor gene annotations target druggability data regulatory interactions synthetic lethality predictions as well as prognostic associations gene aberrations and coexpression patterns across tumor types the software produces a structured and userfriendly analysis report as its main output where versions of all underlying data resources are explicitly logged the latter being a critical component for reproducible science we demonstrate the usefulness of oncoenrichr through interrogation of two candidate lists from proteomic and crispr screens oncoenrichr is freely available as a webbased workflow hosted by the galaxy platform and can also be accessed as a standalone r package,0
background to understand individual genomes it is necessary to look at the variations that lead to changes in phenotype and possibly to disease however genotype information alone is often not sufficient and additional knowledge regarding the phase of the variation is needed to make correct interpretations interactive visualizations that allow the user to explore the data in various ways can be of great assistance in the process of making well informed decisions but currently there is a lack for visualizations that are able to deal with phased haplotype data results we present inphap an interactive visualization tool for genotype and phased haplotype data inphap features a variety of interaction possibilities such as zooming sorting filtering and aggregation of rows in order to explore patterns hidden in large genetic data sets as a proof of concept we apply inphap to the phased haplotype data set of phase 1 of the 1000 genomes project thereby inphaps ability to show genetic variations on the population as well as on the individuals level is demonstrated for several disease related loci conclusions as of today inphap is the only visual analytical tool that allows the user to explore unphased and phased haplotype data interactively due to its highly scalable design inphap can be applied to large datasets with up to 100 gb of data enabling users to visualize even large scale input data inphap closes the gap between common visualization tools for unphased genotype data and introduces several new features such as the visualization of phased data,0
thanks to the increasing availability of genomics and other biomedical data many machine learning approaches have been proposed for a wide range of therapeutic discovery and development tasks in this survey we review the literature on machine learning applications for genomics through the lens of therapeutic development we investigate the interplay among genomics compounds proteins electronic health records ehr cellular images and clinical texts we identify twentytwo machine learning in genomics applications across the entire therapeutics pipeline from discovering novel targets personalized medicine developing geneediting tools all the way to clinical trials and postmarket studies we also pinpoint seven important challenges in this field with opportunities for expansion and impact this survey overviews recent research at the intersection of machine learning genomics and therapeutic development,0
how to compare whole genome sequences at large scale has not been achieved via conventional methods based on pairwisely basetobase comparison nevertheless no attention was paid to handle inonesitting a number of genomes crossing genetic category chromosome plasmid and phage with farther divergences much less or no homologous over large size ranges from kbp to mbp we created a new method genomefingerprinter to unambiguously produce threedimensional coordinates from a sequence followed by one threedimensional plot and six twodimensional trajectory projections to illustrate whole genome fingerprints we further developed a set of concepts and tools and thereby established a new method universal genome fingerprint analysis we demonstrated their applications through case studies on over a hundred of genome sequences particularly we defined the total genetic component configuration tgcc ie chromosome plasmid and phage for describing a strain as a system and the universal genome fingerprint map ugfm of tgcc for differentiating a strain as a universal system as well as the systematic comparative genomics scg for comparing inonesitting a number of genomes crossing genetic category in diverse strains by using ugfm ugfmtgcc and ugfmtgccscg we compared a number of genome sequences with farther divergences chromosome plasmid and phage bacterium archaeal bacterium and virus over large size ranges 6kbp5mbp giving new insights into critical problematic issues in microbial genomics in the postgenomic era this paper provided a new method for rapidly computing geometrically visualizing and intuitively comparing genome sequences at fingerprint level and hence established a new method of universal genome fingerprint analysis for systematic comparative genomics,0
extracting genetic information from a full range of sequencing data is important for understanding diseases we propose a novel method to effectively explore the landscape of genetic mutations and aggregate them to predict cancer type we used multinomial logistic regression nonsmooth nonnegative matrix factorization nsnmf and support vector machine svm to utilize the full range of sequencing data aiming at better aggregating genetic mutations and improving their power in predicting cancer types specifically we introduced a classifier to distinguish cancer types using somatic mutations obtained from wholeexome sequencing data mutations were identified from multiple cancers and scored using sift pp2 and cadd and grouped at the individual gene level the nsnmf was then applied to reduce dimensionality and to obtain coefficient and basis matrices a feature matrix was derived from the obtained matrices to train a classifier for cancer type classification with the svm model we have demonstrated that the classifier was able to distinguish the cancer types with reasonable accuracy in fivefold crossvalidations using mutation counts as features the average prediction accuracy was 771 sem01 significantly outperforming baselines and outperforming models using mutation scores as features using the factor matrices derived from the nsnmf we identified multiple genes and pathways that are significantly associated with each cancer type this study presents a generic and complete pipeline to study the associations between somatic mutations and cancers the discovered genes and pathways associated with each cancer type can lead to biological insights the proposed method can be adapted to other studies for disease classification and pathway discovery,0
discovering all the genetic causes of a phenotype is an important goal in functional genomics in this paper we combine an experimental design for multiple independent detections of the genetic causes of a phenotype with a highthroughput sequencing analysis that maximizes sensitivity for comprehensively identifying them testing this approach on a set of 24 mutant strains generated for a metabolic phenotype with many known genetic causes we show that this pathwaybased phenotype sequencing analysis greatly improves sensitivity of detection compared with previous methods and reveals a wide range of pathways that can cause this phenotype we demonstrate our approach on a metabolic reengineering phenotype the pepoaa metabolic node in e coli which is crucial to a substantial number of metabolic pathways and under renewed interest for biofuel research out of 2157 mutations in these strains pathwayphenoseq discriminated just five gene groups 12 genes as statistically significant causes of the phenotype experimentally these five gene groups and the next two highscoring pathwayphenoseq groups either have a clear connection to the pep metabolite level or offer an alternative path of producing oxaloacetate oaa and thus clearly explain the phenotype these highscoring gene groups also show strong evidence of positive selection pressure compared with strictly neutral selection in the rest of the genome,0
the em doublecutandjoin dcj operation introduced by yancopoulos emphet al allows minimum edit distance to be computed by modeling all possible classical rearrangement operations such as inversions fusions fissions translocations and transpositions in lineartime between two genomes however there is lack of visualization tool that can effectively present dcj operations that will help biologists to use dcj operation in this paper a new visualization program is introduced dcjvis to create a diagram of each dcj operation necessary to transform between the genomes of two distinct organisms by describing a possible sequence of genome graphs based on the selected gene adjacency on the source genome for the dcj operation our program is the first visualization tool for dcj operations using circular layout specifically the genomes of textitsaccharomyces cerevisiae and textitcandida albicans are used to demonstrate the functionality of this program and provide an example of the type of problem this program can solve for biologists,0
spanning two decades the encyclopaedia of dna elements encode is a collaborative research project that aims to identify all the functional elements in the human and mouse genomes to best serve the scientific community all data generated by the consortium is shared through a webportal with no access restrictions the fourth and final phase of the project added a diverse set of new samples including those associated with human disease and a wide range of new assays aimed at detection characterization and validation of functional genomic elements the encode data portal hosts results from over 23000 functional genomics experiments over 800 functional elements characterization experiments including in vivo transgenic enhancer assays reporter assays and crispr screens along with over 60000 results of computational and integrative analyses including imputations predictions and genome annotations the encode data coordination center dcc is responsible for development and maintenance of the data portal along with the implementation and utilisation of the encode uniform processing pipelines to generate uniformly processed data here we report recent updates to the data portal specifically we have completely redesigned the home page improved search interface added several new pages to highlight collections of biologically related data deeply profiled cell lines immune cells alzheimers disease rnaprotein interactions degron matrix and a matrix of experiments organised by human donors added singlecell experiments and enhanced the cart interface for visualisation and download of userselected datasets,0
identifying genes associated with complex human diseases is one of the main challenges of human genetics and computational medicine to answer this question millions of genetic variants get screened to identify a few of importance to increase the power of identifying genes associated with diseases and to account for other potential sources of protein function aberrations we propose a novel factorgraph based model where much of the biological knowledge is incorporated through factors and priors our extensive simulations show that our method has superior sensitivity and precision compared to variantaggregating and differential expression methods our integrative approach was able to identify important genes in breast cancer identifying genes that had coding aberrations in some patients and regulatory abnormalities in others emphasizing the importance of data integration to explain the disease in a larger number of patients,0
lineage tracing the determination and mapping of progeny arising from single cells is an important approach enabling the elucidation of mechanisms underlying diverse biological processes ranging from development to disease we developed a dynamic sequencebased barcode for lineage tracing and have demonstrated its performance in c elegans a model organism whose lineage tree is well established the strategy we use creates lineage trees based upon the introduction of specific mutations into cells and the propagation of these mutations to daughter cells at each cell division we present an experimental proof of concept along with a corresponding simulation and analytical model for deeper understanding of the coding capacity of the system by introducing mutations in a predictable manner using crisprcas9 our technology will enable more complete investigations of cellular processes,0
the edit distance under the dcj model can be computed in linear time for genomes with equal content or with indels but it becomes nphard in the presence of duplications a problem largely unsolved especially when indels are considered in this paper we compare two mainstream methods to deal with duplications and associate them with indels one by deletion namely dcjindelexemplar distance versus the other by gene matching namely dcjindelmatching distance we design branchandbound algorithms with set of optimization methods to compute exact distances for both furthermore median problems are discussed in alignment with both of these distance methods which are to find a median genome that minimizes distances between itself and three given genomes linkernighan lk heuristic is leveraged and powered up by subgraph decomposition and search space reduction technologies to handle median computation a wide range of experiments are conducted on synthetic data sets and real data sets to show pros and cons of these two distance metrics per se as well as putting them in the median computation scenario,0
understanding functional organization of genetic information is a major challenge in modern biology following the initial publication of the human genome sequence in 2001 advances in highthroughput measurement technologies and efficient sharing of research material through community databases have opened up new views to the study of living organisms and the structure of life in this thesis novel computational strategies have been developed to investigate a key functional layer of genetic information the human transcriptome which regulates the function of living cells through protein synthesis the key contributions of the thesis are general exploratory tools for highthroughput data analysis that have provided new insights to cellbiological networks cancer mechanisms and other aspects of genome function a central challenge in functional genomics is that highdimensional genomic observations are associated with high levels of complex and largely unknown sources of variation by combining statistical evidence across multiple measurement sources and the wealth of background information in genomic data repositories it has been possible to solve some the uncertainties associated with individual observations and to identify functional mechanisms that could not be detected based on individual measurement sources statistical learning and probabilistic models provide a natural framework for such modeling tasks open source implementations of the key methodological contributions have been released to facilitate further adoption of the developed methods by the research community,0
recent emergence of nextgeneration dna sequencing technology has enabled acquisition of genetic information at unprecedented scales in order to determine the genetic blueprint of an organism sequencing platforms typically employ socalled shotgun sequencing strategy to oversample the target genome with a library of relatively short overlapping reads the order of nucleotides in the reads is determined by processing the acquired noisy signals generated by the sequencing instrument assembly of a genome from potentially erroneous short reads is a computationally daunting task even in the scenario where a reference genome exists errors and gaps in the reference and perfect repeat regions in the target further render the assembly challenging and cause inaccuracies in this paper we formulate the referenceguided sequence assembly problem as the inference of the genome sequence on a bipartite graph and solve it using a messagepassing algorithm the proposed algorithm can be interpreted as the wellknown classical belief propagation scheme under a certain prior unlike existing stateoftheart methods the proposed algorithm combines the information provided by the reads without needing to know reliability of the short reads socalled quality scores relation of the messagepassing algorithm to a provably convergent power iteration scheme is discussed to evaluate and benchmark the performance of the proposed technique we find an analytical expression for the probability of error of a genieaided maximum a posteriori map decision scheme results on both simulated and experimental data demonstrate that the proposed messagepassing algorithm outperforms commonly used stateoftheart tools and it nearly achieves the performance of the aforementioned map decision scheme,0
in this paper association results from genomewide association studies gwas are combined with a deep learning framework to test the predictive capacity of statistically significant single nucleotide polymorphism snps associated with obesity phenotype our approach demonstrates the potential of deep learning as a powerful framework for gwas analysis that can capture information about snps and the important interactions between them basic statistical methods and techniques for the analysis of genetic snp data from populationbased genomewide studies have been considered statistical association testing between individual snps and obesity was conducted under an additive model using logistic regression four subsets of loci after qualitycontrol qc and association analysis were selected pvalues lower than 1x105 5 snps 1x104 32 snps 1x103 248 snps and 1x102 2465 snps a deep learning classifier is initialised using these sets of snps and finetuned to classify obese and nonobese observations using a deep learning classifier model and genetic variants with pvalue 1x102 2465 snps it was possible to obtain results se09604 sp09712 gini09817 logloss01150 auc09908 and mse00300 as the pvalue increased an evident deterioration in performance was observed results demonstrate that single snp analysis fails to capture the cumulative effect of less significant variants and their overall contribution to the outcome in disease prediction which is captured using a deep learning framework,0
microarray is one of the essential technologies used by the biologist to measure genomewide expression levels of genes in a particular organism under some particular conditions or stimuli as microarrays technologies have become more prevalent the challenges of analyzing these data for getting better insight about biological processes have essentially increased due to availability of artificial intelligence based sophisticated computational techniques such as artificial neural networks fuzzy logic genetic algorithms and many other natureinspired algorithms it is possible to analyse microarray gene expression data in more better way here we reviewed artificial intelligence based techniques for the analysis of microarray gene expression data further challenges in the field and future work direction have also been suggested,0
in order to associate complex traits with genetic polymorphisms genomewide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms when handling these datasets which exceed the main memory of contemporary computers one faces two distinct challenges 1 millions of polymorphisms come at the cost of hundreds of gigabytes of genotype data which can only be kept in secondary storage 2 the relatedness of the test population is represented by a covariance matrix which for large populations can only fit in the combined main memory of a distributed architecture in this paper we present solutions for both challenges the genotype data is streamed from and to secondary storage using a double buffering technique while the covariance matrix is kept across the main memory of a distributed memory system we show that these methods sustain highperformance and allow the analysis of enormous dataset,0
biologists rely heavily on the language of information coding and transmission that is commonplace in the field of information theory as developed by claude shannon but there is open debate about whether such language is anything more than facile metaphor philosophers of biology have argued that when biologists talk about information in genes and in evolution they are not talking about the sort of information that shannons theory addresses first philosophers have suggested that shannon theory is only useful for developing a shallow notion of correlation the socalled causal sense of information second they typically argue that in genetics and evolutionary biology information language is used in a semantic sense whereas semantics are deliberately omitted from shannon theory neither critique is wellfounded here we propose an alternative to the causal and semantic senses of information a transmission sense of information in which an object x conveys information if the function of x is to reduce by virtue of its sequence properties uncertainty on the part of an agent who observes x the transmission sense not only captures much of what biologists intend when they talk about information in genes but also brings shannons theory back to the fore by taking the viewpoint of a communications engineer and focusing on the decision problem of how information is to be packaged for transport this approach resolves several problems that have plagued the information concept in biology and highlights a number of important features of the way that information is encoded stored and transmitted as genetic sequence,0
metagenomics is a powerful approach to study genetic content of environmental samples that has been strongly promoted by ngs technologies to cope with massive data involved in modern metagenomic projects recent tools rely on the analysis of kmers shared between the read to be classified and sampled reference genomes within this general framework we show in this work that spaced seeds provide a significant improvement of classification accuracy as opposed to traditional contiguous kmers we support this thesis through a series a different computational experiments including simulations of largescale metagenomic projects scripts and programs used in this study as well as supplementary material are available from,0
finding the largest few principal components of a matrix of genetic data is a common task in genomewide association studies gwass both for dimensionality reduction and for identifying unwanted factors of variation we describe a simple random matrix model for matrices that arise in gwass showing that the singular values have a bulk behavior that obeys a marchenkopastur distributed with a handful of large outliers we also implement golubkahanlanczos gkl bidiagonalization in the julia programming language providing thick restarting and a choice between full and partial reorthogonalization strategies to control numerical roundoff our implementation of gkl bidiagonalization is up to 36 times faster than software tools used commonly in genomics data analysis for computing principal components such as eigensoft and flashpca which use dense lapack routines and randomized subspace iteration respectively,0
systematic characterization of biological effects to genetic perturbation is essential to the application of molecular biology and biomedicine however the experimental exhaustion of genetic perturbations on the genomewide scale is challenging here we show that transcriptionnet a deep learning model that integrates multiple biological networks to systematically predict transcriptional profiles to three types of genetic perturbations based on transcriptional profiles induced by genetic perturbations in the l1000 project rna interference rnai clustered regularly interspaced short palindromic repeat crispr and overexpression oe transcriptionnet performs better than existing approaches in predicting inducible gene expression changes for all three types of genetic perturbations transcriptionnet can predict transcriptional profiles for all genes in existing biological networks and increases perturbational gene expression changes for each type of genetic perturbation from a few thousand to 26945 genes transcriptionnet demonstrates strong generalization ability when comparing predicted and true gene expression changes on different external tasks overall transcriptionnet can systemically predict transcriptional consequences induced by perturbing genes on a genomewide scale and thus holds promise to systemically detect gene function and enhance drug development and target discovery,0
in order to associate complex traits with genetic polymorphisms genomewide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms when handling these datasets which exceed the main memory of contemporary computers one faces two distinct challenges 1 millions of polymorphisms and thousands of phenotypes come at the cost of hundreds of gigabytes of data which can only be kept in secondary storage 2 the relatedness of the test population is represented by a relationship matrix which for large populations can only fit in the combined main memory of a distributed architecture in this paper by using distributed resources such as cloud or clusters we address both challenges the genotype and phenotype data is streamed from secondary storage using a double buffer ing technique while the relationship matrix is kept across the main memory of a distributed memory system with the help of these solutions we develop separate algorithms for studies involving only one or a multitude of traits we show that these algorithms sustain highperformance and allow the analysis of enormous datasets,0
reconstruction of gene regulatory networks is the process of identifying gene dependency from gene expression profile through some computation techniques in our human body though all cells pose similar genetic material but the activation state may vary this variation in the activation of genes helps researchers to understand more about the function of the cells researchers get insight about diseases like mental illness infectious disease cancer disease and heart disease from microarray technology etc in this study a cancerspecific gene regulatory network has been constructed using a simple and novel machine learning approach in first step linear regression algorithm provided us the significant genes those expressed themselves differently next regulatory relationships between the identified genes has been computed using pearson correlation coefficient finally the obtained results have been validated with the available databases and literatures we can identify the hub genes and can be targeted for the cancer diagnosis,0
motivation working with a large number of genomes simultaneously is of great interest in genetic population and comparative genomics research bubbles discovery in multigenomes coloured de bruijn graph for de novo genome assembly is a problem that can be translated to cycles enumeration in graph theory cycle enumerations algorithms in big and complex de bruijn graphs are time consuming specialised fast algorithms for efficient bubble search are needed for coloured de bruijn graph variant calling applications in coloured de bruijn graphs bubble paths coverages are used in downstream variants calling analysis results in this paper we introduce a fast parallel graph search for different kmer cycle sizes coloured path coverages are used for snp prediction the graph search method uses a combined multinode and multicore design to speeds up cycles enumeration the search algorithm uses an index extracted from the raw assembly of a coloured de bruijn graph stored in a hash table the index is distributed across different cpucores in a shared memory hpc compute node to build undirected subgraphs then search independently and simultaneously specific cycle sizes this same index can also be split between several hpc compute nodes to take advantage of as many cpucores available to the user the local neighbourhood parallel search approach reduces the graphs complexity and facilitate cycles search of a multicolour de bruijn graph the search algorithm is incorporated into cyc application and tested on a number of schizosaccharomyces pombe genomes availability cyc is an opensource software available at,0
rnas are essential molecules that carry genetic information vital for life with profound implications for drug development and biotechnology despite this importance rna research is often hindered by the vast literature available on the topic to streamline this process we introduce rnagpt a multimodal rna chat model designed to simplify rna discovery by leveraging extensive rna literature rnagpt integrates rna sequence encoders with linear projection layers and stateoftheart large language models llms for precise representation alignment enabling it to process useruploaded rna sequences and deliver concise accurate responses built on a scalable training pipeline rnagpt utilizes rnaqa an automated system that gathers rna annotations from rnacentral using a divideandconquer approach with gpt4o and latent dirichlet allocation lda to efficiently handle large datasets and generate instructiontuning samples our experiments indicate that rnagpt effectively addresses complex rna queries thereby facilitating rna research additionally we present rnaqa a dataset of 407616 rna samples for modality alignment and instruction tuning further advancing the potential of rna research tools,0
genegene interactions have long been recognized to be fundamentally important to understand genetic causes of complex disease traits at present identifying genegene interactions from genomewide casecontrol studies is computationally and methodologically challenging in this paper we introduce a simple but powerful method named boolean operation based screening and testingboost to discover unknown genegene interactions that underlie complex diseases boost allows examining all pairwise interactions in genomewide casecontrol studies in a remarkably fast manner we have carried out interaction analyses on seven data sets from the wellcome trust case control consortium wtccc each analysis took less than 60 hours on a standard 30 ghz desktop with 4g memory running windows xp system the interaction patterns identified from the type 1 diabetes data set display significant difference from those identified from the rheumatoid arthritis data set while both data sets share a very similar hit region in the wtccc report boost has also identified many undiscovered interactions between genes in the major histocompatibility complex mhc region in the type 1 diabetes data set in the coming era of largescale interaction mapping in genomewide casecontrol studies our method can serve as a computationally and statistically useful tool,0
highcontent cellular imaging transcriptomics and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function however the biological determinants through which changes in multiomics measurements influence cellular morphology have not yet been systematically explored and the degree to which cell imaging could potentially enable the prediction of multiomics directly from cell imaging data is therefore currently unclear here we address the question of whether it is possible to predict bulk multiomics measurements directly from cell images using image2omics a deep learning approach that predicts multiomics in a cell population directly from highcontent images of cells stained with multiplexed fluorescent dyes we perform an experimental evaluation in geneedited macrophages derived from human induced pluripotent stem cells hipsc under multiple stimulation conditions and demonstrate that image2omics achieves significantly better performance in predicting transcriptomics and proteomics measurements directly from cell images than predictions based on the mean observed training set abundance we observed significant predictability of abundances for 4927 1872 95 ci 652 3552 and 3521 1338 95 ci 410 3221 transcripts out of 26137 in m1 and m2stimulated macrophages respectively and for 422 846 95 ci 058 2583 and 697 1398 95 ci 241 3283 proteins out of 4986 in m1 and m2stimulated macrophages respectively our results show that some transcript and protein abundances are predictable from cell imaging and that cell imaging may potentially in some settings and depending on the mechanisms of interest and desired performance threshold even be a scalable and resourceefficient substitute for multiomics measurements,0
dna methylation is a wellstudied genetic modification crucial to regulate the functioning of the genome its alterations play an important role in tumorigenesis and tumorsuppression thus studying dna methylation data may help biomarker discovery in cancer since public data on dna methylation become abundant and considering the high number of methylated sites features present in the genome it is important to have a method for efficiently processing such large datasets relying on big data technologies we propose bigbiocl an algorithm that can apply supervised classification methods to datasets with hundreds of thousands of features it is designed for the extraction of alternative and equivalent classification models through iterative deletion of selected features we run experiments on dna methylation datasets extracted from the cancer genome atlas focusing on three tumor types breast kidney and thyroid carcinomas we perform classifications extracting several methylated sites and their associated genes with accurate performance results suggest that bigbiocl can perform hundreds of classification iterations on hundreds of thousands of features in few hours moreover we compare the performance of our method with other stateoftheart classifiers and with a widespread dna methylation analysis method based on network analysis finally we are able to efficiently compute multiple alternative classification models and extract from dnamethylation large datasets a set of candidate genes to be further investigated to determine their active role in cancer bigbiocl results of experiments and a guide to carry on new experiments are freely available on github,0
nextgeneration sequencing techniques have facilitated a large scale analysis of human genetic variation despite the advances in sequencing speeds the computational discovery of structural variants is not yet standard it is likely that many variants have remained undiscovered in most sequenced individuals here we present a novel internal segment size based approach which organizes all including also concordant reads into a read alignment graph where maxcliques represent maximal contradictionfree groups of alignments a specifically engineered algorithm then enumerates all maxcliques and statistically evaluates them for their potential to reflect insertions or deletions indels for the first time in the literature we compare a large range of stateoftheart approaches using simulated illumina reads from a fully annotated genome and present various relevant performance statistics we achieve superior performance rates in particular on indels of sizes 20100 which have been exposed as a current major challenge in the sv discovery literature and where prior insert size based approaches have limitations in that size range we outperform even split read aligners we achieve good results also on real data where we make a substantial amount of correct predictions as the only tool which complement the predictions of splitread aligners clever is open source gpl and available from,0
background elucidating gene regulatory networks is crucial for understanding normal cell physiology and complex pathologic phenotypes existing computational methods for the genomewide reverse engineering of such networks have been successful only for lower eukaryotes with simple genomes here we present aracne a novel algorithm using microarray expression profiles specifically designed to scale up to the complexity of regulatory networks in mammalian cells yet general enough to address a wider range of network deconvolution problems this method uses an information theoretic approach to eliminate the majority of indirect interactions inferred by coexpression methods results we prove that aracne reconstructs the network exactly asymptotically if the effect of loops in the network topology is negligible and we show that the algorithm works well in practice even in the presence of numerous loops and complex topologies we assess aracnes ability to reconstruct transcriptional regulatory networks using both a realistic synthetic dataset and a microarray dataset from human b cells on synthetic datasets aracne achieves very low error rates and outperforms established methods such as relevance networks and bayesian networks application to the deconvolution of genetic networks in human b cells demonstrates aracnes ability to infer validated transcriptional targets of the c myc protooncogene we also study the effects of mis estimation of mutual information on network reconstruction and show that algorithms based on mutual information ranking are more resilient to estimation errors,0
cellular phenotypes are determined by the dynamical activity of networks of coregulated genes elucidating such networks is crucial for the understanding of normal cell physiology as well as for the dissection of complex pathologic phenotypes existing methods for such reverse engineering of genetic networks from microarray expression data have been successful only in prokaryotes e coli and lower eukaryotes s cerevisiae with relatively simple genomes additionally they have mostly attempted to reconstruct average properties about the network connectivity without capturing the highly conditional nature of the interactions in this paper we extend the aracne algorithm which we recently introduced and successfully applied to the reconstruction of wholegenome transcriptional networks from mammalian cells precisely to link the existence of specific network structures to the expression or lack thereof of specific regulator genes this is accomplished by analyzing thousands of alternative network topologies generated by constraining the data set on the presence or absence of putative regulator genes by considering interactions that are consistently supported across several such constraints we identify many transcriptional interactions that would not have been detectable by the original method by selecting genes that produce statistically significant changes in network topology we identify novel candidate regulator genes further analysis shows that transcription factors kinases phosphatases and other gene families known to effect biochemical interactions are significantly overrepresented among the set of candidate regulator genes identified in silico indirectly supporting the validity of the approach,0
influenced by breakthroughs in llms singlecell foundation models are emerging while these models show successful performance in cell type clustering phenotype classification and gene perturbation response prediction it remains to be seen if a simpler model could achieve comparable or better results especially with limited data this is important as the quantity and quality of singlecell data typically fall short of the standards in textual data used for training llms singlecell sequencing often suffers from technical artifacts dropout events and batch effects these challenges are compounded in a weakly supervised setting where the labels of cell states can be noisy further complicating the analysis to tackle these challenges we present scotgm streamlined with less than 500k parameters making it approximately 100x more compact than the foundation models offering an efficient alternative scotgm is an unsupervised model grounded in the inductive bias that the scrnaseq data can be generated from a combination of the finite multivariate gaussian distributions the core function of scotgm is to create a probabilistic latent space utilizing a gmm as its prior distribution and distinguish between distinct cell populations by learning their respective marginal pdfs it uses a hitandrun markov chain sampler to determine the ot plan across these pdfs within the gmm framework we evaluated our model against a crisprmediated perturbation dataset called cropseq consisting of 57 onegene perturbations our results demonstrate that scotgm is effective in cell state classification aids in the analysis of differential gene expression and ranks genes for target identification through a recommender system it also predicts the effects of singlegene perturbations on downstream gene regulation and generates synthetic scrnaseq data conditioned on specific cell states,0
gene translation is the process in which intracellular macromolecules called ribosomes decode genetic information in the mrna chain into the corresponding proteins gene translation includes several steps during the elongation step ribosomes move along the mrna in a sequential manner and link aminoacids together in the corresponding order to produce the proteins the homogeneous ribosome flow modelhrfm is a deterministic computational model for translationelongation under the assumption of constant elongation rates along the mrna chain the hrfm is described by a set of n firstorder nonlinear ordinary differential equations where n represents the number of sites along the mrna chain the hrfm also includes two positive parameters ribosomal initiation rate and the constant elongation rate in this paper we show that the steadystate translation rate in the hrfm is a concave function of its parameters this means that the problem of determining the parameter values that maximize the translation rate is relatively simple our results may contribute to a better understanding of the mechanisms and evolution of translationelongation we demonstrate this by using the theoretical results to estimate the initiation rate in m musculus embryonic stem cell the underlying assumption is that evolution optimized the translation mechanism for the infinitedimensional hrfm we derive a closedform solution to the problem of determining the initiation and transition rates that maximize the protein translation rate we show that these expressions provide good approximations for the optimal values in the ndimensional hrfm already for relatively small values of n these results may have applications for synthetic biology where an important problem is to reengineer genomic systems in order to maximize the protein production rate,0
how a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology the cells grow divide and acquire differentiated characteristics through poorly understood molecular processes a key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories providing an analytical framework for dissecting individual cells molecular decisions during replication and differentiation although genetically engineered lineagetracing methods have advanced the field they are either infeasible or ethically constrained in many organisms in contrast modern singlecell technologies can measure highcontent molecular profiles eg transcriptomes in a wide range of biological systems here we introduce celltreeqm a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for treegraph inference by formulating lineage reconstruction as a treemetric learning problem we have systematically explored supervised weakly supervised and unsupervised training settings and present a lineage reconstruction benchmark to facilitate comprehensive evaluation of our learning method we benchmarked the method on 1 synthetic data modeled via brownian motion with independent noise and spurious signals and 2 lineageresolved singlecell rna sequencing datasets experimental results show that celltreeqm recovers lineage structures with minimal supervision and limited data offering a scalable framework for uncovering cell lineage relationships in challenging animal models to our knowledge this is the first method to cast cell lineage inference explicitly as a metric learning task paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage,0
socioeconomic constructs and urban topology are crucial drivers of human mobility patterns during the coronavirus disease 2019 pandemic these patterns were reshaped in their components the spatial dimension represented by the daily travelled distance and the temporal dimension expressed as the synchronization time of commuting routines here leveraging locationbased data from deidentified mobile phone users we observed that during lockdowns restrictions the decrease of spatial mobility is interwoven with the emergence of asynchronous mobility dynamics the lifting of restriction in urban mobility allowed a faster recovery of the spatial dimension compared with the temporal one moreover the recovery in mobility was different depending on urbanization levels and economic stratification in rural and lowincome areas the spatial mobility dimension suffered a more considerable disruption when compared with urbanized and highincome areas in contrast the temporal dimension was more affected in urbanized and highincome areas than in rural and lowincome areas,0
modeling and predicting human mobility trajectories in urban areas is an essential task for various applications the recent availability of largescale human movement data collected from mobile devices have enabled the development of complex human mobility prediction models however human mobility prediction methods are often trained and tested on different datasets due to the lack of opensource largescale human mobility datasets amid privacy concerns posing a challenge towards conducting fair performance comparisons between methods to this end we created an opensource anonymized metropolitan scale and longitudinal 90 days dataset of 100000 individuals human mobility trajectories using mobile phone location data the location pings are spatially and temporally discretized and the metropolitan area is undisclosed to protect users privacy the 90day period is composed of 75 days of businessasusual and 15 days during an emergency to promote the use of the dataset we will host a human mobility prediction data challenge humob challenge 2023 using the human mobility dataset which will be held in conjunction with acm sigspatial 2023,0
despite their importance for urban planning traffic forecasting and the spread of biological and mobile viruses our understanding of the basic laws governing human motion remains limited thanks to the lack of tools to monitor the time resolved location of individuals here we study the trajectory of 100000 anonymized mobile phone users whose position is tracked for a six month period we find that in contrast with the random trajectories predicted by the prevailing levy flight and random walk models human trajectories show a high degree of temporal and spatial regularity each individual being characterized by a time independent characteristic length scale and a significant probability to return to a few highly frequented locations after correcting for differences in travel distances and the inherent anisotropy of each trajectory the individual travel patterns collapse into a single spatial probability distribution indicating that despite the diversity of their travel history humans follow simple reproducible patterns this inherent similarity in travel patterns could impact all phenomena driven by human mobility from epidemic prevention to emergency response urban planning and agent based modeling,0
human mobility is subject to collective dynamics that are the outcome of numerous individual choices smart card data which originated as a means of facilitating automated fare collections has emerged as an invaluable source for analyzing human mobility patterns a variety of clustering and segmentation techniques has been adopted and adapted for applications ranging from passenger demand market segmentation to the analysis of urban activity locations in this paper we provide a systematic review of the stateoftheart on clustering public transport users based on their temporal or spatialtemporal characteristics as well as studies that use the patter to characterize individual stations lines or urban areas furthermore a critical review of the literature reveals an important distinction between studies focusing on the intrapersonal variability of travel patterns versus those concerned with the interpersonal variability of travel patterns we synthesize the key analysis approaches and based on which identify and outline the following directions for further research i predictions of passenger travel patterns ii decision support for service planning and policy evaluation iii enhanced geographical characterization of users travel patterns iv from demand analytics towards behavioral analytics,0
this research introduces a mathematical framework to comprehending human mobility patterns integrating mathematical modeling and economic analysis the study focuses on latentvariable networks investigating the dynamics of human mobility using stochastic models by examining actual origindestination data the research reveals scaling relations and uncovers the economic implications of mobility patterns such as the income elasticity of travel demand the mathematical analysis commences with the development of a stochastic model based on inhomogeneous random graphs to construct a visitation model with multipurpose drivers for travel demand a directed multigraph with weighted edges is considered incorporating trip costs and labels to represent factors like distance traveled and travel time the study gains insights into the structural properties and dynamic correlations of human mobility networks to derive analytical and computational solutions for key network metrics including scalefree behavior of the strength and degree distribution together with the estimation of assortativity and clustering coefficient additionally the models validity is assessed through a realworld case study of the new york metropolitan area the analysis of this data exposes clear scaling relations in commuting patterns confirming theoretical predictions and validating the efficacy of the mathematical model the model further explains a series of scaling behaviors in origindestination flows among areas of a region successfully reproducing statistical regularities observed in realworld cases using extensive human mobility datasets in particular the models application to estimating income elasticity of travel demand bears significant implications for urban and transport economics,0
human mobility is an important characteristic of human behavior but since tracking personalized position to high temporal and spatial resolution is difficult most studies on human mobility patterns rely largely on mathematical models seminal models which assume frequently visited locations tend to be revisited reproduce a wide range of statistical features including collective mobility fluxes and numerous scaling laws however these models cannot be verified at a timescale relevant to our daily travel patterns as most available data do not provide the necessary temporal resolution in this work we reexamined human mobility mechanisms via comprehensive cellphone position data recorded at a high frequency up to every second we found that the next location visited by users is not their most frequently visited ones in many cases instead individuals exhibit origindependent pathpreferential patterns in their short timescale mobility these behaviors are prominent when the temporal resolution of the data is high and are thus overlooked in most previous studies incorporating measured quantities from our high frequency data into conventional human mobility models shows contradictory statistical results we finally revealed that the individual preferential transition mechanism characterized by the firstorder markov process can quantitatively reproduce the observed travel patterns at both individual and population levels at all relevant timescales,0
outbreaks of infectious diseases present a global threat to human health and are considered a major healthcare challenge one major driver for the rapid spatial spread of diseases is human mobility in particular the travel patterns of individuals determine their spreading potential to a great extent these travel behaviors can be captured and modelled using novel locationbased data sources eg smart travel cards social media etc previous studies have shown that individuals who cannot be characterized by their most frequently visited locations spread diseases farther and faster however these studies are based on gps data and mobile call records which have position uncertainty and do not capture explicit contacts it is unclear if the same conclusions hold for large scale realworld transport networks in this paper we investigate how mobility patterns impact disease spread in a largescale public transit network of empirical data traces in contrast to previous findings our results reveal that individuals with mobility patterns characterized by their most frequently visited locations and who typically travel large distances pose the highest spreading risk,0
in this paper we present a threestep methodological framework including location identification bias modification and outofsample validation so as to promote human mobility analysis with social media data more specifically we propose ways of identifying personal activityspecific places and commuting patterns in beijing china based on weibo chinas twitter checkin records as well as modifying sample bias of checkin data with population synthesis technique an independent citywide travel logistic survey is used as the benchmark for validating the results obvious differences are discerned from weibo users and survey respondents activitymobility patterns while there is a large variation of population representativeness between data from the two sources after bias modification the similarity coefficient between commuting distance distributions of weibo data and survey observations increases substantially from 23 to 63 synthetic data proves to be a satisfactory costeffective alternative source of mobility information the proposed framework can inform many applications related to human mobility ranging from transportation through urban planning to transport emission modelling,0
a range of early studies have been conducted to illustrate human mobility patterns using different tracking data such as dollar notes cell phones and taxicabs here we explore human mobility patterns based on massive tracking data of us flights both topological and geometric properties are examined in detail we found that topological properties such as traffic volume between airports and degree of connectivity of individual airports including both in and outdegrees follow a power law distribution but not a geometric property like travel lengths the travel lengths exhibit an exponential distribution rather than a power law with an exponential cutoff as previous studies illustrated we further simulated human mobility on the established topologies of airports with various moving behaviors and found that the mobility patterns are mainly attributed to the underlying binary topology of airports and have little to do with other factors such as moving behaviors and geometric distances apart from the above findings this study adopts the headtail division rule which is regularity behind any heavytailed distribution for extracting individual airports the adoption of this rule for data processing constitutes another major contribution of this paper keywords scaling of geographic space headtail division rule power law geographic information agentbased simulations,0
human mobility is a key component of largescale spatialtransmission models of infectious diseases correctly modeling and quantifying human mobility is critical for improving epidemic control policies but may be hindered by incomplete data in some regions of the world here we explore the opportunity of using proxy data or models for individual mobility to describe commuting movements and predict the diffusion of infectious disease we consider three european countries and the corresponding commuting networks at different resolution scales obtained from official census surveys from proxy data for human mobility extracted from mobile phone call records and from the radiation model calibrated with census data metapopulation models defined on the three countries and integrating the different mobility layers are compared in terms of epidemic observables we show that commuting networks from mobile phone data well capture the empirical commuting patterns accounting for more than 87 of the total fluxes the distributions of commuting fluxes per link from both sources of data mobile phones and census are similar and highly correlated however a systematic overestimation of commuting traffic in the mobile phone data is observed this leads to epidemics that spread faster than on census commuting networks however preserving the order of infection of newly infected locations match in the epidemic invasion pattern is sensitive to initial conditions the radiation model shows higher accuracy with respect to mobile phone data when the seed is central in the network while the mobile phone proxy performs better for epidemics seeded in peripheral locations results suggest that different proxies can be used to approximate commuting patterns across different resolution scales in spatial epidemic simulations in light of the desired accuracy in the epidemic outcome under study,0
a human is a thing that moves in space like all things that move in space we can in principle use differential equations to describe their motion as a set of functions that maps time to position and velocity acceleration and so on with inanimate objects we can reliably predict their trajectories by using differential equations that account for up to the secondorder time derivative of their position as is commonly done in analytical mechanics with animate objects though and with humans in particular we do not know the cardinality of the set of equations that define their trajectory we may be tempted to think for example that by reason of their complexity in cognition or behaviour as compared to say a rock then the motion of humans requires a more complex description than the one generally used to describe the motion of physical systems in this paper we examine a realworld dataset on human mobility and consider the information that is added by each computed but denoised additional time derivative and find the maximum order of derivatives of the position that for that particular dataset cannot be expressed as a linear transformation of the previous in this manner we identify the dimensionality of a minimal model that correctly describes the observed trajectories we find that every higherorder derivative after the acceleration is linearly dependent upon one of the previous timederivatives this measure is robust against noise and the choice for differentiation techniques that we use to compute the timederivatives numerically as a function of the measured position this result imposes empirical constraints on the possible sets of differential equations that can be used to describe the kinematics of a moving human,0
getting insights on human mobility patterns and being able to reproduce them accurately is of the utmost importance in a wide range of applications from public health to transport and urban planning still the relationship between the effort individuals will invest in a trip and its purpose importance is not taken into account in the individual mobility models that can be found in the recent literature here we address this issue by introducing a model hypothesizing a relation between the importance of a trip and the distance traveled in most practical cases quantifying such importance is undoable we overcome this difficulty by focusing on shopping trips for which we have empirical data and by taking the price of items as a proxy our model is able to reproduce the longtailed distribution in travel distances empirically observed and to explain the scaling relationship between distance traveled and item value found in the data,0
understanding the patterns of mobility of individuals is crucial for a number of reasons from city planning to disaster management there are two common ways of quantifying the amount of travel between locations by direct observations that often involve privacy issues eg tracking mobile phone locations or by estimations from models typically such models build on accurate knowledge of the population size at each location however when this information is not readily available their applicability is rather limited as mobile phones are ubiquitous our aim is to investigate if mobility patterns can be inferred from aggregated mobile phone call data alone using data released by orange for ivory coast we show that human mobility is well predicted by a simple model based on the frequency of mobile phone calls between two locations and their geographical distance we argue that the strength of the model comes from directly incorporating the social dimension of mobility furthermore as only aggregated call data is required the model helps to avoid potential privacy problems,0
from the perspective of human mobility the covid19 pandemic constituted a natural experiment of enormous reach in space and time here we analyse the inherent multiple scales of human mobility using facebook movement maps collected before and during the first uk lockdown first we obtain the prelockdown uk mobility graph and employ multiscale community detection to extract in an unsupervised manner a set of robust partitions into flow communities at different levels of coarseness the partitions so obtained capture intrinsic mobility scales with better coverage than nuts regions which suffer from mismatches between human mobility and administrative divisions furthermore the flow communities in the fine scale partition match well the uk travel to work areas ttwas but also capture mobility patterns beyond commuting to work we also examine the evolution of mobility under lockdown and show that mobility first reverted towards fine scale flow communities already found in the prelockdown data and then expanded back towards coarser flow communities as restrictions were lifted the improved coverage induced by lockdown is well captured by a linear decay shock model which allows us to quantify regional differences both in the strength of the effect and the recovery time from the lockdown shock,0
although a lot of attentions have been paid to human mobility the relationship between travel pattern with city structure is still unclear here we probe into this relationship by analyzing the metro passenger trip datathere are two unprecedented findings one from the average view a linear law exists between the individuals travel distance with his original distance to city center the mechanism underlying is a travel pattern we called ring aggregation ie the daily movement of city passengers is just aggregating to a ring with roughly equal distance to city centerinterestingly for the round trips the daily travel pattern can be regarded as a switching between the home ring at outer area with the office ring at the inner area second this linear law and ring aggregation pattern seems to be an exclusive characteristic of the metro system it can not be found in short distance transportation modes such as bicycle and taxi neither as multiple transportation modes this means the ring aggregation pattern is a token of the relationship between travel pattern with city structure in the large scale space,0
uncovering human mobility patterns is of fundamental importance to the understanding of epidemic spreading urban transportation and other socioeconomic dynamics embodying spatiality and human travel according to the direct travel diaries of volunteers we show the absence of scaling properties in the displacement distribution at the individual levelwhile the aggregated displacement distribution follows a power law with an exponential cutoff given the constraint on total travelling cost this aggregated scaling law can be analytically predicted by the mixture nature of human travel under the principle of maximum entropy a direct corollary of such theory is that the displacement distribution of a single mode of transportation should follow an exponential law which also gets supportive evidences in known data we thus conclude that the travelling cost shapes the displacement distribution at the aggregated level,0
despite the long history of modelling human mobility we continue to lack a highly accurate approach with low data requirements for predicting mobility patterns in cities here we present a populationweighted opportunities model without any adjustable parameters to capture the underlying driving force accounting for human mobility patterns at the city scale we use various mobility data collected from a number of cities with different characteristics to demonstrate the predictive power of our model we find that insofar as the spatial distribution of population is available our model offers universal prediction of mobility patterns in good agreement with real observations including distance distribution destination travel constraints and flux in contrast the models that succeed in modelling mobility patterns in countries are not applicable in cities which suggests that there is a diversity of human mobility at different spatial scales our model has potential applications in many fields relevant to mobility behaviour in cities without relying on previous mobility measurements,0
understanding human mobility is crucial for urban and transport studies in cities peoples daily activities provide valuable insight such as where people live work shop leisure or eat during midday or afterwork hours however such activities are changed due to travel behaviours after covid19 in cities this study examines the mobility patterns captured from mobile phone apps to explore the behavioural patterns established since the covid19 lockdowns triggered a series of changes in urban environments,0
human mobility has been empirically observed to exhibit levy flight characteristics and behaviour with powerlaw distributed jump size the fundamental mechanisms behind this behaviour has not yet been fully explained in this paper we analyze urban human mobility and we propose to explain the levy walk behaviour observed in human mobility patterns by decomposing them into different classes according to the different transportation modes such as walkrun bicycle trainsubway or cartaxibus our analysis is based on two reallife gps datasets containing approximately 10 and 20 million gps samples with transportation mode information we show that human mobility can be modelled as a mixture of different transportation modes and that these single movement patterns can be approximated by a lognormal distribution rather than a powerlaw distribution then we demonstrate that the mixture of the decomposed lognormal flight distributions associated with each modality is a powerlaw distribution providing an explanation to the emergence of levy walk patterns that characterize human mobility patterns,0
the metaverse promises a shift in the way humans interact with each other and with their digital and physical environments the lack of geographical boundaries and travel costs in the metaverse prompts us to ask if the fundamental laws that govern human mobility in the physical world apply we collected data on avatar movements along with their network mobility extracted from nft purchases we find that despite the absence of commuting costs an individuals inclination to explore new locations diminishes over time limiting movement to a small fraction of the metaverse we also find a lack of correlation between land prices and visitation a deviation from the patterns characterizing the physical world finally we identify the scaling laws that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics rather than the nature of space and cost of movement,0
the constrained outbreak of covid19 in mainland china has recently been regarded as a successful example of fighting this highly contagious virus both the short period in about three months of transmission and the subexponential increase of confirmed cases in mainland china have proved that the chinese authorities took effective epidemic prevention measures such as case isolation travel restrictions closing recreational venues and banning public gatherings these measures can of course effectively control the spread of the covid19 pandemic meanwhile they may dramatically change the human mobility patterns such as the daily transportationrelated behaviors of the public to better understand the impact of covid19 on transportationrelated behaviors and to provide more targeted antiepidemic measures we use the huge amount of human mobility data collected from baidu maps a widelyused web mapping service in china to look into the detail reaction of the people there during the pandemic to be specific we conduct datadriven analysis on transportationrelated behaviors during the pandemic from the perspectives of 1 means of transportation 2 type of visited venues 3 checkin time of venues 4 preference on origindestination distance and 5 origintransportationdestination patterns for each topic we also give our specific insights and policymaking suggestions given that the covid19 pandemic is still spreading in more than 200 countries and territories worldwide infecting millions of people the insights and suggestions provided here may help fight covid19,0
this article proposes a method to quantitatively measure the resilience of transportation systems using gps data from taxis the granularity of the gps data necessary for this analysis is relatively coarse it only requires coordinates for the beginning and end of trips the metered distance and the total travel time the method works by computing the historical distribution of pace normalized travel times between various regions of a city and measuring the pace deviations during an unusual event this method is applied to a dataset of nearly 700 million taxi trips in new york city which is used to analyze the transportation infrastructure resilience to hurricane sandy the analysis indicates that hurricane sandy impacted traffic conditions for more than five days and caused a peak delay of two minutes per mile practically it identifies that the evacuation caused only minor disruptions but significant delays were encountered during the postdisaster reentry process since the implementation of this method is very efficient it could potentially be used as an online monitoring tool representing a first step toward quantifying city scale resilience with coarse gps data,0
human mobility patterns are surprisingly structured in spite of many hard to model factors such as climate culture and socioeconomic opportunities aggregate migration rates obey a universal parameterfree radiation model recent work has further shown that the detailed spectral decomposition of these flows defined as the number of individuals that visit a given location with frequency f from a distance r away also obeys simple rules namely scaling as a universal inverse square law in the combination rf however this surprising regularity derived on general grounds has not been explained through microscopic mechanisms of individual behavior here we confirm this by analyzing largescale cell phone datasets from three distinct regions and show that a direct consequence of this scaling law is that the average travel energy spent by visitors to a given location is constant across space a finding reminiscent of the wellknown travel budget hypothesis of human movement the attractivity of different locations which we define by the total number of visits to that location also admits nontrivial spatiallyclustered structure the observed pattern is consistent with the wellknown central place theory in urban geography as well as with the notion of weber optimality in spatial economy hinting to a collective human capacity of optimizing recurrent movements we close by proposing a simple microscopic human mobility model which simultaneously captures all our empirical findings our results have relevance for transportation urban planning geography and other disciplines in which a deeper understanding of aggregate human mobility is key,0
understanding and modeling human mobility is central to challenges in transport planning sustainable urban design and public health despite decades of effort simulating individual mobility remains challenging because of its complex contextdependent and exploratory nature here we present mobilitygen a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales by linking behavioral attributes with environmental context mobilitygen reproduces key patterns such as scaling laws for location visits activity time allocation and the coupled evolution of travel mode and destination choices it reflects spatiotemporal variability and generates diverse plausible and novel mobility patterns consistent with the built environment beyond standard validation mobilitygen yields insights not attainable with earlier models including how access to urban space varies across travel modes and how copresence dynamics shape social exposure and segregation our work establishes a new framework for mobility simulation paving the way for finegrained datadriven studies of human behavior and its societal implications,0
understanding human mobility is critical for decision support in areas from urban planning to infectious diseases control prior work has focused on tracking daily logs of outdoor mobility without considering relevant context which contain a mixture of regular and irregular human movement for a range of purposes and thus diverse effects on the dynamics have been ignored this study aims to focus on irregular human movement of different metapopulations with various purposes we propose approaches to estimate the predictability of mobility in different contexts with our survey data from international and domestic visitors to australia we found that the travel patterns of europeans visiting for holidays are less predictable than those visiting for education while east asian visitors show the opposite patterns ie more predictable for holidays than for education domestic residents from the most populous australian states exhibit the most unpredictable patterns while visitors from less populated states show the highest predictable movement,0
the individual movements of large numbers of people are important in many contexts from urban planning to disease spreading datasets that capture human mobility are now available and many interesting features have been discovered including the ultraslow spatial growth of individual mobility however the detailed substructures and spatiotemporal flows of mobility the sets and sequences of visited locations have not been well studied we show that individual mobility is dominated by small groups of frequently visited dynamically close locations forming primary habitats capturing typical daily activity along with subsidiary habitats representing additional travel these habitats do not correspond to typical contexts such as home or work the temporal evolution of mobility within habitats which constitutes most motion is universal across habitats and exhibits scaling patterns both distinct from all previous observations and unpredicted by current models the delay to enter subsidiary habitats is a primary factor in the spatiotemporal growth of human travel interestingly habitats correlate with nonmobility dynamics such as communication activity implying that habitats may influence processes such as information spreading and revealing new connections between human mobility and social networks,0
the study of human mobility patterns is of both theoretical and practical values in many aspects for longdistance travels a few research endeavors have shown that the displacements of human travels follow the powerlaw distribution however controversies remain in the issue of the scaling law of human mobility in intraurban areas in this work we focus on the mobility pattern of taxi passengers by examining five datasets of the three metropolitans of new york dalian and nanjing through statistical analysis we find that the lognormal distribution with a powerlaw tail can best approximate both the displacement and the duration time of taxi trips as well as the vacant time of taxicabs in all the examined cities the universality of scaling law of human mobility is subsequently discussed in accordance with the data analytics,0
public stakeholders implement several policies and regulations to tackle gender gaps fostering the change in the cultural constructs associated with gender one way to quantify if such changes elicit gender equality is by studying mobility in this work we study the daily mobility patterns of women and men occurring in medelln colombia in two years 2005 and 2017 specifically we focus on the spatiotemporal differences in the travels and find that purpose of travel and occupation characterise each gender differently we show that women tend to make shorter trips corroborating ravensteins laws of migration our results indicate that urban mobility in colombia seems to behave in agreement with the archetypal case studied by ravenstein,0
understanding human mobility is crucial for a broad range of applications from disease prediction to communication networks most efforts on studying human mobility have so far used private and low resolution data such as call data records here we propose twitter as a proxy for human mobility as it relies on publicly available data and provides high resolution positioning when users opt to geotag their tweets with their current location we analyse a twitter dataset with more than six million geotagged tweets posted in australia and we demonstrate that twitter can be a reliable source for studying human mobility patterns our analysis shows that geotagged tweets can capture rich features of human mobility such as the diversity of movement orbits among individuals and of movements within and between cities we also find that short and longdistance movers both spend most of their time in large metropolitan areas in contrast with intermediatedistance movers movements reflecting the impact of different modes of travel our study provides solid evidence that twitter can indeed be a useful proxy for tracking and predicting human movement,0
bikes are among the healthiest greenest and most affordable means of transportation for a better future city but mobility patterns of riders with different income were rarely studied due to limitations on collecting data newly emergent dockless bikesharing platforms that record detailed information regarding each trip provide us a unique opportunity attribute to its better usage flexibility and accessibility dockless bikesharing platforms are booming over the past a few years worldwide and reviving the riding fashion in cities in this work by exploiting massive riding records in two megacities from a dockless bikesharing platform we reveal that individual mobility patterns including radius of gyration and average travel distance are similar among users with different income which indicates that human beings all follow similar physical rules however collective mobility patterns including average range and diversity of visitation and commuting directions all exhibit different behaviors and spatial patterns across income categories hotspot locations that attract more cycling activities are quite different over groups and locations where users reside are of a low user ratio for both higher and lower income groups lower income groups are inclined to visit less flourishing locations and commute towards the direction to the city center in both cities and of a smaller mobility diversity in beijing but a larger diversity in shanghai in addition differences on mobility patterns among socioeconomic categories are more evident in beijing than in shanghai our findings would be helpful on designing better promotion strategies for dockless bikesharing platforms and towards the transition to a more sustainable green transportation,0
human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns the growing accessibility to ict information from portable digital media has recently opened the possibility of exploring human behavior at high spatiotemporal resolutions mobile phone records geolocated tweets checkins from foursquare or geotagged photos have contributed to this purpose at different scales from cities to countries in different world areas many previous works lacked however details on the individuals attributes such as age or gender in this work we analyze creditcard records from barcelona and madrid and by examining the geolocated creditcard transactions of individuals living in the two provinces we find that the mobility patterns vary according to gender age and occupation differences in distance traveled and travel purpose are observed between younger and older people but curiously either between males and females of similar age while mobility displays some generic features here we show that sociodemographic characteristics play a relevant role and must be taken into account for mobility and epidemiological modelization,0
it is very important to understand urban mobility patterns because most trips are concentrated in urban areas in the paper a new model is proposed to model collective human mobility in urban areas the model can be applied to predict individual flows not only in intracity but also in countries or a larger range based on the model it can be concluded that the exponential law of distance distribution is attributed to decreasing exponentially of average density of human travel demands since the distribution of human travel demands only depends on urban planning population distribution regional functions and so on it illustrates that these inherent properties of cities are impetus to drive collective human movements,0
as a significant factor in urban planning traffic forecasting and prediction of epidemics modeling patterns of human mobility draws intensive attention from researchers for decades powerlaw distribution and its variations are observed from quite a few realworld human mobility datasets such as the movements of banking notes trackings of cell phone users locations and trajectories of vehicles in this paper we build models for 20 million trajectories with fine granularity collected from more than 10 thousand taxis in beijing in contrast to most models observed in human mobility data the taxis traveling displacements in urban areas tend to follow an exponential distribution instead of a powerlaw similarly the elapsed time can also be well approximated by an exponential distribution worth mentioning analysis of the interevent time indicates the bursty nature of human mobility similar to many other human activities,0
in the advent of a pervasive presence of location sharing services researchers gained an unprecedented access to the direct records of human activity in space and time this paper analyses geolocated twitter messages in order to uncover global patterns of human mobility based on a dataset of almost a billion tweets recorded in 2012 we estimate volumes of international travelers in respect to their country of residence we examine mobility profiles of different nations looking at the characteristics such as mobility rate radius of gyration diversity of destinations and a balance of the inflows and outflows the temporal patterns disclose the universal seasons of increased international mobility and the peculiar national nature of overseen travels our analysis of the community structure of the twitter mobility network obtained with the iterative network partitioning reveals spatially cohesive regions that follow the regional division of the world finally we validate our result with the global tourism statistics and mobility models provided by other authors and argue that twitter is a viable source to understand and quantify global mobility patterns,0
this study leverages mobile phone data to analyze human mobility patterns in developing countries especially in comparison to more industrialized countries developing regions such as the ivory coast are marked by a number of factors that may influence mobility such as less infrastructural coverage and maturity less economic resources and stability and in some cases more cultural and languagebased diversity by comparing mobile phone data collected from the ivory coast to similar data collected in portugal we are able to highlight both qualitative and quantitative differences in mobility patterns such as differences in likelihood to travel as well as in the time required to travel that are relevant to consideration on policy infrastructure and economic development our study illustrates how cultural and linguistic diversity in developing regions such as ivory coast can present challenges to mobility models that perform well and were conceptualized in less culturally diverse regions finally we address these challenges by proposing novel techniques to assess the strength of borders in a regional partitioning scheme and to quantify the impact of border strength on mobility model accuracy,0
mobile phone data has enabled the timely and finegrained study human mobility call detail records generated at call events allow building descriptions of mobility at different resolutions and with different spatial temporal and social granularity individual trajectories are the basis for longterm observation of mobility patterns and identify factors of human dynamics here we propose a systematic analysis to characterize mobility network flows and topology and assess their impact into individual traces discrete flowbased descriptors are used to classify and understand human mobility patterns at multiple scales this framework is suitable to assess urban planning optimize transportation measure the impact of external events and conditions monitor internal dynamics and profile users according to their movement patterns,0
the gravity model of human mobility has successfully described the deterrence of travels with distance in urban mobility patterns while a broad spectrum of deterrence was found across different cities yet it is not empirically clear if movement patterns in a single city could also have a spectrum of distance exponents denoting a varying deterrence depending on the origin and destination regions in the city by analyzing the travel data in the twelve most populated cities of the united states of america we empirically find that the distance exponent governing the deterrence of travels significantly varies within a city depending on the traffic volumes of the origin and destination regions despite the diverse traffic landscape of the cities analyzed a common pattern is observed for the distance exponents the exponent value tends to be higher between regions with larger traffic volumes while it tends to be lower between regions with smaller traffic volumes this indicates that our method indeed reveals the hidden diversity of gravity laws that would be overlooked otherwise,0
predicting human mobility flows at different spatial scales is challenged by the heterogeneity of individual trajectories and the multiscale nature of transportation networks as vast amounts of digital traces of human behaviour become available an opportunity arises to improve mobility models by integrating into them proxy data on mobility collected by a variety of digital platforms and locationaware services here we propose a hybrid model of human mobility that integrates a largescale publicly available dataset from a popular photosharing system with the classical gravity model under a stacked regression procedure we validate the performance and generalizability of our approach using two groundtruth datasets on air travel and daily commuting in the united states using two different crossvalidation schemes we show that the hybrid model affords enhanced mobility prediction at both spatial scales,0
human mobility has attracted attentions from different fields of studies such as epidemic modeling traffic engineering traffic prediction and urban planning in this survey we review major characteristics of human mobility studies including from trajectorybased studies to studies using graph and network theory in trajectorybased studies statistical measures such as jump length distribution and radius of gyration are analyzed in order to investigate how people move in their daily life and if it is possible to model this individual movements and make prediction based on them using graph in mobility studies helps to investigate the dynamic behavior of the system such as diffusion and flow in the network and makes it easier to estimate how much one part of the network influences another by using metrics like centrality measures we aim to study population flow in transportation networks using mobility data to derive models and patterns and to develop new applications in predicting phenomena such as congestion human mobility studies with the new generation of mobility data provided by cellular phone networks arise new challenges such as data storing data representation data analysis and computation complexity a comparative review of different data types used in current tools and applications of human mobility studies leads us to new approaches for dealing with mentioned challenges,0
there is a contradiction at the heart of our current understanding of individual and collective mobility patterns on one hand a highly influential stream of literature on human mobility driven by analyses of massive empirical datasets finds that human movements show no evidence of characteristic spatial scales there human mobility is described as scalefree on the other hand in geography the concept of scale referring to meaningful levels of description from individual buildings through neighborhoods cities regions and countries is central for the description of various aspects of human behavior such as socioeconomic interactions or political and cultural dynamics here we resolve this apparent paradox by showing that daytoday human mobility does indeed contain meaningful scales corresponding to spatial containers restricting mobility behavior the scalefree results arise from aggregating displacements across containers we present a simple model which given a persons trajectory infers their neighborhoods cities and so on as well as the sizes of these geographical containers we find that the containers characterizing the trajectories of more than 700000 individuals do indeed have typical sizes we show that our model generates highly realistic trajectories without overfitting and provides a new lens through which to understand the differences in mobility behaviour across countries gender groups and urbanrural areas,0
predicting human mobility between locations has practical applications in transportation science spatial economics sociology and many other fields for more than 100 years many human mobility prediction models have been proposed among which the gravity model analogous to newtons law of gravitation is widely used another classical model is the intervening opportunity io model which indicates that an individual selecting a destination is related to both the destinations opportunities and the intervening opportunities between the origin and the destination the io model established from the perspective of individual selection behavior has recently triggered the establishment of many new io class models although these io class models can achieve accurate prediction at specific spatiotemporal scales an io class model that can describe an individuals destination selection behavior at different spatiotemporal scales is still lacking here we develop a universal opportunity model that considers two human behavioral tendencies one is the exploratory tendency and the other is the cautious tendency our model establishes a new framework in io class models and covers the classical radiation model and opportunity priority selection model furthermore we use various mobility data to demonstrate our models predictive ability the results show that our model can better predict human mobility than previous io class models moreover this model can help us better understand the underlying mechanism of the individuals destination selection behavior in different types of human mobility,0
the description of complex human mobility patterns is at the core of many important applications ranging from urbanism and transportation to epidemics containment data about collective human movements once scarce has become widely available thanks to new sources such as phone cdr gps devices or smartphone apps nevertheless it is still common to rely on a single dataset by implicitly assuming that it is a valid instance of universal dynamics regardless of factors such as data gathering and processing techniques here we test such an overarching assumption on an unprecedented scale by comparing human mobility datasets obtained from 7 different datasources tracing over 500 millions individuals in 145 countries we report wide quantifiable differences in the resulting mobility networks and in particular in the displacement distribution previously thought to be universal these variations that do not necessarily imply that the human mobility is not universal also impact processes taking place on these networks as we show for the specific case of epidemic spreading our results point to the crucial need for disclosing the data processing and overall to follow good practices to ensure the robustness and the reproducibility of the results,0
human mobility is investigated using a continuum approach that allows to calculate the probability to observe a trip to anyarbitrary region and the fluxes between any two regions the considered description offers a general and unified framework in which previously proposed mobility models like the gravity model the intervening opportunities model and the recently introduced radiation model are naturally resulting as special cases a new form of radiation model is derived and its validity is investigated using observational data offered by commuting trips obtained from the united states census data set and the mobility fluxesextracted from mobile phone data collected in a western european country the new modeling paradigm offered by this description suggests that the complex topological features observed in large mobility and transportation networks may be the result of a simple stochastic process taking place on an inhomogeneous landscape,0
the recent availability of large databases allows to study macroscopic properties of many complex systems however inferring a model from a fit of empirical data without any knowledge of the dynamics might lead to erroneous interpretations we illustrate this in the case of human mobility and foraging human patterns where empirical longtailed distributions of jump sizes have been associated to scalefree superdiffusive random walks called lvy flights here we introduce a new class of accelerated random walks where the velocity changes due to acceleration kicks at random times which combined with a peaked distribution of travel times displays a jump length distribution that could easily be misinterpreted as a truncated power law but that is not governed by large fluctuations this stochastic model allows us to explain empirical observations about the movements of 780000 private vehicles in italy and more generally to get a deeper quantitative understanding of human mobility,0
largescale human mobility data is a key resource in datadriven policy making and across many scientific fields most recently mobility data was extensively used during the covid19 pandemic to study the effects of governmental policies and to inform epidemic models largescale mobility is often measured using digital tools such as mobile phones however it remains an open question how truthfully these digital proxies represent the actual travel behavior of the general population here we examine mobility datasets from multiple countries and identify two fundamentally different types of bias caused by unequal access to and unequal usage of mobile phones we introduce the concept of data generation bias a previously overlooked type of bias which is present when the amount of data that an individual produces influences their representation in the dataset we find evidence for data generation bias in all examined datasets in that highwealth individuals are overrepresented with the richest 20 contributing over 50 of all recorded trips substantially skewing the datasets this inequality is consequential as we find mobility patterns of different wealth groups to be structurally different where the mobility networks of highwealth users are denser and contain more longrange connections to mitigate the skew we present a framework to debias data and show how simple techniques can be used to increase representativeness using our approach we show how biases can severely impact outcomes of dynamic processes such as epidemic simulations where biased data incorrectly estimates the severity and speed of disease transmission overall we show that a failure to account for biases can have detrimental effects on the results of studies and urge researchers and practitioners to account for datafairness in all future studies of human mobility,0
big mobility datasets bmd have shown many advantages in studying human mobility and evaluating the performance of transportation systems however the quality of bmd remains poorly understood this study evaluates biases in bmd and develops mitigation methods using google and apple mobility data as examples this study compares them with benchmark data from governmental agencies spatiotemporal discrepancies between bmd and benchmark are observed and their impacts on transportation applications are investigated emphasizing the urgent need to address these biases to prevent misguided policymaking this study further proposes and tests a bias mitigation method it is shown that the mitigated bmd could generate valuable insights into largescale public transit systems across 100 us counties revealing regional disparities of the recovery of transit systems from the covid19 this study underscores the importance of caution when using bmd in transportation research and presents effective mitigation strategies that would benefit practitioners,0
recent years have witnessed an explosion of extensive geolocated datasets related to human movement enabling scientists to quantitatively study individual and collective mobility patterns and to generate models that can capture and reproduce the spatiotemporal structures and regularities in human trajectories the study of human mobility is especially important for applications such as estimating migratory flows traffic forecasting urban planning and epidemic modeling in this survey we review the approaches developed to reproduce various mobility patterns with the main focus on recent developments this review can be used both as an introduction to the fundamental modeling principles of human mobility and as a collection of technical methods applicable to specific mobilityrelated problems the review organizes the subject by differentiating between individual and population mobility and also between shortrange and longrange mobility throughout the text the description of the theory is intertwined with realworld applications,0
human mobility is a fundamental aspect of social behavior with broad applications in transportation urban planning and epidemic modeling however for decades new mathematical formulas to model mobility phenomena have been scarce and usually discovered by analogy to physical processes such as the gravity model and the radiation model these sporadic discoveries are often thought to rely on intuition and luck in fitting empirical data here we propose a systematic approach that leverages symbolic regression to automatically discover interpretable models from human mobility data our approach finds several wellknown formulas such as the distance decay effect and classical gravity models as well as previously unknown ones such as an exponentialpowerlaw decay that can be explained by the maximum entropy principle by relaxing the constraints on the complexity of model expressions we further show how key variables of human mobility are progressively incorporated into the model making this framework a powerful tool for revealing the underlying mathematical structures of complex social phenomena directly from observational data,0
in the past decade large scale mobile phone data have become available for the study of human movement patterns these data hold an immense promise for understanding human behavior on a vast scale and with a precision and accuracy never before possible with censuses surveys or other existing data collection techniques there is already a significant body of literature that has made key inroads into understanding human mobility using this exciting new data source and there have been several different measures of mobility used however existing mobile phone based mobility measures are inconsistent inaccurate and confounded with social characteristics of local context new measures would best be developed immediately as they will influence future studies of mobility using mobile phone data in this article we do exactly this we discuss problems with existing mobile phone based measures of mobility and describe new methods for measuring mobility that address these concerns our measures of mobility which incorporate both mobile phone records and detailed gis data are designed to address the spatial nature of human mobility to remain independent of social characteristics of context and to be comparable across geographic regions and time we also contribute a discussion of the variety of uses for these new measures in developing a better understanding of how human mobility influences microlevel human behaviors and wellbeing and macrolevel social organization and change,0
mobility is a complex phenomenon encompassing diverse transportation modes infrastructure elements and human behaviors tackling the persistent challenges of congestion pollution and accessibility requires a range of modeling approaches to optimize these systems while ai offers transformative potential it should not be the sole solution parsimonious models remain crucial in generating innovative concepts and tools and fostering collaborative efforts among researchers policymakers and industry stakeholders,0
human mobility forms the backbone of contact patterns through which infectious diseases propagate fundamentally shaping the spatiotemporal dynamics of epidemics and pandemics while traditional models are often based on the assumption that all individuals have the same probability of infecting every other individual in the population a socalled random homogeneous mixing they struggle to capture the complex and heterogeneous nature of realworld human interactions recent advancements in datadriven methodologies and computational capabilities have unlocked the potential of integrating highresolution human mobility data into epidemic modeling significantly improving the accuracy timeliness and applicability of epidemic risk assessment contact tracing and intervention strategies this review provides a comprehensive synthesis of the current landscape in human mobilityinformed epidemic modeling we explore diverse sources and representations of human mobility data and then examine the behavioral and structural roles of mobility and contact in shaping disease transmission dynamics furthermore the review spans a wide range of epidemic modeling approaches ranging from classical compartmental models to networkbased agentbased and machine learning models and we also discuss how mobility integration enhances risk management and response strategies during epidemics by synthesizing these insights the review can serve as a foundational resource for researchers and practitioners bridging the gap between epidemiological theory and the dynamic complexities of human interaction while charting clear directions for future research,0
we examine the nonmarkovian nature of human mobility by exposing the inability of markov models to capture criticality in human mobility in particular the assumed markovian nature of mobility was used to establish a theoretical upper bound on the predictability of human mobility expressed as a minimum error probability limit based on temporally correlated entropy since its inception this bound has been widely used and empirically validated using markov chains we show that recurrentneural architectures can achieve significantly higher predictability surpassing this widely used upper bound in order to explain this anomaly we shed light on several underlying assumptions in previous research works that has resulted in this bias by evaluating the mobility predictability on realworld datasets we show that human mobility exhibits scaleinvariant longrange correlations bearing similarity to a powerlaw decay this is in contrast to the initial assumption that human mobility follows an exponential decay this assumption of exponential decay coupled with lempelziv compression in computing fanos inequality has led to an inaccurate estimation of the predictability upper bound we show that this approach inflates the entropy consequently lowering the upper bound on human mobility predictability we finally highlight that this approach tends to overlook longrange correlations in human mobility this explains why recurrentneural architectures that are designed to handle longrange structural correlations surpass the previously computed upper bound on mobility predictability,0
while the fat tailed jump size and the waiting time distributions characterizing individual human trajectories strongly suggest the relevance of the continuous time random walk ctrw models of human mobility no one seriously believes that human traces are truly random given the importance of human mobility from epidemic modeling to traffic prediction and urban planning we need quantitative models that can account for the statistical characteristics of individual human trajectories here we use empirical data on human mobility captured by mobile phone traces to show that the predictions of the ctrw models are in systematic conflict with the empirical results we introduce two principles that govern human trajectories allowing us to build a statistically selfconsistent microscopic model for individual human mobility the model not only accounts for the empirically observed scaling laws but also allows us to analytically predict most of the pertinent scaling exponents,0
we provide a brief review of human mobility science and present three key areas where we expect to see substantial advancements we start from the mind and discuss the need to better understand how spatial cognition shapes mobility patterns we then move to societies and argue the importance of better understanding new forms of transportation we conclude by discussing how algorithms shape mobility behaviour and provide useful tools for modellers finally we discuss how progress in these research directions may help us address some of the challenges our society faces today,0
the recent outbreak of a novel coronavirus and its rapid spread underlines the importance of understanding human mobility enclosed spaces such as public transport vehicles eg buses and trains offer a suitable environment for infections to spread widely and quickly investigating the movement patterns and the physical encounters of individuals on public transit systems is thus critical to understand the drivers of infectious disease outbreaks for instance previous work has explored the impact of recurring patterns inherent in human mobility on disease spread but has not considered other dimensions such as the distance travelled or the number of encounters here we consider multiple mobility dimensions simultaneously to uncover critical information for the design of effective intervention strategies we use one month of citywide smart card travel data collected in sydney australia to classify bus passengers along three dimensions namely the degree of exploration the distance travelled and the number of encounters additionally we simulate disease spread on the transport network and trace the infection paths we investigate in detail the transmissions between the classified groups while varying the infection probability and the suspension time of pathogens our results show that characterizing individuals along multiple dimensions simultaneously uncovers a complex infection interplay between the different groups of passengers that would remain hidden when considering only a single dimension we also identify groups that are more influential than others given specific disease characteristics which can guide containment and vaccination efforts,0
driven by access to large volumes of movement data the study of human mobility has grown rapidly over the past decades the field has shown that human mobility is scalefree proposed models to generate scalefree moving distance distributions and explained how the scalefree distribution arises it has not however explicitly addressed how mobility is structured by geographical constraints how mobility relates to the outlines of landmasses lakes and rivers by the placement of buildings roadways and cities based on millions of moves we show how separating the effect of geography from mobility choices reveals a power law spanning five orders of magnitude to do so we incorporate geography via the pair distribution function that encapsulates the structure of locations on which mobility occurs showing how the spatial distribution of human settlements shapes human mobility our approach bridges the gap between distance and opportunitybased models of human mobility,0
over the past decade scientific studies have used the growing availability of large tracking datasets to enhance our understanding of human mobility behavior however so far data processing pipelines for the varying data collection methods are not standardized and consequently limit the reproducibility comparability and transferability of methods and results in quantitative human mobility analysis this paper presents trackintel an opensource python library for human mobility analysis trackintel is built on a standard data model for human mobility used in transport planning that is compatible with different types of tracking data we introduce the main functionalities of the library that covers the full lifecycle of human mobility analysis including processing steps according to the conceptual data model read and write interfaces as well as analysis functions eg data quality assessment travel mode prediction and location labeling we showcase the effectiveness of the trackintel library through a case study with four different tracking datasets trackintel can serve as an essential tool to standardize mobility data analysis and increase the transparency and comparability of novel research on human mobility,0
covid19 has been affecting every aspect of societal life including human mobility since december 2019 in this paper we study the impact of covid19 on human mobility patterns at the state level within the united states from the temporal perspective we find that the change of mobility patterns does not necessarily correlate with government policies and guidelines but is more related to peoples awareness of the pandemic which is reflected by the search data from google trends our results show that it takes on average 14 days for the mobility patterns to adjust to the new situation from the spatial perspective we conduct a statelevel network analysis and clustering using the mobility data from multiscale dynamic human mobility flow dataset as a result we find that 1 states in the same cluster have shorter geographical distances 2 a 14day delay again is found between the time when the largest number of clusters appears and the peak of coronavirusrelated search queries on google trends and 3 a major reduction in other network flow properties namely degree closeness and betweenness of all states from the week of march 2 to the week of april 6 the week of the largest number of clusters,0
gyration radius of individuals trajectory plays a key role in quantifying human mobility patterns of particular interests empirical analyses suggest that the growth of gyration radius is slow versus time except the very early stage and may eventually arrive to a steady value however up to now the underlying mechanism leading to such a possibly steady value has not been well understood in this letter we propose a simplified human mobility model to simulate individuals daily travel with three sequential activities commuting to workplace going to do leisure activities and returning home with the assumption that individual has constant travel speed and inferior limit of time at home and work we prove that the daily moving area of an individual is an ellipse and finally get an exact solution of the gyration radius the analytical solution well captures the empirical observation reported in we also find that in spite of the heterogeneous displacement distribution in the population level individuals in our model have characteristic displacements indicating a completely different mechanism to the one proposed by song et al,0
human travelling behaviours are markedly regular to a large extent predictable and mostly driven by biological necessities eg sleeping eating and social constructs eg school schedules synchronisation of labour not surprisingly such predictability is influenced by an array of factors ranging in scale from individual eg preference choices and social eg household groups all the way to global scale eg mobility restrictions in a pandemic in this work we explore how spatiotemporal patterns in individuallevel mobility which we refer to as emphpredictability states carry a large degree of information regarding the nature of the regularities in mobility our findings indicate the existence of contextual and activity signatures in predictability states pointing towards the potential for more sophisticated datadriven approaches to shortterm higherorder mobility predictions beyond frequentistprobabilistic methods,0
uncovering the mechanism behind the scaling law in human trajectories is of fundamental significance in understanding many spatiotemporal phenomena in combination of the exploration and the preferential returns we propose a simple dynamical model mainly based on the cascading processes to capture the human mobility patterns by the numerical simulations and analytical studies we show more than five statistical characters that are well consistent with the empirical observations including several type of scaling anomalies and the ultraslow diffusion property implying the cascading processes associated with the other two mechanisms are indeed a key in the understanding of human mobility activities moreover both of the diverse individual mobility and aggregated scaling movelengths bridging the micro and macro patterns in human mobility our model provides deeper understandings on the emergence of human mobility patterns,0
recently many empirical studies uncovered that animal foraging migration and human traveling obey levy flights with an exponent around 2 inspired by the deluge of h1n1 this year in this paper the effects of levy flights mobility pattern on epidemic spreading is studied from a network perspective we construct a spatial weighted network which possesses levy flight spatial property under a restriction of total energy the energy restriction is represented by the limitation of total travel distance within a certain time period of an individual we find that the exponent 2 is the epidemic threshold of sis spreading dynamics moreover at the threshold the speed of epidemics spreading is highest the results are helpful for the understanding of the effect of mobility pattern on epidemic spreading,0
we present a study on human mobility at small spatial scales differently from large scale mobility recently studied through dollarbill tracking and mobile phone data sets within one big country or continent we report brownian features of human mobility at smaller scales in particular the scaling exponents found at the smallest scales is typically close to onehalf differently from the larger values for the exponent characterizing mobility at larger scales we carefully analyze 12month data of the eduroam database within the portuguese university of minho a full procedure is introduced with the aim of properly characterizing the human mobility within the network of access points composing the wireless system of the university in particular measures of flux are introduced for estimating a distance between access points this distance is typically noneuclidean since the spatial constraints at such small scales distort the continuum space on which human mobility occurs since two different exponents are found depending on the scale human motion takes place we raise the question at which scale the transition from brownian to nonbrownian motion takes place in this context we discuss how the numerical approach can be extended to larger scales using the full eduroam in europe and in asia for uncovering the transition between both dynamical regimes,0
recent advances in human mobility research have revealed consistent pairwise characteristics in movement behavior yet existing mobility models often overlook the spatial and topological structure of mobility networks by analyzing millions of devices anonymized cell phone trajectories we uncover a distinct modular organization within these networks demonstrating that movements within spatial modules differ significantly from those between modules this finding challenges the conventional assumption of uniform mobility dynamics and underscores the influence of heterogeneous environments on human movement inspired by switching behaviors in animal movement patterns we introduce a novel switch mechanism to differentiate movement modes allowing our model to accurately reproduce both the modular structures of trajectory networks and spatial mobility patterns our results provide new insights into the dynamics of human mobility and its impact on network formation with broad applications in traffic prediction disease transmission modeling and urban planning beyond advancing the theoretical and practical understanding of mobility networks this work opens new avenues for understanding societal dynamics at large,0
the intrinsic factor that drives the human movement remains unclear for decades while our observations from intraurban and interurban trips both demonstrate a universal law in human mobility be specific the probability from one location to another is inversely proportional to the number of population living in locations which are closer than the destination a simple rankbased model is then presented which is parameterless but predicts human flows with a convincing fidelity besides comparison with other models shows that our model is more stable and fundamental at different spatial scales by implying the strong correlation between human mobility and social relationship,0
the statistical properties of human mobility have been studied in the framework of complex systems physics taking advantage from the new datasets made available by the information and communication technologies the distributions of mobility path lengths and of trip duration have been considered to discover the fingerprints of complexity characters but the role of the different transportation means on the statistical properties of urban mobility has not been studied in deep in this paper we cope with the problem of pointing out the existence of universal features for different type of individual mobility pedestrian cycling and vehicular urban mobility in particular we propose the use of travel time as universal energy for the mobility and we define a simple survival model that explains the travel time distribution of the different types of mobility the analysis is performed in the metropolitan area of bologna italy where gps datasets were available on individual trips using different transport means our results could suggest how to plan the different transportation networks to realize a multimodal mobility compatibly with the citizens propensities to use the different transport means,0
the study of human mobility patterns is a crucially important research field for its impact on several socioeconomic aspects and in particular the measure of regularity patters of human mobility can provide a acrosstheboard view of many social distancing variables in epidemics such as human movement trends physical interpersonal distances and population density we will show that the notion of information entropy is also strongly related to demographic and economic trends by the use and analysis of realtime data in the present research paper we address three different problems first we provide an evidencebased analytical approach which relates the human mobility patterns social distancing attitudes and population density with entropic measures which depict for erraticity of human contact behaviors second we investigate the correlations between the aggregated mobility and entropic measures versus five external economic indicators finallywe show how entropic measures represents a useful tool for testing the limitations of typical assumptions in epidemiological and mobility models,0
the covid19 pandemic has created an urgent need for mathematical models that can project epidemic trends and evaluate the effectiveness of mitigation strategies to forecast the transmission of covid19 a major challenge is the accurate assessment of the multiscale human mobility and how they impact the infection through close contacts by combining the stochastic agentbased modeling strategy and hierarchical structures of spatial containers corresponding to the notion of places in geography this study proposes a novel model mobcov to study the impact of human traveling behaviour and individual health conditions on the disease outbreak and the probability of zero covid in the population specifically individuals perform powerlaw type of local movements within a container and global transport between differentlevel containers frequent short movements inside a smalllevel container eg a road or a county and a large population size influence the local crowdedness of people which accelerates the infection and regional transmission travels between largelevel containers eg cities and nations facilitate global spread and outbreak moreover dynamic infection and recovery in the population are able to drive the bifurcation of the system to a zerocovid state or a live with covid state depending on the mobility patterns population number and health conditions reducing total population and local people accumulation as well as restricting global travels help achieve zerocovid in summary the mobcov model considers more realistic human mobility in a wide range of spatial scales and has been designed with equal emphasis on performance low simulation cost accuracy ease of use and flexibility it is a useful tool for researchers and politicians to investigate the pandemic dynamics and plan actions against the disease,0
understanding dynamic human mobility changes and spatial interaction patterns at different geographic scales is crucial for assessing the impacts of nonpharmaceutical interventions such as stayathome orders during the covid19 pandemic in this data descriptor we introduce a regularlyupdated multiscale dynamic human mobility flow dataset across the united states with data starting from march 1st 2020 by analyzing millions of anonymous mobile phone users visits to various places provided by safegraph the daily and weekly dynamic origintodestination od population flows are computed aggregated and inferred at three geographic scales census tract county and state there is high correlation between our mobility flow dataset and openly available data sources which shows the reliability of the produced data such a high spatiotemporal resolution human mobility flow dataset at different geographic scales over time may help monitor epidemic spreading dynamics inform public health policy and deepen our understanding of human behavior changes under the unprecedented public health crisis this uptodate od flow open data can support many other social sensing and transportation applications,0
human mobility patterns deeply affect the dynamics of many social systems in this paper we empirically analyze the realworld human movements based gps records and observe rich scaling properties in the temporalspatial patterns as well as an abnormal transition in the speeddisplacement patterns we notice that the displacements at the population level show significant positive correlation indicating a cascadelike nature in human movements furthermore our analysis at the individual level finds that the displacement distributions of users with strong correlation of displacements are closer to power laws implying a relationship between the positive correlation of the series of displacements and the form of an individuals displacement distribution these findings from our empirical analysis show a factor directly relevant to the origin of the scaling properties in human mobility,0
the dynamics of human mobility characterizes the trajectories humans follow during their daily activities and is the foundation of processes from epidemic spreading to traffic prediction and information recommendation in this paper we investigate a massive data set of human activity including both online behavior of browsing websites and offline one of visiting towers based mobile terminations the nonmarkovian character observed from both online and offline cases is suggested by the scaling law in the distribution of dwelling time at individual and collective levels respectively furthermore we argue that the lower entropy and higher predictability in human mobility for both online and offline cases may origin from this nonmarkovian character however the distributions of individual entropy and predictability show the different degrees of nonmarkovian character from online to offline cases to accounting for nonmarkovian character in human mobility we introduce a protype model with three basic ingredients emphpreferential return inertial effect and exploration to reproduce the dynamic process of online and offline human mobility in comparison with standard and biased random walk models with assumption of markov process the proposed model is able to obtain characters much closer to these empirical observations,0
the covid19 pandemic has brought both tangible and intangible damage to our society many researchers studied about its societal impacts in the countries that had implemented strong social distancing measures such as stayathome orders among them human mobility has been studied extensively due to its importance in flattening the curve however mobility has not been actively studied in the context of mild social distancing insufficient understanding of human mobility in diverse contexts might provide limited implications for any technological interventions to alleviate the situation to this end we collected a dataset consisting of more than 1m daily smart device users in the thirdlargest city of south korea which has implemented mild social distancing policies we analyze how covid19 shaped human mobility in the city from geographical socioeconomic and sociopolitical perspectives we also examine mobility changes for points of interest and special occasions such as transportation stations and the case of legislative elections we identify a typology of populations through these analyses as a means to provide design implications for technological interventions this paper contributes to social sciences through indepth analyses of human mobility and to the cscw community with new design challenges and potential implications,0
we report on a datadriven investigation aimed at understanding the dynamics of message spreading in a realworld dynamical network of human proximity we use data collected by means of a proximitysensing network of wearable sensors that we deployed at three different social gatherings simultaneously involving several hundred individuals we simulate a message spreading process over the recorded proximity network focusing on both the topological and the temporal properties we show that by using an appropriate technique to deal with the temporal heterogeneity of proximity events a universal statistical pattern emerges for the delivery times of messages robust across all the data sets our results are useful to set constraints for generic processes of data dissemination as well as to validate established models of human mobility and proximity that are frequently used to simulate realistic behaviors,0
travel patterns can be impacted by abnormal events assessing the impacts has important implications for relief operations and improving preparedness or planning for future events conventionally the assessment is done followed by data collection from postevent surveys which are economically costly suffering lowresponse rate timeconsuming and usually delayed for months or even years after an event leading to inefficient and unreliable assessment and creating obstacles for relief organizations to reach people in need penetration of smartphones and services enabled by them continuously generate large amount of trajectory data eg call records data appbased data containing trajectories of massive users these trajectory data are passively and timely collected and without additional cost and contain information of travel patterns of the massive number of individuals in a region for a prolonged time period eg months to years we propose a framework to assessing the impacts on travel patterns using these data utilizing the passively collected trajectory data the proposed framework seeks to capturing and understanding the full spectrum of travel pattern changes which helps to assess who when and how people in a certain area were impacted the proposed framework is applied to a mobile phone trajectory dataset containing about halfyear trajectories of a million anonymous users to assess the impacts of hurricane harvey the secondcostliest hurricane in us history the results are validated and show that the proposed framework can provide a comprehensive assessment of impacts of harvey on travel patterns which could guide the response to and the recovery from the impacts,0
human mobility is influenced by environmental change and natural disasters researchers have used trip distance distribution radius of gyration of movements and individuals visited locations to understand and capture human mobility patterns and trajectories however our knowledge of human movements during natural disasters is limited owing to both a lack of empirical data and the low precision of available data here we studied human mobility using highresolution movement data from individuals in new york city during and for several days after hurricane sandy in 2012 we found the human movements followed truncated powerlaw distributions during and after hurricane sandy although the  value was noticeably larger during the first 24 hours after the storm struck also we examined two parameters the center of mass and the radius of gyration of each individuals movements we found that their values during perturbation states and steady states are highly correlated suggesting human mobility data obtained in steady states can possibly predict the perturbation state our results demonstrate that human movement trajectories experienced significant perturbations during hurricanes but also exhibited high resilience we expect the study will stimulate future research on the perturbation and inherent resilience of human mobility under the influence of natural disasters for example mobility patterns in coastal urban areas could be examined as tropical cyclones approach gain or dissipate in strength and as the path of the storm changes understanding nuances of human mobility under the influence of disasters will enable more effective evacuation emergency response planning and development of strategies and policies to reduce fatality injury and economic loss,0
predictive models for human mobility have important applications in many fields such as traffic control ubiquitous computing and contextual advertisement the predictive performance of models in literature varies quite broadly from as high as 93 to as low as under 40 in this work we investigate which factors influence the accuracy of nextplace prediction using a highprecision location dataset of more than 400 users for periods between 3 months and one year we show that it is easier to achieve high accuracy when predicting the timebin location than when predicting the next place moreover we demonstrate how the temporal and spatial resolution of the data can have strong influence on the accuracy of prediction finally we uncover that the exploration of new locations is an important factor in human mobility and we measure that on average 2025 of transitions are to new places and approx 70 of locations are visited only once we discuss how these mechanisms are important factors limiting our ability to predict human mobility,0
recent seminal works on human mobility have shown that individuals constantly exploit a small set of repeatedly visited locations a concurrent literature has emphasized the explorative nature of human behavior showing that the number of visited places grows steadily over time how to reconcile these seemingly contradicting facts remains an open question here we analyze highresolution multiyear traces of sim40000 individuals from 4 datasets and show that this tension vanishes when the longterm evolution of mobility patterns is considered we reveal that mobility patterns evolve significantly yet smoothly and that the number of familiar locations an individual visits at any point is a conserved quantity with a typical size of sim25 locations we use this finding to improve stateoftheart modeling of human mobility furthermore shifting the attention from aggregated quantities to individual behavior we show that the size of an individuals set of preferred locations correlates with the number of her social interactions this result suggests a connection between the conserved quantity we identify which as we show can not be understood purely on the basis of time constraints and the dunbar number describing a cognitive upper limit to an individuals number of social relations we anticipate that our work will spark further research linking the study of human mobility and the cognitive and behavioral sciences,0
mobility data at eu scale can help understand the dynamics of the pandemic and possibly limit the impact of future waves still since a reliable and consistent method to measure the evolution of contagion at international level is missing a systematic analysis of the relationship between human mobility and virus spread has never been conducted a notable exceptions are france and italy for which data on excess deaths an indirect indicator which is generally considered to be less affected by national and regional assumptions are available at department and municipality level respectively using this information together with anonymised and aggregated mobile data this study shows that mobility alone can explain up to 92 of the initial spread in these two eu countries while it has a slow decay effect after lockdown measures meaning that mobility restrictions seem to have effectively contribute to save lives it also emerges that internal mobility is more important than mobility across provinces and that the typical lagged positive effect of reduced human mobility on reducing excess deaths is around 1420 days an analogous analysis relative to spain for which an igg sarscov2 antibody screening study at province level is used instead of excess deaths statistics confirms the findings the same approach adopted in this study can be easily extended to other european countries as soon as reliable data on the spreading of the virus at a suitable level of granularity will be available looking at past data relative to the initial phase of the outbreak in eu member states this study shows in which extent the spreading of the virus and human mobility are connected,0
accurate human mobility prediction underpins many important applications across a variety of domains including epidemic modelling transport planning and emergency responses due to the sparsity of mobility data and the stochastic nature of peoples daily activities achieving precise predictions of peoples locations remains a challenge while recently developed large language models llms have demonstrated superior performance across numerous languagerelated tasks their applicability to human mobility studies remains unexplored addressing this gap this article delves into the potential of llms for human mobility prediction tasks we introduce a novel method llmmob which leverages the language understanding and reasoning capabilities of llms for analysing human mobility data we present concepts of historical stays and context stays to capture both longterm and shortterm dependencies in human movement and enable timeaware prediction by using time information of the prediction target additionally we design contextinclusive prompts that enable llms to generate more accurate predictions comprehensive evaluations of our method reveal that llmmob excels in providing accurate and interpretable predictions highlighting the untapped potential of llms in advancing human mobility prediction techniques we posit that our research marks a significant paradigm shift in human mobility modelling transitioning from building complex domainspecific models to harnessing generalpurpose llms that yield accurate predictions through language instructions the code for this work is available at,0
unlike the lockdown measures taken in some countries or cities the japanese government declared a state of emergency soe under which people were only requested to reduce their contact with other people by at least 70 while some local governments also implemented their own mobilityreduction measures that had no legal basis the effects of these measures are still unclear thus in this study we investigate changes in travel patterns in response to the covid19 outbreak and related policy measures in japan using longitudinal aggregated mobile phone data specifically we consider daily travel patterns as networks and analyze their structural changes by applying a framework for analyzing temporal networks used in network science the cluster analysis with the network similarity measures across different dates showed that there are six main types of mobility patterns in the three major metropolitan areas of japan i weekends and holidays prior to the covid19 outbreak ii weekdays prior to the covid19 outbreak iii weekends and holidays before and after the soe iv weekdays before and after the soe v weekends and holidays during the soe and vi weekdays during the soe it was also found that travel patterns might have started to change from march 2020 when most schools were closed and that the mobility patterns after the soe returned to those prior to the soe interestingly we found that after the lifting of the soe travel patterns remained similar to those during the soe for a few days suggesting the possibility that selfrestraint continued after the lifting of the soe moreover in the case of the nagoya metropolitan area we found that people voluntarily changed their travel patterns when the number of cases increased,0
bikesharing has gradually become one adopted sustainable transportation mode recent years to bring us many social environmental economic and healthrelated benefits and rewards there is increased research toward better understanding of bikesharing systems bss in urban environments however our comprehension remains incomplete on the patterns and characteristics of bss in this paper aiming to help improving sustainability in multimodal transportation through bss we perform a systematic data analysis to examine underlying patterns and characteristics of the system dynamics in a bikeshare network and to acquire implications of the patterns and characteristics for decision making as a case study we use trip history data from the capital bikeshare system in the washington dc area and some additional data sources the study covers seven important aspects of bikeshare transportation systems which are respectively trip demand and flow operating activities use and idle times trip purpose origindestination flows mobility and safety for these aspects by using appropriate statistical methods and geographic techniques we investigate travel patterns and characteristics of bss from data to evaluate the qualitative and quantitative impacts of the inputs from key stakeholders on main measures of effectiveness such as trip costs mobility safety quality of service and operational efficiency where key stakeholders include road users system operators and city we also disclose some new patterns and characteristics of bss to advance the knowledge on travel behaviors finally we briefly summarize our findings and discuss the implications of the patterns and characteristics for datadriven decision supports from the relations between bss and key stakeholders for promoting bikeshare utilization and transforming urban transportation to be more sustainable,0
we propose to use social networking data to validate mobility models for pervasive mobile adhoc networks manets and delay tolerant networks dtns the random waypoint rwp and erdosrenyi er models have been a popular choice among researchers for generating mobility traces of nodes and relationships between them not only rwp and er are useful in evaluating networking protocols in a simulation environment but they are also used for theoretical analysis of such dynamic networks however it has been observed that neither relationships among people nor their movements are random instead human movements frequently contain repeated patterns and friendship is bounded by distance we used social networking site gowalla to collect create and validate models of human mobility and relationships for analysis and evaluations of applications in opportunistic networks such as sensor networks and transportation models in civil engineering in doing so we hope to provide more humanlike movements and social relationship models to researchers to study problems in complex and mobile networks,0
recent availability of geolocalized data capturing individual human activity together with the statistical data on international migration opened up unprecedented opportunities for a study on global mobility in this paper we consider it from the perspective of a multilayer complex network built using a combination of three datasets twitter flickr and official migration data those datasets provide different but equally important insights on the global mobility while the first two highlight shortterm visits of people from one country to another the last one migration shows the longterm mobility perspective when people relocate for good and the main purpose of the paper is to emphasize importance of this multilayer approach capturing both aspects of human mobility at the same time so we start from a comparative study of the network layers comparing short and long term mobility through the statistical properties of the corresponding networks such as the parameters of their degree centrality distributions or parameters of the corresponding gravity model being fit to the network we also focus on the differences in country ranking by their short and longterm attractiveness discussing the most noticeable outliers finally we apply this multilayered human mobility network to infer the structure of the global society through a community detection approach and demonstrate that consideration of mobility from a multilayer perspective can reveal important global spatial patterns in a way more consistent with other available relevant sources of international connections in comparison to the spatial structure inferred from each network layer taken separately,0
homework commuting has always attracted significant research attention because of its impact on human mobility one of the key assumptions in this domain of study is the universal uniformity of commute times however a true comparison of commute patterns has often been hindered by the intrinsic differences in data collection methods which make observation from different countries potentially biased and unreliable in the present work we approach this problem through the use of mobile phone call detail records cdrs which offers a consistent method for investigating mobility patterns in wholly different parts of the world we apply our analysis to a broad range of datasets at both the country and city scale additionally we compare these results with those obtained from vehicle gps traces in milan while different regions have some unique commute time characteristics we show that the homework time distributions and average values within a single region are indeed largely independent of commute distance or country portugal ivory coast and bostondespite substantial spatial and infrastructural differences furthermore a comparative analysis demonstrates that such distanceindependence holds true only if we consider multimodal commute behaviorsas consistent with previous studies in caronly milan gps traces and carheavy saudi arabia commute datasets we see that commute time is indeed influenced by commute distance,0
in this paper we extend some ideas of statistical physics to describe the properties of human mobility from a physical point of view we consider the statistical empirical laws of private cars mobility taking advantage of a gps database which contains a sampling of the individual trajectories of 2 of the whole vehicle population in an italian region our aim is to discover possible universal laws that can be related to the dynamical cognitive features of individuals analyzing the empirical trip length distribution we study if the travel time can be used as universal cost function in a mesoscopic model of mobility we discuss the implications of the elapsed times distribution between successive trips that shows an underlying benfords law and we study the rank distribution of the average visitation frequency to understand how people organize their daily agenda we also propose simple stochastic models to suggest possible explanations of the empirical observations and we compare our results with analogous results on statistical properties of human mobility presented in the literature,0
human mobility is a fundamental process underpinning socioeconomic life and urban structure classic theories such as egocentric activity spaces and central place theory provide crucial insights into specific facets of movement like homecentricity and hierarchical spatial organization however identifying universal characteristics or an underlying principle that quantitatively links these disparate perspectives has remained a challenge here we reveal such a connection by analyzing the spatial structure of individual daily mobility trajectories using networkbased modules we discover a universal scaling law the spatial extent radius of these mobility modules expands sublinearly with increasing distance from home a pattern consistent across three orders of magnitude furthermore we demonstrate that these modules precisely map onto the nested hierarchy of urban systems corresponding to local citylevel and regional scales as distance from home increases these findings deepen our understanding of human mobility dynamics and demonstrate the profound connection between classical urban theory human geography and mobility studies,0
modeling of human mobility is critical to address questions in urban planning and transportation as well as global challenges in sustainability public health and economic development however our understanding and ability to model mobility flows within and between urban areas are still incomplete at one end of the modeling spectrum we have simple socalled gravity models which are easy to interpret and provide modestly accurate predictions of mobility flows at the other end we have complex machine learning and deep learning models with tens of features and thousands of parameters which predict mobility more accurately than gravity models at the cost of not being interpretable and not providing insight on human behavior here we show that simple machinelearned closedform models of mobility are able to predict mobility flows more accurately overall than either gravity or complex machine and deep learning models at the same time these models are simple and gravitylike and can be interpreted in terms similar to standard gravity models furthermore these models work for different datasets and at different scales suggesting that they may capture the fundamental universal features of human mobility,0
human movements in the real world and in cyberspace affect not only dynamical processes such as epidemic spreading and information diffusion but also social and economical activities such as urban planning and personalized recommendation in online shopping despite recent efforts in characterizing and modeling human behaviors in both the real and cyber worlds the fundamental dynamics underlying human mobility have not been well understood we develop a minimal memorybased random walk model in limited space for reproducing with a single parameter the key statistical behaviors characterizing human movements in both spaces the model is validated using big data from mobile phone and online commerce suggesting memorybased random walk dynamics as the universal underpinning for human mobility regardless of whether it occurs in the real world or in cyberspace,0
as a key energy challenge we urgently require a better understanding of how growing urban populations interact with municipal energy systems and the resulting impact on energy demand across city neighborhoods which are dense hubs of both consumer population and co2 emissions currently the physical characteristics of urban infrastructure are the main determinants in predictive modeling of the demand side of energy in our rapidly growing urban areas overlooking influence related to fluctuating human activities here we show how applying intraurban human mobility as an indicator for interactions of the population with local energy systems can be translated into spatial imprints to predict the spatial distribution of energy use in urban settings our findings establish human mobility as an important element in explaining the spatial structure underlying urban energy flux and demonstrate the utility of a human mobility driven approach for predicting future urban energy demand with implications for co2 emission strategies,0
the recent availability of digital traces generated by phone calls and online logins has significantly increased the scientific understanding of human mobility until now however limited data resolution and coverage have hindered a coherent description of human displacements across different spatial and temporal scales here we characterise mobility behaviour across several orders of magnitude by analysing 850 individuals digital traces sampled every 16 seconds for 25 months with 10 meters spatial resolution we show that the distributions of distances and waiting times between consecutive locations are best described by lognormal distributions and that natural timescales emerge from the regularity of human mobility we point out that lognormal distributions also characterise the patterns of discovery of new places implying that they are not a simple consequence of the routine of modern life,0
understanding human mobility patterns how people move in their everyday lives is an interdisciplinary research field it is a question with roots back to the 19th century that has been dramatically revitalized with the recent increase in data availability models of human mobility often take the population distribution as a starting point another sometimes more accurate data source is landuse maps in this paper we discuss how the intracity movement patterns and consequently population distribution can be predicted from such data sources as a link between land use and mobility we show that the purposes of peoples trips are strongly correlated with the land use of the trips origin and destination we calibrate validate and discuss our model using survey data,0
understanding the spatiotemporal patterns of human mobility is crucial for addressing societal challenges such as epidemic control and urban transportation optimization despite advancements in data collection the complexity and scale of mobility data continue to pose significant analytical challenges existing methods often result in losing locationspecific details and fail to fully capture the intricacies of human movement this study proposes a twostep dimensionality reduction framework to overcome existing limitations first we construct a potential landscape of human flow from origindestination od matrices using combinatorial hodge theory preserving essential spatial and structural information while enabling an intuitive visualization of flow patterns second we apply principal component analysis pca to the potential landscape systematically identifying major spatiotemporal patterns by implementing this twostep reduction method we reveal significant shifts during a pandemic characterized by an overall declines in mobility and stark contrasts between weekdays and holidays these findings underscore the effectiveness of our framework in uncovering complex mobility patterns and provide valuable insights into urban planning and public health interventions,0
given the rapid recent trend of urbanization a better understanding of how urban infrastructure mediates socioeconomic interactions and economic systems is of vital importance while the accessibility of locationenabled devices as well as largescale datasets of human activities has fueled significant advances in our understanding there is little agreement on the linkage between socioeconomic status and its influence on movement patterns in particular the role of inequality here we analyze a heavily aggregated and anonymized summary of global mobility and investigate the relationships between socioeconomic status and mobility across a hundred cities in the us and brazil we uncover two types of relationships finding either a clear connection or littletono interdependencies the former tend to be characterized by low levels of public transportation usage inequitable access to basic amenities and services and segregated clusters of communities in terms of income with the latter class showing the opposite trends our findings provide useful lessons in designing urban habitats that serve the larger interests of all inhabitants irrespective of their economic status,0
the advent of geographic online social networks such as foursquare where users voluntarily signal their current location opens the door to powerful studies on human movement in particular the fine granularity of the location data with gps accuracy down to 10 meters and the worldwide scale of foursquare adoption are unprecedented in this paper we study urban mobility patterns of people in several metropolitan cities around the globe by analyzing a large set of foursquare users surprisingly while there are variations in human movement in different cities our analysis shows that those are predominantly due to different distributions of places across different urban environments moreover a universal law for human mobility is identified which isolates as a key component the rankdistance factoring in the number of places between origin and destination rather than pure physical distance as considered in some previous works building on our findings we also show how a rankbased movement model accurately captures real human movements in different cities our results shed new light on the driving factors of urban human mobility with potential applications for urban planning locationbased advertisement and even social studies,0
despite the growing popularity of human mobility studies that collect gps location data the problem of determining the minimum required length of gps monitoring has not been addressed in the current statistical literature in this paper we tackle this problem by laying out a theoretical framework for assessing the temporal stability of human mobility based on gps location data we define several measures of the temporal dynamics of human spatiotemporal trajectories based on the average velocity process and on activity distributions in a spatial observation window we demonstrate the use of our methods with data that comprise the gps locations of 185 individuals over the course of 18 months our empirical results suggest that gps monitoring should be performed over periods of time that are significantly longer than what has been previously suggested furthermore we argue that gps study designs should take into account demographic groups keywords density estimation global positioning systems gps human mobility spatiotemporal trajectories temporal dynamics,0
urbanization drives the epidemiology of infectious diseases to many threats and new challenges in this research we study the interplay between human mobility and dengue outbreaks in the complex urban environment of the citystate of singapore we integrate both stylized and mobile phone datadriven mobility patterns in an agentbased transmission model in which humans and mosquitoes are represented as agents that go through the epidemic states of dengue we monitor with numerical simulations the systemlevel response to the epidemic by comparing our results with the observed cases reported during the 2013 and 2014 outbreaks our results show that human mobility is a major factor in the spread of vectorborne diseases such as dengue even on the short scale corresponding to intracity distances we finally discuss the advantages and the limits of mobile phone data and potential alternatives for assessing valuable mobility patterns for modeling vectorborne diseases outbreaks in cities,0
in june 2020 arizona us emerged as one of the worlds worst coronavirus disease 2019covid19 spots after the stayathome order was lifted in the middle of may however with the decisions to reimpose restrictions the number of covid19 cases has been declining and arizona is considered to be a good model in slowing the epidemic in this paper we aimed to examine the covid19 situation in arizona and assess the impact of human mobility change we constructed the mobility integrated metapopulation susceptibleinfectiousremoved model and fitted to publicly available datasets on covid19 cases and mobility changes in arizona our simulations showed that by reducing human mobility the peak time was delayed and the final size of the epidemic was decreased in all three regions our analysis suggests that rapid and effective decision making is crucial to control human mobility and therefore covid19 epidemics until a vaccine is available reimplementations of mobility restrictions in response to the increase of new covid19 cases might need to be considered in arizona and beyond,0
urban science has largely relied on universal models rendering the heterogeneous and locally specific nature of cities effectively invisible here we introduce a topological framework that defines and detects localities in human mobility networks we empirically demonstrate that these human mobility network localities are rigorous geometric entities that map directly to geographic localities revealing that human mobility networks lie on manifolds of dimension 5 this representation provides a compact theoretical foundation for spatial embedding and enables efficient applications to facility location and propagation modeling our approach reconciles local heterogeneity with universal representation offering a new pathway toward a more comprehensive urban science,0
social distancing and stayathome are among the few measures that are known to be effective in checking the spread of a pandemic such as covid19 in a given population the patterns of dependency between such measures and their effects on disease incidence may vary dynamically and across different populations we described a new computational framework to measure and compare the temporal relationships between human mobility and new cases of covid19 across more than 150 cities of the united states with relatively high incidence of the disease we used a novel application of optimal transport for computing the distance between the normalized patterns induced by bivariate time series for each pair of cities thus we identified 10 clusters of cities with similar temporal dependencies and computed the wasserstein barycenter to describe the overall dynamic pattern for each cluster finally we used cityspecific socioeconomic covariates to analyze the composition of each cluster,0
the generation of realistic spatiotemporal trajectories of human mobility is of fundamental importance in a wide range of applications such as the developing of protocols for mobile adhoc networks or whatif analysis in urban ecosystems current generative algorithms fail in accurately reproducing the individuals recurrent schedules and at the same time in accounting for the possibility that individuals may break the routine during periods of variable duration in this article we present ditras diarybased trajectory simulator a framework to simulate the spatiotemporal patterns of human mobility ditras operates in two steps the generation of a mobility diary and the translation of the mobility diary into a mobility trajectory we propose a datadriven algorithm which constructs a diary generator from real data capturing the tendency of individuals to follow or break their routine we also propose a trajectory generator based on the concept of preferential exploration and preferential return we instantiate ditras with the proposed diary and trajectory generators and compare the resulting algorithm with real data and synthetic data produced by other generative algorithms built by instantiating ditras with several combinations of diary and trajectory generators we show that the proposed algorithm reproduces the statistical properties of real trajectories in the most accurate way making a step forward the understanding of the origin of the spatiotemporal patterns of human mobility,0
while large scale mobility data has become a popular tool to monitor the mobility patterns during the covid19 pandemic the impacts of noncompulsory measures in tokyo japan on human mobility patterns has been understudied here we analyze the temporal changes in human mobility behavior social contact rates and their correlations with the transmissibility of covid19 using mobility data collected from more than 200k anonymized mobile phone users in tokyo the analysis concludes that by april 15th 1 week into state of emergency human mobility behavior decreased by around 50 resulting in a 70 reduction of social contacts in tokyo showing the effectiveness of noncompulsory measures furthermore the reduction in datadriven human mobility metrics showed correlation with the decrease in estimated effective reproduction number of covid19 in tokyo such empirical insights could inform policy makers on deciding sufficient levels of mobility reduction to contain the disease,0
airborne diseases including covid19 raise the question of transmission risk in public transportation systems however quantitative analysis of the effectiveness of transmission risk mitigation methods in public transportation is lacking the paper develops a transmission risk modeling framework based on the wellsriley model using as inputs transit operating characteristics schedule origindestination od demand and virus characteristics the model is sensitive to various factors that operators can control as well as external factors that may be subject of broader policy decisions eg mask wearing the model is utilized to assess transmission risk as a function of od flows planned operations and factors such as maskwearing ventilation and infection rates using actual data from the massachusetts bay transportation authority mbta red line the paper explores the transmission risk under different infection rate scenarios both in magnitude and spatial characteristics the paper assesses the combined impact from viral load related factors and passenger load factors increasing frequency can mitigate transmission risk but cannot fully compensate for increases in infection rates imbalanced passenger distribution on different cars of a train is shown to increase the overall systemwide infection probability spatial infection rate patterns should also be taken into account during policymaking as it is shown to impact transmission risk for lines with branches demand distribution among the branches is important and headway allocation adjustment among branches to balance the load on trains to different branches can help reduce risk,0
studies using massive passively data collected from communication technologies have revealed many ubiquitous aspects of social networks helping us understand and model social media information diffusion and organizational dynamics more recently these data have come tagged with geographic information enabling studies of human mobility patterns and the science of cities we combine these two pursuits and uncover reproducible mobility patterns amongst social contacts first we introduce measures of mobility similarity and predictability and measure them for populations of users in three large urban areas we find individuals visitations patterns are far more similar to and predictable by social contacts than strangers and that these measures are positively correlated with tie strength unsupervised clustering of hourly variations in mobility similarity identifies three categories of social ties and suggests geography is an important feature to contextualize social relationships we find that the composition of a users ego network in terms of the type of contacts they keep is correlated with mobility behavior finally we extend a popular mobility model to include movement choices based on social contacts and compare its ability to reproduce empirical measurements with two additional models of mobility,0
while conventional shared demandresponsive transportation sdrt systems mostly operate on a doortodoor policy the usage of meeting points for the pickup and dropoff of user groups can offer several advantages like fewer stops and less total travelled mileage moreover it offers the possibility to select only feasible and welldefined locations where a safe deboarding is possible this paper presents a threestep workflow for solving the sdrt problem with meeting points sdrtmp firstly the customers are clustered into similar groups then meeting and divergence points are determined for each cluster finally a parallel neighbourhood search algorithm is applied to create the vehicle routes further a simulation with realistic pickup and dropoff locations based on map data is performed in order to demonstrate the impact of using meeting points for sdrt systems in contrast to the doortodoor service although the average passenger travel time is higher due to enhanced walking and waiting times the experiment highlights a reduction of operator resources required to serve all customers,0
many of our routines and activities are linked to our ability to move be it commuting to work shopping for groceries or meeting friends yet factors that limit the individuals ability to fully realise their mobility needs will ultimately affect the opportunities they can have access to eg cultural activities professional interactions one important aspect frequently overlooked in human mobility studies is how gendercentred issues can amplify other sources of mobility disadvantages eg socioeconomic inequalities unevenly affecting the pool of opportunities men and women have access to in this work we leverage on a combination of computational statistical and informationtheoretical approaches to investigate the existence of systematic discrepancies in the mobility diversity ie the diversity of travel destinations of 1 men and women from different socioeconomic backgrounds and 2 work and nonwork travels our analysis is based on datasets containing multiple instances of largescale official travel surveys carried out in three major metropolitan areas in south america medelln and bogot in colombia and so paulo in brazil our results indicate the presence of general discrepancies in the urban mobility diversities related to the gender and socioeconomic characteristics of the individuals lastly this paper sheds new light on the possible origins of genderlevel human mobility inequalities contributing to the general understanding of disaggregated patterns in human mobility,0
many modern and growing cities are facing declines in public transport usage with few efficient methods to explain why in this article we show that urban mobility patterns and transport mode choices can be derived from cellphone call detail records coupled with public transport data recorded from smart cards specifically we present new data mining approaches to determine the spatial and temporal variability of public and private transportation usage and transport mode preferences across singapore our results which were validated by singapores quadriennial household interview travel survey hits revealed that there are 35 hits 35 million million and 43 hits 44 million million interdistrict passengers by public and private transport respectively along with classifying which transportation connections are weak or underserved the analysis shows that the mode share of public transport use increases from 38 percent in the morning to 44 percent around midday and 52 percent in the evening,0
the information collected by mobile phone operators can be considered as the most detailed information on human mobility across a large part of the population the study of the dynamics of human mobility using the collected geolocations of users and applying it to predict future users locations has been an active field of research in recent years in this work we study the extent to which social phenomena are reflected in mobile phone data focusing in particular in the cases of urban commute and major sports events we illustrate how these events are reflected in the data and show how information about the events can be used to improve predictability in a simple model for a mobile phone users location,0
the massive amounts of geolocation data collected from mobile phone records has sparked an ongoing effort to understand and predict the mobility patterns of human beings in this work we study the extent to which social phenomena are reflected in mobile phone data focusing in particular in the cases of urban commute and major sports events we illustrate how these events are reflected in the data and show how information about the events can be used to improve predictability in a simple model for a mobile phone users location,0
we apply a simple clustering algorithm to a large dataset of cellular telecommunication records reducing the complexity of mobile phone users full trajectories and allowing for simple statistics to characterize their properties for the case of two clusters we quantify how clustered human mobility is how much of a users spatial dispersion is due to motion between clusters and how spatially and temporally separated clusters are from one another,0
researches on the human mobility have made great progress in many aspects but the longterm and longdistance migration behavior is lack of indepth and extensive research because of the difficult in accessing to household data in this paper we use the resume data to discover the human migration behavior on the large scale scope it is found that the asymmetry in the flow structure which reflects the influence of population competition is caused by the difference of attractiveness among cities this flow structure can be approximately described by the gravity model of spatial economics besides the value of scaling exponent of distance function in the gravity model is less than the value of shortterm travel behavior it means that compared with the shortterm travel behavior the longterm human migration behavior is less sensitive moreover the scaling coefficients of each variable in the gravity model are investigated the result shows that the economic level is a mainly factor on the migration,0
human mobility networks can reveal insights into resilience phenomena such as population response to impacts on and recovery from crises the majority of human mobility network resilience characterizations however focus mainly on macroscopic network properties little is known about variation in measured resilience characteristics ie the extent of impact and recovery duration across macroscopic substructure motif and microscopic mobility scales to address this gap in this study we examine the human mobility network in eight parishes in louisiana usa impacted by the 2021 hurricane ida we constructed human mobility networks using locationbased data and examined three sets of measures 1 macroscopic measures such as network density giant component size and modularity 2 substructure measures such motif distribution and 3 microscopic mobility measures such as the radius of gyration and average travel distance to determine the extent of impact and duration of recovery for each measure we established the baseline values and examined the fluctuation of measures during the perturbation caused by hurricane ida the results reveal the variation of impact extent and recovery duration obtained from different sets of measures at different scales macroscopic measures such as giant components tend to recover more quickly than substructure and microscopic measures in fact microscopic measures tend to recover more slowly than measures in other scales these findings suggest that resilience characteristics in human mobility networks are scalevariant and thus a single measure at a particular scale may not be representative of the perturbation impacts and recovery duration in the network as a whole these results spotlight the need to use measures at different scales to properly characterize resilience in human mobility networks,0
identifying statistical patterns characterizing human trajectories is crucial for public health traffic engineering city planning and epidemic modeling recent developments in global positioning systems and mobile phone networks have enabled the collection of substantial information on human movement analyses of these data have revealed various power laws in the temporal and spatial statistical patterns of human mobility for example jump size and waiting time distributions follow power laws zipfs law was also established for the frequency of visits to each location and rank relationship stsim t exists between time t and the number of sites visited up to that time t recently a universal law of visitation for human mobility was established specifically the number of people per unit area rf who reside at distance r from a particular location and visit that location f times in a given period is inversely proportional to the square of rf ie rf propto rf2 holds the exploration and preferential return epr model and its improved versions have been proposed to reproduce the above scaling laws however some rules that follow the power law are preinstalled in the epr model we propose a simple walking model to generate movements toward and away from a target via a single mechanism by relaxing the concept of approaching a target our model can reproduce the abovementioned power laws and some of the rules used in the epr model are generated these results provide a new perspective on why or how the scaling laws observed in human mobility behavior arise,0
in recent years we have seen scientists attempt to model and explain human dynamics and in particular human movement many aspects of our complex life are affected by human movements such as disease spread and epidemics modeling city planning wireless network development and disaster relief to name a few given the myriad of applications it is clear that a complete understanding of how people move in space can lead to huge benefits to our society in most of the recent works scientists have focused on the idea that people movements are biased towards frequentlyvisited locations according to them human movement is based on an explorationexploitation dichotomy in which individuals choose new locations exploration or return to frequentlyvisited locations exploitation in this work we focus on the concept of recency we propose a model in which exploitation in human movement also considers recentlyvisited locations and not solely frequentlyvisited locations we test our hypothesis against different empirical data of human mobility and show that our proposed model is able to better explain the human trajectories in these datasets,0
recent research highlighted the scaling property of human and animal mobility an interesting issue is that the exponents of scaling law for animals and humans in different situations are quite different this paper proposes a general optimization model a random walker following scaling laws whose traveling distances in each step obey a power law distribution with exponent  tries to diversify its visiting places under a given total traveling distance with a homereturn probability the results show that different optimal exponents in between 1 and 2 can emerge naturally therefore the scaling property of human and animal mobility can be understood in our framework where the discrepancy of the scaling law exponents is due to the homereturn constraint under the maximization of the visiting places diversity,0
the urban spatial structure represents the distribution of public and private spaces in cities and how people move within them while it usually evolves slowly it can change fast during largescale emergency events as well as due to urban renewal in rapidly developing countries this work presents an approach to delineate such urban dynamics in quasirealtime through a human mobility metric the mobility centrality index ks as a case study we tracked the urban dynamics of eleven spanish cities during the covid19 pandemic results revealed that their structures became more monocentric during the lockdown in the first wave but kept their regular spatial structures during the second wave to provide a more comprehensive understanding of mobility from home we also introduce a dimensionless metric kshbt which measures the extent of homebased travel and provides statistical insights into the transmission of covid19 by utilizing individual mobility data our metrics enable the detection of changes in the urban spatial structure,0
the impact of the ongoing covid19 pandemic is being felt in all spheres of our lives cutting across the boundaries of nation wealth religions or race from the time of the first detection of infection among the public the virus spread though almost all the countries in the world in a short period of time with humans as the carrier of the virus the spreading process necessarily depends on the their mobility after being infected not only in the primary spreading process but also in the subsequent spreading of the mutant variants human mobility plays a central role in the dynamics therefore on one hand travel restrictions of varying degree were imposed and are still being imposed by various countries both nationally and internationally on the other hand these restrictions have severe fall outs in businesses and livelihood in general therefore it is an optimization process exercised on a global scale with multiple changing variables here we review the techniques and their effects on optimization or proposed optimizations of human mobility in different scales carried out by data driven machine learning and model approaches,0
human mobility in an urban area is complicated the origins destinations and transport methods of each person differ the quantitative description of urban human mobility has recently attracted the attention of researchers and it highly related to urban science problems herein combined with physics inspiration we introduce a revised electric circuit model recm in which moving people are regarded as charged particles and analogical concepts of electromagnetism such as human conductivity and human potential enable us to capture the characteristics of urban human mobility we introduce the unit system ensure the uniqueness of the calculation result and reduce the computation cost of the algorithm to 110000 compared with the original ecm making the model more universal and easier to use we compared features including human conductivity and potential between different cities in japan to show our improvement of the universality and the application range of the model furthermore based on inspiration of physics we propose a route generation model rgm to simulate a human flow pattern that automatically determines suitable routes between a given origin and destination as a source and sink respectively these discoveries are expected to lead to new approaches to the solution of urban science problems,0
this paper proposes a hierarchical bayesian network model bnm to quantitatively evaluate the resilience of urban transportation infrastructure based on systemic thinkings and sustainability perspectives we investigate the longterm resilience of the road transportation systems in four cities of china from 1998 to 2017 namely beijing tianjin shanghai and chongqing respectively the model takes into account the factors involved in stages of design construction operation management and innovation of urban road transportation which collected from multisource data platforms we test the model with the forward inference sensitivity analysis and backward inference the result shows that the overall resilience of all four cities transportation infrastructure is within a moderate range with values between 50 to 60 although they all have an everincreasing economic level beijing and tianjin demonstrate a clear v shape in the longterm transportation resilience which indicates a strong multidimensional dynamic and nonlinear characteristic in resilienceeconomic coupling effect additionally the results obtained from the sensitivity analysis and backward inference suggest that urban decisionmakers should pay more attention to the capabilities of quick rebuilding and making changes to cope with future disturbance as an exploratory study this study clarifies the concepts of longterm multidimensional resilience and specific hazardrelated resilience and provides an effective decisionsupport tool for stakeholders when building sustainable infrastructure,0
the burgeoning availability of sensing technology and locationbased data is driving the expansion of analysis of human mobility networks in science and engineering research as well as in epidemic forecasting and mitigation urban planning traffic engineering emergency response and business development however studies employ datasets provided by different locationbased data providers and the extent to which the human mobility measures and results obtained from different datasets are comparable is not known to address this gap in this study we examined three prominent locationbased data sources spectus xmode and veraset to analyze human mobility networks across metropolitan areas at different scales global substructure and microscopic dissimilar results were obtained from the three datasets suggesting the sensitivity of network models and measures to datasets this finding has important implications for building generalized theories of human mobility and urban dynamics based on different datasets the findings also highlighted the need for groundtruthed human movement datasets to serve as the benchmark for testing the representativeness of human mobility datasets researchers and decisionmakers across different fields of science and technology should recognize the sensitivity of human mobility results to dataset choice and develop procedures for groundtruthing the selected datasets in terms of representativeness of data points and transferability of results,0
autonomous vehicles avs are about to be used in transportation systems in the near future to increase the level of safety and throughput of these vehicles dedicated lanes for avs have been suggested in past studies as exclusive mobility infrastructure for these types of vehicles although these lanes can bring obvious advantages in a transportation network overall performance of these lanes especially in urban areas and in micro level sight is not clear this study aims to examine the efficiency of dedicated lanes for avs in roundabout with an unbalanced traffic flow pattern four factors of travel time delay time speed of vehicles and queue of vehicles have been selected as variables of traffic performance at the roundabout two microscopic traffic simulation software aimsun and sidra intersection were used to examine the impact of the av dedicated lanes at the roundabout this study shows that the effects of an imbalanced traffic pattern in a roundabout are higher when the penetration rate of avs is lower and also dedicated lanes in roundabouts leg may improve traffic performance indicators when the penetration rate of avs is higher but this improvement is not significant,0
understanding the relationship between spatial structures of cities and environmental hazard exposures such as urban heat is essential for urban health and sustainability planning however a critical knowledge gap exists in terms of the extent to which sociospatial networks shaped by human mobility exacerbate or alleviate urban heat exposures of populations in cities in this study we utilize locationbased data to construct human mobility networks in twenty metropolitan areas in the us the human mobility networks are analyzed in conjunction with the urban heat characteristics of spatial areas we identify areas with high and low urban heat exposure and evaluate visitation patterns of populations residing in high and low urban heat areas to other spatial areas with similar and dissimilar urban heat exposure the results reveal the presence of urban heat traps in the majority of the studied metropolitan areas in which populations residing in high heat exposure areas primarily visit areas with high heat exposure the results also show a small percentage of human mobility to produce urban heat escalate visitations from low heat areas to high heat areas and heat escapes movements from high heat areas to low heat areas the findings from this study provide a better understanding of urban heat exposure in cities based on patterns of human mobility these finding contribute to a broader understanding of the intersection of human network dynamics and environmental hazard exposures in cities to inform more integrated urban design and planning to promote health and sustainability,0
community formation in sociospatial human networks is one of the important mechanisms for mitigating hazard impacts of extreme weather events research is scarce regarding latent network characteristics shaping community formation in human mobility networks during natural disasters here we examined human mobility networks in harris county texas in the context of the managed power outage forced by 2021 winter storm uri to detect communities and to evaluate latent characteristics in those communities we examined three characteristics in the communities formed within human mobility networks hazardexposure heterophily sociodemographic homophily and socialconnectedness strength the results show that population movements were shaped by sociodemographic homophily heterophilic hazard exposure and social connectedness strength our results also indicate that a community encompassing more highimpact areas would motivate population movements to areas with weaker social connectedness our findings reveal important characteristics shaping community formation in human mobility networks in hazard response specific to managed power outages formed communities are spatially colocated underscoring a best management practice to avoid prolonged power outages among areas within communities thus improving hazard exposure heterophily the findings have implications for power utility operators to account for the characteristics of sociospatial human networks when determining the patterns of managed power outages,0
previous studies have shown that human movement is predictable to a certain extent at different geographic scales existing prediction techniques exploit only the past history of the person taken into consideration as input of the predictors in this paper we show that by means of multivariate nonlinear time series prediction techniques it is possible to increase the forecasting accuracy by considering movements of friends people or more in general entities with correlated mobility patterns ie characterised by high mutual information as inputs finally we evaluate the proposed techniques on the nokia mobile data challenge and cabspotting datasets,0
the spread of infectious diseases is often influenced by human mobility across different geographical regions although numerous studies have investigated how diseases like sars and covid19 spread from china to various global locations there remains a gap in understanding how the movement of individuals contributes to disease transmission on a more personal or humantohuman level typically researchers have employed the concept of metapopulation movement to analyze how diseases move from one location to another this paper shifts focus to the dynamics of disease transmission incorporating the critical factor of distance between an infected person and a healthy individual during human movement the study delves into the impact of distance on various parameters of epidemiological dynamics throughout human mobility mathematical expressions for important epidemiological metrics such as the basic reproduction number r0 and the critical infection rate critical are derived in relation to the distance between individuals the results indicate that the proposed model closely aligns with observed patterns of covid19 spread based on the analysis done on the available datasets,0
detecting regional spatial structures based on spatial interactions is crucial in applications ranging from urban planning to traffic control in the big data era various movement trajectories are available for studying spatial structures this research uses large scale shanghai taxi trip data extracted from gpsenabled taxi trajectories to reveal traffic flow patterns and urban structure of the city using the network science methods 15 temporally stable regions reflecting the scope of peoples daily travels are found using community detection method on the network built from short trips which represent residents daily intraurban travels and exhibit a clear pattern in each region taxi traffic flows are dominated by a few hubs and hubs in suburbs impact more trips than hubs in urban areas land use conditions in urban regions are different from those in suburban areas additionally hubs in urban area associate with office buildings and commercial areas more whereas residential land use is more common in suburban hubs the taxi flow structures and land uses reveal the polycentric and layered concentric structure of shanghai finally according to the temporal variations of taxi flows and the diversity levels of taxi trip lengths we explore the total taxi traffic properties of each region and proved the city structure we find external trips across regions also take large proportion of the total traffic in each region especially in suburbs the results could help transportation policy making and shed light on the way to reveal urban structures with big data,0
although a number of studies have investigated human mobility patterns during natural hazards mechanistic models that capture mobility dynamics under largescale perturbations such as extreme floods remain scarce leveraging mobile phone data and building upon recent insights into universal mobility patterns we assess whether the general structure of population flows persists during the extreme floods that struck emiliaromagna italy in 2023 our analysis reveals that the relationship between visitor density distance and visitation frequency remains robust even under extreme flooding conditions to disentangle the effects of distance and visitation frequency we define two aggregated visitor densities the marginal density over frequency and the aggregated density over distance we find that the marginal density over frequency exhibits a timeinvariant powerlaw exponent indicating resilience to flooding disturbances in contrast the aggregated density over distance displays more complex behavior an exponential decay over biweekly periods and a powerlaw decay over a monthly interval we propose that the observed power law emerges from the superposition of exponential distributions across shorter timescales these findings provide new insights into human mobility scaling laws under extreme perturbations highlighting the robustness of visitation patterns and suggesting avenues for improved mechanistic modeling during natural disasters,0
understanding how people move within a geographic area eg a city a country or the whole world is fundamental in several applications from predicting the spatiotemporal evolution of an epidemics to inferring migration patterns mobile phone records provide an excellent proxy of human mobility showing that movements exhibit a high level of memory however the precise role of memory in widely adopted proxies of mobility as mobile phone records is unknown here we use 560 millions of call detail records from senegal to show that standard markovian approaches including higherorder ones fail in capturing real mobility patterns and introduce spurious movements never observed in reality we introduce an adaptive memorydriven approach to overcome such issues at variance with markovian models it is able to realistically model conditional waiting times ie the probability to stay in a specific area depending on individuals historical movements our results demonstrate that in standard mobility models the individuals tend to diffuse faster than what observed in reality whereas the predictions of the adaptive memory approach significantly agree with observations we show that as a consequence the incidence and the geographic spread of a disease could be inadequately estimated when standard approaches are used with crucial implications on resources deployment and policy making during an epidemic outbreak,0
while a social event such as a concert or a food festival is a common experience to people a natural disaster is experienced by a fewer individuals the ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon but devastating natural events on society such as floods based on this idea we present a comparison in terms of human mobility between an extreme local flood that occurred in 2017 in switzerland and social events which took place in the same region in the weeks before and after the inundation using mobile phone location data we show that the severe local flood and social events have a similar impact on human mobility both at the national scale and at a local scale at the national level we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place at the local level instead we detected the anomalies in time series in the number of people travelling each road and railway and we found that the distributions of anomalies and of their clusters are comparable between the flood and the social events hence our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility the proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and in general as complementary approaches for the analyses of complex systems,0
confirmed cases during the early stage of the 2009 h1n1 pdm in various countries showed an age shift between importations and local transmission cases with adults mainly responsible for seeding unaffected regions and children most frequently driving community outbreaks we introduce a multihost stochastic metapopulation model with two age classes to analytically address the role of a heterogeneously mixing population and its associated nonhomogeneous travel behaviors on the risk of a major epidemic we inform the model with statistics on demography mixing and travel behavior for europe and mexico and calibrate it to the 2009 h1n1 pdm early outbreak we varied model parameters to explore the invasion conditions under different scenarios we derive the expression for the global invasion potential of the epidemic that depends on disease transmissibility transportation network and mobility features demographic profile and mixing pattern highly assortative mixing favor the spatial containment of the epidemic this effect being contrasted by an increase in the social activity of adults vs children heterogeneity of the mobility network topology and traffic flows strongly favor the disease invasion as also a larger fraction of children traveling variations in the demography and mixing habits across countries lead to heterogeneous outbreak situations results are compatible with the h1n1 spatial spread observed the work illustrates the importance of agedependent mixing profiles and mobility features in the study of the conditions for the spatial invasion of an emerging influenza pandemic its results allow the immediate assessment of the risk of a major epidemic for a specific scenario upon availability of data and the evaluation of the effectiveness of public health interventions targeting specific age groups their interactions and mobility behaviors,0
the polycentric city model has gained popularity in spatial planning policy since it is believed to overcome some of the problems often present in monocentric metropolises ranging from congestion to difficult accessibility to jobs and services however the concept polycentric city has a fuzzy definition and as a result the extent to which a city is polycentric cannot be easily determined here we leverage the fine spatiotemporal resolution of smart travel card data to infer urban polycentricity by examining how a city departs from a welldefined monocentric model in particular we analyse the human movements that arise as a result of sophisticated forms of urban structure by introducing a novel probabilistic approach which captures the complexity of these human movements we focus on london uk and seoul south korea as our two case studies and we specifically find evidence that london displays a higher degree of monocentricity than seoul suggesting that seoul is likely to be more polycentric than london,0
mobile phone data have been widely used to model the spread of covid19 however quantifying and comparing their predictive value across different settings is challenging their quality is affected by various factors and their relationship with epidemiological indicators varies over time here we adopt a modelfree approach based on transfer entropy to quantify the relationship between mobile phonederived mobility metrics and covid19 cases and deaths in more than 200 european subnational regions we found that past knowledge of mobility does not provide statistically significant information on covid19 cases or deaths in most of the regions in the remaining ones measures of contact rates were often more informative than movements in predicting the spread of the disease while the most predictive metrics between midrange and shortrange movements depended on the region considered we finally identify geographic and demographic factors such as users coverage and commuting patterns that can help determine the best metric for predicting disease incidence in a particular location our approach provides epidemiologists and public health officials with a general framework to evaluate the usefulness of human mobility data in responding to epidemics,0
the vast majority of travel takes place within cities recently new data has become available which allows for the discovery of urban mobility patterns which differ from established results about long distance travel specifically the latest evidence increasingly points to exponential trip length distributions contrary to the scaling laws observed on larger scales in this paper in order to explore the origin of the exponential law we propose a new model which can predict individual flows in urban areas better based on the model we explain the exponential law of intraurban mobility as a result of the exponential decrease in average population density in urban areas indeed both empirical and analytical results indicate that the trip length and the population density share the same exponential decaying rate,0
from footpaths to flight routes human mobility networks facilitate the spread of communicable diseases control and elimination efforts depend on characterizing these networks in terms of connections and flux rates of individuals between contact nodes in some cases transport can be parameterized with gravitytype models or approximated by a diffusive random walk as a alternative we have isolated intranational commercial air traffic as a case study for the utility of nondiffusive heavytailed transport models we implemented new stochastic simulations of a prototypical influenzalike infection focusing on the dense highlyconnected united states air travel network we show that mobility on this network can be described mainly by a power law in agreement with previous studies remarkably we find that the global evolution of an outbreak on this network is accurately reproduced by a twoparameter spacefractional diffusion equation such that those parameters are determined by the air travel network,0
in this paper we deal with the study of travel flows and patterns of people in large populated areas information about the movements of people is extracted from coarsegrained aggregated cellular network data without tracking mobile devices individually mobile phone data are provided by the italian telecommunication company tim and consist of density profiles ie the spatial distribution of people in a given area at various instants of time by computing a suitable approximation of the wasserstein distance between two consecutive density profiles we are able to extract the main directions followed by people ie to understand how the mass of people distribute in space and time the main applications of the proposed technique are the monitoring of daily flows of commuters the organization of large events and more in general the traffic management and control,0
human interactions and mobility shape epidemic dynamics by facilitating disease outbreaks and their spatial spread across regions traditional models often isolate commuting and random mobility as separate behaviors focusing either on short recurrent trips or on random exploratory movements here we propose a unified formalism that allows a smooth transition between commuting and exploratory behavior based on travel and return probabilities we derive an analytical expression for the epidemic threshold revealing a nonmonotonic dependence on recurrence rates while recurrence tends to lower the threshold by increasing agent concentration in highcontact hubs it counterintuitively raises the invasion threshold in lowmobility scenarios suggesting that allowing recurrence may foster local outbreaks while suppressing global epidemics these results provide a comprehensive understanding of the interplay between human mobility patterns and epidemic spread with implications for containment strategies in structured populations,0
this paper investigates the impact of human activity and mobility ham in the spreading dynamics of an epidemic specifically it explores the interconnections between ham and its effect on the early spread of the covid19 virus during the early stages of the pandemic effective reproduction numbers exhibited a high correlation with human mobility patterns leading to a hypothesis that the ham system can be studied as a coupled system with disease spread dynamics this study applies the generalized koopman framework with control inputs to determine the nonlinear disease spread dynamics and the inputoutput characteristics as a locally linear controlled dynamical system the approach solely relies on the snapshots of spatiotemporal data and does not require any knowledge of the systems physical laws we exploit the koopman operator framework by utilizing the hankel dynamic mode decomposition with control hdmdc algorithm to obtain a linear disease spread model incorporating human mobility as a control input the study demonstrated that the proposed methodology could capture the impact of local mobility on the early dynamics of the ongoing global pandemic the obtained locally linear model can accurately forecast the number of new infections for various prediction windows ranging from two to four weeks the study corroborates a leaderfollower relationship between mobility and disease spread dynamics in addition the effect of delay embedding in the hdmdc algorithm is also investigated and reported a case study was performed using covid infection data from florida us and ham data extracted from google community mobility data report,0
in recent years mobility models have been reconsidered based on findings by analyzing some big datasets collected by gps sensors cellphone call records and geotagging to understand the fundamental statistical properties of the frequency of serendipitous human encounters we conducted experiments to collect longterm data on human contact using shortrange wireless communication devices which many people frequently carry in daily life by analyzing the data we showed that the majority of human encounters occur onceinanexperimentalperiod they are ichigo ichie we also found that the remaining more frequent encounters obey a powerlaw distribution they are scalefree to theoretically find the origin of these properties we introduced as a minimal human mobility model homesick lvy walk where the walker stochastically selects moving long distances as well as lvy walk or returning back home using numerical simulations and a simple meanfield theory we offer a theoretical explanation for the properties to validate the mobility model the proposed model is helpful for evaluating longterm performance of routing protocols in delay tolerant networks and mobile opportunistic networks better since some utilitybased protocols select nodes with frequent encounters for message transfer,0
the dense social contact networks and high mobility in congested urban areas facilitate the rapid transmission of infectious diseases typical mechanistic epidemiological models are either based on uniform mixing with adhoc contact processes or need realtime or archived population mobility data to simulate the social networks however the rapid and global transmission of the novel coronavirus sarscov2 has led to unprecedented lockdowns at global and regional scales leaving the archived datasets to limited use while it is often hypothesized that population density is a significant driver in disease propagation the disparate disease trajectories and infection rates exhibited by the different cities with comparable densities require a highresolution description of the disease and its drivers in this study we explore the impact of the creation of containment zones on travel patterns within the city further we use a dynamical networkbased infectious disease model to understand the key drivers of disease spread at subkilometer scales demonstrated in the city of ahmedabad india which has been classified as a sarscov2 hotspot we find that in addition to the contact network and population density road connectivity patterns and ease of transit are strongly correlated with the rate of transmission of the disease given the limited access to realtime traffic data during lockdowns we generate road connectivity networks using opensource imageries and travel patterns from opensource surveys and government reports within the proposed framework we then analyze the relative merits of social distancing enforced lockdowns and enhanced testing and quarantining mitigating the disease spread,0
we investigate a model for spatial epidemics explicitly taking into account bidirectional movements between base and destination locations on individual mobility networks we provide a systematic analysis of generic dynamical features of the model on regular and complex metapopulation network topologies and show that significant dynamical differences exist to ordinary reactiondiffusion and effective force of infection models on a lattice we calculate an expression for the velocity of the propagating epidemic front and find that in contrast to the diffusive systems our model predicts a saturation of the velocity with increasing traveling rate furthermore we show that a fully stochastic system exhibits a novel threshold for attack ratio of an outbreak absent in diffusion and force of infection models these insights not only capture natural features of human mobility relevant for the geographical epidemic spread they may serve as a starting point for modeling important dynamical processes in human and animal epidemiology population ecology biology and evolution,0
the importance of understanding human mobility patterns has led many studies to examine their spatialtemporal scaling laws these studies mainly reveal that human travel can be highly nonhomogeneous with powerlaw scaling distributions of distances and times however investigating and quantifying the extent of variability in time and space when traveling the same air distance has not been addressed so far using taxi data from five large cities we focus on several novel measures of distance and time to explore the spatiotemporal variations of taxi travel routes relative to their typical routes during peak and nonpeak periods to compare all trips using a single measure we calculate the distributions of the ratios between actual travel distances and the average travel distance as well as between actual travel times and the average travel time for all origin destinations od during peak and nonpeak periods in this way we measure the scaling of the distribution of all single trip paths with respect to their mean trip path our results surprisingly demonstrate very broad distributions for both the distance ratio and time ratio characterized by a longtail powerlaw distribution moreover all analyzed cities have larger exponents in peak hours than in nonpeak hours we suggest that the interesting results of shorter trip lengths and times characterized by larger exponents during rush hours are due to the higher availability of travelers in rush hours compared to nonrush hours,0
largescale human mobility datasets play increasingly critical roles in many algorithmic systems business processes and policy decisions unfortunately there has been little focus on understanding bias and other fundamental shortcomings of the datasets and how they impact downstream analyses and prediction tasks in this work we study data production quantifying not only whether individuals are represented in big digital datasets but also how they are represented in terms of how much data they produce we study gps mobility data collected from anonymized smartphones for ten major us cities and find that data points can be more unequally distributed between users than wealth we build models to predict the number of data points we can expect to be produced by the composition of demographic groups living in census tracts and find strong effects of wealth ethnicity and education on data production while we find that bias is a universal phenomenon occurring in all cities we further find that each city suffers from its own manifestation of it and that locationspecific models are required to model bias for each city this work raises serious questions about general approaches to debias human mobility data and urges further research,0
social structures influence a variety of human behaviors including mobility patterns but the extent to which one individuals movements can predict anothers remains an open question further latent information about an individuals mobility can be present in the mobility patterns of both social and nonsocial ties a distinction that has not yet been addressed here we develop a colocation network to distinguish the mobility patterns of an egos social ties from those of nonsocial colocators individuals not socially connected to the ego but who nevertheless arrive at a location at the same time as the ego we apply entropy and predictability measures to analyse and bound the predictive information of an individuals mobility pattern and the flow of that information from their top social ties and from their nonsocial colocators while social ties generically provide more information than nonsocial colocators we find that significant information is present in the aggregation of nonsocial colocators 37 colocators can provide as much predictive information as the top social tie and colocators can replace up to 85 of the predictive information about an ego compared with social ties that can replace up to 94 of the egos predictability the presence of predictive information among nonsocial colocators raises privacy concerns given the increasing availability of realtime mobility traces from smartphones individuals sharing data may be providing actionable information not just about their own movements but the movements of others whose data are absent both known and unknown individuals,0
motivated by recent findings that human mobility is proxy for crime behavior in big cities and that there is a superlinear relationship between the peoples movement and crime this article aims to evaluate the impact of how these findings influence police allocation more precisely we shed light on the differences between an allocation strategy in which the resources are distributed by clusters of floating population and conventional allocation strategies in which the police resources are distributed by an administrative area typically based on resident population we observed a substantial difference in the distributions of police resources allocated following these strategies what evidences the imprecision of conventional police allocation methods,0
complex networks provide a suitable framework to characterize air traffic previous works described the world air transport network as a graph where direct flights are edges and commercial airports are vertices in this work we focus instead on the properties of flight delays in the us air transportation network we analyze flight performance data in 2010 and study the topological structure of the network as well as the aircraft rotation the properties of flight delays including the distribution of total delays the dependence on the day of the week and the hourbyhour evolution within each day are characterized paying special attention to flights accumulating delays longer than 12 hours we find that the distributions are robust to changes in takeoff or landing operations different moments of the year or even different airports in the contiguous states however airports in remote areas hawaii alaska puerto rico can show peculiar distributions biased toward long delays additionally we show that long delayed flights have an important dependence on the destination airport,0
with remarkable significance in migration prediction global disease mitigation urban planning and many others an arresting challenge is to predict human mobility fluxes between any two locations a number of methods have been proposed against the above challenge including the gravity model the intervening opportunity model the radiation model the populationweighted opportunity model and so on despite their theoretical elegance all models ignored an intuitive and important ingredient in individual decision about where to go that is the possible congestion on the way and the possible crowding in the destination here we propose a microscopic mechanism underlying mobility decisions named destination choice game dcg which takes into account the crowding effects resulted from spatial interactions among individuals in comparison with the stateoftheart models the present one shows more accurate prediction on mobility fluxes across wide scales from intracity trips to intercity travels and further to internal migrations the wellknown gravity model is proved to be the equilibrium solution of a degenerated dcg neglecting the crowding effects in the destinations,0
vectorborne epidemics are the result of the combination of different factors such as the crossed contagions between humans and vectors their demographic distribution and human mobility among others the current availability of information about the former ingredients demands their incorporation to current mathematical models for vectorborne disease transmission here relying on metapopulation dynamics we propose a framework whose results are in fair agreement with those obtained from mechanistic simulations this framework allows us to derive an expression of the epidemic threshold capturing with high accuracy the conditions leading to the onset of epidemics driven by these insights we obtain a prevalence indicator to rank the patches according to the risk of being affected by a vectorborne disease we illustrate the utility of this epidemic risk indicator by reproducing the spatial distribution dengue cases reported in the city of santiago de cali colombia from 2015 to 2016,0
the last decade has witnessed the emergence of massive mobility data sets such as tracks generated by gps devices call detail records and geotagged posts from social media platforms these data sets have fostered a vast scientific production on various applications of mobility analysis ranging from computational epidemiology to urban planning and transportation engineering a strand of literature addresses data cleaning issues related to raw spatiotemporal trajectories while the second line of research focuses on discovering the statistical laws that govern human movements a significant effort has also been put on designing algorithms to generate synthetic trajectories able to reproduce realistically the laws of human mobility last but not least a line of research addresses the crucial problem of privacy proposing techniques to perform the reidentification of individuals in a database a view on state of the art cannot avoid noticing that there is no statistical software that can support scientists and practitioners with all the aspects mentioned above of mobility data analysis in this paper we propose scikitmobility a python library that has the ambition of providing an environment to reproduce existing research analyze mobility data and simulate human mobility habits scikitmobility is efficient and easy to use as it extends pandas a popular python library for data analysis moreover scikitmobility provides the user with many functionalities from visualizing trajectories to generating synthetic data from analyzing statistical patterns to assessing the privacy risk related to the analysis of mobility data sets,0
the rise of uber as the global alternative taxi operator has attracted a lot of interest recently aside from the media headlines which discuss the new phenomenon eg on how it has disrupted the traditional transportation industry policy makers economists citizens and scientists have engaged in a discussion that is centred around the means to integrate the new generation of the sharing economy services in urban ecosystems in this work we aim to shed new light on the discussion by taking advantage of a publicly available longitudinal dataset that describes the mobility of yellow taxis in new york city in addition to movement this data contains information on the fares paid by the taxi customers for each trip as a result we are given the opportunity to provide a first head to head comparison between the iconic yellow taxi and its modern competitor uber in one of the worlds largest metropolitan centres we identify situations when uber x the cheapest version of the uber taxi service tends to be more expensive than yellow taxis for the same journey we also demonstrate how ubers economic model effectively takes advantage of well known patterns in human movement finally we take our analysis a step further by proposing a new mobile application that compares taxi prices in the city to facilitate travellers taxi choices hoping to ultimately to lead to a reduction of commuter costs our study provides a case on how big datasets that become public can improve urban services for consumers by offering the opportunity for transparency in economic sectors that lack up to date regulations,0
migration patterns are complex and contextdependent with the distances migrants travel varying greatly depending on socioeconomic and demographic factors while global migration studies often focus on western countries there is a crucial gap in our understanding of migration dynamics within the african continent particularly in west africa using data from over 60000 individuals from eight west african countries this study examines the determinants of migration distance in the region our analysis reveals a bimodal distribution of migration distances while most migrants travel locally within a hundred km a smaller yet significant portion undertakes longdistance journeys often exceeding 3000 km socioeconomic factors such as employment status marital status and level of education play a decisive role in determining migration distances unemployed migrants for instance travel substantially farther 1467 km on average than their employed counterparts 295 km furthermore we find that conflictinduced migration is particularly variable with migrants fleeing violence often undertaking longer and riskier journeys our findings highlight the importance of considering both local and longdistance migration in policy decisions and support systems as well as the need for a comprehensive understanding of migration in nonwestern contexts this study contributes to the broader discourse on human mobility by providing new insights into migration patterns in western africa which in turn has implications for global migration research and policy development,0
the first goal of this study is to quantify the magnitude and spatial variability of air quality changes in the us during the covid19 pandemic we focus on two federally regulated pollutants nitrogen dioxide no2 and fine particulate matter pm25 observed concentrations at all available ground monitoring sites 240 and 480 for no2 and pm25 respectively were compared between april 2020 and april of the prior five years 20152019 as the baseline large statistically significant decreases in no2 concentrations were found at more than 65 of the monitoring sites with an average drop of 2 ppb when compared to the mean of the previous five years the same patterns are confirmed by satellitederived no2 column totals from nasa omi pm25 concentrations from the ground monitoring sites however were more likely to be higher the second goal of this study is to explain the different responses of the two pollutants during the covid19 pandemic the hypothesis put forward is that the shelterinplace measures affected peoples driving patterns most dramatically thus passenger vehicle no2 emissions were reduced commercial vehicles and electricity demand for all purposes remained relatively unchanged thus pm25 concentrations did not drop significantly to establish a correlation between the observed no2 changes and the extent to which people were sheltering in place we use a mobility index which was produced and made public by descartes labs this mobility index aggregates cell phone usage at the county level to capture changes in human movement over time we found a strong correlation between the observed decreases in no2 concentrations and decreases in human mobility by contrast no discernible pattern was detected between mobility and pm25 concentrations changes suggesting that decreases in personalvehicle traffic alone may not be effective at reducing pm25 pollution,0
with the aim to contribute to humanitarian response to disasters and violent events scientists have proposed the development of analytical tools that could identify emergency events in realtime using mobile phone data the assumption is that dramatic and discrete changes in behavior measured with mobile phone data will indicate extreme events in this study we propose an efficient system for spatiotemporal detection of behavioral anomalies from mobile phone data and compare sites with behavioral anomalies to an extensive database of emergency and nonemergency events in rwanda our methodology successfully captures anomalous behavioral patterns associated with a broad range of events from religious and official holidays to earthquakes floods violence against civilians and protests our results suggest that human behavioral responses to extreme events are complex and multidimensional including extreme increases and decreases in both calling and movement behaviors we also find significant temporal and spatial variance in responses to extreme events our behavioral anomaly detection system and extensive discussion of results are a significant contribution to the longterm project of creating an effective realtime event detection system with mobile phone data and we discuss the implications of our findings for future research to this end keywords big data call detail record emergency events human mobility,0
in 2020 countries affected by the covid19 pandemic implemented various nonpharmaceutical interventions to contrast the spread of the virus and its impact on their healthcare systems and economies using italian data at different geographic scales we investigate the relationship between human mobility which subsumes many facets of the populations response to the changing situation and the spread of covid19 leveraging mobile phone data from february through september 2020 we find a striking relationship between the decrease in mobility flows and the net reproduction number we find that the time needed to switch off mobility and bring the net reproduction number below the critical threshold of 1 is about one week moreover we observe a strong relationship between the number of days spent above such threshold before the lockdowninduced drop in mobility flows and the total number of infections per 100k inhabitants estimating the statistical effect of mobility flows on the net reproduction number over time we document a 2week lag positive association strong in march and april and weaker but still significant in june our study demonstrates the value of big mobility data to monitor the epidemic and inform control interventions during its unfolding,0
infectious diseases usually originate from a specific location within a city due to the heterogenous distribution of population and public facilities and the structural heterogeneity of human mobility network embedded in space infectious diseases break out at different locations would cause different transmission risk and control difficulty this study aims to investigate the impact of initial outbreak locations on the risk of spatiotemporal transmission and reveal the driving force behind highrisk outbreak locations first integrating mobile phone location data we built a slir susceptiblelatentinfectiousremovedbased metapopulation model to simulate the spreading process of an infectious disease ie covid19 across finegrained intraurban regions ie 649 communities of shenzhen city china based on the simulation model we evaluated the transmission risk caused by different initial outbreak locations by proposing three indexes including the number of infected cases casenum the number of affected regions regionnum and the spatial diffusion range spatialrange finally we investigated the contribution of different influential factors to the transmission risk via machine learning models results indicates that different initial outbreak locations would cause similar casenum but different regionnum and spatialrange to avoid the epidemic spread quickly to more regions it is necessary to prevent epidemic breaking out in locations with high populationmobility flow density while to avoid epidemic spread to larger spatial range remote regions with long daily trip distance of residents need attention those findings can help understand the transmission risk and driving force of initial outbreak locations within cities and make precise prevention and control strategies in advance,0
the movement changes the underlying spatial representation of the participated mobile objects or nodes in real world scenario such mobile nodes can be part of any biological network transportation network social network human interaction etc the change in the geometry leads to the change in various desirable properties of realworld networks especially in human interaction networks in real life human movement is concerned for better lifestyle where they form their new connections due to the geographical changes therefore in this paper we design a model for geometric networks with mobile nodes gnmn and conduct a comprehensive statistical analysis of their properties we analyze the effect of node mobility by evaluating key network metrics such as connectivity node degree distribution second hop neighbors and centrality measures through extensive simulations we observe significant variations in the behavior of geometric networks with mobile nodes,0
predicting human mobility patterns has many practical applications in urban planning traffic engineering infectious disease epidemiology emergency management and locationbased services developing a universal model capable of accurately predicting the mobility fluxes between locations is a fundamental and challenging problem in regional economics and transportation science here we propose a new parameterfree model named opportunity priority selection model as an alternative in human mobility prediction the basic assumption of the model is that an individual will select destination locations that present higher opportunity benefits than the location opportunities of the origin and the intervening opportunities between the origin and destination we use real mobility data collected from a number of cities and countries to demonstrate the predictive ability of this simple model the results show that the new model offers universal predictions of intracity and intercity mobility patterns that are consistent with real observations thus suggesting that the proposed model better captures the mechanism underlying human mobility than previous models,0
in march of this year covid19 was declared a pandemic and it continues to threaten public health this global health crisis imposes limitations on daily movements which have deteriorated every sector in our society understanding public reactions to the virus and the nonpharmaceutical interventions should be of great help to fight covid19 in a strategic way we aim to provide tangible evidence of the human mobility trends by comparing the daybyday variations across the us largescale public mobility at an aggregated level is observed by leveraging mobile device location data and the measures related to social distancing our study captures spatial and temporal heterogeneity as well as the sociodemographic variations regarding the pandemic propagation and the nonpharmaceutical interventions all mobility metrics adapted capture decreased public movements after the national emergency declaration the population staying home has increased in all states and becomes more stable after the stayathome order with a smaller range of fluctuation there exists overall mobility heterogeneity between the income or population density groups the public had been taking active responses voluntarily staying home more to the instate confirmed cases while the stayathome orders stabilize the variations the study suggests that the public mobility trends conform with the government message urging to stay home we anticipate our datadriven analysis offers integrated perspectives and serves as evidence to raise public awareness and consequently reinforce the importance of social distancing while assisting policymakers,0
human travel patterns are commonly studied as networks in which the points of departure and destination are encoded as nodes and the travel frequency between two points is recorded as a weighted edge however because travelers often visit multiple destinations which constitute pathways an analysis incorporating pathway statistics is expected to be more informative over an approach based solely on pairwise frequencies hence in this study we apply a higherorder network representation framework to identify characteristic travel patterns from foreign visitor pathways in japan we expect that the results herein are mainly useful for marketing research in the tourism industry,0
we introduce a community detection method that finds clusters in network timeseries by introducing an algorithm that finds significantly interconnected nodes across time these connections are either increasing decreasing or constant over time significance of nodal connectivity within a set is judged using the weighted configuration null model at each timepoint then a novel significancetesting scheme is used to assess connectivity at all time points and the direction of its timetrend we apply this method to bikeshare networks in new york city and chicago and taxicab pickups and dropoffs in new york to find and illustrate patterns in human mobility in urban zones results show stark geographical patterns in clusters that are growing and declining in relative usage across time and potentially elucidate latent economic or demographic trends,0
accurate modelling of local population movement patterns is a core contemporary concern for urban policymakers affecting both the short term deployment of public transport resources and the longer term planning of transport infrastructure yet while macrolevel population movement models such as the gravity and radiation models are well developed microlevel alternatives are in much shorter supply with most macromodels known to perform badly in smaller geographic confines in this paper we take a first step to remedying this deficit by leveraging two novel datasets to analyse where and why macrolevel models of human mobility break down at small scales in particular we use an anonymised aggregate dataset from a major mobility app and combine this with freely available data from openstreetmap concerning landuse composition of different areas around the county of oxfordshire in the united kingdom we show where different models fail and make the case for a new modelling strategy which moves beyond rough heuristics such as distance and population size towards a detailed granular understanding of the opportunities presented in different areas of the city,0
understanding human mobility during disastrous events is crucial for emergency planning and disaster management here we develop a methodology involving the construction of timevarying multilayer networks in which edges encode observed movements between spatial regions census tracts and network layers encode different movement categories according to industry sectors eg visitations to schools hospitals and grocery stores this approach provides a rich characterization of human mobility thereby complementing studies examining the riskaversion activities of evacuation and sheltering in place focusing on the 2021 texas winter storm as a case study which led to many casualties we find that people largely reduced their movements to ambulatory healthcare services restaurants and schools but prioritized movements to grocery stores and gas stations additionally we study the predictability of nodes in and outdegrees in the multilayer networks which encode movements into and out of census tracts we find that inward movements are harder to predict than outward movements and even more so during this winter storm our findings about the reduction prioritization and predictability of sectorspecific human movements could inform mobilityrelated decisions arising from future extreme weather events,0
to contain the pandemic of coronavirus covid19 in mainland china the authorities have put in place a series of measures including quarantines social distancing and travel restrictions while these strategies have effectively dealt with the critical situations of outbreaks the combination of the pandemic and mobility controls has slowed chinas economic growth resulting in the first quarterly decline of gross domestic product gdp since gdp began to be calculated in 1992 to characterize the potential shrinkage of the domestic economy from the perspective of mobility we propose two new economic indicators the new venues created nvc and the volumes of visits to venue v3 as the complementary measures to domestic investments and consumption activities using the data of baidu maps the historical records of these two indicators demonstrated strong correlations with the past figures of chinese gdp while the status quo has dramatically changed this year due to the pandemic we hereby presented a quantitative analysis to project the impact of the pandemic on economies using the recent trends of nvc and v3 we found that the most affected sectors would be traveldependent businesses such as hotels educational institutes and public transportation while the sectors that are mandatory to human life such as workplaces residential areas restaurants and shopping sites have been recovering rapidly analysis at the provincial level showed that the selfsufficient and selfsustainable economic regions with internal supplies production and consumption have recovered faster than those regions relying on global supply chains,0
cellular phones are now offering an ubiquitous means for scientists to observe life how people act move and respond to external influences they can be utilized as measurement devices of individual persons and for groups of people of the social context and the related interactions the picture of human life that emerges shows complexity which is manifested in such data in properties of the spatiotemporal tracks of individuals we extract from smartphonebased data for a set of persons important locations such as home work and so forth over fixed length timeslots covering the days in the dataset this set of typical places is heavytailed a powerlaw distribution with an exponent close to 17 to analyze the regularities and stochastic features present the days are classified for each person into regular personal patterns to this are superimposed fluctuations for each day this randomness is measured by life entropy computed both before and after finding the clustering so as to subtract the contribution of a number of patterns the main issue that we then address is how predictable individuals are in their mobility the patterns and entropy are reflected in the predictability of the mobility of the life both individually and on average we explore the simple approaches to guess the location from the typical behavior and of exploiting the transition probabilities with time from location or activity a to b the patterns allow an enhanced predictability at least up to a few hours into the future from the current location such fixed habits are most clearly visible in the workingday length,0
urban road networks are well known to have universal characteristics and scaleinvariant patterns despite the different geographical and historical environments of cities previous studies on universal characteristics of the urban road networks mostly have paid attention to their network properties but often ignored the spatial networked structures to fill the research gap we explore the underlying spatial patterns of road networks in doing so we inspect the travelroute efficiency in a given road network across 70 global cities which provides information on the usage pattern and functionality of the road structure the efficiency is quantified by the detour patterns of the travel routes estimated by the detour index di the di is a longstanding popular measure but its spatiality has been barely considered so far in this study we probe the behavior of di with respect to spatial variables by scanning the network radially from a city center through empirical analysis we first discover universal properties in di throughout most cities which are summarized as a constant behavior of di regardless of the radial position from a city center and clear collapse into a single curve for dis for various radii with respect to the angular distance especially the latter enables us to know the scaling factor in the length scale we also reveal that the coreperiphery spatial structure of the roads induces the universal pattern which is supported by an artificial road network model furthermore we visualize the spatial di pattern on the city map to figure out the cityspecific characteristics the most and least efficient connections of several representative cities show the potential for practical implications in analyzing individual cities,0
we introduce a surveillance strategy specifically designed for urban areas to enhance preparedness and response to disease outbreaks by leveraging the unique characteristics of human behavior within urban contexts by integrating data on individual residences and travel patterns we construct a mixing matrix that facilitates the identification of critical pathways that ease pathogen transmission across urban landscapes enabling targeted testing strategies our approach not only enhances public health systems ability to provide early epidemiological alerts but also underscores the variability in strategy effectiveness based on urban layout we prove the feasibility of our mobilityinformed policies by mapping essential mobility flows to major transit stations showing that few resources focused on specific stations yields a more effective surveillance than nontargeted approaches this study emphasizes the critical role of integrating human behavioral patterns into epidemic management strategies to improve the preparedness and resilience of major cities against future outbreaks,0
we model human mobility as a combinatorial allocation process treating trips as distinguishable balls assigned to locationbins and generating origindestination od networks from this analogy we construct a unified threescale framework enumerative probabilistic and continuum graphon ensembles and prove a renormalization theorem showing that in the large sparse regime these representations converge to a universal mixedpoisson law the framework yields compact formulas for key mobility observables including destination occupancy vacancy of unvisited sites coverage a stoppingtime extension of the coupon collector problem and overflow beyond finite capacities simulations with gravitylike kernels calibrated on empirical od data closely match the asymptotic predictions by connecting exact combinatorial models with continuum analysis the results offer a principled toolkit for synthetic network generation congestion assessment and the design of sustainable urban mobility policies,0
extreme heat is a growing threat to both individual livelihoods and broader economies killing a growing number of people each year as temperatures rise in many parts of the world and limiting productivity many studies document the link between heat waves and mortality or morbidity and others explore the economic consequences of them but few are able to determine how populations respond to the shock of extreme heat in daytoday activity toward this end we investigate the link between human mobility and ambient temperature examining indonesia india and mexico we show that extreme heat reduces mobility by up to 10 in urban settings with losses concentrated midday we examine the shape of the relationship finding that while heat reduces activity very hot days and very long heat waves may induce more of it indicating different adaptation effects are stronger in poorer areas twinning these models with climate projections we show that without adaptation mobility may fall 12 per year on aggregate with certain seasons and places seeing activity fall by as much as 10 according to our estimates small cities will face the highest relative losses and large cities will experience the greatest absolute impacts,0
multilevel modeling is increasingly relevant in the context of modelling and simulation since it leads to several potential benefits such as software reuse and integration the split of semantically separated levels into submodels the possibility to employ different levels of detail and the potential for parallel execution the coupling that inevitably exists between the submodels however implies the need for maintaining consistency between the various components more so when different simulation paradigms are employed eg sequential vs parallel discrete vs continuous in this paper we argue that multilevel modelling is well suited for the simulation of human mobility since it naturally leads to the decomposition of the model into two layers the micro and macro layer where individual entities micro and longrange interactions macro are described in this paper we investigate the challenges of multilevel modeling and describe some preliminary results using prototype implementations of multilayer simulators in the context of epidemic diffusion and vehicle pollution,0
for transportation hubs leveraging pedestrian flows for commercial activities presents an effective strategy for funding maintenance and infrastructure improvements however this introduces new challenges as consumer behaviors can disrupt pedestrian flow and efficiency to optimize both retail potential and pedestrian efficiency careful strategic planning in store layout and facility dimensions was done by expert judgement due to the complexity in pedestrian dynamics in the retail areas of transportation hubs this paper introduces an attentionbased movement model to simulate these dynamics by simulating retail potential of an area through the duration of visual attention it receives and pedestrian efficiency via speed loss in pedestrian walking behaviors the study further explores how design features can influence the retail potential and pedestrian efficiency in a bidirectional corridor inside a transportation hub project webpage,0
next place prediction algorithms are invaluable tools capable of increasing the efficiency of a wide variety of tasks ranging from reducing the spreading of diseases to better resource management in areas such as urban planning in this work we estimate upper and lower limits on the predictability of human mobility to help assess the performance of competing algorithms we do this using gps traces from 604 individuals participating in a multi year long experiment the copenhagen networks study earlier works focusing on the prediction of a participants whereabouts in the next time bin have found very high upper limits 90 we show that these upper limits are highly dependent on the choice of a spatiotemporal scales and mostly reflect stationarity ie the fact that people tend to not move during small changes in time this leads us to propose an alternative approach which aims to predict the next location rather than the location in the next bin our approach is independent of the temporal scale and introduces a natural length scale by removing the effects of stationarity we show that the predictability of the next location is significantly lower 71 than the predictability of the location in the next bin,0
this study investigates travel behavior determinants based on a multiday travel survey conducted in the region of ghent belgium due to the limited data reliability of the data sample and the influence of outliers exerted on classical principal component analysis robust principal component analysis robpca is employed in order to reveal the explanatory variables responsible for most of the variability interpretation of the results is eased by utilizing rospca the application of rospca reveals six distinct principal components where each is determined by a few variables among others our results suggest a key role of variable categories such as journey purposerelated impedance and journey inherent constraints surprisingly the variables associated with journey timing turn out to be less important finally our findings reveal the critical role of outliers in travel behavior analysis this suggests that a systematic understanding of how outliers contribute to observed mobility behavior patterns as derived from travel surveys is needed in this regard the proposed methods serve for processing raw data typically used in activitybased modelling,0
the exploitation of high volume of geolocalized data from social sport tracking applications of outdoor activities can be useful for natural resource planning and to understand the human mobility patterns during leisure activities this geolocalized data represents the selection of hike activities according to subjective and objective factors such as personal goals personal abilities trail conditions or weather conditions in our approach human mobility patterns are analysed from trajectories which are generated by hikers we propose the generation of the trail network identifying special points in the overlap of trajectories trail crossings and trailheads define our network and shape topological features we analyse the trail network of balearic islands as a case of study using complex weighted network theory the analysis is divided into the four seasons of the year to observe the impact of weather conditions on the network topology the number of visited places does not decrease despite the large difference in the number of samples of the two seasons with larger and lower activity it is in summer season where it is produced the most significant variation in the frequency and localization of activities from inland regions to coastal areas finally we compare our model with other related studies where the network possesses a different purpose one finding of our approach is the detection of regions with relevant importance where landscape interventions can be applied in function of the communities,0
given its wideranging and longlasting impacts covid19 especially its spatial spreading dynamics has received much attention knowledge of such dynamics helps public health professionals and city managers devise and deploy efficient contacttracing and treatment measures however most existing studies focus on aggregate mobility flows and have rarely exploited the widely available disaggregatelevel human mobility data in this paper we propose a personalized pagerank ppr method to estimate covid19 transmission risks based on a bipartite network of people and locations the method incorporates both mobility patterns of individuals and their spatiotemporal interactions to validate the applicability and relevance of the proposed method we examine the interplay between the spread of covid19 cases and intracity mobility patterns in a small synthetic network and a realworld mobility network from hong kong china based on transit smart card data we compare the recall sensitivity accuracy and spearmans correlation coefficient between the estimated transmission risks and number of actual cases based on various mass tracing and testing strategies including pprbased pagerank prbased locationbased routebased and base case no strategy the results show that the pprbased method achieves the highest efficiency accuracy and spearmans correlation coefficient with the actual case number this demonstrates the value of ppr for transmission risk estimation and the importance of incorporating individual mobility patterns for efficient contacttracing and testing,0
despite the growing recognition of the importance of inclusive transportation policies nationwide there is still a gap as the existing transportation models often fail to capture the unique travel behavior of people with disabilities this research study focuses on understanding the mode choice behavior of individuals with travellimited disabilities and comparing the group with no such disability the study identified key factors influencing mode preferences for both groups by utilizing utahs household travel survey simulation algorithm and multinomial logit model explanatory variables include household and sociodemographic attributes personal trip characteristics and built environment variables the analysis revealed intriguing trends including a shift towards carpooling among disabled individuals people with disabilities placed less emphasis on travel time saving a lower value of travel time for people with disabilities is potentially due to factors like parttime work reduced transit fare and no or shared cost for carpooling despite a 50 fare reduction for the disabled group transit accessibility remains a significant barrier in their choice of transit mode in downtown areas people with no disability were found to choose transit compared to driving whereas disabled people preferred carpooling travelers with no driving licenses and disabled people who use transit daily showed complex travel patterns among multiple modes the study emphasizes the need for accessible and inclusive transportation options such as improved public transit services shorter first and last miles in transit and better connectivity for nonmotorized modes to cater to the unique needs of disabled travelers the findings of this study have significant policy implications such as an inclusive mode choice modeling framework for creating a more sustainable and inclusive transportation system,0
in 2020 and 2021 the spread of covid19 was globally addressed by imposing restrictions on the distance of individual travel recent literature has uncovered a clear pattern in human mobility that underlies the complexity of urban mobility r cdot f the product of distance traveled r and frequency of return f per user to a given location is invariant across space this paper asks whether the invariant rcdot f also serves as a driver for epidemic spread so that the risk associated with human movement can be modeled by a unifying variable rcdot f we use two largescale datasets of individual human mobility to show that there is in fact a simple relation between r and f and both speed and spatial dispersion of disease spread this discovery could assist in modeling spread of disease and inform travel policies in future epidemics based not only on travel distance r but also on frequency of return f,0
estimating dynamic origindestination od traffic flow is crucial for understanding traffic patterns and the traffic network while dynamic origindestination estimation dode has been studied for decades as a useful tool for estimating traffic flow few existing models have considered its potential in evaluating the influence of policy on travel activity this paper proposes a datadriven approach to estimate od traffic flow using sensor data on highways and local roads we extend prior dode models to improve accuracy and realism in order to estimate how policies affect od traffic flow in large urban networks we applied our approach to a case study in los angeles county where we developed a traffic network estimated od traffic flow between health districts during covid19 and analyzed the relationship between od traffic flow and demographic characteristics such as income our findings demonstrate that the proposed approach provides valuable insights into traffic flow patterns and their underlying demographic factors for a largescale traffic network specifically our approach allows for evaluating the impact of policy changes on travel activity the approach has practical applications for transportation planning and traffic management enabling a better understanding of traffic flow patterns and the impact of policy changes on travel activity,0
covid19 is highly transmissible and containing outbreaks requires a rapid and effective response because infection may be spread by people who are presymptomatic or asymptomatic substantial undetected transmission is likely to occur before clinical cases are diagnosed thus when outbreaks occur there is a need to anticipate which populations and locations are at heightened risk of exposure in this work we evaluate the utility of aggregate human mobility data for estimating the geographic distribution of transmission risk we present a simple procedure for producing spatial transmission risk assessments from nearrealtime population mobility data we validate our estimates against three welldocumented covid19 outbreak scenarios in australia two of these were welldefined transmission clusters and one was a community transmission scenario our results indicate that mobility data can be a good predictor of geographic patterns of exposure risk from transmission centres particularly in scenarios involving workplaces or other environments associated with habitual travel patterns for community transmission scenarios our results demonstrate that mobility data adds the most value to risk predictions when case counts are low and spatially clustered our method could assist health systems in the allocation of testing resources and potentially guide the implementation of geographicallytargeted restrictions on movement and social interaction,0
increasingly available highfrequency location datasets derived from smartphones provide unprecedented insight into trajectories of human mobility these datasets can play a significant and growing role in informing preparedness and response to natural disasters however limited tools exist to enable rapid analytics using mobility data and tend not to be tailored specifically for disaster risk management we present an opensource pythonbased toolkit designed to conduct replicable and scalable postdisaster analytics using gps location data privacy system capabilities and potential expansions of textitmobilkit are discussed,0
in ad hoc wireless networking units are connected to each other rather than to a central fixed infrastructure constructing and maintaining such networks create several tradeoff problems between robustness communication speed power consumption etc that bridges engineering computer science and the physics of complex systems in this work we address the role of mobility patterns of the agents on the optimal tuning of a smallworld type network construction method by this method the network is updated periodically and held static between the updates we investigate the optimal updating times for different scenarios of the movement of agents modeling for example the fattailed trip distances and periodicities of human travel we find that these mobility patterns affect the power consumption in nontrivial ways and discuss how these effects can best be handled,0
travel activities have been widely applied to quantify spatial interactions between places regions and nations in this paper we model the spatial connectivities between 652 traffic analysis zones tazs in beijing by a taxi od dataset first we unveil the gravitational structure of intraurban spatial connectivities of beijing on overall the intertaz interactions are well governed by the gravity model gij pipjdij where pi pj are degrees of taz i j and dij the distance between them with a goodnessoffit around 08 second the network based analysis well reveals the polycentric form of beijing last we detect the semantics of intertaz connectivities based on their spatiotemporal patterns we further find that intertaz connections deviating from the gravity model can be well explained by link semantics,0
human migration is a type of human mobility where a trip involves a person moving with the intention of changing their home location predicting human migration as accurately as possible is important in city planning applications international trade spread of infectious diseases conservation planning and public policy development traditional human mobility models such as gravity models or the more recent radiation model predict human mobility flows based on population and distance features only these models have been validated on commuting flows a different type of human mobility and are mainly used in modeling scenarios where large amounts of prior ground truth mobility data are not available one downside of these models is that they have a fixed form and are therefore not able to capture more complicated migration dynamics we propose machine learning models that are able to incorporate any number of exogenous features to predict origindestination human migration flows our machine learning models outperform traditional human mobility models on a variety of evaluation metrics both in the task of predicting migrations between us counties as well as international migrations in general predictive machine learning models of human migration will provide a flexible base with which to model human migration under different whatif conditions such as potential sea level rise or population growth scenarios,0
multimodal transportation systems can be represented as timeresolved multilayer networks where different transportation modes connecting the same set of nodes are associated to distinct network layers their quantitative description became possible recently due to openly accessible datasets describing the geolocalised transportation dynamics of large urban areas advancements call for novel analytics which combines earlier established methods and exploits the inherent complexity of the data here our aim is to provide a novel userbased methodological framework to represent public transportation systems considering the total travel time its variability across the schedule and taking into account the number of transfers necessary using this framework we analyse public transportation systems in several french municipal areas we incorporate travel routes and times over multiple transportation modes to identify efficient transportation connections and nontrivial connectivity patterns the proposed method enables us to quantify the networks overall efficiency as compared to the specific demand and to the car alternative,0
understanding and predicting outbreaks of contagious diseases are crucial to the development of society and public health especially for underdeveloped countries however challenging problems are encountered because of complex epidemic spreading dynamics influenced by spatial structure and human dynamics including both human mobility and human interaction intensity we propose a systematical model to depict nationwide epidemic spreading in cte divoire which integrates multiple factors such as human mobility human interaction intensity and demographic features we provide insights to aid in modeling and predicting the epidemic spreading process by datadriven simulation and theoretical analysis which is otherwise beyond the scope of local evaluation and geometrical views we show that the requirement that the average local basic reproductive number to be greater than unity is not necessary for outbreaks of epidemics the observed spreading phenomenon can be roughly explained as a heterogeneous diffusionreaction process by redefining mobility distance according to the human mobility volume between nodes which is beyond the geometrical viewpoint however the heterogeneity of human dynamics still poses challenges to precise prediction,0
understanding the patterns of human mobility between cities has various applications from transport engineering to spatial modeling of the spreading of contagious diseases we adopt a citycentric datadriven perspective to quantify such patterns and introduce the mobility signature as a tool for understanding how a city or a region is embedded in the wider mobility network we demonstrate the potential of the mobility signature approach through two applications that build on mobilephonebased data from finland first we use mobility signatures to show that the wellknown radiation model is more accurate for mobility flows associated with larger cities while the traditional gravity model appears a better fit for less populated areas second we illustrate how the sarscov2 pandemic disrupted the mobility patterns in finland in the spring of 2020 these two cases demonstrate the ability of the mobility signatures to quickly capture features of mobility flows that are harder to extract using more traditional methods,0
human mobility is a key factor in spatial disease dynamics and related phenomena in computational models host mobility is typically modelled by diffusion in space or on metapolulation networks alternatively an effective force of infection across distance has been introduced to capture spatial dispersal implicitly both approaches do not account for important aspects of natural human mobility diffusion does not capture the high degree of predictability in natural human mobility patters eg the high percentage of return movements to individuals base location the effective force of infection approach assumes immediate equilibrium with respect to dispersal these conditions are typically not met in natural scenarios we investigate an epidemiological model that explicitly captures natural individual mobility patterns we systematically investigate generic dynamical features of the model on regular lattices as well as metapopulation networks and show that generally the model exhibits significant dynamical differences in comparison to ordinary diffusion and effective force of infection models for instance the natural human mobility model exhibits a saturation of wave front speeds and a novel type of invasion threshold that is a function of the return rate in mobility patterns in the light of these new findings and with the availability of precise and pervasive data on human mobility our approach provides a framework for a more sophisticated modeling of spatial disease dynamics,0
in this paper we study how interactions between populations impact epidemic spread we extend the classical seir model to include both integrationbased disease transmission simulation and population flow our model differs from existing ones by having a more detailed representation of travel patterns without losing tractability this allows us to study the epidemic consequence of interregional travel with high fidelity in particular we define emphtravel cadence as a twodimensional measure of interregional travel and show that both dimensions modulate epidemic spread this technical insight leads to policy recommendations pointing to a family of simple policy trajectories that can effectively curb epidemic spread while maintaining a basic level of mobility,0
human mobility is becoming an accessible field of study thanks to the progress and availability of tracking technologies as a common feature of smart phones we describe an example of a scalable experiment exploiting these circumstances at a public outdoor fair in barcelona spain participants were tracked while wandering through an open space with activity stands attracting their attention we develop a general modeling framework based on langevin dynamics which allows us to test the influence of two distinct types of ingredients on mobility reactive or contextdependent factors modelled by means of a force field generated by attraction points in a given spatial configuration and active or inherent factors modelled from intrinsic movement patterns of the subjects the additive and constructive framework model accounts for the observed features starting with the simplest model purely random walkers as a reference we progressively introduce different ingredients such as persistence memory and perceptual landscape aiming to untangle active and reactive contributions and quantify their respective relevance the proposed approach may help in anticipating the spatial distribution of citizens in alternative scenarios and in improving the design of public events based on a factsbased approach,0
host mobility plays a fundamental role in the spatial spread of infectious diseases previous theoretical works based on the integration of network theory into the metapopulation framework have shown that the heterogeneities that characterize real mobility networks favor the propagation of epidemics nevertheless the studies conducted so far assumed the mobility process to be either markovian or nonmarkovian with a fixed traveling time scale available statistics however show that the time spent by travelers at destination is characterized by wide fluctuations ranging between a single day up to several months such varying length of stay crucially affects the chance and duration of mixing events among hosts and may therefore have a strong impact on the spread of an emerging disease here we present an analytical and computational study of epidemic processes on a complex subpopulation network where travelers have memory of their origin and spend a heterogeneously distributed time interval at their destination through analytical calculations and numerical simulations we show that the heterogeneity of the length of stay alters the expression of the threshold between local outbreak and global invasion and moreover it changes the epidemic behavior of the system in case of a global outbreak additionally our theoretical framework allows us to study the effect of changes in the traveling behavior in response to the infection by considering a scenario in which sick individuals do not leave their home location finally we compare the results of our nonmarkovian framework with those obtained with a classic markovian approach and find relevant differences between the two in the estimate of the epidemic invasion potential as well as of the timing and the pattern of its spatial spread,0
healthy and liveable neighbourhoods have increasingly been recognised as essential components of sustainable urban development yet ambiguity surrounding their definition and constituent elements presents challenges in understanding and evaluating neighbourhood profiles highlighting the need for a more detailed and systematic assessment this research develops a composite liveability index for greater london based on metrics related to the proximity density and diversity of pois along with population density and investigates how neighbourhood liveability relates to active travel behaviour the index is based on the principles of the 15minute city paradigm moreno et al 2021 and developed in line with oecd guidelines for composite indicators european union and joint research centre 2008 the index revealed distinct spatial patterns of neighbourhood liveability with high liveability neighbourhoods predominantly clustered in inner london decomposing the index provided further insights into the strengths and weaknesses of each neighbourhood footfall modelling using ordinary least squares ols and geographically weighted regression gwr indicates a generally positive relationship between liveability and footfall with spatial variation in the strength of this association this research offers a new perspective on conceptualising and measuring liveability demonstrating its role as an urban attractor that fosters social interaction and active engagement,0
in the era of mobile computing understanding human mobility patterns is crucial in order to better design protocols and applications many studies focus on different aspects of human mobility such as peoples points of interests routes traffic individual mobility patterns among others in this work we propose to look at human mobility through a social perspective ie analyze the impact of social groups in mobility patterns we use the mit reality mining proximity trace to detect track and investigate groups evolution throughout time our results show that group meetings happen in a periodical fashion and present daily and weekly periodicity we analyze how groups dynamics change over day hours and find that group meetings lasting longer are those with less changes in members composition and with members having stronger social bonds with each other our findings can be used to propose meeting prediction algorithms opportunistic routing and information diffusion protocols taking advantage of those revealed properties,0
trip flow between areas is a fundamental metric for human mobility research given its identification with travel demand and its relevance for transportation and urban planning many models have been developed for its estimation these models focus on flow intensity disregarding the information provided by the local mobility orientation a fieldtheoretic approach can overcome this issue and handling both intensity and direction at once here we propose a general vectorfield representation starting from individuals trajectories valid for any type of mobility by introducing four models of spatial exploration we show how individuals elections determine the mesoscopic properties of the mobility field distance optimization in long displacements and randomlike local exploration are necessary to reproduce empirical field features observed in chinese logistic data and in new york city foursquare checkins our framework is an essential tool to capture hidden symmetries in mesoscopic urban mobility it establishes a benchmark to test the validity of mobility models and opens the doors to the use of field theory in a wide spectrum of applications,0
shared rides are often considered to be a promising travel alternative that could efficiently pool people together while offering a doortodoor service notwithstanding even though demand distribution patterns are expected to greatly affect the potential for ridepooling their impact remains unknown in this study we explore the shareability of various demand patterns we devise a set of experiments tailored to identify the most promising demand patterns for introducing ridepooling services by varying the number of centers the dispersion of destinations around each of these centers and the trip length distribution when matching trips into rides we do not only ensure their mutual compatibility in time and space but also that shared rides are only composed by travellers who find the ridepooling offer to be more attractive than the private ridehailing alternative given the tradeoffs between travel time fare and discomfort we measure the shareability potential using a series of metrics related to the extent to which passenger demand can be assigned to shared rides our findings indicate that introducing a ridepooling service can reduce vehiclehours by 1859 under a fixed demand level and depending on the concentration of travel destinations around the center and the trip length distribution system efficiency correlates positively with the former and negatively with the latter a shift from a monocentric to a polycentric demand pattern is found to have a limited impact on the prospects of shared rides,0
probability distributions of human displacements has been fit with exponentially truncated lvy flights or fat tailed pareto inverse power law probability distributions thus people usually stay within a given location for example the city of residence but with a nonvanishing frequency they visit nearby or far locations too herein we show that an important empirical distribution of human displacements range from 1 to 1000 km can be well fit by three consecutive pareto distributions with simple integer exponents equal to 1 2 and gtrapprox 3 these three exponents correspond to three displacement range zones of about 1 km lesssim r lesssim 10 km 10 km lesssim r lesssim 300 km and 300 km lesssim r lesssim 1000 km respectively these three zones can be geographically and physically well determined as displacements within a city visits to nearby cities that may occur within just oneday trips and visit to far locations that may require multidays trips the incremental integer values of the three exponents can be easily explained with a threescale mobility costbenefit model for human displacements based on simple geometrical constrains essentially people would divide the space into three major regions close medium and far distances and would assume that the travel benefits are randomlyuniformly distributed mostly only within specific urbanlike areas,0
in this paper a novel transport planning model system tpms is formulated which is built on the concepts of supernetworks multimodality integrity and calibration in the proposed formulation activity travel pattern atp choice facets including the choices of activity activity sequence mode departure time and parking location are all unified into a timedependent supernetwork the proposed model accounts for the dynamicity of the network including timeofday and congestion effects these help capturing the interdependencies among all different attributes of a full transport planning system moreover the proposed tpms explicitly formulates an operating capacitated public transport system to allow visiting locations multiple times and to alleviate the complexity of the proposed supernetwork a novel multivisit vehicle routing problem is proposed which does not enumerate the node and link visits in order to calibrate the model based on the major travel attributes of the travel survey data a set of splitting ratios are introduced to distribute trips on the supernetwork the model uses the splitting ratios to integrate the supernetwork and the traffic assignment model in a unified tpms structure at last numerical examples are provided to demonstrate the advantages of the proposed approach,0
existing empirical work has focused on assessing the effectiveness of nonpharmaceutical interventions on human mobility to contain the spread of covid19 less is known about the ways in which the covid19 pandemic has reshaped the spatial patterns of population movement within countries anecdotal evidence of an urban exodus from large cities to rural areas emerged during early phases of the pandemic across western societies yet these claims have not been empirically assessed traditional data sources such as censuses offer coarse temporal frequency to analyse population movement over shorttime intervals drawing on a data set of 21 million observations from facebook users we aim to analyse the extent and evolution of changes in the spatial patterns of population movement across the ruralurban continuum in britain over an 18month period from march 2020 to august 2021 our findings show an overall and sustained decline in population movement during periods of high stringency measures with the most densely populated areas reporting the largest reductions during these periods we also find evidence of higherthanaverage mobility from highly dense population areas to low densely populated areas lending some support to claims of largescale population movements from large cities yet we show that these trends were temporary overall mobility levels trended back to precoronavirus levels after the easing of nonpharmaceutical interventions following these interventions we also found a reduction in movement to low density areas and a rise in mobility to high density agglomerations overall these findings reveal that while covid19 generated shock waves leading to temporary changes in the patterns of population movement in britain the resulting vibrations have not significantly reshaped the prevalent structures in the national pattern of population movement,0
in this chapter we discuss urban mobility from a complexity science perspective first we give an overview of the datasets that enable this approach such as mobile phone records locationbased social network traces or gps trajectories from sensors installed on vehicles we then review the empirical and theoretical understanding of the properties of human movements including the distribution of travel distances and times the entropy of trajectories and the interplay between exploration and exploitation of locations next we explain generative and predictive models of individual mobility and their limitations due to intrinsic limits of predictability finally we discuss urban transport from a systemic perspective including systemwide challenges like ridesharing multimodality and sustainable transport,0
we present a collection of networks that describe the travel patterns between municipalities in mexico between 2020 and 2021 using anonymized mobile device geolocation data we constructed directed weighted networks representing the normalized volume of travels between municipalities we analysed changes in global graph total weight sum local centrality measures and mesoscale community structure network features we observe that changes in these features are associated with factors such as covid19 restrictions and population size in general events in early 2020 when initial covid19 restrictions were implemented induced more intense changes in network features whereas later events had a less notable impact in network features we believe these networks will be useful for researchers and decision makers in the areas of transportation infrastructure planning epidemic control and network science at large,0
as the discovery of nonpoissonian statistics of human mobility trajectories more attention has been paid to understanding the role of these patterns in different dynamics in this study we first introduce the heterogeneous mobility of mobile agents into dynamical networks and then investigate the forwarding strategy on the heterogeneous dynamical networks we find that the faster speed and the higher proportion of highspeed agents can enhance the network throughput and reduce the mean traveling time in the case of random forwarding a hierarchical structure in the dependence of highspeed is observed the network throughput remains unchanged in small and large highspeed value it is interesting to find that the slightly preferential forwarding to highspeed agents can maximize the network capacity through theoretical analysis and numerical simulations we show that the optimal forwarding ratio stems from local structural heterogeneity of lowspeed agents,0
travel restrictions may reduce the spread of a contagious disease that threatens public health in this study we investigate what effect different levels of travel restrictions may have on the speed and geographical spread of an outbreak of a disease similar to sars we use a stochastic simulation model of the swedish population calibrated with survey data of travel patterns between municipalities in sweden collected over three years we find that a ban on journeys longer than 50 km drastically reduces the speed and the geographical spread of outbreaks even with when compliance is less than 100 the result is found to be robust for different rates of intermunicipality transmission intensities travel restrictions may therefore be an effective way to mitigate the effect of a future outbreak,0
social networks provide a new perspective for enterprises to better understand their customers and have attracted substantial attention in industry however inferring high quality customer social networks is a great challenge while there are no explicit customer relations in many traditional oltp environments in this paper we study this issue in the field of passenger transport and introduce a new member to the family of social networks which is named cotravel networks consisting of passengers connected by their cotravel behaviors we propose a novel method to infer high quality cotravel networks of civil aviation passengers from their cobooking behaviors derived from the pnrs passenger naming records in our method to accurately evaluate the strength of ties we present a measure of cojourney times to count the cotravel times of complete journeys between passengers we infer a high quality cotravel network based on a large encrypted pnr dataset and conduct a series of network analyses on it the experimental results show the effectiveness of our inferring method as well as some special characteristics of cotravel networks such as the sparsity and high aggregation compared with other kinds of social networks it can be expected that such cotravel networks will greatly help the industry to better understand their passengers so as to improve their services more importantly we contribute a special kind of social networks with high strength of ties generated from very close and high cost travel behaviors for further scientific researches on human travel behaviors group travel patterns highend travel market evolution etc from the perspective of social networks,0
this study estimates the relationships between street network characteristics and transportsector co2 emissions across every urban area in the world and investigates whether they are the same across development levels and urban design paradigms the prior literature has estimated relationships between street network design and transport emissions including greenhouse gases implicated in climate change primarily through case studies focusing on certain world regions or relatively small samples of cities complicating generalizability and applicability for evidenceinformed practice our worldwide study finds that straighter moreconnected and lessoverbuilt street networks are associated with lower transport emissions all else equal importantly these relationships vary across development levels and design paradigms yet most prior literature reports findings from urban areas that are outliers by global standards planners need a better empirical base for evidenceinformed practice in understudied regions particularly the rapidly urbanizing global south,0
the objective of this study is to propose and test an adaptive reinforcement learning model that can learn the patterns of human mobility in a normal context and simulate the mobility during perturbations caused by crises such as flooding wildfire and hurricanes understanding and predicting human mobility patterns such as destination and trajectory selection can inform emerging congestion and road closures raised by disruptions in emergencies data related to human movement trajectories are scarce especially in the context of emergencies which places a limitation on applications of existing urban mobility models learned from empirical data models with the capability of learning the mobility patterns from data generated in normal situations and which can adapt to emergency situations are needed to inform emergency response and urban resilience assessments to address this gap this study creates and tests an adaptive reinforcement learning model that can predict the destinations of movements estimate the trajectory for each origin and destination pair and examine the impact of perturbations on humans decisions related to destinations and movement trajectories the application of the proposed model is shown in the context of houston and the flooding scenario caused by hurricane harvey in august 2017 the results show that the model can achieve more than 76 precision and recall the results also show that the model could predict traffic patterns and congestion resulting from to urban flooding the outcomes of the analysis demonstrate the capabilities of the model for analyzing urban mobility during crises which can inform the public and decisionmakers about the response strategies and resilience planning to reduce the impacts of crises on urban mobility,0
understanding human mobility from a microscopic point of view may represent a fundamental breakthrough for the development of a statistical physics for cognitive systems and it can shed light on the applicability of macroscopic statistical laws for social systems even if the complexity of individual behaviors prevents a true microscopic approach the introduction of mesoscopic models allows the study of the dynamical properties for the nonstationary states of the considered system we propose to compute various entropy measures of the individual mobility patterns obtained from gps data that record the movements of private vehicles in the florence district in order to point out new features of human mobility related to the use of time and space and to define the dynamical properties of a stochastic model that could generate similar patterns moreover we can relate the predictability properties of human mobility to the distribution of time passed between two successive trips our analysis suggests the existence of a hierarchical structure in the mobility patterns which divides the performed activities into three different categories according to the time cost with different information contents we show that a markov process defined by using the individual mobility network is not able to reproduce this hierarchy which seems the consequence of different strategies in the activity choice our results could contribute to the development of governance policies for a sustainable mobility in modern cities,0
understanding human mobility is of vital importance for urban planning epidemiology and many other fields that aim to draw policies from the activities of humans in space despite recent availability of large scale data sets related to human mobility such as gps traces mobile phone data etc it is still true that such data sets represent a subsample of the population of interest and then might give an incomplete picture of the entire population in question notwithstanding the abundant usage of such inherently limited data sets the impact of sampling biases on mobility patterns is unclear we do not have methods available to reliably infer mobility information from a limited data set here we investigate the effects of sampling using a data set of millions of taxi movements in new york city on the one hand we show that mobility patterns are highly stable once an appropriate simple rescaling is applied to the data implying negligible loss of information due to subsampling over long time scales on the other hand contrasting an appropriate null model on the weighted network of vehicle flows reveals distinctive features which need to be accounted for accordingly we formulate a supersampling methodology which allows us to reliably extrapolate mobility data from a reduced sample and propose a number of networkbased metrics to reliably assess its quality and that of other human mobility models our approach provides a well founded way to exploit temporal patterns to save effort in recording mobility data and opens the possibility to scale up data from limited records when information on the full system is needed,0
abrupt changes in the environment such as unforeseen events due to climate change have triggered massive and precipitous changes in human mobility the ability to quickly predict traffic patterns in different scenarios has become more urgent to support shortterm operations and longterm transportation planning this requires modeling entire metropolitan areas to recognize the upstream and downstream effects on the network however there is a wellknown tradeoff between increasing the level of detail of a model and decreasing computational performance to achieve the level of detail required for traffic microsimulation current implementations often compromise by simulating small spatial scales and those that operate at larger scales often require access to expensive highperformance computing systems or have computation times on the order of days or weeks that discourage productive research and realtime planning this paper addresses the performance shortcomings by introducing a new platform manta microsimulation analysis for network traffic assignment for traffic microsimulation at the metropolitanscale manta employs a highly parallelized gpu implementation that is capable of running metropolitanscale simulations within a few minutes the runtime to simulate all morning trips using halfsecond timesteps for the ninecounty san francisco bay area is just over four minutes not including routing and initialization this computational performance significantly improves the state of the art in largescale traffic microsimulation manta expands the capacity to analyze detailed travel patterns and travel choices of individuals for infrastructure planning,0
the mobility behavior of human beings is predictable to a varying degree eg depending on the traits of their personality such as the trait extraversion introversion the mobility of introvert users may be more dominated by routines and habitual movement patterns resulting in a more predictable mobility behavior on the basis of their own location history while in contrast extrovert users get about a lot and are explorative by nature which may hamper the prediction of their mobility however socially more active and extrovert users meet more people and share information experiences believes thoughts etc with others which in turn leads to a high interdependency between their mobility and social lives using a large lbsn dataset his paper investigates the interdependency between human mobility and social proximity the influence of social networks on enhancing location prediction of an individual and the transmission of social trendsinfluences within social networks,0
in this paper the authors develop a method of detecting correlations between epidemic patterns in different regions that are due to human movement and introduce a null model in which the travelinduced correlations are cancelled they apply this method to the welldocumented cases of seasonal influenza outbreaks in the united states and france in the united states using data for 19722002 the authors observed strong shortrange correlations between several states and their immediate neighbors as well as robust longrange spreading patterns resulting from large domestic airtraffic flows the stability of these results over time allowed the authors to draw conclusions about the possible impact of travel restrictions on epidemic spread the authors also applied this method to the case of france 19842004 and found that on the regional scale there was no transportation mode that clearly dominated disease spread the simplicity and robustness of this method suggest that it could be a useful tool for detecting transmission channels in the spread of epidemics,0
crime has been previously explained by social characteristics of the residential population and as stipulated by crime pattern theory might also be linked to human movements of nonresidential visitors yet a full empirical validation of the latter is lacking the prime reason is that prior studies are limited to aggregated statistics of human visitors rather than mobility flows and because of that neglect the temporal dynamics of individual human movements as a remedy we provide the first work which studies the ability of granular human mobility in describing and predicting crime concentrations at an hourly scale for this purpose we propose the use of data from location technology platforms this type of data allows us to trace individual transitions and therefore we succeed in distinguishing different mobility flows that i are incoming or outgoing from a neighborhood ii remain within it or iii refer to transitions where people only pass through the neighborhood our evaluation infers mobility flows by leveraging an anonymized dataset from foursquare that includes almost 148 million consecutive checkins in three major us cities according to our empirical results mobility flows are significantly and positively linked to crime these findings advance our theoretical understanding as they provide confirmatory evidence for crime pattern theory furthermore our novel use of digital location services data proves to be an effective tool for crime forecasting it also offers unprecedented granularity when studying the connection between human mobility and crime,0
mobility is a fundamental feature of human life and through it our interactions with the world and people around us generate complex and consequential social phenomena social segregation one such process is increasingly acknowledged as a product of ones entire lived experience rather than mere residential location increasingly granular sources of data on human mobility have evidenced how segregation persists outside the home in workplaces cafes and on the street yet there remains only a weak evidential link between the production of social segregation and urban policy this study addresses this gap through an assessment of the role of the urban transportation systems in shaping social segregation using cityscale gps mobility data and a novel probabilistic mobility framework we establish social interactions at the scale of transportation infrastructure by rail and bus service segment individual roads and city blocks the outcomes show how social segregation is more than a single process in space but varying by time of day urban design and structure and service design these findings reconceptualize segregation as a product of likely encounters during ones daily mobility practice we then extend these findings through exploratory simulations highlighting how transportation policy to promote sustainable transport may have potentially unforeseen impacts on segregation the study underscores that to understand social segregation and achieve positive social change urban policymakers must consider the broadest impacts of their interventions and seek to understand their impact on the daily lived experience of their citizens,0
in response to the coronavirus disease 2019 covid19 pandemic governments have encouraged and ordered citizens to practice social distancing particularly by working and studying at home intuitively only a subset of people have the ability to practice remote work however there has been little research on the disparity of mobility adaptation across different income groups in us cities during the pandemic the authors worked to fill this gap by quantifying the impacts of the pandemic on human mobility by income in greater houston texas in this paper we determined human mobility using pseudonymized spatially disaggregated cell phone location data a longitudinal study across estimated income groups was conducted by measuring the total travel distance radius of gyration number of visited locations and pertrip distance in april 2020 compared to the data in a baseline an apparent disparity in mobility was found across estimated income groups in particular there was a strong negative correlation  090 between a travelers estimated income and travel distance in april disparities in mobility adaptability were further shown since those in higher income brackets experienced larger percentage drops in the radius of gyration and the number of distinct visited locations than did those in lower income brackets the findings of this study suggest a need to understand the reasons behind the mobility inflexibility among lowincome populations during the pandemic the study illuminates an equity issue which may be of interest to policy makers and researchers alike in the wake of an epidemic,0
the spatial structure of populations is a key element in the understanding of the large scale spreading of epidemics motivated by the recent empirical evidence on the heterogeneous properties of transportation and commuting patterns among urban areas we present a thorough analysis of the behavior of infectious diseases in metapopulation models characterized by heterogeneous connectivity and mobility patterns we derive the basic reactiondiffusion equation describing the metapopulation system at the mechanistic level and derive an early stage dynamics approximation for the subpopulation invasion dynamics the analytical description uses degree block variables that allows us to take into account arbitrary degree distribution of the metapopulation network we show that along with the usual single population epidemic threshold the metapopulation network exhibits a global threshold for the subpopulation invasion we find an explicit analytic expression for the invasion threshold that determines the minimum number of individuals traveling among subpopulations in order to have the infection of a macroscopic number of subpopulations the invasion threshold is a function of factors such as the basic reproductive number the infectious period and the mobility process and it is found to decrease for increasing network heterogeneity we provide extensive mechanistic numerical monte carlo simulations that recover the analytical finding in a wide range of metapopulation network connectivity patterns the results can be useful in the understanding of recent data driven computational approaches to disease spreading in large transportation networks and the effect of containment measures such as travel restrictions,0
using us nationwide travel surveys for 1995 2001 2009 and 2017 this study compares millennials with their previous generation gen xers in terms of their automobile travel across different neighborhood patterns at the age of 16 to 28 years old millennials have lower daily personal vehicle miles traveled and car trips than gen xers in urban higherdensity and suburban lowerdensity neighborhoods such differences remain unchanged after adjusting for the socioeconomic vehicle ownership life cycle yearspecific and regionalspecific factors in addition the associations between residential density and automobile travel for the 16 to 28yearold millennials are flatter than that for gen xers controlling for the aforementioned covariates these generational differences remain for the 24 to 36yearold millennials during the period when the us economy was recovering from the recession these findings show that in both urban and suburban neighborhoods millennials in the us are less autocentric than the previous generation during early life stages regardless of economic conditions whether such difference persists over later life stages remains an open question and is worth continuous attention,0
to improve the routing decisions of individual drivers and the management policies designed by traffic operators one needs reliable estimates of travel time distributions since congestion caused by both recurrent patterns eg rush hours and nonrecurrent events eg traffic incidents leads to potentially substantial delays in highway travel times we focus on a framework capable of incorporating both effects to this end we propose to work with the markovian velocity model mvm based on an environmental background process that tracks both random and semipredictable events affecting the vehicle speeds in a highway network we show how to operationalize this flexible datadriven model in order to obtain the travel time distribution for a vehicle departing at a known day and time to traverse a given path specifically we detail how to structure the background process and set the speed levels corresponding to the different states of this process first for the inclusion of nonrecurrent events we study incident data to describe the random durations of the incident and interincident times both of them depend on the time of day but we identify periods in which they can be considered timeindependent second for an estimation of the speed patterns in both incident and interincident regime loop detector data for each of the identified periods is studied in numerical examples that use road network detector data of the dutch highway network we obtain the travel time distribution estimates that arise under different traffic regimes and illustrate the advantages compared to traditional traveltime prediction methods,0
human behaviors exhibit ubiquitous correlations in many aspects such as individual and collective levels temporal and spatial dimensions content social and geographical layers with rich internet data of online behaviors becoming available it attracts academic interests to explore human mobility similarity from the perspective of social network proximity existent analysis shows a strong correlation between online social proximity and offline mobility similari ty namely mobile records between friends are significantly more similar than between strangers and those between friends with common neighbors are even more similar we argue the importance of the number and diversity of com mon friends with a counter intuitive finding that the number of common friends has no positive impact on mobility similarity while the diversity plays a key role disagreeing with previous studies our analysis provides a novel view for better understanding the coupling between human online and offline behaviors and will help model and predict human behaviors based on social proximity,0
we present and test a sequential learning algorithm for the shortterm prediction of human mobility this novel approach pairs the exponential weights forecaster with a very large ensemble of experts the experts are individual sequence prediction algorithms constructed from the mobility traces of 10 million roaming mobile phone users in a european country average prediction accuracy is significantly higher than that of individual sequence prediction algorithms namely constant order markov models derived from the users own data that have been shown to achieve high accuracy in previous studies of human mobility prediction the algorithm uses only time stamped location data and accuracy depends on the completeness of the expert ensemble which should contain redundant records of typical mobility patterns the proposed algorithm is applicable to the prediction of any sufficiently large dataset of sequences,0
the accurate modeling of individual movement in cities has significant implications for policy decisions across various sectors existing research emphasizes the universality of human mobility positing that simple models can capture populationlevel movements however populationlevel accuracy does not guarantee consistent performance across all individuals by overlooking individual differences universality laws may accurately describe certain groups while less precisely representing others resulting in aggregate accuracy from a balance of discrepancies using largescale mobility data we assess individuallevel accuracy of a universal model the exploration and preferential return epr by examining deviations from expected behavior in two scaling laws one related to exploration and the other to return patterns our findings reveal that while the model can describe populationwide movement patterns it displays widespread deviations linked to individuals behavioral traits socioeconomic status and lifestyles contradicting model assumptions like nonbursty exploration and preferential return specifically individuals poorly represented by the epr model tend to visit routine locations in sequences exploring rarely but in a bursty manner when they do among socioeconomic factors income most strongly correlates with significant deviations consequently spatial inhomogeneity emerges in model accuracy with lower performance concentrated in urbanized densely populated areas underscoring policy implications our results show that emphasizing populationwide models can propagate socioeconomic inequalities by poorly representing vulnerable population sectors,0
we investigate the difference in the spread of covid19 between the states won by donald trump red and the states won by hillary clinton blue in the 2016 presidential election by mining transportation patterns of us residents from march 2020 to july 2020 to ensure a fair comparison we first use a kmeans clustering method to group the 50 states into five clusters according to their population area and population density we then characterize daily transportation patterns of the residents of different states using the mean percentage of residents traveling and the number of trips per person for each state we study the correlations between travel patterns and infection rate for a 2month period before and after the official states reopening dates we observe that during the lockdown red and blue states both displayed strong positive correlations between their travel patterns and infection rates however after states reopened we find that red states had higher travelinfection correlations than blue states in all five state clusters we find that the residents of both red and blue states displayed similar travel patterns during the period post the reopening of states leading us to conclude that on average the residents in red states might be mobilizing less safely than the residents in blue states furthermore we use temperature data to attempt to explain the difference in the way residents travel and practice safety measures,0
human mobility plays a key role on the transformation of local disease outbreaks into global pandemics thus the inclusion of human movements into epidemic models has become mandatory for understanding current epidemic episodes and to design efficient prevention policies following this challenge here we develop a markovian framework which enables to address the impact of recurrent mobility patterns on the epidemic onset at different temporal scales this formalism is validated by comparing their predictions with results from mechanistic simulations the fair agreement between both theory and simulations enables to get an analytical expression for the epidemic threshold which captures the critical conditions triggering epidemic outbreaks finally by performing an exhaustive analysis of this epidemic threshold we reveal that the impact of tuning human mobility on the emergence of diseases is strongly affected by the temporal scales associated to both epidemiological and mobility processes,0
rapid advances in modern communication technology are enabling the accumulation of largescale highresolution observational data of spatiotemporal movements of humans classification and prediction of human mobility based on the analysis of such data carry great potential in applications such as urban planning as well as being of theoretical interest a robust theoretical framework is therefore required to study and properly understand human motion here we perform the eigenmode analysis of human motion data gathered from mobile communication records which allows us to explore the scaling properties and characteristics of human motion,0
human flow data are rich behavioral data relevant to peoples decisionmaking regarding where to live work go shopping etc and provide vital information for identifying city centers however it is not as easy to understand massive relational data and datasets have often been reduced merely to the statistics of trip counts at destinations discarding relational information from origin to destination in this study we propose an alternative center identification method based on human mobility data this method extracts the scalar potential field of human trips based on combinatorial hodge theory it detects not only statistically significant attractive locations as the sinks of human trips but also significant origins as the sources of trips as a case study we identify the sinks and sources of commuting and shopping trips in the tokyo metropolitan area this aimspecific analysis leads to a combinatorial classification of city centers based on the distinct aspects of human mobility the proposed method can be applied to other mobility datasets with relevant properties and helps us examine the complex spatial structures in contemporary metropolitan areas from the multiple perspectives of human mobility,0
agentbased models have proven to be useful tools in supporting decisionmaking processes in different application domains the advent of modern computers and supercomputers has enabled these bottomup approaches to realistically model human mobility and contact behavior the covid19 pandemic showcased the urgent need for detailed and informative models that can answer research questions on transmission dynamics we present a sophisticated agentbased model to simulate the spread of respiratory diseases the model is highly modularized and can be used on various scales from a small collection of buildings up to cities or countries although not being the focus of this paper the model has undergone performance engineering on a single core and provides an efficient intra and intersimulation parallelization for timecritical decisionmaking processes in order to allow answering research questions on individual level resolution nonpharmaceutical intervention strategies such as face masks or venue closures can be implemented for particular locations or agents in particular we allow for sophisticated testing and isolation strategies to study the effects of minimalinvasive infectious disease mitigation with realistic human mobility patterns for the region of brunswick germany we study the effects of different interventions between march 1st and may 30 2021 in the sarscov2 pandemic our analyses suggest that symptomindependent testing has limited impact on the mitigation of disease dynamics if the dark figure in symptomatic cases is high furthermore we found that quarantine length is more important than quarantine efficiency but that with sufficient symptomatic control also short quarantines can have a substantial effect,0
the city is a complex system that evolves through its inherent social and economic interactions mediating the movements of people and resources urban street networks offer a spatial footprint of these activities consequently their structural characteristics have been of great interest in the literature in comparison relatively limited attention has been devoted to the interplay between street structure and its functional usage ie the movement patterns of people and resources to address this we study the shape of 472040 spatiotemporally optimized travel routes in the 92 most populated cities in the world the routes are sampled in a geographically unbiased way such that their properties can be mapped on to each city with their summary statistics capturing mesoscale connectivity patterns representing the complete space of possible movement in cities the collective morphology of routes exhibits a directional bias that could be described as influenced by the attractive or repulsive forces resulting from congestion accessibility and travel demand that relate to various socioeconomic factors to capture this feature we propose a simple metric inness that maps this force field an analysis of the morphological patterns of individual cities reveals structural and socioeconomic commonalities among cities with similar inness patterns in particular that they cluster into groups that are correlated with their size and putative stage of urban development as measured by a series of socioeconomic and infrastructural indicators our results lend weight to the insight that levels of urban socioeconomic development are intrinsically tied to increasing physical connectivity and diversity of road hierarchies,0
the complex interplay between population movements in space and nonhomogeneous mixing patterns have so far hindered the fundamental understanding of the conditions for spatial invasion through a general theoretical framework to address this issue we present an analytical modelling approach taking into account such interplay under general conditions of mobility and interactions in the simplifying assumption of two population classes we describe a spatially structured population with nonhomogeneous mixing and travel behaviour through a multihost stochastic epidemic metapopulation model different population partitions mixing patterns and mobility structures are considered along with a specific application for the study of the role of age partition in the early spread of the 2009 h1n1 pandemic influenza we provide a complete mathematical formulation of the model and derive a semianalytical expression of the threshold condition for global invasion of an emerging infectious disease in the metapopulation system a rich solution space is found that depends on the social partition of the population the pattern of contacts across groups and their relative social activity the travel attitude of each class and the topological and traffic features of the mobility network reducing the activity of the less social group and reducing the crossgroup mixing are predicted to be the most efficient strategies for controlling the pandemic potential in the case the less active group constitutes the majority of travellers if instead traveling is dominated by the more social class our model predicts the existence of an optimal acrossgroups mixing that maximises the pandemic potential of the disease whereas the impact of variations in the activity of each group is less important,0
the covid19 pandemic is changing the world in unprecedented and unpredictable ways human mobility is at the epicenter of that change as the greatest facilitator for the spread of the virus to study the change in mobility to evaluate the efficiency of mobility restriction policies and to facilitate a better response to possible future crisis we need to properly understand all mobility data sources at our disposal our work is dedicated to the study of private mobility sources gathered and released by large technological companies this data is of special interest because unlike most public sources it is focused on people not transportation means ie its unit of measurement is the closest thing to a person in a western society a phone furthermore the sample of society they cover is large and representative on the other hand this sort of data is not directly accessible for anonymity reasons thus properly interpreting its patterns demands caution aware of that we set forth to explore the behavior and interrelations of private sources of mobility data in the context of spain this country represents a good experimental setting because of its large and fast pandemic peak and for its implementation of a sustained generalized lockdown we find private mobility sources to be both correlated and complementary using them we evaluate the efficiency of implemented policies and provide a insights into what new normal means in spain,0
the analysis and characterization of human mobility using populationlevel mobility models is important for numerous applications ranging from the estimation of commuter flows in cities to modeling trade flows between countries however almost all of these applications have focused on large spatial scales which typically range between intracity scales to intercountry scales in this paper we investigate populationlevel human mobility models on a much smaller spatial scale by using them to estimate customer mobility flow between supermarket zones we use anonymized ordered customerbasket data to infer empirical mobility flow in supermarkets and we apply variants of the gravity and interveningopportunities models to fit this mobility flow and estimate the flow on unseen data we find that a doublyconstrained gravity model and an extended radiation model which is a type of interveningopportunities model can successfully estimate 6570 of the flow inside supermarkets using a gravity model as a case study we then investigate how to reduce congestion in supermarkets using mobility models we model each supermarket zone as a queue and we use a gravity model to identify store layouts with low congestion which we measure either by the maximum number of visits to a zone or by the total mean queue size we then use a simulatedannealing algorithm to find store layouts with lower congestion than a supermarkets original layout in these optimized store layouts we find that popular zones are often in the perimeter of a store our research gives insight both into how customers move in supermarkets and into how retailers can arrange stores to reduce congestion it also provides a case study of human mobility on small spatial scales,0
human mobility contact patterns and their interplay are key aspects of our social behavior that shape the spread of infectious diseases across different regions in the light of new evidence and data sets about these two elements epidemic models should be refined to incorporate both the heterogeneity of human contacts and the complexity of mobility patterns here we propose a theoretical framework that allows accommodating these two aspects in the form of a set of markovian equations we validate these equations with extensive mechanistic simulations and derive analytically the epidemic threshold the expression of this critical value allows us to evaluate its dependence on the specific demographic distribution the structure of mobility flows and the heterogeneity of contact patterns thus shedding light on the microscopic mechanisms responsible for the epidemic detriment driven by recurrent mobility patterns reported in the literature,0
estimating temporal patterns in travel times along road segments in urban settings is of central importance to traffic engineers and city planners in this work we propose a methodology to leverage coarsegrained and aggregated travel time data to estimate the streetlevel travel times of a given metropolitan area our main focus is to estimate travel times along the arterial road segments where relevant data are often unavailable the central idea of our approach is to leverage easytoobtain aggregated data sets with broad spatial coverage such as the data published by uber movement as the fabric over which other expensive finegrained datasets such as loop counter and probe data can be overlaid our proposed methodology uses a graph representation of the road network and combines several techniques such as graphbased routing trip sampling graph sparsification and leastsquares optimization to estimate the streetlevel travel times using sampled trips and weighted shortestpath routing we iteratively solve constrained leastsquares problems to obtain the travel time estimates we demonstrate our method on the los angeles metropolitanarea street network where aggregated travel time data is available for trips between traffic analysis zones additionally we present techniques to scale our approach via a novel graph pseudosparsification technique,0
climate change is altering the frequency and intensity of wildfires leading to increased evacuation events that disrupt human mobility and socioeconomic structures these disruptions affect access to resources employment and housing amplifying existing vulnerabilities within communities understanding the interplay between climate change wildfires evacuation patterns and socioeconomic factors is crucial for developing effective mitigation and adaptation strategies to contribute to this challenge we use highdefinition mobile phone records to analyse evacuation patterns during the wildfires in valparaso chile that took place between february 23 2024 this data allows us to track the movements of individuals in the disaster area providing insight into how people respond to largescale evacuations in the context of severe wildfires we apply a causal inference approach that combines regression discontinuity and differenceindifferences methodologies to observe evacuation behaviours during wildfires with a focus on socioeconomic stratification this approach allows us to isolate the impact of the wildfires on different socioeconomic groups by comparing the evacuation patterns of affected populations before and after the event while accounting for underlying trends and discontinuities at the threshold of the disaster we find that many people spent nights away from home with those in the lowest socioeconomic segment stayed away the longest in general people reduced their travel distance during the evacuation and the lowest socioeconomic group moved the least initially movements became more random as people sought refuge in a rush but eventually gravitated towards areas with similar socioeconomic status our results show that socioeconomic differences play a role in evacuation dynamics providing useful insights for response planning,0
synthetic opioids are the most common drugs involved in druginvolved overdose mortalities in the us the center for disease control and prevention reported that in 2018 about 70 of all drug overdose deaths involved opioids and 67 of all opioidinvolved deaths were accounted for by synthetic opioids in this study we investigated the spread of synthetic opioids between 2013 and 2020 in the us we analyzed the relationship between the spatiotemporal pattern of synthetic opioidinvolved deaths and another key opioid heroin and compared patterns of deaths involving these two types of drugs during this period spatial connections and human mobility between counties were incorporated into a graph convolutional neural network model to represent and analyze the spread of synthetic opioidinvolved deaths in the context of previous heroininvolved death patterns,0
a longstanding expectation is that large dense and cosmopolitan areas support socioeconomic mixing and exposure between diverse individuals it has been difficult to assess this hypothesis because past approaches to measuring socioeconomic mixing have relied on static residential housing data rather than reallife exposures between people at work in places of leisure and in home neighborhoods here we develop a new measure of exposure segregation es that captures the socioeconomic diversity of everyday encounters leveraging cell phone mobility data to represent 16 billion exposures among 96 million people in the united states we measure exposure segregation across 382 metropolitan statistical areas msas and 2829 counties we discover that exposure segregation is 67 higher in the 10 largest metropolitan statistical areas msas than in small msas with fewer than 100000 residents this means that contrary to expectation residents of large cosmopolitan areas have significantly less exposure to diverse individuals second we find evidence that large cities offer a greater choice of differentiated spaces targeted to specific socioeconomic groups a dynamic that accounts for this increase in everyday socioeconomic segregation third we discover that this segregationincreasing effect is countered when a citys hubs eg shopping malls are positioned to bridge diverse neighborhoods and thus attract people of all socioeconomic statuses overall our findings challenge a longstanding conjecture in human geography and urban design and highlight how built environment can both prevent and facilitate exposure between diverse individuals,0
a mass of traces of human activities show diverse dynamic patterns in this paper we comprehensively investigate the dynamic pattern of human attention defined by the quantity of interests on subdisciplines in an online academic communication forum both the expansion and exploration of human attention have a powerlaw scaling relation with browsing actions of which the exponent is close to that in onedimension random walk furthermore the memory effect of human attention is characterized by the powerlaw distributions of both the return interval time and return interval steps which is reinforced by studying the attention shift that monotonically increase with the interval order between pairs of continuously segmental sequences of expansion at last the observing dynamic pattern of human attention in the browsing process is analytically described by a dynamic model whose generic mechanism is analogy to that of human spatial mobility thus our work not only enlarges the research scope of human dynamics but also provides an insight to understand the relationship between the interest transitivity in online activities and human spatial mobility in real world,0
the recent availability of digital traces from information and communications technologies ict has facilitated the study of both individual and populationlevel movement with unprecedented spatiotemporal resolution enabling us to better understand a plethora of socioeconomic processes such as urbanization transportation impact on the environment and epidemic spreading to name a few using empirical spatiotemporal trends several mobility models have been proposed to explain the observed regularities in human movement with the advent of the world wide web a new type of virtual mobility has emerged that has begun to supplant many traditional facets of human activity here we conduct a systematic analysis of physical and virtual movement uncovering both similarities and differences in their statistical patterns the differences manifest themselves primarily in the temporal regime as a signature of the spatial and economic constraints inherent in physical movement features that are predominantly absent in the virtual space we demonstrate that once one moves to the timeindependent space of events ie the sequences of visited locations these differences vanish and the statistical patterns of physical and virtual mobility are identical the observed similarity in navigating these markedly different domains point towards a common mechanism governing the movement patterns a feature we describe through a metropolishastings type optimization model where individuals navigate locations through decisionmaking processes resembling a costbenefit analysis of the utility of locations in contrast to existing phenomenological models of mobility we show that our model can reproduce the commonalities in the empirically observed statistics with minimal input,0
understanding human mobility is crucial for applications such as forecasting epidemic spreading planning transport infrastructure and urbanism in general while traditionally mobility information has been collected via surveys the pervasive adoption of mobile technologies has brought a wealth of real time data the easy access to this information opens the door to study theoretical questions so far unexplored in this work we show for a series of worldwide cities that commuting daily flows can be mapped into a well behaved vector field fulfilling the divergence theorem and which is besides irrotational this property allows us to define a potential for the field that can become a major instrument to determine separate mobility basins and discern contiguous urban areas we also show that empirical fluxes and potentials can be well reproduced and analytically characterized using the socalled gravity model while other models based on intervening opportunities have serious difficulties,0
we address and discuss recent trends in the analysis of big data sets with the emphasis on studying multiscale phenomena applications of big data analysis in different scientific fields are described and two particular examples of multiscale phenomena are explored in more detail the first one deals with wind power production at the scale of single wind turbines the scale of entire wind farms and also at the scale of a whole country using open source data we show that the wind power production has an intermittent character at all those three scales with implications for defining adequate strategies for stable energy production the second example concerns the dynamics underlying human mobility which presents different features at different scales for that end we analyze 12month data of the eduroam database within portuguese universities and find that at the smallest scales typically within a set of a few adjacent buildings the characteristic exponents of average displacements are different from the ones found at the scale of one country or one continent,0
using smartphone location data from colombia mexico and indonesia we investigate how nonpharmaceutical policy interventions intended to mitigate the spread of the covid19 pandemic impact human mobility in all three countries we find that following the implementation of mobility restriction measures human movement decreased substantially importantly we also uncover large and persistent differences in mobility reduction between wealth groups on average users in the top decile of wealth reduced their mobility up to twice as much as users in the bottom decile for decisionmakers seeking to efficiently allocate resources to response efforts these findings highlight that smartphone location data can be leveraged to tailor policies to the needs of specific socioeconomic groups especially the most vulnerable,0
major interventions have been introduced worldwide to slow down the spread of the sarscov2 virus largescale lockdowns of human movements are effective in reducing the spread but they come at a cost of significantly limited societal functions we show that natural human movements are statistically diverse and the spread of the disease is significantly influenced by a small group of active individuals and gathering venues we find that interventions focused on these most mobile individuals and popular venues reduce both the peak infection rate and the total infected population while retaining high social activity levels these trends are seen consistently in simulations with real human mobility data of different scales resolutions and modalities from multiple cities across the world the observation implies that compared to broad sweeping interventions more heterogeneous strategies that are targeted based on the network effects in human mobility provide a better balance between pandemic control and regular social activities,0
in this work we address the connection between population density centers in urban areas and the nature of human flows between such centers in shaping the vulnerability to the onset of contagious diseases a study of 163 cities chosen from four different continents reveals a universal trend whereby the risk induced by human mobility increases in those cities where mobility flows are predominantly between high population density centers we apply our formalism to the spread of sarscov2 in the united states providing a plausible explanation for the observed heterogeneity in the spreading process across cities armed with this insight we propose realistic mitigation strategies less severe than lockdowns based on modifying the mobility in cities our results suggest that an optimal control strategy involves an asymmetric policy that restricts flows entering the most vulnerable areas but allowing residents to continue their usual mobility patterns,0
modelling host behavioral change in response to epidemics is important to describe disease dynamics and many previous studies proposed mathematical models describing it indeed the epidemic of covid19 clearly demonstrated that people changed their activity in response to the epidemic which subsequently modified the disease dynamics to predict the behavioral change relevant to the disease dynamics we need to know the epidemic situation eg the number of reported cases at the moment of decision to change behavior however it is difficult to identify the timing of decisionmaking in this study we analyzed travel accommodation reservation data in four prefectures of japan to observe decisionmaking timings and how it responded to the changing epidemic situation during japans coronavirus disease 2019 eight waves until february 2023 to this end we defined mobility avoidance index to indicate peoples decision of mobility avoidance and quantified it using the timeseries of the accommodation bookingcancellation data our analysis revealed semiquantitative rules for daytoday decisionmaking of human mobility under a given epidemic situation we observed matches of the peak dates of the index and the number of reported cases additionally we found that mobility avoidance index increaseddecreased linearly with the logarithmic number of reported cases during the first epidemic wave this pattern agrees with weberfechner law in psychophysics we also found that the slope of the mobility avoidance index against the change of the logarithmic number of reported cases were similar among the waves while the intercept of that was much reduced as the first epidemic wave passed by it suggests that the peoples response became weakened after the first experience as if the number of reported cases were multiplied by a constant small factor,0
researchers face the tradeoff between publishing mobility data along with their papers while simultaneously protecting the privacy of the individuals in addition to the fundamental anonymization process other techniques such as spatial discretization and in certain cases location concealing or complete removal are applied to achieve these dual objectives the primary research question is whether concealing the observation area is an adequate form of protection or whether human mobility patterns in urban areas are inherently revealing of location the characteristics of the mobility data such as the number of activity records or the number of unique users in a given spatial unit reveal the silhouette of the urban landscape which can be used to infer the identity of the city in question it was demonstrated that even without disclosing the exact location the patterns of human mobility can still reveal the urban area from which the data was collected the presented locating method was tested on other cities using different open data sets and against coarser spatial discretization units while publishing mobility data is essential for research it was demonstrated that concealing the observation area is insufficient to prevent the identification of the urban area furthermore using larger discretization units alone is an ineffective solution to the problem of the observation area reidentification instead of obscuring the observation area noise should be added to the trajectories to prevent user identification,0
with the rapid increase in ridehailing rh use a need to better understand and regulate the industry arises this paper analyzes a years worth of rh trip data from the greater chicago area to study rh trip patterns more than 104 million trips were analyzed for trip rates the results show that the total number of trips remained stable over the year with pooled trips steadily decreasing from 20 to 9 percent people tend to use rh more on weekends compared to weekdays specifically weekend rh trip counts per day are on average 20 percent higher than weekday trip counts the results of this work will help policy makers and transportation administrators better understand the nature of rh trips which in turn allows for the design of a better regulation and guidance system for the ridehailing industry,0
the ubiquitous use of mobile devices and associated internet services generates vast volumes of geolocated data offering valuable insights into human behaviors and their interactions with urban environments over the past decade mobile phone data have proven indispensable in various fields such as demography geography transport planning and epidemiology they enable researchers to examine human mobility patterns on unprecedented scales and analyze the spatial structure and function of cities the relationship between mobile phone data and land use has also been extensively explored particularly in inferring land use patterns from spatiotemporal activity however many studies rely on call detail records cdr or extended detail records xdr which may not capture specific mobile application usage this study aims to address this gap by mapping mobile service usage diversity in 20 french cities and investigating its correlation with land use distribution utilizing a shannon diversity index the study evaluates mobile service usage diversity based on hourly traffic volume data from 17 mobile services furthermore the study compares temporal diversity with land distribution both within and among cities,0
we experience air traffic delays every day but are there any recurrent patterns in these delays in this study we investigate the recurrence of delay propagation patterns in japans domestic air transport network in 2019 by integrating delay causality networks and temporal network analysis additionally we examine characteristics unique to delay propagation by comparing delay causality networks with corresponding randomized networks generated by a directed configuration model as a result we found that the structure of the delay propagation patterns can be classified into several groups the identified groups exhibit statistically significant differences in total delay time and average outdegree with different airports playing central roles in spreading delays the results also suggest that some delay propagation patterns are particularly prominent during specific times of the year which could be influenced by japans seasonal and geographical factors moreover we discovered that specific network motifs appear significantly more or less frequently in delay causality networks than their corresponding randomized counterparts this characteristic is particularly pronounced in groups with more significant delays these results suggest that delays propagate following specific directional patterns which could significantly contribute to predicting air traffic delays we expect the present study to trigger further research on recurrent and nonrecurrent natures of air traffic delay propagation,0
recent outbreaks of ebola and dengue viruses have again elevated the significance of the capability to quickly predict disease spread in an emergent situation however existing approaches usually rely heavily on the timeconsuming census processes or the privacysensitive call logs leading to their unresponsive nature when facing the abruptly changing dynamics in the event of an outbreak in this paper we study the feasibility of using largescale twitter data as a proxy of human mobility to model and predict disease spread we report that for australia twitter users distribution correlates well the censusbased population distribution and that the twitter users travel patterns appear to loosely follow the gravity law at multiple scales of geographic distances ie national level state level and metropolitan level the radiation model is also evaluated on this dataset though it has shown inferior fitness as a result of australias sparse population and large landmass the outcomes of the study form the cornerstones for future work towards a modelbased responsive prediction method from twitter data for disease spread,0
while platial representations are being developed for sedentary entities a parallel and useful endeavour would be to consider time in socalled platiotemporal representations that would also expand notions of mobility in giscience that are solely dependent on euclidean space and time besides enhancing such aspects of place and mobility via spatiotemporal we also include human aspects of these representations via considerations of the sociological notions of mobility via the mobilities paradigm that can systematically introduce representation of both platial information along with mobilities associated with moving places we condense these aspects into platial mobility a novel conceptual framework as an integration in giscience and the mobilities paradigm in sociology that denotes movement of places in our platiotemporal and sociologybased representations as illustrative cases for further study using platial mobility as a framework we explore its benefits and methodological aspects toward developing better understanding for disaster management disaster risk reduction and pandemics we then discuss some of the illustrative use cases to clarify the concept of platial mobility and its application prospects in the areas of disaster management disaster risk reduction and pandemics these use cases which include flood events and the ongoing covid19 pandemic have led to displaced and restricted communities having to change practices and places which would be particularly amenable to the conceptual framework developed in our work,0
among the realistic ingredients to be considered in the computational modeling of infectious diseases human mobility represents a crucial challenge both on the theoretical side and in view of the limited availability of empirical data in order to study the interplay between smallscale commuting flows and longrange airline traffic in shaping the spatiotemporal pattern of a global epidemic we i analyze mobility data from 29 countries around the world and find a gravity model able to provide a global description of commuting patterns up to 300 kms ii integrate in a worldwide structured metapopulation epidemic model a timescale separation technique for evaluating the force of infection due to multiscale mobility processes in the disease dynamics commuting flows are found on average to be one order of magnitude larger than airline flows however their introduction into the worldwide model shows that the large scale pattern of the simulated epidemic exhibits only small variations with respect to the baseline case where only airline traffic is considered the presence of short range mobility increases however the synchronization of subpopulations in close proximity and affects the epidemic behavior at the periphery of the airline transportation infrastructure the present approach outlines the possibility for the definition of layered computational approaches where different modeling assumptions and granularities can be used consistently in a unifying multiscale framework,0
in recent years human mobility research has discovered universal patterns capable of describing how people move these regularities have been shown to partly depend on individual and environmental characteristics eg gender ruralurban country in this work we show that lifecourse events such as job loss can disrupt individual mobility patterns adversely affecting individuals wellbeing and potentially increasing the risk of social and economic inequalities we show that job loss drives a significant change in the exploratory behaviour of individuals with changes that intensify over time since job loss our findings shed light on the dynamics of employmentrelated behavior at scale providing a deeper understanding of key components in human mobility regularities these drivers can facilitate targeted social interventions to support the most vulnerable populations,0
the use of mobile phone call detail records and device location data for the calling patterns movements and social contacts of individuals has proven to be valuable for devising models and understanding of their mobility and behaviour patterns in this study we investigate weighted exposurenetworks of human daily activities in the capital region of finland as a proxy for contacts between postal code areas during the prepandemic year 2019 and pandemic years 2020 2021 and early 2022 we investigate the suitability of gravity and radiation type models for reconstructing the exposurenetworks based on geospatial and population mobility information for this we use a mobile phone dataset of aggregated daily visits from a postal code area to cellphone grid locations and treat it as a bipartite network to create weighted one mode projections using a weighted cooccurrence function we fit a gravitation model and a radiation model to the averaged weekly and yearly projection networks with geospatial and socioeconomic variables of the postal code areas and their populations we also consider an extended gravity type model comprising of additional postal area information such as distance via public transportation and population density the results show that the cooccurrence of human activities or exposure between postal code areas follows both the gravity and radiation type interactions once fitted to the empirical network the effects of the pandemic beginning in 2020 can be observed as a decrease of the overall activity as well as of the exposure of the projected networks in general the results show that the postal code level networks changed to be more proximity weighted after the pandemic began following the government imposed nonpharmaceutical interventions with differences based on the geospatial and socioeconomic structure of the areas,0
the availability of largescale datasets collected via mobile phones has opened up opportunities to study human mobility at an individual level the granular nature of these datasets calls for the design of summary statistics that can be used to describe succinctly mobility patterns in this work we show that the radius of gyration a popular summary statistic to quantify the extent of an individuals whereabouts suffers from a sensitivity to outliers and is incapable of capturing mobility organised around multiple centres we propose a natural generalisation of the radius of gyration to a polycentric setting as well as a novel metric to assess the quality of its description with these notions we propose a method to identify the centres in an individuals mobility and apply it to two large mobility datasets with sociodemographic features showing that a polycentric description can capture features that a monocentric model is incapable of,0
since the primary mode of respiratory virus transmission is persontoperson interaction we are required to reconsider physical interaction patterns to mitigate the number of people infected with covid19 while research has shown that nonpharmaceutical interventions npi had an evident impact on national mobility patterns we investigate the relative regional mobility behaviour to assess the effect of human movement on the spread of covid19 in particular we explore the impact of human mobility and social connectivity derived from facebook activities on the weekly rate of new infections in germany between march 3rd and june 22nd 2020 our results confirm that reduced social activity lowers the infection rate accounting for regional and temporal patterns the extent of social distancing quantified by the percentage of people staying put within a federal administrative district has an overall negative effect on the incidence of infections additionally our results show spatial infection patterns based on geographic as well as social distances,0
human movement is used as an indicator of human activity in modern society the velocity of moving humans is calculated based on position information obtained from mobile phones the level of human activity as recorded by velocity varies throughout the day therefore velocity can be used to identify the intervals of highest and lowest activity more specifically we obtained mobilephone gps data from the people around shibuya station in tokyo which has the highest population density in japan from these data we observe that velocity tends to consistently increase with the changes in social activities for example during the earthquake in kumamoto prefecture in april 2016 the activity on that day was much lower than usual in this research we focus on natural disasters such as earthquakes owing to their significant effects on human activities in developed countries like japan in the event of a natural disaster in another developed country considering the change in human behavior at the time of the disaster eg the 2016 kumamoto great earthquake from the viewpoint of velocity allows us to improve our planning for mitigation measures thus we analyze the changes in human activity through velocity calculations in shibuya tokyo and compare times of disasters with normal times,0
human mobility contributes to the fast spatiotemporal propagation of infectious diseases during an outbreak monitoring the infection situation on either side of an international border is very crucial as there is always a higher risk of disease importation associated with crossborder migration mechanistic models are effective tools to investigate the consequences of crossborder mobility on disease dynamics and help in designing effective control strategies however in practice due to the unavailability of crossborder mobility data it becomes difficult to propose reliable modelbased strategies in this study we propose a method for estimating crossborder mobility flux between any pair of regions that share an international border from the observed difference in the timing of the infection peak in each region assuming the underlying disease dynamics is governed by a susceptibleinfectedrecovered sir model we employ stochastic simulations to obtain the maximum likelihood crossborder mobility estimate for any pair of regions where the difference in peak time can be measured we then investigate how the estimate of crossborder mobility flux varies depending on the disease transmission rate which is a key epidemiological parameter we further show that the uncertainty in mobility flux estimates decreases for higher disease transmission rates and larger observed differences in peak timing finally as a case study we apply the method to some selected regions along the polandgermany border which are directly connected through multiple modes of transportation and quantify the crossborder fluxes from the covid19 cases data during the period 20rm th february 2021 to 20rm th june 2021,0
human mobility and activity patterns mediate contagion on many levels including the spatial spread of infectious diseases diffusion of rumors and emergence of consensus these patterns however are often dominated by specific locations and recurrent flows and poorly modeled by the random diffusive dynamics generally used to study them here we develop a theoretical framework to analyze contagion within a network of locations where individuals recall their geographic origins we find a phase transition between a regime in which the contagion affects a large fraction of the system and one in which only a small fraction is affected this transition cannot be uncovered by continuous deterministic models due to the stochastic features of the contagion process and defines an invasion threshold that depends on mobility parameters providing guidance for controlling contagion spread by constraining mobility processes we recover the threshold behavior by analyzing diffusion processes mediated by real human commuting data,0
walking and cycling commonly referred to as active travel have become integral components of modern transport planning recently there has been growing recognition of the substantial role that active travel can play in making cities more liveable sustainable and healthy as opposed to traditional vehiclecentred approaches this shift in perspective has spurred interest in developing new data sets of varying resolution levels to represent for instance walking and cycling street networks this has also led to the development of tailored computational tools and quantitative methods to model and analyse active travel flows in response to this surge in active travelrelated data and methods our study develops a methodological framework primarily focused on walking and cycling as modes of commuting we explore commonly used data sources and tools for constructing and analysing walking and cycling networks with a particular emphasis on distance as a key factor that influences describes and predicts commuting behaviour our ultimate aim is to investigate the role of different network distances in predicting active commuting flows to achieve this we analyse the flows in the constructed networks by looking at the detour index of shortest paths we then use the greater london area as a case study and construct a spatial interaction model to investigate the observed commuting patterns through the different networks our results highlight the differences between chosen data sets the uneven spatial distribution of their performance throughout the city and its consequent effect on the spatial interaction model and prediction of walking and cycling commuting flows,0
this paper develops a novel twolayer hierarchical classifier that increases the accuracy of traditional transportation mode classification algorithms this paper also enhances classification accuracy by extracting new frequency domain features many researchers have obtained these features from global positioning system data however this data was excluded in this paper as the system use might deplete the smartphones battery and signals may be lost in some areas our proposed twolayer framework differs from previous classification attempts in three distinct ways 1 the outputs of the two layers are combined using bayes rule to choose the transportation mode with the largest posterior probability 2 the proposed framework combines the new extracted features with traditionally used time domain features to create a pool of features and 3 a different subset of extracted features is used in each layer based on the classified modes several machine learning techniques were used including knearest neighbor classification and regression tree support vector machine random forest and a heterogeneous framework of random forest and support vector machine results show that the classification accuracy of the proposed framework outperforms traditional approaches transforming the time domain features to the frequency domain also adds new features in a new space and provides more control on the loss of information consequently combining the time domain and the frequency domain features in a large pool and then choosing the best subset results in higher accuracy than using either domain alone the proposed twolayer classifier obtained a maximum classification accuracy of 9702,0
the pervasive use of new mobile devices has allowed a better characterization in space and time of human concentrations and mobility in general besides its theoretical interest describing mobility is of great importance for a number of practical applications ranging from the forecast of disease spreading to the design of new spaces in urban environments while classical data sources such as surveys or census have a limited level of geographical resolution eg districts municipalities counties are typically used or are restricted to generic workdays or weekends the data coming from mobile devices can be precisely located both in time and space most previous works have used a single data source to study human mobility patterns here we perform instead a crosscheck analysis by comparing results obtained with data collected from three different sources twitter census and cell phones the analysis is focused on the urban areas of barcelona and madrid for which data of the three types is available we assess the correlation between the datasets on different aspects the spatial distribution of people concentration the temporal evolution of people density and the mobility patterns of individuals our results show that the three data sources are providing comparable information even though the representativeness of twitter geolocated data is lower than that of mobile phone and census data the correlations between the population density profiles and mobility patterns detected by the three datasets are close to one in a grid with cells of 2x2 and 1x1 square kilometers this level of correlation supports the feasibility of interchanging the three data sources at the spatiotemporal scales considered,0
as a highly infectious respiratory disease covid19 has become a pandemic that threatens global health without an effective treatment nonpharmaceutical interventions such as travel restrictions have been widely promoted to mitigate the outbreak current studies analyze mobility metrics such as travel distance however there is a lack of research on interzonal travel flow and its impact on the pandemic our study specifically focuses on the intercounty mobility pattern and its influence on the covid19 spread in the united states to retrieve realworld mobility patterns we utilize an integrated set of mobile device location data including over 100 million anonymous devices we first investigate the nationwide temporal trend and spatial distribution of intercounty mobility then we zoom in on the epicenter of the us outbreak new york city and evaluate the impacts of its outflow on other counties finally we develop a loglinear doublerisk model at the county level to quantify the influence of both external risk imported by intercounty mobility flows and the internal risk defined as the vulnerability of a county in terms of population with highrisk phenotypes our study enhances the situation awareness of intercounty mobility in the us and can help improve nonpharmaceutical interventions for covid19,0
the implementation of social distancing policies is key to reducing the impact of the current covid19 pandemic however their effectiveness ultimately depends on human behavior in the united states compliance with social distancing policies has widely varied thus far during the pandemic but what drives such variability through six open datasets including actual human mobility we estimated the association between mobility and the growth rate of covid19 cases across 3107 us counties generalizing previous reports in addition data from the 2016 us presidential election was used to measure how the association between mobility and covid19 growth rate differed based on voting patterns a significant association between political leaning and the covid19 growth rate was measured our results demonstrate that political orientation may inform models predicting the impact of policies in reducing the spread of covid19,0
quantification of the overall characteristics of urban mobility using coarsegrained methods is crucial for urban management planning and sustainable development although some recent studies have provided quantification methods for coarsegrained numerical information regarding urban mobility a method that can simultaneously capture numerical and spatial information remains an outstanding problem here we use mathematical vectors to depict human mobility with mobility magnitude representing numerical information and mobility direction representing spatial information we then define anisotropy and centripetality metrics by vector computation to measure imbalance in direction distribution and orientation toward the city center of mobility flows respectively as a case study we apply our method to 60 chinese cities and identify three mobility patterns strong monocentric weak monocentric and polycentric to better understand mobility pattern we further study the allometric scaling of the average commuting distance and the spatiotemporal variations of the two metrics in different patterns finally we build a microscopic model to explain the key mechanisms driving the diversity in anisotropy and centripetality our work offers a comprehensive method that considers both numerical and spatial information to quantify and classify the overall characteristics of urban mobility enhancing our understanding of the structure and evolution of urban mobility systems,0
since the development of the original schelling model of urban segregation several enhancements have been proposed but none have considered the impact of mobility constraints on model dynamics recent studies have shown that human mobility follows specific patterns such as a preference for short distances and dense locations this paper proposes a segregation model incorporating mobility constraints to make agents select their location based on distance and location relevance our findings indicate that the mobilityconstrained model produces lower segregation levels but takes longer to converge than the original schelling model we identified a few persistently unhappy agents from the minority group who cause this prolonged convergence time and lower segregation level as they move around the grid centre our study presents a more realistic representation of how agents move in urban areas and provides a novel and insightful approach to analyzing the impact of mobility constraints on segregation models we highlight the significance of incorporating mobility constraints when policymakers design interventions to address urban segregation,0
the development of mobile phones has largely increased human interactions whilst the use of these devices for communication has received significant attention there has been little analysis of more passive interactions through census data on casual social groups this work suggests a clear pattern of mobile phones being carried in peoples hands without the person using it that is not looking at it moreover this study suggests that when individuals join members of the opposite sex there is a clear tendency to stop holding mobile phones whilst walking although it is not clear why people hold their phones whilst walking in such large proportions 38 of solitary women and 31 of solitary men we highlight several possible explanation for holding the device including the need to advertise status and affluence to maintain immediate connection with friends and family and to mitigate feelings related to anxiety and security,0
with the booming economy in china many researches have pointed out that the improvement of regional transportation infrastructure among other factors had an important effect on economic growth utilizing a largescale dataset which includes 35 billion entry and exit records of vehicles along highways generated from toll collection systems we attempt to establish the relevance of middistance land transport patterns to regional economic status through transportation network analyses we apply standard measurements of complex networks to analyze the highway transportation networks a set of traffic flow features are computed and correlated to the regional economic development indicator the multilinear regression models explain about 89 to 96 of the variation of cities gdp across three provinces in china we then fit gravity models using annual traffic volumes of cars buses and freight trucks between pairs of cities for each province separately as well as for the whole dataset we find the temporal changes of distancedecay effects on spatial interactions between cities in transportation networks which link to the economic development patterns of each province we conclude that transportation big data reveal the status of regional economic development and contain valuable information of human mobility production linkages and logistics for regional management and planning our research offers insights into the investigation of regional economic development status using highway transportation big data,0
urban mobility plays a crucial role in the functioning of cities influencing economic activity accessibility and quality of life however the effectiveness of analytical models in understanding urban mobility patterns can be significantly affected by the spatial scales employed in the analysis this paper explores the impact of spatial scales on the performance of the gravity model in explaining urban mobility patterns using public transport flow data in singapore the model is evaluated across multiple spatial scales of origin and destination locations ranging from individual bus stops and train stations to broader regional aggregations results indicate the existence of an optimal intermediate spatial scale at which the gravity model performs best at the finest scale where individual transport nodes are considered the model exhibits poor performance due to noisy and highly variable travel patterns conversely at larger scales model performance also suffers as overaggregation of transport nodes results in excessive generalisation which obscures the underlying mobility dynamics furthermore distancebased spatial aggregation of transport nodes proves to outperform administrative boundarybased aggregation suggesting that actual urban organisation and movement patterns may not necessarily align with imposed administrative divisions these insights highlight the importance of selecting appropriate spatial scales in mobility analysis and urban modelling in general offering valuable guidance for urban and transport planning efforts aimed at enhancing mobility in complex urban environments,0
rapidly mutating pathogens may be able to persist in the population and reach an endemic equilibrium by escaping hosts acquired immunity for such diseases multiple biological environmental and populationlevel mechanisms determine the dynamics of the outbreak including pathogens epidemiological traits eg transmissibility infectious period and duration of immunity seasonality interaction with other circulating strains and hosts mixing and spatial fragmentation here we study a susceptibleinfectedrecoveredsusceptible model on a metapopulation where individuals are distributed in subpopulations connected via a network of mobility flows through extensive numerical simulations we explore the phase space of pathogens persistence and map the dynamical regimes of the pathogen following emergence our results show that spatial fragmentation and mobility play a key role in the persistence of the disease whose maximum is reached at intermediate mobility values we describe the occurrence of different phenomena including local extinction and emergence of epidemic waves and assess the conditions for large scale spreading findings are highlighted in reference to previous works and to real scenarios our work uncovers the crucial role of hosts mobility on the ecological dynamics of rapidly mutating pathogens opening the path for further studies on disease ecology in the presence of a complex and heterogeneous environment,0
in the post year 2000 era the technologies that facilitate human communication have rapidly multiplied while the adoption of these technologies has hugely impacted the behaviour and sociality of people specifically in urban but also in rural environments their digital footprints on different data bases have become an active area of research the existence and accessibility of such large populationlevel datasets has allowed scientists to study and model innate human tendencies and social patterns in an unprecedented way that complements traditional research approaches like questionnaire studies in this review we focus on data analytics and modelling research we call social physics as it has been carried out using the mobile phone data sets to get insight into the various aspects of human sociality burstiness in communication mobility patterns and daily rhythms,0
urban transportation systems grow over time as city populations grow and move and their transportation needs evolve typical network growth models such as preferential attachment grow the network node by node whereas rail and metro systems grow by adding entire lines with all their nodes the objective of this paper is to see if any canonical regular network forms such as stars or grids capture the growth patterns of urban metro systems for which we have historical data in terms of old maps data from these maps reveal that the systems pearson degree correlation grows increasingly from initially negative values toward positive values over time and in some cases becomes decidedly positive we have derived closed form expressions for degree correlation and clustering coefficient for a variety of canonical forms that might be similar to metro systems of all those examined only a few types patterned after a wide area network wan with a coreperiphery structure show similar positivetrending degree correlation as network size increases this suggests that large metro systems either are designed or evolve into the equivalent of message carriers that seek to balance travel between arbitrary nodedestination pairs with avoidance of congestion in the central regions of the network keywords metro subway urban transport networks degree correlation,0
the structure of heterogeneous networks and human mobility patterns profoundly influence the spreading of endemic diseases in smallscale communities individuals engage in social interactions within confined environments such as homes and workplaces where daily routines facilitate virus transmission through predictable mobility pathways here we introduce a metapopulation model grounded in a microscopic markov chain approach to simulate susceptibleinfectedsusceptible dynamics within structured populations there are two primary types of nodes homes and destinations where individuals interact and transmit infections through recurrent mobility patterns we derive analytical expressions for the epidemic threshold and validate our theoretical findings through comparative simulations on wattsstrogatz and barabsialbert networks the experimental results reveal a nonlinear relationship between mobility probability and the epidemic threshold indicating that further increases can inhibit disease transmission beyond a certain critical mobility level,0
recently developed techniques to acquire highquality human mobility data allow largescale simulations of the spread of infectious diseases with high spatial and temporal resolutionanalysis of such data has revealed the oversimplification of existing theoretical frameworks to infer the final epidemic size or influential nodes from the network topology here we propose a spectral decompositionbased framework for the quantitative analysis of epidemic processes on realistic networks of human proximity derived from urban mobility data common wisdom suggests that modes with larger eigenvalues contribute more to the epidemic dynamics however we show that hidden dominant structures namely modes with smaller eigenvalues but a greater contribution to the epidemic dynamics exist in the proximity network this framework provides a basic understanding of the relationship between urban human motion and epidemic dynamics and will contribute to strategic mitigation policy decisions,0
in the past years we have witnessed the emergence of the new discipline of computational social science which promotes a new datadriven and computationbased approach to social sciences in this article we discuss how the availability of new technologies such as online social media and mobile smartphones has allowed researchers to passively collect human behavioral data at a scale and a level of granularity that were just unthinkable some years ago we also discuss how these digital traces can then be used to prove or disprove existing theories and develop new models of human behavior,0
new york has become one of the worstaffected covid19 hotspots and a pandemic epicenter due to the ongoing crisis this paper identifies the impact of the pandemic and the effectiveness of government policies on human mobility by analyzing multiple datasets available at both macro and micro levels for the new york city using data sources related to population density aggregated population mobility public rail transit use vehicle use hotspot and nonhotspot movement patterns and human activity agglomeration we analyzed the interborough and intraborough moment for new york city by aggregating the data at the borough level we also assessed the internodal population movement amongst hotspot and nonhotspot points of interest for the month of march and april 2020 results indicate a drop of about 80 in peoples mobility in the city beginning in midmarch the movement to and from manhattan showed the most disruption for both public transit and road traffic the city saw its first case on march 1 2020 but disruptions in mobility can be seen only after the second week of march when the shelter in place orders was put in effect owing to people working from home and adhering to stayathome orders manhattan saw the largest disruption to both inter and intraborough movement but the risk of spread of infection in manhattan turned out to be high because of higher hotspotlinked movements the stayathome restrictions also led to an increased population density in brooklyn and queens as people were not commuting to manhattan insights obtained from this study would help policymakers better understand human behavior and their response to the news and governmental policies,0
novel aspects of human dynamics and social interactions are investigated by means of mobile phone data using extensive phone records resolved in both time and space we study the mean collective behavior at large scales and focus on the occurrence of anomalous events we discuss how these spatiotemporal anomalies can be described using standard percolation theory tools we also investigate patterns of calling activity at the individual level and show that the interevent time of consecutive calls is heavytailed this finding which has implications for dynamics of spreading phenomena in social networks agrees with results previously reported on other human activities,0
the policies implemented to hinder the covid19 outbreak represent one of the largest critical events in history the understanding of this process is fundamental for crafting and tailoring postdisaster relief in this work we perform a massive data analysis through geolocalized data from 13m facebook users on how such a stress affected mobility patterns in france italy and uk we find that the general reduction of the overall efficiency in the network of movements is accompanied by geographical fragmentation with a massive reduction of longrange connections the impact however differs among nations according to their initial mobility structure indeed we find that the mobility network after the lockdown is more concentrated in the case of france and uk and more distributed in italy such a process can be approximated through percolation to quantify the substantial impact of the lockdown,0
this study considers the availability of room opportunities collected from a japanese hotel booking site we empirically analyze the daily number of room opportunities for four areas to determine the migration trends of travelers we discuss a finite mixture of poisson distributions and the emalgorithm as its parameter estimation method we further propose a method to infer the probability of opportunities existing for each observation we characterize demandsupply situations by means of relationship between the averaged room prices and the probability of opportunity existing,0
natural disasters can significantly disrupt human mobility in urban areas studies have attempted to understand and quantify such disruptions using crowdsourced mobility data sets however limited research has studied the justice issues of mobility data in the context of natural disasters the lack of research leaves us without an empirical foundation to quantify and control the possible biases in the data this study using 2017 hurricane harvey as a case study explores three aspects of mobility data that could potentially cause injustice representativeness quality and precision we find representativeness being a major factor contributing to mobility data injustice there is a persistent disparity of representativeness across neighborhoods of different socioeconomic characteristics before during and after the hurricanes landfall additionally we observed significant drops of data precision during the hurricane adding uncertainty to locate people and understand their movements during extreme weather events the findings highlight the necessity in understanding and controlling the possible bias of mobility data as well as developing practical tools through data justice lenses in collecting and analyzing data during disasters,0
the covid19 the disease caused by the novel coronavirus 2019 sarscov2 has caused graving woes across the globe since first reported in the epicenter wuhan hubei china december 2019 the spread of covid19 in china has been successfully curtailed by massive travel restrictions that put more than 900 million people housebound for more than two months since the lockdown of wuhan on 23 january 2020 when other provinces in china followed suit here we assess the impact of chinas massive lockdowns and travel restrictions reflected by the changes in mobility patterns before and during the lockdown period we quantify the synchrony of mobility patterns across provinces and within provinces using these mobility data we calibrate movement flow between provinces in combination with an epidemiological compartment model to quantify the effectiveness of lockdowns and reductions in disease transmission our analysis demonstrates that the onset and phase of local community transmission in other provinces depends on the cumulative population outflow received from the epicenter hubei as such infections can propagate further into other interconnected places both near and far thereby necessitating synchronous lockdowns moreover our datadriven modeling analysis shows that lockdowns and consequently reduced mobility lag a certain time to elicit an actual impact on slowing down the spreading and ultimately putting the epidemic under check in spite of the vastly heterogeneous demographics and epidemiological characteristics across china mobility data shows that massive travel restrictions have been applied consistently via a topdown approach along with high levels of compliance from the bottom up,0
we model the mobility of mobile phone users to study the fundamental spreading patterns characterizing a mobile virus outbreak we find that while bluetooth viruses can reach all susceptible handsets with time they spread slowly due to human mobility offering ample opportunities to deploy antiviral software in contrast viruses utilizing multimedia messaging services could infect all users in hours but currently a phase transition on the underlying call graph limits them to only a small fraction of the susceptible users these results explain the lack of a major mobile virus breakout so far and predict that once a mobile operating systems market share reaches the phase transition point viruses will pose a serious threat to mobile communications,0
mobile phone datasets allow for the analysis of human behavior on an unprecedented scale the social network temporal dynamics and mobile behavior of mobile phone users have often been analyzed independently from each other using mobile phone datasets in this article we explore the connections between various features of human behavior extracted from a large mobile phone dataset our observations are based on the analysis of communication data of 100000 anonymized and randomly chosen individuals in a dataset of communications in portugal we show that clustering and principal component analysis allow for a significant dimension reduction with limited loss of information the most important features are related to geographical location in particular we observe that most people spend most of their time at only a few locations with the help of clustering methods we then robustly identify home and office locations and compare the results with official census data finally we analyze the geographic spread of users frequent locations and show that commuting distances can be reasonably well explained by a gravity model,0
we develop a multipleevents model and exploit within and between country variation in the timing type and level of intensity of various public policies to study their dynamic effects on the daily incidence of covid19 and on population mobility patterns across 135 countries we remove concurrent policy bias by taking into account the contemporaneous presence of multiple interventions the main result of the paper is that cancelling public events and imposing restrictions on private gatherings followed by school closures have quantitatively the most pronounced effects on reducing the daily incidence of covid19 they are followed by workplace as well as stayathome requirements whose statistical significance and levels of effect are not as pronounced instead we find no effects for international travel controls public transport closures and restrictions on movements across cities and regions we establish that these findings are mediated by their effect on population mobility patterns in a manner consistent with timeuse and epidemiological factors,0
as cities struggle to adapt to more peoplecentered urbanism transportation planning and engineering must innovate to expand the street network strategically in order to ensure efficiency but also to deter sprawl here we conducted a study of over 200 cities around the world to understand the impact that the patterns of deceleration points in streets due to traffic signs has in trajectories done from motorized vehicles we demonstrate that there is a ubiquitous nonlinear relationship between time and distance in the optimal trajectories within each city more precisely given a specific period of time  without any traffic one can move on average up to the distance left langle d right rangle sim we found a superlinear relationship for almost all cities in which 10 this points to an efficiency of scale when traveling large distances meaning the average speed will be higher for longer trips when compared to shorter trips we demonstrate that this efficiency is a consequence of the spatial distribution of large segments of streets without deceleration points favoring access to routes in which a vehicle can cross large distances without stops these findings show that cities must consider how their street morphology can affect travel speed,0
urban morphology has long been recognized as a factor shaping human mobility yet comparative and formal classifications of urban form across metropolitan areas remain limited building on theoretical principles of urban structure and advances in unsupervised learning we systematically classified the built environment of nine us metropolitan areas using structural indicators such as density connectivity and spatial configuration the resulting morphological types were linked to mobility patterns through descriptive statistics marginal effects estimation and post hoc statistical testing here we show that distinct urban forms are systematically associated with different mobility behaviors such as reticular morphologies being linked to significantly higher public transport use marginal effect 049 and reduced car dependence 041 while organic forms are associated with increased car usage 044 and substantial declines in public transport 047 and active mobility 030 these effects are statistically robust p 1e19 highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research these results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design,0
human mobility a pivotal aspect of urban dynamics displays a profound and multifaceted relationship with urban sustainability despite considerable efforts analyzing mobility patterns over decades the ranking dynamics of urban mobility has received limited attention this study aims to contribute to the field by investigating changes in rank and size of hourly inflows to various locations across 60 chinese cities throughout the day we find that the ranksize distribution of hourly inflows over the course of the day is stable across cities to uncover the microdynamics beneath the stable aggregate distribution amidst shifting location inflows we analyzed consecutivehour inflow size and ranking variations our findings reveal a dichotomy locations with higher daily average inflow display a clear monotonic trend with more pronounced increases or decreases in consecutivehour inflow in contrast ranking variations exhibit a nonmonotonic pattern distinguished by the stability of not only the top and bottom rankings but also those in moderatelyinflowed locations finally we compare ranking dynamics across cities using a ranking metric the rank turnover the results advance our understanding of urban mobility dynamics providing a basis for applications in urban planning and traffic engineering,0
the patterns of human mobility play a key role in the spreading of infectious diseases and thus represent a key ingredient of epidemic modeling and forecasting unfortunately as the covid19 pandemic has dramatically highlighted for the vast majority of countries there is no availability of granular mobility data this hinders the possibility of developing computational frameworks to monitor the evolution of the disease and to adopt timely and adequate prevention policies here we show how this problem can be addressed in the case study of italy we build a multiplex mobility network based solely on open data and implement a sir metapopulation model that allows scenario analysis through datadriven stochastic simulations the mobility flows that we estimate are in agreement with realtime proprietary data from smartphones our modeling approach can thus be useful in contexts where highresolution mobility data is not available,0
predicting human displacements is crucial for addressing various societal challenges including urban design traffic congestion epidemic management and migration dynamics while predictive models like deep learning and markov models offer insights into individual mobility they often struggle with outofroutine behaviours our study introduces an approach that dynamically integrates individual and collective mobility behaviours leveraging collective intelligence to enhance prediction accuracy evaluating the model on millions of privacypreserving trajectories across three us cities we demonstrate its superior performance in predicting outofroutine mobility surpassing even advanced deep learning methods spatial analysis highlights the models effectiveness near urban areas with a high density of points of interest where collective behaviours strongly influence mobility during disruptive events like the covid19 pandemic our model retains predictive capabilities unlike individualbased models by bridging the gap between individual and collective behaviours our approach offers transparent and accurate predictions crucial for addressing contemporary mobility challenges,0
an intriguing open question is whether measurements made on big data recording human activities can yield us highfidelity proxies of socioeconomic development and wellbeing can we monitor and predict the socioeconomic development of a territory just by observing the behavior of its inhabitants through the lens of big data in this paper we design a datadriven analytical framework that uses mobility measures and social measures extracted from mobile phone data to estimate indicators for socioeconomic development and wellbeing we discover that the diversity of mobility defined in terms of entropy of the individual users trajectories exhibits i significant correlation with two different socioeconomic indicators and ii the highest importance in predictive models built to predict the socioeconomic indicators our analytical framework opens an interesting perspective to study human behavior through the lens of big data by means of new statistical indicators that quantify and possibly nowcast the wellbeing and the socioeconomic development of a territory,0
this study identifies the limitations and underlying characteristics of urban mobility networks that influence the performance of the gravity model the gravity model is a widelyused approach for estimating and predicting population flows in urban mobility networks assuming the scalefree property prior studies have reported good performance results for the gravity model at certain levels of aggregation however the characteristics of urban mobility networks might vary depending on the spatial and temporal resolutions of data hence the sensitivity of gravity model performance to variation in the level of aggregation of data and the temporal and spatial scale of urban mobility networks needs to be examined the basic gravity model is tested on urban mobility networks on an hourly and daily scale using finegrained locationbased human mobility data for multiple us metropolitan counties to address this gap the findings suggest that 1 finerscale urban mobility networks do not demonstrate a scalefree property 2 the performance of the basic gravity model decays for predicting population flow in the finerscale urban mobility networks 3 the variations in population density distribution and mobility network structure and properties across counties do not significantly influence the performance of gravity models hence gravity models may not be suitable for modeling urban mobility networks with daily or hourly aggregation of census tract to census tract movements the findings highlight the need for new urban mobility network models or machine learning approaches to predict finescale and high temporalresolution urban mobility networks better,0
transportation networks serve as windows into the complex world of urban systems by properly characterizing a road network we can therefore better understand its encompassing urban system this study offers a geometrical approach towards capturing inherent properties of urban road networks it offers a robust and efficient methodology towards defining and extracting three relevant indicators of road networks area line and point thresholds through measures of their grid equivalents by applying the methodology to 50 us urban systems we successfully observe differences between eastern versus western coastal versus inland and old versus young cities moreover we show that many socioeconomic characteristics as well as travel patterns within urban systems are directly correlated with their corresponding area line and point thresholds,0
in this research we propose a series of methodologies to mine transit riders travel pattern and behavioral preferences and then we use these knowledges to adjust and optimize the transit systems contributions are 1 to increase the data validity a we propose a novel approach to rectify the time discrepancy of data between the afc automated fare collection systems and avl automated vehicle location system our approach transforms data events into signals and applies time domain correlation the detect and rectify their relative discrepancies b by combining historical data and passengers ticketing time stamps we induct and compensate missing information in avl datasets 2 to infer passengers alighting point we introduce a maximum probabilistic model incorporating passengers home place to recover their complete transit trajectory from semicomplete boarding recordsthen we propose an enhance activity identification algorithm which is capable of specifying passengers shortterm activity from ordinary transfers finally we analyze the temporalspatial characteristic of transit ridership 3 to discover passengers travel demands we integrate each passengers trajectory data in multiple days and construct a hybrid trip graph htg we then use a depth search algorithm to derive the spatially closed transit trip chains finally we use closed transit trip chains of passengers to study their travel pattern from various perspectives finally we analyze urban transit corridors by aggregating the passengers critical transit chains4 we derive eight influential factors and then construct passengers choice models under various scenarios next we validate our model using ridership redistribute simulations finally we conduct a comprehensive analysis on passengers temporal choice preference and use this information to optimize urban transit systems,0
fighting the covid19 pandemic most countries have implemented nonpharmaceutical interventions like wearing masks physical distancing lockdown and travel restrictions because of their economic and logistical effects tracking mobility changes during quarantines is crucial in assessing their efficacy and predicting the virus spread chile one of the worsthit countries in the world unlike many other countries implemented quarantines at a more localized level shutting down small administrative zones rather than the whole country or large regions given the nonobvious effects of these localized quarantines tracking mobility becomes even more critical in chile to assess the impact on human mobility of the localized quarantines in chile we analyze a mobile phone dataset made available by telefnica chile which comprises 31 billion extended detail records and 54 million users covering the period february 26th to september 20th 2020 from these records we derive three epidemiologically relevant metrics describing the mobility within and between comunas the datasets made available can be used to fight the covid19 epidemics particularly for localized quarantines less understood effect,0
residential mobility is deeply entangled with all aspects of huntergatherer life ways and is therefore an issue of central importance in huntergatherer studies huntergatherers vary widely in annual rates of residential mobility and understanding the sources of this variation has long been of interest to anthropologists and archaeologists since mobility is to a large extent driven by the need for a continuous supply of food a natural framework for addressing this question is provided by the metabolic theory of ecology this provides a powerful framework for formulating formal testable hypotheses concerning evolutionary and ecological constraints on the scale and variation of huntergatherer residential mobility we evaluate these predictions using extant data and show strong support for the hypotheses we show that the overall scale of huntergatherer residential mobility is predicted by average human body size and the limited capacity of mobile huntergatherers to store energy internally we then show that the majority of variation in residential mobility observed across cultures is predicted by energy availability in local ecosystems our results demonstrate that largescale evolutionary and ecological processes common to all plants and animals constrain huntergatherers in predictable ways as they move through territories to effectively exploit resources over the course of a year moreover our results extend the scope of the metabolic theory of ecology by showing how it successfully predicts variation in the behavioral ecology of populations within a species,0
persuasive scholarship presents how individual daily travel habits implicate congestion environmental pollution and the travel experience however the empirical characteristics and dynamics of travel habits remain poorly understood quantifying both our individual travel habits and how these habits aggregate to form systemwide dynamics is of critical importance to enable the smart design of public transit systems that are better tailored to our daily mobility needs we contribute to this need through the development and implementation of a new measurement framework capturing the peakedness of users departure time distributions departure time peakedness reflects a users tendency to repeatedly choose the same departure time for a given origindestination trip offering a clearer and more intuitive representation of regularity and habitual patterns compared to traditional metrics like standard deviation or entropy our framework demonstrates that systemwide departure time peakedness can be decomposed into individual users departure time peakedness and the alignment of their peak times this allows for a systematic analysis of both individual and collective behaviours we apply our framework to departure time data from a 12month period encompassing 5947907 bus journeys made by 29640 individuals across three urban networks within a large regional metropolis our findings reveal that departure time peakedness is more deeply tied to inherent passengerspecific characteristics such as passenger type rather than external factors like weather or holidays additionally individuallevel departure time peakedness shows notable dynamics over time indicating that habitual routines can evolve in the long term while systemlevel peakedness exhibits remarkable longterm stability,0
ambient exposure to fine particulate matters of diameters smaller than 25m pm25 has been identified as one critical cause for respiratory disease disparities in exposure to pm25 among income groups at individual residences are known to exist and are easy to calculate existing approaches for exposure assessment however do not capture the exposure implied by the dynamic mobility of city dwellers that accounts for a large proportion of the exposure outside homes to overcome the challenge of gauging the exposure to pm25 for city dwellers we analyzed billions of anonymized and privacyenhanced locationbased data generated by mobile phone users in harris county texas to characterize the mobility patterns of the populations and associated exposure we introduce the metric for exposure extent based on the time people spent at places with the air pollutant and examine the disparities in mobilitybased exposure across income groups our results show that pm25 emissions disproportionately expose lowincome populations due to their mobility activities people with higherthanaverage income are exposed to lower levels of pm25 emissions these disparities in mobilitybased exposure are the result of frequent visits of lowincome people to the industrial sectors of urban areas with high pm25 emissions and the larger mobility scale of these people for life needs the results inform about environmental justice and public health strategies not only to reduce the overall pm25 exposure but also to mitigate the disproportional impacts on lowincome populations the findings also suggest that an integration of extensive finescale population mobility and pollution emissions data can unveil new insights into inequality in air pollution exposures at the urban scale,0
human mobility in cities is shaped not only by visible structures such as highways rivers and parks but also by invisible barriers rooted in socioeconomic segregation uneven access to amenities and administrative divisions yet identifying and quantifying these barriers at scale and their relative importance on peoples movements remains a major challenge neural embedding models originally developed for language offer a powerful way to capture the complexity of human mobility from largescale data here we apply this approach to 254 million observed trajectories across 11 major us cities learning mobility embeddings that reveal how people move through urban space these mobility embeddings define a functional distance between places one that reflects behavioral rather than physical proximity and allow us to detect barriers between neighborhoods that are geographically close but behaviorally disconnected we find that the strongest predictors of these barriers are differences in access to amenities administrative borders and residential segregation by income and race these invisible borders are concentrated in urban cores and persist across cities spatial scales and time periods physical infrastructure such as highways and parks plays a secondary but still significant role especially at short distances we also find that individuals who cross barriers tend to do so outside of traditional commuting hours and are more likely to live in areas with greater racial diversity and higher transit use or income together these findings reveal how spatial social and behavioral forces structure urban accessibility and provide a scalable framework to detect and monitor barriers in cities with applications in planning policy evaluation and equity analysis,0
we introduce a basic model for human mobility that accounts for the different dynamics arising from individuals embarking on short trips and returning to their home locations and individuals relocating to a new home the differences between the two modes of motion comes to light on contrasting two recent studies one tracking the geographical location of dollar bills citebrockmann the other that of mobile cell phones citegonzalez trips introduce two characteristic time scales the time between trips  and the duration of each trip  and relocations introduces a third time scale t for the time between relocations in practice tsimrm years simrm months and simrm days so the three time scales are widely separated traditionally studies incorporating human motion assume only a single mode using a generic rate to account for all types of motion,0
the availability of massive datasets describing human mobility offers the possibility to design simulation tools to monitor and improve the resilience of transport systems in response to traumatic events such as natural and manmade disasters eg floods terroristic attacks etc in this perspective we propose achilles an application to model peoples movements in a given transport system mode through a multiplex network representation based on mobility data achilles is a webbased application which provides an easytouse interface to explore the mobility fluxes and the connectivity of every urban zone in a city as well as to visualize changes in the transport system resulting from the addition or removal of transport modes urban zones and single stops notably our application allows the user to assess the overall resilience of the transport network by identifying its weakest node ie urban achilles heel with reference to the ancient greek mythology to demonstrate the impact of achilles for humanitarian aid we consider its application to a realworld scenario by exploring human mobility in singapore in response to flood prevention,0
human mobility and other social activity patterns influence various aspects of society such as urban planning traffic predictions crisis resilience and epidemic prevention the behaviour of individuals like their communication frequencies and movements are shaped by societal and socioeconomic factors in addition the differences in the geolocation of people as well as their gender and age cast effects on their activity patterns in this study we focus on investigating these patterns by using mobile phone data specifically the call detail records cdrs to analyze the social communication and mobility patterns of people this dataset can provide us insight into the individual and populationlevel behaviours in rural and urban environments on a daily weekly and seasonal basis the results of our analyses show that in the urban areas people have high calling activity but low mobility while in the rural areas they show the opposite behaviour ie low calling activity combined with high mobility overall there is a decreasing trend in peoples mobility through the year even though their calling activity remained consistent except for the holidays during which time the communication frequency drops markedly we have also observed that there are significant differences in the mobility between the work days and free days finally the age and gender of individuals have also been observed to play a role in the seasonal patterns differently in urban and rural areas,0
lvy flights represent the best strategy to randomly search for a target in an unknown environment and have been widely observed in many animal species here we inspect and discuss recent results concerning human behavior and cognition different studies have shown that human mobility can be described in terms of lvy flights while fresh evidence indicates that the same pattern accounts for human mental searches in online gambling sites thus lvy flights emerge as a unifying concept with broad crossdisciplinary implications we argue that the ubiquity of such a pattern both in behavior and cognition suggests that the brain regions responsible for this behavior are likely to be evolutionarily old ie no frontal cortex is involved and that fmri techniques might help to confirm this hypothesis,0
human mobility is crucial to understand the transmission pattern of covid19 on spatially embedded geographic networks this pattern seems unpredictable and the propagation appears unstoppable resulting in over 350000 death tolls in the us by the end of 2020 here we create the spatiotemporal intercounty mobility network using 10 tb terabytes trajectory data of 30 million smart devices in the us in the first six months of 2020 we investigate its bound percolation by removing the weakly connected edges the mobility network becomes vulnerable and prone to reach its criticality and thus experience surprisingly abrupt phase transitions despite the complex behaviors of the mobility network we devised a novel approach to identify a small manageable set of recurrent critical bridges connecting the giant component and the secondlargest component these adaptive links located across the united states played a key role as valves connecting components in divisions and regions during the pandemic beyond our numerical results unveil that network characteristics determine the critical thresholds and the bridge locations the findings provide new insights into managing and controlling the connectivity of mobility networks during unprecedented disruptions the work can also potentially offer practical future infectious diseases both globally and locally,0
what predicts a neighborhoods resilience and adaptability to essential public health policies and shelterinplace regulations that prevent the harmful spread of covid19 to answer this question in this paper we present a novel application of human mobility patterns and human behavior in a network setting we analyze mobility data in new york city over two years from january 2019 to december 2020 and create weekly mobility networks between census block groups by aggregating point of interest level visit patterns our results suggest that both the socioeconomic and geographic attributes of neighborhoods significantly predict neighborhood adaptability to the shelterinplace policies active at that time that is our findings and simulation results reveal that in addition to factors such as race education and income geographical attributes such as access to amenities in a neighborhood that satisfy community needs were equally important factors for predicting neighborhood adaptability and the spread of covid19 the results of our study provide insights that can enhance urban planning strategies that contribute to pandemic alleviation efforts which in turn may help urban areas become more resilient to exogenous shocks such as the covid19 pandemic,0
sequential visiontolanguage or visual storytelling has recently been one of the areas of focus in computer vision and language modeling domains though existing models generate narratives that read subjectively well there could be cases when these models miss out on generating stories that account and address all prospective human and animal characters in the image sequences considering this scenario we propose a model that implicitly learns relationships between provided characters and thereby generates stories with respective characters in scope we use the vist dataset for this purpose and report numerous statistics on the dataset eventually we describe the model explain the experiment and discuss our current status and future work,0
we present a novel approach to classify causal micronarratives from text these narratives are sentencelevel explanations of the causes andor effects of a target subject the approach requires only a subjectspecific ontology of causes and effects and we demonstrate it with an application to inflation narratives using a humanannotated dataset spanning historical and contemporary us news articles for training we evaluate several large language models llms on this multilabel classification task the bestperforming modela finetuned llama 31 8bachieves f1 scores of 087 on narrative detection and 071 on narrative classification comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements this research establishes a framework for extracting causal micronarratives from realworld data with wideranging applications to social science research,0
computational visual storytelling produces a textual description of events and interpretations depicted in a sequence of images these texts are made possible by advances and crossdisciplinary approaches in natural language processing generation and computer vision we define a computational creative visual storytelling as one with the ability to alter the telling of a story along three aspects to speak about different environments to produce variations based on narrative goals and to adapt the narrative to the audience these aspects of creative storytelling and their effect on the narrative have yet to be explored in visual storytelling this paper presents a pipeline of taskmodules object identification singleimage inferencing and multiimage narration that serve as a preliminary design for building a creative visual storyteller we have piloted this design for a sequence of images in an annotation task we present and analyze the collected corpus and describe plans towards automation,0
one of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate humanlike stories in this paper we propose a hierarchical deep learning architecture based on encoderdecoder networks to address this problem to better help our network maintain this context while also generating long and diverse sentences we incorporate natural language image descriptions along with the images themselves to generate each story sentence we evaluate our system on the visual storytelling vist dataset and show that our method outperforms stateoftheart techniques on a suite of different automatic evaluation metrics the empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling,0
centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology the research in automatic storytelling has recently turned towards emotional storytelling in which characters emotions play an important role in the plot development however these studies mainly use emotion to generate propositional statements in the form a feels affection towards b or a confronts b at the same time emotional behavior does not boil down to such propositional descriptions as humans display complex and highly variable patterns in communicating their emotions both verbally and nonverbally in this paper we analyze how emotions are expressed nonverbally in a corpus of fan fiction short stories our analysis shows that stories written by humans convey character emotions along various nonverbal channels we find that some nonverbal channels such as facial expressions and voice characteristics of the characters are more strongly associated with joy while gestures and body postures are more likely to occur with trust based on our analysis we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters emotions,0
visual storytelling includes two important parts coherence between the story and images as well as the story structure for image to text neural network models similar images in the sequence would provide close information for story generator to obtain almost identical sentence however repeatedly narrating same objects or events will undermine a good story structure in this paper we proposed an intersentence diverse beam search to generate a more expressive story comparing to some recent models of visual storytelling task which generate story without considering the generated sentence of the previous picture our proposed method can avoid generating identical sentence even given a sequence of similar pictures,0
in this paper we collect an anthology of 100 visual stories from authors who participated in our systematic creative process of improvised storybuilding based on image sequences following close reading and thematic analysis of our anthology we present five themes that characterize the variations found in this creative visual storytelling process 1 narrating what is in vision vs envisioning 2 dynamically characterizing entitiesobjects 3 sensing experiential information about the scenery 4 modulating the mood 5 encoding narrative biases in understanding the varied ways that people derive stories from images we offer considerations for collecting storydriven training data to inform automatic story generation in correspondence with each theme we envision narrative intelligence criteria for computational visual storytelling as creative reliable expressive grounded and responsible from these criteria we discuss how to foreground creative expression account for biases and operate in the bounds of visual storyworlds,0
digital humanities is an important subject because it enables developments in history literature and films in this paper we perform an empirical study of a chinese historical text records of the three kingdoms textitrecords and a historical novel of the same story romance of the three kingdoms textitromance we employ natural language processing techniques to extract characters and their relationships then we characterize the social networks and sentiments of the main characters in the historical text and the historical novel we find that the social network in textitromance is more complex and dynamic than that of textitrecords and the influence of the main characters differs these findings shed light on the different styles of storytelling in the two literary genres and how the historical novel complicates the social networks of characters to enrich the literariness of the story,0
literary texts are usually rich in meanings and their interpretation complicates corpus studies and automatic processing there have been several attempts to create collections of literary texts with annotation of literary elements like the authors speech characters events scenes etc however they resulted in small collections and standalone rules for annotation the present article describes an experiment on lexical annotation of text worlds in a literary work and quantitative methods of their comparison the experiment shows that for a wellagreed tag assignment annotation rules should be set much more strictly however if borders between text worlds and other elements are the result of a subjective interpretation they should be modeled as fuzzy entities,0
selecting an appropriate book is crucial for fostering reading habits in children while children exhibit varying levels of complexity when generating oral narratives the question arises do childrens books also differ in narrative complexity this study explores the narrative dynamics of literary texts used in schools focusing on how their complexity evolves across different grade levels using wordrecurrence graph analysis we examined a dataset of 1627 literary texts spanning 13 years of education the findings reveal significant exponential growth in connectedness particularly during the first three years of schooling mirroring patterns observed in childrens oral narratives these results highlight the potential of literary texts as a tool to support the development of literacy skills,0
large language models llms have demonstrated notable creative abilities in generating literary texts including poetry and short stories however prior research has primarily centered on english with limited exploration of nonenglish literary traditions and without standardized methods for assessing creativity in this paper we evaluate the capacity of llms to generate persian literary text enriched with culturally relevant expressions we build a dataset of usergenerated persian literary spanning 20 diverse topics and assess model outputs along four creativity dimensionsoriginality fluency flexibility and elaborationby adapting the torrance tests of creative thinking to reduce evaluation costs we adopt an llm as a judge for automated scoring and validate its reliability against human judgments using intraclass correlation coefficients observing strong agreement in addition we analyze the models ability to understand and employ four core literary devices simile metaphor hyperbole and antithesis our results highlight both the strengths and limitations of llms in persian literary text generation underscoring the need for further refinement,0
emotions are a crucial part of compelling narratives literature tells us about people with goals desires passions and intentions emotion analysis is part of the broader and larger field of sentiment analysis and receives increasing attention in literary studies in the past the affective dimension of literature was mainly studied in the context of literary hermeneutics however with the emergence of the research field known as digital humanities dh some studies of emotions in a literary context have taken a computational turn given the fact that dh is still being formed as a field this direction of research can be rendered relatively new in this survey we offer an overview of the existing body of research on emotion analysis as applied to literature the research under review deals with a variety of topics including tracking dramatic changes of a plot development network analysis of a literary text and understanding the emotionality of texts among other topics,0
the paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative ai the study analyzes 250 stories authored by crowdworkers in june 2019 and 80 stories generated by gpt35 and gpt4 in march 2023 by merging methods from narratology and inferential statistics both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human the proposed experimental paradigm allows a direct and controlled comparison between human and llmgenerated storytelling responses to the pygmalionesque prompts confirm the pervasive presence of the pygmalion myth in the collective imaginary of both humans and large language models all solicited narratives present a scientific or technological pursuit the analysis reveals that narratives from gpt35 and particularly gpt4 are more progressive in terms of gender roles and sexuality than those written by humans while ai narratives with default settings and no additional prompting can occasionally provide innovative plot twists they offer less imaginative scenarios and rhetoric than humanauthored texts the proposed framework argues that fiction can be used as a window into human and aibased collective imaginary and social dimensions,0
this study investigates global properties of literary and nonliterary texts within the literary texts a distinction is made between canonical and noncanonical works the central hypothesis of the study is that the three text types nonliterary literarycanonical and literarynoncanonical exhibit systematic differences with respect to structural design features as correlates of aesthetic responses in readers to investigate these differences we compiled a corpus containing texts of the three categories of interest the jena textual aesthetics corpus two aspects of global structure are investigated variability and selfsimilar fractal patterns which reflect longrange correlations along texts we use four types of basic observations i the frequency of postags per sentence ii sentence length iii lexical diversity in chunks of text and iv the distribution of topic probabilities in chunks of texts these basic observations are grouped into two more general categories a the lowlevel properties i and ii which are observed at the level of the sentence reflecting linguistic decoding and b the highlevel properties iii and iv which are observed at the textual level reflecting comprehension the basic observations are transformed into time series and these time series are subject to multifractal detrended fluctuation analysis mfdfa our results show that lowlevel properties of texts are better discriminators than highlevel properties for the three text types under analysis canonical literary texts differ from noncanonical ones primarily in terms of variability fractality seems to be a universal feature of text more pronounced in nonliterary than in literary texts beyond the specific results of the study we intend to open up new perspectives on the experimental study of textual aesthetics,0
texts written in old literary finnish represent the first literary work ever written in finnish starting from the 16th century there have been several projects in finland that have digitized old publications and made them available for research use however using modern nlp methods in such data poses great challenges in this paper we propose an approach for simultaneously normalizing and lemmatizing old literary finnish into modern spelling our best model reaches to 963 accuracy in texts written by agricola and 877 accuracy in other contemporary outofdomain text our method has been made freely available on zenodo and github,0
in natural language processing nlp semantic matching algorithms have traditionally relied on the feature of word cooccurrence to measure semantic similarity while this feature approach has proven valuable in many contexts its simplistic nature limits its analytical and explanatory power when used to understand literary texts to address these limitations we propose a more transparent approach that makes use of story structure and related elements using a bert language model pipeline we label prose and epic poetry with story element labels and perform semantic matching by only considering these labels as features this new method story grammar semantic matching guides literary scholars to allusions and other semantic similarities across texts in a way that allows for characterizing patterns and literary technique,0
we present a dataset of manually annotated relationships between characters in literary texts in order to support the training and evaluation of automatic methods for relation type prediction in this domain makazhanov et al 2014 kokkinakis 2013 and the broader computational analysis of literary character elson et al 2010 bamman et al 2014 vala et al 2015 flekova and gurevych 2015 in this work we solicit annotations from workers on amazon mechanical turk for 109 texts ranging from homers iliad to joyces ulysses on four dimensions of interest for a given pair of characters we collect judgments as to the coarsegrained category professional social familial finegrained category friend lover parent rival employer and affinity positive negative neutral that describes their primary relationship in a text we do not assume that this relationship is static we also collect judgments as to whether it changes at any point in the course of the text,0
beyond the local constraints imposed by grammar words concatenated in long sequences carrying a complex message show statistical regularities that may reflect their linguistic role in the message in this paper we perform a systematic statistical analysis of the use of words in literary english corpora we show that there is a quantitative relation between the role of content words in literary english and the shannon information entropy defined over an appropriate probability distribution without assuming any previous knowledge about the syntactic structure of language we are able to cluster certain groups of words according to their specific role in the text,0
we present the project dialogism novel corpus or pdnc an annotated dataset of quotations for english literary texts pdnc contains annotations for 35978 quotations across 22 fulllength novels and is by an order of magnitude the largest corpus of its kind each quotation is annotated for the speaker addressees type of quotation referring expression and character mentions within the quotation text the annotated attributes allow for a comprehensive evaluation of models of quotation attribution and coreference for literary texts,0
given the rise of a new approach to mt neural mt nmt and its promising performance on different text types we assess the translation quality it can attain on what is perceived to be the greatest challenge for mt literary text specifically we target novels arguably the most popular type of literary text we build a literaryadapted nmt system for the englishtocatalan translation direction and evaluate it against a system pertaining to the previous dominant paradigm in mt statistical phrasebased mt pbsmt to this end for the first time we train mt systems both nmt and pbsmt on large amounts of literary text over 100 million words and evaluate them on a set of twelve widely known novels spanning from the the 1920s to the present day according to the bleu automatic evaluation metric nmt is significantly better than pbsmt p 001 on all the novels considered overall nmt results in a 11 relative improvement 3 points absolute over pbsmt a complementary human evaluation on three of the books shows that between 17 and 34 of the translations depending on the book produced by nmt versus 8 and 20 with pbsmt are perceived by native speakers of the target language to be of equivalent quality to translations produced by a professional human translator,0
large language models llms excel in understanding and generating text but struggle with providing professional literary criticism for works with profound thoughts and complex narratives this paper proposes glass greimas literary analysis via semiotic square a structured analytical framework based on greimas semiotic square gss to enhance llms ability to conduct indepth literary analysis glass facilitates the rapid dissection of narrative structures and deep meanings in narrative works we propose the first dataset for gssbased literary criticism featuring detailed analyses of 48 works then we propose quantitative metrics for gssbased literary criticism using the llmasajudge paradigm our frameworks results compared with expert criticism across multiple works and llms show high performance finally we applied glass to 39 classic works producing original and highquality analyses that address existing research gaps this research provides an aibased tool for literary research and education offering insights into the cognitive mechanisms underlying literary engagement,0
large language models llms have shown great potential in automated story generation but challenges remain in maintaining longform coherence and providing users with intuitive and effective control retrievalaugmented generation rag has proven effective in reducing hallucinations in text generation however the use of structured data to support generative storytelling remains underexplored this paper investigates how knowledge graphs kgs can enhance llmbased storytelling by improving narrative quality and enabling userdriven modifications we propose a kgassisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants participants created their own story prompts generated stories and edited knowledge graphs to shape their narratives through quantitative and qualitative analysis our findings demonstrate that knowledge graphs significantly enhance story quality in actionoriented and structured narratives within our system settings additionally editing the knowledge graph increases users sense of control making storytelling more engaging interactive and playful,0
we present two deep learning approaches to narrative text understanding for character relationship modelling the temporal evolution of these relations is described by dynamic word embeddings that are designed to learn semantic changes over time an empirical analysis of the corresponding character trajectories shows that such approaches are effective in depicting dynamic evolution a supervised learning approach based on the stateoftheart transformer model bert is used instead to detect static relations between characters the empirical validation shows that such events eg two characters belonging to the same family might be spotted with good accuracy even when using automatically annotated data this provides a deeper understanding of narrative plots based on the identification of key facts standard clustering techniques are finally used for character dealiasing a necessary preprocessing step for both approaches overall deep learning models appear to be suitable for narrative text understanding while also providing a challenging and unexploited benchmark for general natural language understanding,0
how well do modern longcontext language models understand literary fiction we explore this question via the task of literary evidence retrieval repurposing the relic dataset of that et al 2022 to construct a benchmark where the entire text of a primary source eg the great gatsby is provided to an llm alongside literary criticism with a missing quotation from that work this setting in which the model must generate the missing quotation mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination we curate a highquality subset of 292 examples through extensive filtering and human verification our experiments show that recent reasoning models such as gemini pro 25 can exceed human expert performance 625 vs 50 accuracy in contrast the best openweight model achieves only 291 accuracy highlighting a wide gap in interpretive reasoning between open and closedweight models despite their speed and apparent accuracy even the strongest models struggle with nuanced literary signals and overgeneration signaling open challenges for applying llms to literary analysis we release our dataset and evaluation code to encourage future work in this direction,0
the gutenberg literary english corpus glec jacobs 2018a provides a rich source of textual data for research in digital humanities computational linguistics or neurocognitive poetics in this study we address differences among the different literature categories in glec as well as differences between authors we report the results of three studies providing i topic and sentiment analyses for six text categories of glec ie children and youth essays novels plays poems stories and its 100 authors ii novel measures of semantic complexity as indices of the literariness creativity and book beauty of the works in glec eg jane austens six novels and iii two experiments on text classification and authorship recognition using novel features of semantic complexity the data on two novel measures estimating a texts literariness intratextual variance and stepwise distance van cranenburgh et al 2019 revealed that plays are the most literary texts in glec followed by poems and novels computation of a novel index of text creativity gray et al 2016 revealed poems and plays as the most creative categories with the most creative authors all being poets milton pope keats byron or wordsworth we also computed a novel index of perceived beauty of verbal art kintsch 2012 for the works in glec and predict that emma is the theoretically most beautiful of austens novels finally we demonstrate that these novel measures of semantic complexity are important features for text classification and authorship recognition with overall predictive accuracies in the range of 75 to 97 our data pave the way for future computational and empirical studies of literature or experiments in reading psychology and offer multiple baselines and benchmarks for analysing and validating other book corpora,0
this paper presents analysis of 30 literary texts written in english by different authors for each text there were created time series representing length of sentences in words and analyzed its fractal properties using two methods of multifractal analysis mfdfa and wtmm both methods showed that there are texts which can be considered multifractal in this representation but a majority of texts are not multifractal or even not fractal at all out of 30 books only a few have socorrelated lengths of consecutive sentences that the analyzed signals can be interpreted as real multifractals an interesting direction for future investigations would be identifying what are the specific features which cause certain texts to be multifractal and other to be monofractal or even not fractal at all,0
this paper introduces a multilevel multilabel text classification dataset comprising over 3000 documents the dataset features literary and critical texts from 19thcentury ottoman turkish and russian it is the first study to apply large language models llms to this dataset sourced from prominent literary periodicals of the era the texts have been meticulously organized and labeled this was done according to a taxonomic framework that takes into account both their structural and semantic attributes articles are categorized and tagged with bibliometric metadata by human experts we present baseline classification results using a classical bagofwords bow naive bayes model and three modern llms multilingual bert falcon and llamav2 we found that in certain cases bag of words bow outperforms large language models llms emphasizing the need for additional research especially in lowresource language settings this dataset is expected to be a valuable resource for researchers in natural language processing and machine learning especially for historical and lowresource languages the dataset is publicly available1,0
literary analysis criticism or studies is a largely valued field with dedicated journals and researchers which remains mostly within the humanities scope text analytics is the computeraided process of deriving information from texts in this article we describe a simple and generic model for performing literary analysis using text analytics the method relies on statistical measures of 1 token and sentence sizes and 2 wordnet synset features these measures are then used in principal component analysis where the texts to be analyzed are observed against shakespeare and the bible regarded as reference literature the model is validated by analyzing selected works from james joyce 18821941 one of the most important writers of the 20th century we discuss the consistency of this approach the reasons why we did not use other techniques eg partofspeech tagging and the ways by which the analysis model might be adapted and enhanced,0
the exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling however automated data analysis systems still face challenges in leveraging large language models llms for data insight discovery augmented analysis and data storytelling this paper introduces the multidimensional data storytelling framework mdsf based on large language models for automated insight generation and contextaware storytelling the framework incorporates advanced preprocessing techniques augmented analysis algorithms and a unique scoring mechanism to identify and prioritize actionable insights the use of finetuned llms enhances contextual understanding and generates narratives with minimal manual intervention the architecture also includes an agentbased mechanism for realtime storytelling continuation control key findings reveal that mdsf outperforms existing methods across various datasets in terms of insight ranking accuracy descriptive quality and narrative coherence the experimental evaluation demonstrates mdsfs ability to automate complex analytical tasks reduce interpretive biases and improve user satisfaction user studies further underscore its practical utility in enhancing content structure conclusion extraction and richness of detail,0
extracting metaphors and analogies from free text requires highlevel reasoning abilities such as abstraction and language understanding our study focuses on the extraction of the concepts that form metaphoric analogies in literary texts to this end we construct a novel dataset in this domain with the help of domain experts we compare the outofthebox ability of recent large language models llms to structure metaphoric mappings from fragments of texts containing proportional analogies the models are further evaluated on the generation of implicit elements of the analogy which are indirectly suggested in the texts and inferred by human readers the competitive results obtained by llms in our experiments are encouraging and open up new avenues such as automatically extracting analogies and metaphors from text instead of investing resources in domain experts to manually label data,0
at the intersection of creative text generation and literary theory this study explores the role of literary metaphor and its capacity to generate a range of meanings in this regard literary metaphor is vital to the development of any particular language to investigate whether the inclusion of original figurative language improves textual quality we trained an lstmbased language model in afrikaans the network produces phrases containing compellingly novel figures of speech specifically the emphasis falls on how ai might be utilised as a defamiliarisation technique which disrupts expected uses of language to augment poetic expression providing a literary perspective on text generation the paper raises thoughtprovoking questions on aesthetic value interpretation and evaluation,0
oral narrative skills are strong predictors of later literacy development this study examines the features of oral narratives from children who were identified by experts as requiring intervention using simple machine learning methods we analyse recorded stories from four and fiveyearold afrikaans and isixhosaspeaking children consistent with prior research we identify lexical diversity unique words and lengthbased features mean utterance length as indicators of typical development but features like articulation rate prove less informative despite crosslinguistic variation in partofspeech patterns the use of specific verbs and auxiliaries associated with goaldirected storytelling is correlated with a reduced likelihood of requiring intervention our analysis of two linguistically distinct languages reveals both languagespecific and shared predictors of narrative proficiency with implications for early assessment in multilingual contexts,0
we consider the task of predicting how literary a text is with a gold standard from human ratings aside from a standard bigram baseline we apply rich syntactic tree fragments mined from the training set and a series of handpicked features our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features and explains 760 of the variation in literary ratings,0
we introduce the first dataset for sequential visiontolanguage and explore how this data may be used for the task of visual storytelling the first release of this dataset sind v1 includes 81743 unique photos in 20211 sequences aligned to both descriptive caption and story language we establish several strong baselines for the storytelling task and motivate an automatic metric to benchmark progress modelling concrete description as well as figurative and social language as provided in this dataset and the storytelling task has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more humanlike understanding of grounded event structure and subjective expression,0
story understanding and analysis have long been challenging areas within natural language understanding automated narrative analysis requires deep computational semantic representations along with syntactic processing moreover the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches in this paper we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved the framework enables the extraction of highlevel and lowlevel concepts conveyed through the narrative using dictionarybased sentiment analysis our approach applies a custom lexicon built with the labmtsimple storylab module the custom lexicon is based on the valence arousal and dominance scores from the nrcvad dataset furthermore the framework advances the analysis by clustering similar sentiment plots using wards hierarchical clustering technique experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story,0
arabic rhetoric is the field of arabic linguistics which governs the art and science of conveying a message with greater beauty impact and persuasiveness the field is as ancient as the arabic language itself and is found extensively in classical and contemporary arabic poetry free verse and prose in practical terms it is the intelligent use of word order figurative speech and linguistic embellishments to enhance message delivery despite the volumes that have been written about it and the high status accorded to it there is no way to objectively know whether a speaker or writer has used arabic rhetoric in a given text to what extent and why there is no objective way to compare the use of arabic rhetoric across genres authors or epochs it is impossible to know which of preislamic poetry andalucian arabic poetry or modern literary genres are richer in arabic rhetoric the aim of the current study was to devise a way to measure the density of the literary devices which constitute arabic rhetoric in a given text as a proxy marker for arabic rhetoric itself a comprehensive list of 84 of the commonest literary devices and their definitions was compiled a system of identifying literary devices in texts was constructed a method of calculating the density of literary devices based on the morpheme count of the text was utilised four electronic tools and an analogue tool were created to support the calculation of an arabic texts rhetorical literary device density including a website and online calculator additionally a technique of reporting the distribution of literary devices used across the three subdomains of arabic rhetoric was created the output of this project is a working tool which can accurately report the density of arabic rhetoric in any arabic text or speech,0
the birth and rapid development of large language models llms have caused quite a stir in the field of literature once considered unattainable ais role in literary creation is increasingly becoming a reality in genres such as poetry jokes and short stories numerous ai tools have emerged offering refreshing new perspectives however its difficult to further improve the quality of these works this is primarily because understanding and appreciating a good literary work involves a considerable threshold such as knowledge of literary theory aesthetic sensibility interdisciplinary knowledge therefore authoritative data in this area is quite lacking additionally evaluating literary works is often complex and hard to fully quantify which directly hinders the further development of ai creation to address this issue this paper attempts to explore the mysteries of literary texts from the perspective of llms using ancient chinese poetry as an example for experimentation first we collected a variety of ancient poems from different sources and had experts annotate a small portion of them then we designed a range of comprehension metrics based on llms to evaluate all these poems finally we analyzed the correlations and differences between various poem collections to identify literary patterns through our experiments we observed a series of enlightening phenomena that provide technical support for the future development of highlevel literary creation based on llms,0
semantic annotation of long texts such as novels remains an open challenge in natural language processing nlp this research investigates the problem of detecting person entities and assigning them unique identities ie recognizing people especially main characters in novels we prepared a method for person entity linkage named entity recognition and disambiguation and new testing datasets the datasets comprise 1300 sentences from 13 classic novels of different genres that a novel reader had manually annotated our process of identifying literary characters in a text implemented in protagonisttagger comprises two stages 1 named entity recognition ner of persons 2 named entity disambiguation ned matching each recognized person with the literary characters full name based on approximate text matching the protagonisttagger achieves both precision and recall of above 83 on the prepared testing sets finally we gathered a corpus of 13 fulltext novels tagged with protagonisttagger that comprises more than 35000 mentions of literary characters,0
art is the lie that enables us to realize the truth pablo picasso for centuries humans have dedicated themselves to producing arts to convey their imagination the advancement in technology and deep learning in particular has caught the attention of many researchers trying to investigate whether art generation is possible by computers and algorithms using generative adversarial networks gans applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized this survey takes a comprehensive look at the recent works using gans for generating visual arts music and literary text a performance comparison and description of the various gan architecture are also presented finally some of the key challenges in art generation using gans are highlighted along with recommendations for future work,0
storytelling plays a central role in human socializing and entertainment however much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction in this paper we introduce the task of collaborative storytelling where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it we present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far we constructed the storytelling system by tuning a publiclyavailable large scale language model on a dataset of writing prompts and their accompanying fictional works we identify generating sufficiently humanlike utterances to be an important technical issue and propose a sampleandrank approach to improve utterance quality quantitative evaluation shows that our approach outperforms a baseline and we present qualitative evaluation of our systems capabilities,0
visual storytelling is a challenging multimodal task between vision language where the purpose is to generate a story for a stream of images its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent the aim of this work is to balance between these aspects by treating visual storytelling as a superset of image captioning an approach quite different compared to most of prior relevant studies this means that we firstly employ a visiontolanguage model for obtaining captions of the input images and then these captions are transformed into coherent narratives using languagetolanguage methods our multifarious evaluation shows that integrating captioning and storytelling under a unified framework has a positive impact on the quality of the produced stories in addition compared to numerous previous studies this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested lastly we propose a new metrictool named ideality that can be used to simulate how far some results are from an oracle model and we apply it to emulate humanlikeness in visual storytelling,0
we study the variation of word frequencies in russian literary texts our findings indicate that the standard deviation of a words frequency across texts depends on its average frequency according to a power law with exponent 062 showing that the rarer words have a relatively larger degree of frequency volatility ie burstiness several latent factors models have been estimated to investigate the structure of the word frequency distribution the dependence of a words frequency volatility on its average frequency can be explained by the asymmetry in the distribution of latent factors,0
written texts reflect an authors perspective making the thorough analysis of literature a key research method in fields such as the humanities and social sciences however conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns to address this gap we propose a method that leverages large language models llms to extract and organize these structures into a hierarchical framework we validate this approach by analyzing public opinions on generative ai collected by japans agency for cultural affairs comparing the narratives of supporters and critics our analysis provides clearer visualization of the factors influencing divergent opinions on generative ai offering deeper insights into the structures of agreement and disagreement,0
in this paper we conducted a multiperspective comparative narrative analysis cna on three prominent llms gpt35 palm2 and llama2 we applied identical prompts and evaluated their outputs on specific tasks ensuring an equitable and unbiased comparison between various llms our study revealed that the three llms generated divergent responses to the same prompt indicating notable discrepancies in their ability to comprehend and analyze the given task human evaluation was used as the gold standard evaluating four perspectives to analyze differences in llm performance,0
we are proposing a tool able to gather information on social networks from narrative texts its name is chaplin characters and places interaction network implemented in vbnet characters and places of the narrative works are extracted in a list of raw words aided by the interface the user selects names out of them after this choice the tool allows the user to enter some parameters and according to them creates a network where the nodes are the characters and places and the edges their interactions edges are labelled by performances the output is a gv file written in the dot graph scripting language which is rendered by means of the free open source software graphviz,0
previous work on visual storytelling mainly focused on exploring image sequence as evidence for storytelling and neglected textual evidence for guiding story generation motivated by human storytelling process which recalls stories for familiar images we exploit textual evidence from similar images to help generate coherent and meaningful stories to pick the images which may provide textual experience we propose a twostep ranking method based on image object recognition techniques to utilize textual information we design an extended seq2seq model with twochannel encoder and attention experiments on the vist dataset show that our method outperforms stateoftheart baseline models without heavy engineering,0
the improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling it is therefore important to quantitatively understand the nature of generated stories and how they differ from human storytelling we augment the reddit writingprompts dataset with short stories generated by gpt35 given the same prompts we quantify and compare the emotional and descriptive features of storytelling from both generative processes human and machine along a set of six dimensions we find that generated stories differ significantly from human stories along all six dimensions and that human and machine generations display similar biases when grouped according to the narrative pointofview and gender of the main protagonist we release our dataset and code at,0
we propose to use affect as a proxy for mood in literary texts in this study we explore the differences in computationally detecting tone versus detecting mood methodologically we utilize affective word embeddings to look at the affective distribution in different text segments we also present a simple yet efficient and effective method of enhancing emotion lexicons to take both semantic shift and the domain of the text into account producing realworld congruent results closely matching both contemporary and modern qualitative analyses,0
storytelling has always been vital for human nature from ancient times humans have used stories for several objectives including entertainment advertisement and education various analyses have been conducted by researchers and creators to determine the way of producing good stories the deep relationship between stories and emotions is a prime example with the advancement in deep learning technology computers are expected to understand and generate stories this survey paper is intended to summarize and further contribute to the development of research being conducted on the relationship between stories and emotions we believe creativity research is not to replace humans with computers but to find a way of collaboration between humans and computers to enhance the creativity with the intention of creating a new intersection between computational storytelling research and human creative writing we introduced creative techniques used by professional storytellers,0
we present a mechanistic analysis of literary style in gpt2 identifying individual neurons that discriminate between exemplary prose and rigid aigenerated text using herman melvilles bartleby the scrivener as a corpus we extract activation patterns from 355 million parameters across 32768 neurons in late layers we find 27122 statistically significant discriminative neurons p 005 with effect sizes up to d 14 through systematic ablation studies we discover a paradoxical result while these neurons correlate with literary text during analysis removing them often improves rather than degrades generated prose quality specifically ablating 50 highdiscriminating neurons yields a 257 improvement in literary style metrics this demonstrates a critical gap between observational correlation and causal necessity in neural networks our findings challenge the assumption that neurons which activate on desirable inputs will produce those outputs during generation with implications for mechanistic interpretability research and ai alignment,0
sentiment analysis is widely used to quantify sentiment in text but its application to literary texts poses unique challenges due to figurative language stylistic ambiguity as well as sentiment evocation strategies traditional dictionarybased tools often underperform especially for lowresource languages and transformer models while promising typically output coarse categorical labels that limit finegrained analysis we introduce a novel continuous sentiment scoring method based on concept vector projection trained on multilingual literary data which more effectively captures nuanced sentiment expressions across genres languages and historical periods our approach outperforms existing tools on english and danish texts producing sentiment scores whose distribution closely matches human ratings enabling more accurate analysis and sentiment arc modeling in literature,0
visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images this paper presents a novel approach that leverages recent advancements in multimodal models specifically adapting transformerbased architectures and large multimodal models for the visual storytelling task leveraging the largescale visual storytelling vist dataset our vistgpt model produces visually grounded contextually appropriate narratives we address the limitations of traditional evaluation metrics such as bleu meteor rouge and cider which are not suitable for this task instead we utilize rovist and groovist novel referencefree metrics designed to assess visual storytelling focusing on visual grounding coherence and nonredundancy these metrics provide a more nuanced evaluation of narrative quality aligning closely with human judgment,0
storytelling is fundamental to language including culture conversation and communication in their broadest senses it thus emerges as an essential component of intelligent systems including systems where natural language is not a primary focus or where we do not usually think of a story being involved in this paper we explore the emergence of storytelling as a requirement in embodied conversational agents including its role in educational and health interventions as well as in a generalpurpose computer interface for people with disabilities or other constraints that prevent the use of traditional keyboard and speech interfaces we further present a characterization of storytelling as an inventive fleshing out of detail according to a particular personal perspective and propose the dreamt model to focus attention on the different layers that need to be present in a characterdriven storytelling system most if not all aspects of the dreamt model have arisen from or been explored in some aspect of our implemented research systems but currently only at a primitive and relatively unintegrated level however this experience leads us to formalize and elaborate the dreamt model mnemonically as follows descriptiondialoguedefinitiondenotation realizationrepresentationrole explanationeducationentertainment actualizationactivation motivationmodelling topicalizationtransformation,0
understanding how online media frame issues is crucial due to their impact on public opinion research on framing using natural language processing techniques mainly focuses on specific content features in messages and neglects their narrative elements also the distinction between framing in different sources remains an understudied problem we address those issues and investigate how the framing of healthrelated topics such as covid19 and other diseases differs between conspiracy and mainstream websites we incorporate narrative information into the framing analysis by introducing a novel frame extraction approach based on semantic graphs we find that healthrelated narratives in conspiracy media are predominantly framed in terms of beliefs while mainstream media tend to present them in terms of science we hope our work offers new ways for a more nuanced frame analysis,0
literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language cultural nuances and unique stylistic elements in this work we introduce transagents a novel multiagent framework that simulates the roles and collaborative practices of a human translation company including a ceo senior editor junior editor translator localization specialist and proofreader the translation process is divided into two stages a preparation stage where the team is assembled and comprehensive translation guidelines are drafted and an execution stage that involves sequential translation localization proofreading and a final quality check furthermore we propose two innovative evaluation strategies monolingual human preference mhp which evaluates translations based solely on target language quality and cultural appropriateness and bilingual llm preference blp which leverages large language models like gpt4 for direct text comparison although transagents achieves lower dbleu scores due to the limited diversity of references its translations are significantly better than those of other baselines and are preferred by both human evaluators and llms over traditional human references and gpt4 translations our findings highlight the potential of multiagent collaboration in enhancing translation quality particularly for longer texts,0
this study investigates the internal mechanisms of bert a transformerbased large language model with a focus on its ability to cluster narrative content and authorial style across its layers using a dataset of narratives developed via gpt4 featuring diverse semantic content and stylistic variations we analyze berts layerwise activations to uncover patterns of localized neural processing through dimensionality reduction techniques such as principal component analysis pca and multidimensional scaling mds we reveal that bert exhibits strong clustering based on narrative content in its later layers with progressively compact and distinct clusters while strong stylistic clustering might occur when narratives are rephrased into different text types eg fables scifi kids stories minimal clustering is observed for authorial style specific to individual writers these findings highlight berts prioritization of semantic content over stylistic features offering insights into its representational capabilities and processing hierarchy this study contributes to understanding how transformer models like bert encode linguistic information paving the way for future interdisciplinary research in artificial intelligence and cognitive neuroscience,0
comics offer a compelling yet underexplored domain for computational narrative analysis combining text and imagery in ways distinct from purely textual or audiovisual media we introduce comicscene154 a manually annotated dataset of scenelevel narrative arcs derived from publicdomain comic books spanning diverse genres by conceptualizing comics as an abstraction for narrativedriven multimodal data we highlight their potential to inform broader research on multimodal storytelling to demonstrate the utility of comicscene154 we present a baseline scene segmentation pipeline providing an initial benchmark that future studies can build upon our results indicate that comicscene154 constitutes a valuable resource for advancing computational methods in multimodal narrative understanding and expanding the scope of comic analysis within the natural language processing community,0
literary artefacts are generally indexed and searched based on titles meta data and keywords over the years this searching and indexing works well when userreader already knows about that particular creative textual artefact or document this indexing and search hardly takes into account interest and emotional makeup of readers and its mapping to books when a person is looking for a literary textual artefact heshe might be looking for not only information but also to seek the joy of reading in case of literary artefacts progression of emotions across the key events could prove to be the key for indexing and searching in this paper we establish clusters among literary artefacts based on computational relationships among sentiment progressions using intelligent text analysis we have created a database of 1076 english titles 20 marathi titles and also used database with 16559 titles and their summaries we have proposed sentiment progression based indexing for searching and recommending books this can be used to create personalized clusters of book titles of interest to readers the analysis clearly suggests better searching and indexing when we are targeting book lovers looking for a particular type of book or creative artefact this indexing and searching can find many reallife applications for recommending books,0
chinese literary classics hold significant cultural and educational value offering deep insights into morality history and human nature these works often include classical chinese and complex narratives making them difficult for children to read to bridge this gap we introduce a childfriendly literary adaptation cla task to adapt the chinese literary classic into engaging and accessible text for children however recent large language models llms overlook childrens reading preferences ie vivid character portrayals concise narrative structures and appropriate readability which poses challenges in cla in this paper we propose a method called instructchild which augments the llm with these preferences for adaptation specifically we first obtain the characters personalities and narrative structure as additional information for finegrained instruction tuning then we devise a readability metric as the reward to align the llm with the childrens reading level finally a lookahead decoding strategy is applied to improve the readability of the generated text during inference to support the evaluation of cla task we construct the classic4children dataset which comprises both the original and childfriendly versions of the four great classical novels of chinese literature experimental results show that our instructchild significantly improves automatic and human evaluation performance,0
coreference annotation and resolution is a vital component of computational literary studies however it has previously been difficult to build high quality systems for fiction coreference requires complicated structured outputs and literary text involves subtle inferences and highly varied language new languagemodelbased seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdownlike annotations we create evaluate and release several trained models for coreference as well as a workflow for training new models,0
previous work of ours on semantic storytelling uses text analytics procedures including named entity recognition and event detection in this paper we outline our longerterm vision on semantic storytelling and describe the current conceptual and technical approach in the project that drives our research we develop aibased technologies that are verified by partners from industry one longterm goal is the development of an approach for semantic storytelling that has broad coverage and that is furthermore robust we provide first results on experiments that involve discourse parsing applied to a concrete use case explore the neighbourhood which is based on a semiautomatically collected data set with documents about noteworthy people in one of berlins districts though automatically obtaining annotations for coherence relations from plain text is a nontrivial challenge our preliminary results are promising we envision our approach to be combined with additional features ner coreference resolution knowledge graphs,0
automated longform story generation typically employs longcontext large language models llms for oneshot creation which can produce cohesive but not necessarily engaging content we introduce storytelling with action guidance swag a novel approach to storytelling with llms our approach frames story writing as a search problem through a twomodel feedback loop one llm generates story content and another auxiliary llm is used to choose the next best action to steer the storys future direction our results show that swag can substantially outperform previous endtoend story generation techniques when evaluated by gpt4 and through human evaluation our swag pipeline using only small opensource models surpasses gpt35turbo,0
we address the problem of endtoend visual storytelling given a photo album our model first selects the most representative summary photos and then composes a natural language story for the album for this task we make use of the visual storytelling dataset and a model composed of three hierarchicallyattentive recurrent neural nets rnns to encode the album photos select representative summary photos and compose the story automatic and human evaluations show our model achieves better performance on selection generation and retrieval than baselines,0
storytelling is an openended task that entails creative thinking and requires a constant flow of ideas natural language generation nlg for storytelling is especially challenging because it requires the generated text to follow an overall theme while remaining creative and diverse to engage the reader in this work we introduce a system and a webbased demo fairytailor for humanintheloop visual story cocreation users can create a cohesive childrens fairytale by weaving generated texts and retrieved images with their input fairytailor adds another modality and modifies the text generation process to produce a coherent and creative sequence of text and images to our knowledge this is the first dynamic tool for multimodal story generation that allows interactive coformation of both texts and images it allows users to give feedback on cocreated stories and share their results,0
visual storytelling is a creative and challenging task aiming to automatically generate a storylike description for a sequence of images the descriptions generated by previous visual storytelling approaches lack coherence because they use wordlevel sequence generation methods and do not adequately consider sentencelevel dependencies to tackle this problem we propose a novel hierarchical visual storytelling framework which separately models sentencelevel and wordlevel semantics we use the transformerbased bert to obtain embeddings for sentences and words we then employ a hierarchical lstm network the bottom lstm receives as input the sentence vector representation from bert to learn the dependencies between the sentences corresponding to images and the top lstm is responsible for generating the corresponding word vector representations taking input from the bottom lstm experimental results demonstrate that our model outperforms most closely related baselines under automatic evaluation metrics bleu and cider and also show the effectiveness of our method with human evaluation,0
visual storytelling is an intriguing and complex task that only recently entered the research arena in this work we survey relevant work to date and conduct a thorough error analysis of three very recent approaches to visual storytelling we categorize and provide examples of common types of errors and identify key shortcomings in current work finally we make recommendations for addressing these limitations in the future,0
several complex systems are characterized by presenting intricate characteristics taking place at several scales of time and space these multiscale characterizations are used in various applications including better understanding diseases characterizing transportation systems and comparison between cities among others in particular texts are also characterized by a hierarchical structure that can be approached by using multiscale concepts and methods the multiscale properties of texts constitute a subject worth further investigation in addition more effective approaches to text characterization and analysis can be obtained by emphasizing words with potentially more informational content the present work aims at developing these possibilities while focusing on mesoscopic representations of networks more specifically we adopt an extension to the mesoscopic approach to represent text narratives in which only the recurrent relationships among tagged parts of speech subject verb and direct object are considered to establish connections among sequential pieces of text eg paragraphs the characterization of the texts was then achieved by considering scaledependent complementary methods accessibility symmetry and recurrence signatures in order to evaluate the potential of these concepts and methods we approached the problem of distinguishing between literary genres fiction and nonfiction a set of 300 books organized into the two genres was considered and were compared by using the aforementioned approaches all the methods were capable of differentiating to some extent between the two genres the accessibility and symmetry reflected the narrative asymmetries while the recurrence signature provided a more direct indication about the nonsequential semantic connections taking place along the narrative,0
speech synthesis for poetry is challenging due to specific intonation patterns inherent to poetic speech in this work we propose an approach to synthesise poems with almost human like naturalness in order to enable literary scholars to systematically examine hypotheses on the interplay between text spoken realisation and the listeners perception of poems to meet these special requirements for literary studies we resynthesise poems by cloning prosodic values from a human reference recitation and afterwards make use of finegrained prosody control to manipulate the synthetic speech in a humanintheloop setting to alter the recitation wrt specific phenomena we find that finetuning our tts model on poetry captures poetic intonation patterns to a large extent which is beneficial for prosody cloning and manipulation and verify the success of our approach both in an objective evaluation as well as in human studies,0
narrative data spans all disciplines and provides a coherent model of the world to the reader or viewer recent advancement in machine learning and large language models llms have enable great strides in analyzing natural language however large language models llms still struggle with complex narrative arcs as well as narratives containing conflicting information recent work indicates llms augmented with external knowledge bases can improve the accuracy and interpretability of the resulting models in this work we analyze the effectiveness of applying knowledge graphs kgs in understanding truecrime podcast data from both classical natural language processing nlp and llm approaches we directly compare kgaugmented llms kgllms with classical methods for kg construction topic modeling and sentiment analysis additionally the kgllm allows us to query the knowledge base in natural language and test its ability to factually answer questions we examine the robustness of the model to adversarial prompting in order to test the models ability to deal with conflicting information finally we apply classical methods to understand more subtle aspects of the text such as the use of hearsay and sentiment in narrative construction and propose future directions our results indicate that kgllms outperform llms on a variety of metrics are more robust to adversarial prompts and are more capable of summarizing the text into topics,0
sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative specifically these agents must infer personas of their listeners to produce statements that cater to their interests they must also learn to maintain consistent speaker personas for themselves throughout the narrative so that their counterparts feel involved in a realistic conversation or story however personas are diverse and complex they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems eg a singer is good at singing and may have attended conservatoire in this work we construct a new largescale persona commonsense knowledge graph peacok containing 100k humanvalidated persona facts our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours and distils facts in this schema from both existing commonsense knowledge graphs and largescale pretrained language models our analysis indicates that peacok contains rich and precise world persona inferences that help downstream systems generate more consistent and engaging narratives,0
what can we learn from the classics of finnish literature by using computational emotion analysis this article tries to answer this question by examining how computational methods of sentiment analysis can be used in the study of literary works in conjunction with a qualitative or more traditional approach to literature and affect we present and develop a simple but robust computational approach of affect analysis that uses a carefully curated emotion lexicon adapted to finnish turnofthecentury literary texts combined with word embeddings to map out the semantic emotional spaces of seminal works of finnish literature we focus our qualitative analysis on selected case studies four works by juhani aho minna canth maria jotuni and f e sillanp but provide emotion arcs for a total of 975 finnish novels we argue that a computational analysis of a texts lexicon can be valuable in evaluating the large distribution of the emotional valence in a text and provide guidelines to help other researchers replicate our findings we show that computational approaches have a place in traditional studies on affect in literature as a support tool for closereadingbased analyses but also allowing for largescale comparison between for example genres or national canons,0
public narratives pns are key tools for leadership development and civic mobilization yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation in this work we propose a novel computational framework that leverages large language models llms to automate the qualitative annotation of public narratives using a codebook we codeveloped with subjectmatter experts we evaluate llm performance against that of expert annotators our work reveals that llms can achieve nearhumanexpert performance achieving an average f1 score of 080 across 8 narratives and 14 codes we then extend our analysis to empirically explore how pn framework elements manifest across a larger dataset of 22 stories lastly we extrapolate our analysis to a set of political speeches establishing a novel lens in which to analyze political rhetoric in civic spaces this study demonstrates the potential of llmassisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling,0
when considering the opening part of 1800 short stories we find that the first dozen paragraphs of the average narrative follow an action principle as defined in arxiv230906600 when the order of the paragraphs is shuffled the average no longer exhibits this property the findings show that there is a preferential direction we take in semantic space when starting a story possibly related to a common western storytelling tradition as implied by aristotle in poetics,0
this study examines the variability of chatgpt machine translation mt outputs across six different configurations in four languageswith a focus on creativity in a literary text we evaluate gpt translations in different text granularity levels temperature settings and prompting strategies with a creativity score formula we found that prompting chatgpt with a minimal instruction yields the best creative translations with translate the following text into creatively at the temperature of 10 outperforming other configurations and deepl in spanish dutch and chinese nonetheless chatgpt consistently underperforms compared to human translation ht,0
existing methods in the visual storytelling field often suffer from the problem of generating general descriptions while the image contains a lot of meaningful contents remaining unnoticed the failure of informative story generation can be concluded to the models incompetence of capturing enough meaningful concepts the categories of these concepts include entities attributes actions and events which are in some cases crucial to grounded storytelling to solve this problem we propose a method to mine the crossmodal rules to help the model infer these informative concepts given certain visual input we first build the multimodal transactions by concatenating the cnn activations and the word indices then we use the association rule mining algorithm to mine the crossmodal rules which will be used for the concept inference with the help of the crossmodal rules the generated stories are more grounded and informative besides our proposed method holds the advantages of interpretation expandability and transferability indicating potential for wider application finally we leverage these concepts in our encoderdecoder framework with the attention mechanism we conduct several experiments on the visual storytellingvist dataset the results of which demonstrate the effectiveness of our approach in terms of both automatic metrics and human evaluation additional experiments are also conducted showing that our mined crossmodal rules as additional knowledge helps the model gain better performance when trained on a small dataset,0
we present the task of modeling information propagation in literature in which we seek to identify pieces of information passing from character a to character b to character c only given a description of their activity in text we describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied using this pipeline we analyze the dynamics of information propagation in over 5000 works of fiction finding that information flows through characters that fill structural holes connecting different communities and that characters who are women are depicted as filling this role much more frequently than characters who are men,0
digital humanities and computational literary studies apply text mining methods to investigate literature such automated approaches enable quantitative studies on large corpora which would not be feasible by manual inspection alone however due to copyright restrictions the availability of relevant digitized literary works is limited derived text formats dtfs have been proposed as a solution here textual materials are transformed in such a way that copyrightcritical features are removed but that the use of certain analytical methods remains possible contextualized word embeddings produced by transformerencoders like bert are promising candidates for dtfs because they allow for stateoftheart performance on various analytical tasks and at first sight do not disclose the original text however in this paper we demonstrate that under certain conditions the reconstruction of the original copyrighted text becomes feasible and its publication in the form of contextualized token representations is not safe our attempts to invert bert suggest that publishing the encoder as a black box together with the contextualized embeddings is critical since it allows to generate data to train a decoder with a reconstruction accuracy sufficient to violate copyright laws,0
climate change has increased demands for transparent and comparable corporate climate disclosures yet imitation and symbolic reporting often undermine their value this paper develops a multidimensional framework to assess disclosure maturity among 828 uslisted firms using large language models llms finetuned for climate communication four classifierssentiment commitment specificity and target ambitionextract narrative indicators from sustainability and annual reports which are linked to firm attributes such as emissions market capitalization and sector analyses reveal three insights 1 riskfocused narratives often align with explicit commitments but quantitative targets eg netzero pledges remain decoupled from tone 2 larger and higheremitting firms disclose more commitments and actions than peers though inconsistently with quantitative targets and 3 widespread similarity in disclosure styles suggests mimetic behavior reducing differentiation and decision usefulness these results highlight the value of llms for esg narrative analysis and the need for stronger regulation to connect commitments with verifiable transition strategies,0
recent work has demonstrated that language models can be trained to identify the author of much shorter literary passages than has been thought feasible for traditional stylometry we replicate these results for authorship and extend them to a new dataset measuring novel genre we find that llms are able to distinguish authorship and genre but they do so in different ways some models seem to rely more on memorization while others benefit more from training to learn authorgenre characteristics we then use three methods to probe one highperforming llm for features that define style these include direct syntactic ablations to input text as well as two methods that look at model internals we find that authorial style is easier to define than genrelevel style and is more impacted by minor syntactic decisions and contextual word usage however some traits like pronoun usage and word order prove significant for defining both kinds of literary style,0
the emergence of decentralized social media platforms presents new opportunities and challenges for realtime analysis of public discourse this study introduces cognitivesky an opensource and scalable framework designed for sentiment emotion and narrative analysis on bluesky a federated twitter or xcom alternative by ingesting data through blueskys application programming interface api cognitivesky applies transformerbased models to annotate largescale usergenerated content and produces structured and analyzable outputs these summaries drive a dynamic dashboard that visualizes evolving patterns in emotion activity and conversation topics built entirely on freetier infrastructure cognitivesky achieves both low operational cost and high accessibility while demonstrated here for monitoring mental health discourse its modular design enables applications across domains such as disinformation detection crisis response and civic sentiment analysis by bridging large language models with decentralized networks cognitivesky offers a transparent extensible tool for computational social science in an era of shifting digital ecosystems,0
human communication is often executed in the form of a narrative an account of connected events composed of characters actions and settings a coherent narrative structure is therefore a requisite for a wellformulated narrative be it fictional or nonfictional for informative and effective communication opening up the possibility of a deeper understanding of a narrative by studying its structural properties in this paper we present a networkbased framework for modeling and analyzing the structure of a narrative which is further expanded by incorporating methods from computational linguistics to utilize the narrative text modeling a narrative as a dynamically unfolding system we characterize its progression via the growth patterns of the character network and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords this is a network framework advanced beyond the simple occurrencebased one most often used until now allowing one to utilize the unique characteristics of a given narrative to a high degree given the ubiquity and importance of narratives such advanced networkbased representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions expression of human sentiments and communication,0
roleplaying games rpg are games in which players interact with one another to create narratives the role of players in the rpg is largely based on the interaction between players and their characters this emerging form of shared narrative primarily oral is receiving increasing attention in particular many authors investigated the use of an llm as an actor in the game in this paper we aim to discover to what extent the language of large language models llms exhibit oral or written features when asked to generate an rpg session without human interference we will conduct a linguistic analysis of the lexical and syntactic features of the generated texts and compare the results with analyses of conversations transcripts of human rpg sessions and books we found that llms exhibit a pattern that is distinct from all other text categories including oral conversations human rpg sessions and books our analysis has shown how training influences the way llms express themselves and provides important indications of the narrative capabilities of these tools,0
this essay proposes and explores the concept of collage for the design of ai writing tools transferred from avantgarde literature with four facets 1 fragmenting text in writing interfaces 2 juxtaposing voices content vs command 3 integrating material from multiple sources eg text suggestions and 4 shifting from manual writing to editorial and compositional decisionmaking such as selecting and arranging snippets the essay then employs collage as an analytical lens to analyse the user interface design of recent ai writing tools and as a constructive lens to inspire new design directions finally a critical perspective relates the concerns that writers historically expressed through literary collage to ai writing tools in a broad view this essay explores how literary concepts can help advance design theory around ai writing tools it encourages creators of future writing tools to engage not only with new technological possibilities but also with past writing innovations,0
in this paper we introduce dixit an interactive visual storytelling system that the user interacts with iteratively to compose a short story for a photo sequence the user initiates the process by uploading a sequence of photos dixit first extracts text terms from each photo which describe the objects eg boy bike or actions eg sleep in the photo and then allows the user to add new terms or remove existing terms dixit then generates a short story based on these terms behind the scenes dixit uses an lstmbased model trained on image caption data and framenet to distill terms from each image and utilizes a transformer decoder to compose a contextcoherent story users change images or terms iteratively with dixit to create the most ideal story dixit also allows users to manually edit and rate stories the proposed procedure opens up possibilities for interpretable and controllable visual storytelling allowing users to understand the story formation rationale and to intervene in the generation process,0
we explore the expression of personality and adaptivity through the gestures of virtual agents in a storytelling task we conduct two experiments using four different dialogic stories we manipulate agent personality on the extraversion scale whether the agents adapt to one another in their gestural performance and agent gender our results show that subjects are able to perceive the intended variation in extraversion between different virtual agents independently of the story they are telling and the gender of the agent a second study shows that subjects also prefer adaptive to nonadaptive virtual agents,0
using geometric data analysis our objective is the analysis of narrative with narrative of emotion being the focus in this work the following two principles for analysis of emotion inform our work firstly emotion is revealed not as a quality in its own right but rather through interaction we study the 2way relationship of ilsa and rick in the movie casablanca and the 3way relationship of emma charles and rodolphe in the novel em madame bovary secondly emotion that is expression of states of mind of subjects is formed and evolves within the narrative that expresses external events and personal social physical context in addition to the analysis methodology with key aspects that are innovative the input data used is crucial we use firstly dialogue and secondly broad and general description that incorporates dialogue in a followon study we apply our unsupervised narrative mapping to data streams with very low emotional expression we map the narrative of twitter streams thus we demonstrate map analysis of general narratives,0
what are the best methods of capturing thematic similarity between literary texts knowing the answer to this question would be useful for automatic clustering of book genres or any other thematic grouping this paper compares a variety of algorithms for unsupervised learning of thematic similarities between texts which we call computational thematics these algorithms belong to three steps of analysis text preprocessing extraction of text features and measuring distances between the lists of features each of these steps includes a variety of options we test all the possible combinations of these options every combination of algorithms is given a task to cluster a corpus of books belonging to four pretagged genres of fiction this clustering is then validated against the ground truth genre labels such comparison of algorithms allows us to learn the best and the worst combinations for computational thematic analysis to illustrate the sharp difference between the best and the worst methods we then cluster 5000 random novels from the hathitrust corpus of fiction,0
question answering qa on narrative text poses a unique challenge to current systems requiring a deep understanding of long complex documents however the reliability of narrativeqa the most widely used benchmark in this domain is hindered by noisy documents and flawed qa pairs in this work we introduce literaryqa a highquality subset of narrativeqa focused on literary works using a human and llmvalidated pipeline we identify and correct lowquality qa samples while removing extraneous text from source documents we then carry out a metaevaluation of automatic metrics to clarify how systems should be evaluated on literaryqa this analysis reveals that all ngrambased metrics have a low systemlevel correlation to human judgment while llmasajudge evaluations even with small openweight models can strongly agree with the ranking identified by humans finally we benchmark a set of longcontext llms on literaryqa we release our code and data at,0
research on storytelling over the last 100 years has distinguished at least two levels of narrative representation 1 story or fabula and 2 discourse or sujhet we use this distinction to create fabula tales a computational framework for a virtual storyteller that can tell the same story in different ways through the implementation of general narratological variations such as varying direct vs indirect speech character voice style point of view and focalization a strength of our computational framework is that it is based on very general methods for reusing existing story content either from fables or from personal narratives collected from blogs we first explain how a simple annotation tool allows naive annotators to easily create a deep representation of fabula called a story intention graph and show how we use this representation to generate story tellings automatically then we present results of two studies testing our narratological parameters and showing that different tellings affect the readers perception of the story and characters,0
previous storytelling approaches mostly focused on optimizing traditional metrics such as bleu rouge and cider in this paper we reexamine this problem from a different angle by looking deep into what defines a realisticallynatural and topicallycoherent story to this end we propose three assessment criteria relevance coherence and expressiveness which we observe through empirical analysis could constitute a highquality story to the human eye following this quality guideline we propose a reinforcement learning framework recorl with reward functions designed to capture the essence of these quality criteria experiments on the visual storytelling dataset vist with both automatic and human evaluations demonstrate that our recorl model achieves better performance than stateoftheart baselines on both traditional metrics and the proposed new criteria,0
modernist novels are thought to break with traditional plot structure in this paper we test this theory by applying sentiment analysis to one of the most famous modernist novels to the lighthouse by virginia woolf we first assess sentiment analysis in light of the critique that it cannot adequately account for literary language we use a unique statistical comparison to demonstrate that even simple lexical approaches to sentiment analysis are surprisingly effective we then use the syuzhetr package to explore similarities and differences across modeling methods this comparative approach when paired with literary close reading can offer interpretive clues to our knowledge we are the first to undertake a hybrid model that fully leverages the strengths of both computational analysis and close reading this hybrid model raises new questions for the literary critic such as how to interpret relative versus absolute emotional valence and how to take into account subjective identification our finding is that while to the lighthouse does not replicate a plot centered around a traditional hero it does reveal an underlying emotional structure distributed between characters what we term a distributed heroine model this finding is innovative in the field of modernist and narrative studies and demonstrates that a hybrid method can yield significant discoveries,0
it is known for some time that autocorrelations of words in humanwritten texts decay according to a power law recent works have also shown that the autocorrelations decay in texts generated by llms is qualitatively different from the literary texts solid state physics tie the autocorrelations decay laws to the states of matter in this work we empirically demonstrate that depending on the temperature parameter llms can generate text that can be classified as solid critical state or gas,0
datadriven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text these stories integrate visual aids such as highlighted bars and lines in charts along with textual annotations explaining insights however creating such stories requires a deep understanding of the data and meticulous narrative planning often necessitating human intervention which can be timeconsuming and mentally taxing while large language models llms excel in various nlp tasks their ability to generate coherent and comprehensive data stories remains underexplored in this work we introduce a novel task for data story generation and a benchmark containing 1449 stories from diverse sources to address the challenges of crafting coherent data stories we propose a multiagent framework employing two llm agents designed to replicate the human storytelling process one for understanding and describing the data reflection generating the outline and narration and another for verification at each intermediary step while our agentic framework generally outperforms nonagentic counterparts in both modelbased and human evaluations the results also reveal unique challenges in data story generation,0
in assessing argument strength the notions of what makes a good argument are manifold with the broader trend towards treating subjectivity as an asset and not a problem in nlp new dimensions of argument quality are studied although studies on individual subjective features like personal stories exist there is a lack of largescale analyses of the relation between these features and argument strength to address this gap we conduct regression analysis to quantify the impact of subjective factors emotions storytelling and hedging on two standard datasets annotated for objective argument quality and subjective persuasion as such our contribution is twofold at the level of contributed resources as there are no datasets annotated with all studied dimensions this work compares and evaluates automated annotation methods for each subjective feature at the level of novel insights our regression analysis uncovers different patterns of impact of subjective features on the two facets of argument strength encoded in the datasets our results show that storytelling and hedging have contrasting effects on objective and subjective argument quality while the influence of emotions depends on their rhetoric utilization rather than the domain,0
tamil a dravidian language of south asia is a highly diglossic language with two very different registers in everyday use literary tamil preferred in writing and formal communication and spoken tamil confined to speech and informal media spoken tamil is undersupported in modern nlp systems in this paper we release irumozhi a humanannotated dataset of parallel text in literary and spoken tamil we train classifiers on the task of identifying which variety a text belongs to we use these models to gauge the availability of pretraining data in spoken tamil to audit the composition of existing labelled datasets for tamil and to encourage future work on the variety,0
given the profound impact of narratives across various societal levels from personal identities to international politics it is crucial to understand their distribution and development over time this is particularly important in online spaces on the web narratives can spread rapidly and intensify societal divides and conflicts while many qualitative approaches exist quantifying narratives remains a significant challenge computational narrative analysis lacks frameworks that are both comprehensive and generalizable to address this gap we introduce a numerical narrative representation grounded in structuralist linguistic theory chiefly greimas actantial model represents a narrative through a constellation of six functional character roles these socalled actants are genreagnostic making the model highly generalizable we extract the actants using an opensource llm and integrate them into a narrativestructured text embedding that captures both the semantics and narrative structure of a text we demonstrate the analytical insights of the method on the example of 5000 fulltext news articles from al jazeera and the washington post on the israelpalestine conflict our method successfully distinguishes articles that cover the same topics but differ in narrative structure,0
narrative frames are a powerful way of conceptualizing and communicating complex controversial ideas however automated frame analysis to date has mostly overlooked this framing device in this paper we connect elements of narrativity with fundamental aspects of framing and present a framework which formalizes and operationalizes such aspects we annotate and release a data set of news articles in the climate change domain analyze the dominance of narrative frame components across political leanings and test llms in their ability to predict narrative frames and their components finally we apply our framework in an unsupervised way to elicit components of narrative framing in a second domain the covid19 crisis where our predictions are congruent with prior theoretical work showing the generalizability of our approach,0
generative ai has established the opportunity to readily transform content from one medium to another this capability is especially powerful for storytelling where visual illustrations can illuminate a story originally expressed in text in this paper we focus on the task of narrative scene illustration which involves automatically generating an image depicting a scene in a story motivated by recent progress on texttoimage models we consider a pipeline that uses llms as an interface for prompting texttoimage models to generate scene illustrations given raw story text we apply variations of this pipeline to a prominent story corpus in order to synthesize illustrations for scenes in these stories we conduct a human annotation task to obtain pairwise quality judgments for these illustrations the outcome of this process is the sceneillustrations dataset which we release as a new resource for future work on crossmodal narrative transformation through our analysis of this dataset and experiments modeling illustration quality we demonstrate that llms can effectively verbalize scene knowledge implicitly evoked by story text moreover this capability is impactful for generating and evaluating illustrations,0
this paper assesses the potential for large language models llms to serve as assistive tools in the creative writing process by means of a single indepth case study in the course of the study we develop interactive and multivoice prompting strategies that interleave background descriptions scene setting plot elements instructions that guide composition samples of text in the target style and critical discussion of the given samples we qualitatively evaluate the results from a literary critical perspective as well as from the standpoint of computational creativity a subfield of artificial intelligence our findings lend support to the view that the sophistication of the results that can be achieved with an llm mirrors the sophistication of the prompting,0
large language models llms play a pivotal role in generating vast arrays of narratives facilitating a systematic exploration of their effectiveness for communicating life events in narrative form in this study we employ a zeroshot structured narrative prompt to generate 24000 narratives using openais gpt4 from this dataset we manually classify 2880 narratives and evaluate their validity in conveying birth death hiring and firing events remarkably 8743 of the narratives sufficiently convey the intention of the structured prompt to automate the identification of valid and invalid narratives we train and validate nine machine learning models on the classified datasets leveraging these models we extend our analysis to predict the classifications of the remaining 21120 narratives all the ml models excelled at classifying valid narratives as valid but experienced challenges at simultaneously classifying invalid narratives as invalid our findings not only advance the study of llm capabilities limitations and validity but also offer practical insights for narrative generation and natural language processing applications,0
stories are central to human culture serving to share ideas preserve traditions and foster connections automatic story generation a key advancement in artificial intelligence ai offers new possibilities for creating personalized content exploring creative ideas and enhancing interactive experiences however existing methods struggle to maintain narrative coherence and logical consistency this disconnect compromises the overall storytelling experience underscoring the need for substantial improvements inspired by human cognitive processes we introduce storyteller a novel approach that systemically improves the coherence and consistency of automatically generated stories storyteller introduces a plot node structure based on linguistically grounded subject verb object svo triplets which capture essential story events and ensure a consistent logical flow unlike previous methods storyteller integrates two dynamic modules the storyline and narrative entity knowledge graph nekgthat continuously interact with the story generation process this integration produces structurally sound cohesive and immersive narratives extensive experiments demonstrate that storyteller significantly outperforms existing approaches achieving an 8433 average win rate through human preference evaluation at the same time it is also far ahead in other aspects including creativity coherence engagement and relevance,0
emotions and their evolution play a central role in creating a captivating story in this paper we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling we design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist our models include emotion supervision emosup and two emotionreinforced emorl models the emorl models use special rewards designed to regularize the story generation process through reinforcement learning our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods without sacrificing story quality,0
bengali literature has a rich history of hundreds of years with luminary figures such as rabindranath tagore and kazi nazrul islam however analytical works involving the most recent advancements in nlp have barely scratched the surface utilizing the enormous volume of the collected works from the writers of the language in order to bring attention to the analytical study involving the works of bengali writers and spearhead the text generation endeavours in the style of existing literature we are introducing rabindranet a character level rnn model with stackedlstm layers trained on the works of rabindranath tagore to produce literary works in his style for multiple genres we created an extensive dataset as well by compiling the digitized works of rabindranath tagore from authentic online sources and published as open source dataset on data science platform kaggle,0
capturing readers engagement in fiction is a challenging but important aspect of narrative understanding in this study we collected 23 readers reactions to 2 short stories through eye tracking sentencelevel annotations and an overall engagement scale survey we analyzed the significance of various qualities of the text in predicting how engaging a reader is likely to find it as enjoyment of fiction is highly contextual we also investigated individual differences in our data furthering our understanding of what captivates readers in fiction will help better inform models used in creative narrative generation and collaborative writing tools,0
this paper presents the character decision points detection chadpod task a task of identification of points within narratives where characters make decisions that may significantly influence the storys direction we propose a novel dataset based on cyoalike games graphs to be used as a benchmark for such a task we provide a comparative analysis of different models performance on this task including a couple of llms and several mlms as baselines achieving up to 89 accuracy this underscores the complexity of narrative analysis showing the challenges associated with understanding characterdriven story dynamics additionally we show how such a model can be applied to the existing text to produce linear segments divided by potential branching points demonstrating the practical application of our findings in narrative analysis,0
as narrative extraction systems grow in complexity establishing user trust through interpretable and explainable outputs becomes increasingly critical this paper presents an evaluation of an explainable artificial intelligence xai system for narrative map extraction that provides meaningful explanations across multiple levels of abstraction our system integrates explanations based on topical clusters for lowlevel document relationships connection explanations for event relationships and highlevel structure explanations for overall narrative patterns in particular we evaluate the xai system through a user study involving 10 participants that examined narratives from the 2021 cuban protests the analysis of results demonstrates that participants using the explanations made the users trust in the systems decisions with connection explanations and important event detection proving particularly effective at building user confidence survey responses indicate that the multilevel explanation approach helped users develop appropriate trust in the systems narrative extraction capabilities this work advances the stateoftheart in explainable narrative extraction while providing practical insights for developing reliable narrative extraction systems that support effective humanai collaboration,0
visual storytelling aims to generate a narrative paragraph from a sequence of images automatically existing approaches construct text description independently for each image and roughly concatenate them as a story which leads to the problem of generating semantically incoherent content in this paper we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream a story is then constructed with the guidance of the topic description in order to combine the two generation tasks we propose a multiagent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism we validate our approach on vist dataset where quantitative results ablations and human evaluation demonstrate our methods good ability in generating stories with higher quality compared to stateoftheart methods,0
many implicit inferences exist in text depending on how it is structured that can critically impact the texts interpretation and meaning one such structural aspect present in text with chronology is the order of its presentation for narratives or stories this is known as the narrative order reordering a narrative can impact the temporal causal eventbased and other inferences readers draw from it which in turn can have strong effects both on its interpretation and interestingness in this paper we propose and investigate the task of narrative reordering nareor which involves rewriting a given story in a different narrative order while preserving its plot we present a dataset nareorc with human rewritings of stories within rocstories in nonlinear orders and conduct a detailed analysis of it further we propose novel taskspecific training methods with suitable evaluation metrics we perform experiments on nareorc using stateoftheart models such as bart and t5 and conduct extensive automatic and human evaluations we demonstrate that although our models can perform decently nareor is a challenging task with potential for further exploration we also investigate two applications of nareor generation of more interesting variations of stories and serving as adversarial sets for temporaleventrelated tasks besides discussing other prospective ones such as for pedagogical setups related to language skills like essay writing and applications to medicine involving clinical narratives,0
stories are diverse and highly personalized resulting in a large possible output space for story generation existing endtoend approaches produce monotonous stories because they are limited to the vocabulary and knowledge in a single training dataset this paper introduces kgstory a threestage framework that allows the story generation model to take advantage of external knowledge graphs to produce interesting stories kgstory distills a set of representative words from the input prompts enriches the word set by using external knowledge graphs and finally generates stories based on the enriched word set this distillenrichgenerate framework allows the use of external resources not only for the enrichment phase but also for the distillation and generation phases in this paper we show the superiority of kgstory for visual storytelling where the input prompt is a sequence of five photos and the output is a short story per the human ranking evaluation stories generated by kgstory are on average ranked better than that of the stateoftheart systems our code and output stories are available at,0
as a technically challenging topic visual storytelling aims at generating an imaginary and coherent story with narrative multisentences from a group of relevant images existing methods often generate direct and rigid descriptions of apparent imagebased contents because they are not capable of exploring implicit information beyond images hence these schemes could not capture consistent dependencies from holistic representation impairing the generation of reasonable and fluent story to address these problems a novel knowledgeenriched attention network with groupwise semantic model is proposed three main novel components are designed and supported by substantial experiments to reveal practical advantages first a knowledgeenriched attention network is designed to extract implicit concepts from external knowledge system and these concepts are followed by a cascade crossmodal attention mechanism to characterize imaginative and concrete representations second a groupwise semantic module with secondorder pooling is developed to explore the globally consistent guidance third a unified onestage story generation model with encoderdecoder structure is proposed to simultaneously train and infer the knowledgeenriched attention network groupwise semantic module and multimodal story generation decoder in an endtoend fashion substantial experiments on the popular visual storytelling dataset with both objective and subjective evaluation metrics demonstrate the superior performance of the proposed scheme as compared with other stateoftheart methods,0
the proliferation of biased news narratives across various media platforms has become a prominent challenge influencing public opinion on critical topics like politics health and climate change this paper introduces the navigating news narratives a media bias analysis dataset a comprehensive dataset to address the urgent need for tools to detect and analyze media bias this dataset encompasses a broad spectrum of biases making it a unique and valuable asset in the field of media studies and artificial intelligence the dataset is available at,0
writing a coherent and engaging story is not easy creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline and work and rework iteratively toward perfection automated visual storytelling vist models however make poor use of external knowledge and iterative generation when attempting to create stories this paper introduces prvist a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline prvist then takes this path and learns to generate the final story via an iterative training process this framework produces stories that are superior in terms of diversity coherence and humanness per both automatic and human evaluations an ablation study shows that both plotting and reworking contribute to the models superiority,0
a proper evaluation of stories generated for a sequence of images the task commonly referred to as visual storytelling must consider multiple aspects such as coherence grammatical correctness and visual grounding in this work we focus on evaluating the degree of grounding that is the extent to which a story is about the entities shown in the images we analyze current metrics both designed for this purpose and for general visiontext alignment given their observed shortcomings we propose a novel evaluation tool groovist that accounts for crossmodal dependencies temporal misalignments the fact that the order in which entities appear in the story and the image sequence may not match and human intuitions on visual grounding an additional advantage of groovist is its modular design where the contribution of each component can be assessed and interpreted individually,0
the covid19 pandemic is a global crisis that has been testing every society and exposing the critical role of local politics in crisis response in the united states there has been a strong partisan divide between the democratic and republican partys narratives about the pandemic which resulted in polarization of individual behaviors and divergent policy adoption across regions as shown in this case as well as in most major social issues strongly polarized narrative frameworks facilitate such narratives to understand polarization and other social chasms it is critical to dissect these diverging narratives here taking the democratic and republican political social media posts about the pandemic as a case study we demonstrate that a combination of computational methods can provide useful insights into the different contexts framing and characters and relationships that construct their narrative frameworks which individual posts source from leveraging a dataset of tweets from elite politicians in the us we found that the democrats narrative tends to be more concerned with the pandemic as well as financial and social support while the republicans discuss more about other political entities such as china we then perform an automatic framing analysis to characterize the ways in which they frame their narratives where we found that the democrats emphasize the governments role in responding to the pandemic and the republicans emphasize the roles of individuals and support for small businesses finally we present a semantic role analysis that uncovers the important characters and relationships in their narratives as well as how they facilitate a membership categorization process our findings concretely expose the gaps in the elusive consensus between the two parties our methodologies may be applied to computationally study narratives in various domains,0
recent advances in the performance of large language models llms have sparked debate over whether given sufficient training highlevel human abilities emerge in such generic forms of artificial intelligence ai despite the exceptional performance of llms on a wide range of tasks involving natural language processing and reasoning there has been sharp disagreement as to whether their abilities extend to more creative human abilities a core example is the ability to interpret novel metaphors given the enormous and non curated text corpora used to train llms a serious obstacle to designing tests is the requirement of finding novel yet high quality metaphors that are unlikely to have been included in the training data here we assessed the ability of gpt4 a state of the art large language model to provide naturallanguage interpretations of novel literary metaphors drawn from serbian poetry and translated into english despite exhibiting no signs of having been exposed to these metaphors previously the ai system consistently produced detailed and incisive interpretations human judges blind to the fact that an ai model was involved rated metaphor interpretations generated by gpt4 as superior to those provided by a group of college students in interpreting reversed metaphors gpt4 as well as humans exhibited signs of sensitivity to the gricean cooperative principle in addition for several novel english poems gpt4 produced interpretations that were rated as excellent or good by a human literary critic these results indicate that llms such as gpt4 have acquired an emergent ability to interpret complex metaphors including those embedded in novel poems,0
visual storytelling aims to generate compelling narratives from image sequences existing models often focus on enhancing the representation of the image sequence eg with external knowledge sources or advanced graph structures despite recent progress the stories are often repetitive illogical and lacking in detail to mitigate these issues we present a novel framework which integrates visual representations with pretrained language models and planning our model translates the image sequence into a visual prefix a sequence of continuous embeddings which language models can interpret it also leverages a sequence of questionanswer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative automatic and human evaluation on the vist benchmark huang et al 2016 demonstrates that blueprintbased models generate stories that are more coherent interesting and natural compared to competitive baselines and stateoftheart systems,0
understanding covert narratives and implicit messaging is essential for analyzing bias and sentiment traditional nlp methods struggle with detecting subtle phrasing and hidden agendas this study tackles two key challenges 1 multilabel classification of narratives and subnarratives in news articles and 2 generating concise evidencebased explanations for dominant narratives we finetune a bert model with a recalloriented approach for comprehensive narrative detection refining predictions using a gpt4o pipeline for consistency for narrative explanation we propose a react reasoning acting framework with semantic retrievalbased fewshot prompting ensuring grounded and relevant justifications to enhance factual accuracy and reduce hallucinations we incorporate a structured taxonomy table as an auxiliary knowledge base our results show that integrating auxiliary knowledge in prompts improves classification accuracy and justification reliability with applications in media analysis education and intelligence gathering,0
visual storytelling consists in generating a natural language story given a temporally ordered sequence of images this task is not only challenging for models but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story good in this paper we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work visual grounding coherence and repetitiveness we then use this method to evaluate the stories generated by several models showing that the foundation model llava obtains the best result but only slightly so compared to tapm a 50times smaller visual storytelling model upgrading the visual and language components of tapm results in a model that yields competitive performance with a relatively low number of parameters finally we carry out a human evaluation study whose results suggest that a good story may require more than a humanlike level of visual grounding coherence and repetition,0
we address the problem of visual storytelling ie generating a story for a given sequence of images while each sentence of the story should describe a corresponding image a coherent story also needs to be consistent and relate to both future and past images to achieve this we develop ordered image attention oia oia models interactions between the sentencecorresponding image and important regions in other images of the sequence to highlight the important objects a messagepassinglike algorithm collects representations of those objects in an orderaware manner to generate the storys sentences we then highlight important image attention vectors with an imagesentence attention isa further to alleviate common linguistic mistakes like repetitiveness we introduce an adaptive prior the obtained results improve the meteor score on the vist dataset by 1 in addition an extensive human study verifies coherency improvements and shows that oia and isa generated stories are more focused shareable and imagegrounded,0
automatic storytelling is challenging since it requires generating long coherent natural language to describes a sensible sequence of events despite considerable efforts on automatic story generation in the past prior work either is restricted in plot planning or can only generate stories in a narrow domain in this paper we explore opendomain story generation that writes stories given a title topic as input we propose a planandwrite hierarchical generation framework that first plans a storyline and then generates a story based on the storyline we compare two planning strategies the dynamic schema interweaves story planning and its surface realization in text while the static schema plans out the entire storyline before generating stories experiments show that with explicit storyline planning the generated stories are more diverse coherent and on topic than those generated without creating a full plan according to both automatic and human evaluations,0
sota transformer and dnn short text sentiment classifiers report over 97 accuracy on narrow domains like imdb movie reviews realworld performance is significantly lower because traditional models overfit benchmarks and generalize poorly to different or more open domain texts this paper introduces sentimentarcs a new selfsupervised time series sentiment analysis methodology that addresses the two main limitations of traditional supervised sentiment analysis limited labeled training datasets and poor generalization a large ensemble of diverse models provides a synthetic ground truth for selfsupervised learning novel metrics jointly optimize an exhaustive search across every possible corpusmodel combination the joint optimization over both the corpus and model solves the generalization problem simple visualizations exploit the temporal structure in narratives so domain experts can quickly spot trends identify key features and note anomalies over hundreds of arcs and millions of data points to our knowledge this is the first selfsupervised method for time series sentiment analysis and the largest survey directly comparing realworld model performance on longform narratives,0
understanding narrative text requires capturing characters motivations goals and mental states this paper proposes an entitybased narrative graph eng to model the internalstates of characters in a story we explicitly model entities their interactions and the context in which they appear and learn rich representations for them we experiment with different taskadaptive pretraining objectives indomain training and symbolic inference to capture dependencies between different decisions in the output space we evaluate our model on two narrative understanding tasks predicting character mental states and desire fulfillment and conduct a qualitative analysis,0
we present a neural model for generating short stories from image sequences which extends the image description model by vinyals et al vinyals et al 2015 this extension relies on an encoder lstm to compute a context vector of each story from the image sequence this context vector is used as the first state of multiple independent decoder lstms each of which generates the portion of the story corresponding to each image in the sequence by taking the image embedding as the first input our model showed competitive results with the meteor metric and human ratings in the internal track of the visual storytelling challenge 2018,0
making legal knowledge accessible to nonexperts is crucial for enhancing general legal literacy and encouraging civic participation in democracy however legal documents are often challenging to understand for people without legal backgrounds in this paper we present a novel application of large language models llms in legal education to help nonexperts learn intricate legal concepts through storytelling an effective pedagogical tool in conveying complex and abstract concepts we also introduce a new dataset legalstories which consists of 294 complex legal doctrines each accompanied by a story and a set of multiplechoice questions generated by llms to construct the dataset we experiment with various llms to generate legal stories explaining these concepts furthermore we use an expertintheloop approach to iteratively design multiplechoice questions then we evaluate the effectiveness of storytelling with llms through randomized controlled trials rcts with legal novices on 10 samples from the dataset we find that llmgenerated stories enhance comprehension of legal concepts and interest in law among nonnative speakers compared to only definitions moreover stories consistently help participants relate legal concepts to their lives finally we find that learning with stories shows a higher retention rate for nonnative speakers in the followup assessment our work has strong implications for using llms in promoting teaching and learning in the legal field and beyond,0
the complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language in order to retain and convey such a high dimensional information the statistical properties of our linguistic output has to be highly correlated in time an example are the robust observations still largely not understood of correlations on arbitrary long scales in literary texts in this paper we explain how longrange correlations flow from highly structured linguistic levels down to the building blocks of a text words letters etc by combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text the mechanisms we identify are fairly general and can be equally applied to other hierarchical settings,0
this study uses the palestinian oral history archive poha to investigate how palestinian refugee groups in lebanon sustain a cohesive collective memory of the nakba through shared narratives grounded in halbwachs theory of group memory we employ statistical analysis of pairwise similarity of narratives focusing on the influence of shared gender and location we use textual representation and semantic embeddings of narratives to represent the interviews themselves our analysis demonstrates that shared origin is a powerful determinant of narrative similarity across thematic keywords landmarks and significant figures as well as in semantic embeddings of the narratives meanwhile shared residence fosters cohesion with its impact significantly amplified when paired with shared origin additionally womens narratives exhibit heightened thematic cohesion particularly in recounting experiences of the british occupation underscoring the gendered dimensions of memory formation this research deepens the understanding of collective memory in diasporic settings emphasizing the critical role of oral histories in safeguarding palestinian identity and resisting erasure,0
despite its benefits for childrens skill development and parentchild bonding many parents do not often engage in interactive storytelling by having storyrelated dialogues with their child due to limited availability or challenges in coming up with appropriate questions while recent advances made ai generation of questions from stories possible the fullyautomated approach excludes parent involvement disregards educational goals and underoptimizes for child engagement informed by needfinding interviews and participatory design pd results we developed storybuddy an aienabled system for parents to create interactive storytelling experiences storybuddys design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parentchild bonding and the goal of minimizing parent intervention when busy the pd revealed varied assessment and educational goals of parents which storybuddy addressed by supporting configuring question types and tracking child progress a user study validated storybuddys usability and suggested design insights for future parentai collaboration systems,0
writers such as journalists often use automatic tools to find relevant content to include in their narratives in this paper we focus on supporting writers in the news domain to develop eventcentric narratives given an incomplete narrative that specifies a main event and a context we aim to retrieve news articles that discuss relevant events that would enable the continuation of the narrative we formally define this task and propose a retrieval dataset construction procedure that relies on existing news articles to simulate incomplete narratives and relevant articles experiments on two datasets derived from this procedure show that stateoftheart lexical and semantic rankers are not sufficient for this task we show that combining those with a ranker that ranks articles by reverse chronological order outperforms those rankers alone we also perform an indepth quantitative and qualitative analysis of the results that sheds light on the characteristics of this task,0
automated story plot generation is the task of generating a coherent sequence of plot events causal relations between plot events are believed to increase the perception of story and plot coherence in this work we introduce the concept of soft causal relations as causal relations inferred from commonsense reasoning we demonstrate c2po an approach to narrative generation that operationalizes this concept through causal commonsense plot ordering using humanparticipant protocols we evaluate our system against baseline systems with different commonsense reasoning reasoning and inductive biases to determine the role of soft causal relations in perceived story quality through these studies we also probe the interplay of how changes in commonsense norms across storytelling genres affect perceptions of story quality,0
authorship identification is a process in which the author of a text is identified most known literary texts can easily be attributed to a certain author because they are for example signed yet sometimes we find unfinished pieces of work or a whole bunch of manuscripts with a wide variety of possible authors in order to assess the importance of such a manuscript it is vital to know who wrote it in this work we aim to develop a machine learning framework to effectively determine authorship we formulate the task as a singlelabel multiclass text categorization problem and propose a supervised machine learning framework incorporating stylometric features this task is highly interdisciplinary in that it takes advantage of machine learning information retrieval and natural language processing we present an approach and a model which learns the differences in writing style between 50 different authors and is able to predict the author of a new text with high accuracy the accuracy is seen to increase significantly after introducing certain linguistic stylometric features along with text features,0
visual storytelling systems generate multisentence stories from image sequences in this task capturing contextual information and bridging visual variation bring additional challenges we propose a simple yet effective framework that leverages the generalization capabilities of pretrained foundation models only training a lightweight visionlanguage mapping network to connect modalities while incorporating context to enhance coherence we introduce a multimodal contrastive objective that also improves visual relevance and story informativeness extensive experimental results across both automatic metrics and human evaluations demonstrate that the stories generated by our framework are diverse coherent informative and interesting,0
the task of multiimage cued story generation such as visual storytelling dataset vist challenge is to compose multiple coherent sentences from a given sequence of images the main difficulty is how to generate imagespecific sentences within the context of overall images here we propose a deep learning network model glac net that generates visual stories by combining globallocal glocal attention and context cascading mechanisms the model incorporates two levels of attention ie overall encoding level and image feature level to construct imagedependent sentences while standard attention configuration needs a large number of parameters the glac net implements them in a very simple way via hard connections from the outputs of encoders or image features onto the sentence generators the coherency of the generated story is further improved by conveying cascading the information of the previous sentence to the next sentence serially we evaluate the performance of the glac net on the visual storytelling dataset vist and achieve very competitive results compared to the stateoftheart techniques our code and pretrained models are available here,0
this paper describes the design principles behind jsrealb version 40 a surface realizer written javascript for english or french sentences from a specification inspired by the constituent syntax formalism but for which a dependencybased input notation is also available jsrealb can be used either within a web page or as a nodejs module we show that the seemingly simple process of text realization involves many interesting implementation challenges in order to take into account the specifics of each language jsrealb has a large coverage of english and french and has been used to develop realistic datatotext applications and to reproduce existing literary texts and sentences from universal dependency annotations its source code and that of its applications are available on github the port of this approach to python pyrealb is also presented,0
podcasts have become a central arena for shaping public opinion making them a vital source for understanding contemporary discourse their typically unscripted multithemed and conversational style offers a rich but complex form of data to analyze how podcasts persuade and inform we must examine their narrative structures specifically the narrative frames they employ the fluid and conversational nature of podcasts presents a significant challenge for automated analysis we show that existing large language models typically trained on more structured text such as news articles struggle to capture the subtle cues that human listeners rely on to identify narrative frames as a result current approaches fall short of accurately analyzing podcast narratives at scale to solve this we develop and evaluate a finetuned bert model that explicitly links narrative frames to specific entities mentioned in the conversation effectively grounding the abstract frame in concrete details our approach then uses these granular frame labels and correlates them with highlevel topics to reveal broader discourse trends the primary contributions of this paper are i a novel framelabeling methodology that more closely aligns with human judgment for messy conversational data and ii a new analysis that uncovers the systematic relationship between what is being discussed the topic and how it is being presented the frame offering a more robust framework for studying influence in digital media,0
tracking characters and locations throughout a story can help improve the understanding of its plot structure prior research has analyzed characters and locations from text independently without grounding characters to their locations in narrative time here we address this gap by proposing a new spatial relationship categorization task the objective of the task is to assign a spatial relationship category for every character and location comention within a window of text taking into consideration linguistic context narrative tense and temporal scope to this end we annotate spatial relationships in approximately 2500 book excerpts and train a model using contextual embeddings as features to predict these relationships when applied to a set of books this model allows us to test several hypotheses on mobility and domestic space revealing that protagonists are more mobile than noncentral characters and that women as characters tend to occupy more interior space than men overall our work is the first step towards joint modeling and analysis of characters and places in narrative text,0
the increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities the scale and diversity of such collections however presents particular challenges in identifying and extracting relevant content this paper presents curatr an online platform for the exploration and curation of literature with machine learningsupported semantic search designed within the context of digital humanities scholarship the platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons allowing researches to curate relevant subcorpora from a large corpus of 18th and 19th century digitised texts,0
we propose an unsupervised corpusindependent method to extract keywords from a single text it is based on the spatial distribution of words and the response of this distribution to a random permutation of words as compared to existing methods such as eg yake our method has three advantages first it is significantly more effective at extracting keywords from long texts second it allows inference of two types of keywords local and global third it uncovers basic themes in texts additionally our method is languageindependent and applies to short texts the results are obtained via human annotators with previous knowledge of texts from our database of classical literary works the agreement between annotators is from moderate to substantial our results are supported via humanindependent arguments based on the average length of extracted content words and on the average number of nouns in extracted words we discuss relations of keywords with higherorder textual features and reveal a connection between keywords and chapter divisions,0
we explore boccaccios decameron to see how digital humanities tools can be used for tasks that have limited data in a language no longer in contemporary use medieval italian we focus our analysis on the question do the different storytellers in the text exhibit distinct personalities to answer this question we curate and release a dataset based on the authoritative edition of the text we use supervised classification methods to predict storytellers based on the stories they tell confirming the difficulty of the task and demonstrate that topic modeling can extract thematic storyteller profiles,0
here we introduce narrative context protocol ncp an opensource narrative standard designed to enable narrative interoperability aidriven authoring tools realtime emergent narratives and more by encoding a storys structure in a storyform which is a structured register of its narrative features ncp enables narrative portability across systems as well as intentbased constraints for generative storytelling systems we demonstrate the capabilities of ncp through a yearlong experiment during which an author used ncp and a custom authoring platform to create a playable textbased experience based on her preexisting novella this experience is driven by generative ai with unconstrained natural language input ncp functions as a set of guardrails that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained,0
prior work in stylecontrolled text generation has focused on tasks such as emulating the style of prolific literary authors producing formal or informal text and mitigating toxicity of generated text plentiful demonstrations of these styles are available and as a result modern language models are often able to emulate them either via prompting or discriminative control however in applications such as writing assistants it is desirable for language models to produce text in an authorspecific style on the basis of a potentially small writing sample for example someone writing in a particular dialect may prefer writing suggestions that retain the same dialect we find that instructiontuned language models can struggle to reproduce authorspecific style demonstrated in a prompt instead we propose to guide a language model to generate text in a target style using contrastivelytrained representations that capture stylometric features our approach stylemc combines an authoradapted language model with sequencelevel inference to improve stylistic consistency and is found to be effective in a variety of conditions including unconditional generation and style transfer additionally we find that the proposed approach can serve as an effective anonymization method by editing a document to mask authorship while preserving the original meaning,0
historic variations of spelling poses a challenge for fulltext search or natural language processing on historical digitized texts to minimize the gap between the historic orthography and contemporary spelling usually an automatic orthographic normalization of the historical source material is pursued this report proposes a normalization system for german literary texts from c 17001900 trained on a parallel corpus the proposed system makes use of a machine learning approach using transformer language models combining an encoderdecoder model to normalize individual word types and a pretrained causal language model to adjust these normalizations within their context an extensive evaluation shows that the proposed system provides stateoftheart accuracy comparable with a much larger fully endtoend sentencebased normalization system finetuning a pretrained transformer large language model however the normalization of historical text remains a challenge due to difficulties for models to generalize and the lack of extensive highquality parallel data,0
the proliferation of online news and the increasing spread of misinformation necessitate robust methods for automatic data analysis narrative classification is emerging as a important task since identifying what is being said online is critical for factcheckers policy markers and other professionals working on information studies this paper presents our approach to semeval 2025 task 10 subtask 2 which aims to classify news articles into a predefined twolevel taxonomy of main narratives and subnarratives across multiple languages we propose hierarchical threestep prompting h3prompt for multilingual narrative classification our methodology follows a threestep large language model llm prompting strategy where the model first categorises an article into one of two domains ukrainerussia war or climate change then identifies the most relevant main narratives and finally assigns subnarratives our approach secured the top position on the english test set among 28 competing teams worldwide the code is available at,0
traditionally authorship attribution aa tasks relied on statistical data analysis and classification based on stylistic features extracted from texts in recent years pretrained language models plms have attracted significant attention in text classification tasks however although they demonstrate excellent performance on largescale shorttext datasets their effectiveness remains underexplored for small samples particularly in aa tasks additionally a key challenge is how to effectively leverage plms in conjunction with traditional featurebased methods to advance aa research in this study we aimed to significantly improve performance using an integrated integrative ensemble of traditional featurebased and modern plmbased methods on an aa task in a small sample for the experiment we used two corpora of literary works to classify 10 authors each the results indicate that bert is effective even for smallsample aa tasks both bertbased and classifier ensembles outperformed their respective standalone models and the integrated ensemble approach further improved the scores significantly for the corpus that was not included in the pretraining data the integrated ensemble improved the f1 score by approximately 14 points compared to the bestperforming single model our methodology provides a viable solution for the efficient use of the everexpanding array of data processing tools in the foreseeable future,0
automatic understanding of domain specific texts in order to extract useful relationships for later use is a nontrivial task one such relationship would be between railroad accidents causes and their correspondent descriptions in reports from 2001 to 2016 rail accidents in the us cost more than 46b railroads involved in accidents are required to submit an accident report to the federal railroad administration fra these reports contain a variety of fixed field entries including primary cause of the accidents a coded variable with 389 values as well as a narrative field which is a short text description of the accident although these narratives provide more information than a fixed field entry the terminologies used in these reports are not easy to understand by a nonexpert reader therefore providing an assisting method to fill in the primary cause from such domain specific textsnarratives would help to label the accidents with more accuracy another important question for transportation safety is whether the reported accident cause is consistent with narrative description to address these questions we applied deep learning methods together with powerful word embeddings such as word2vec and glove to classify accident cause values for the primary cause field using the text in the narratives the results show that such approaches can both accurately classify accident causes based on report narratives and find important inconsistencies in accident reporting,0
the generation of a long story consisting of several thousand words is a subtask in the field of long text generationltg previous research has addressed this challenge through outlinebased generation which employs a multistage method for generating outlines into stories however this approach suffers from two common issues almost inevitable theme drift caused by the loss of memory of previous outlines and tedious plots with incoherent logic that are less appealing to human readers in this paper we propose the multiagent story generator structure to improve the multistage method using large language modelsllms as the core components of agents to avoid theme drift we introduce a memory storage model comprising two components a longterm memory storage that identifies the most important memories thereby preventing theme drift and a shortterm memory storage that retains the latest outlines from each generation round to incorporate engaging elements into the story we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline this framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content additionally we establish a multiagent interaction stage to simulate writerreader interaction through dialogue and revise the story text according to feedback to ensure it remains consistent and logical evaluations against previous methods demonstrate that our approach can generate higherquality long stories,0
with the rapid development of multimodal large language models mllms an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models however these benchmarks focus on standalone videos and mainly assess visual elements like human actions and object states in reality contemporary videos often encompass complex and continuous narratives typically presented as a series to address this challenge we propose seriesbench a benchmark consisting of 105 carefully curated narrativedriven series covering 28 specialized tasks that require deep narrative understanding specifically we first select a diverse set of drama series spanning various genres then we introduce a novel longspan narrative annotation method combined with a fullinformation transformation approach to convert manual annotations into diverse task formats to further enhance model capacity for detailed analysis of plot structures and character relationships within series we propose a novel narrative reasoning framework pcdcot extensive results on seriesbench indicate that existing mllms still face significant challenges in understanding narrativedriven series while pcdcot enables these mllms to achieve performance improvements overall our seriesbench and pcdcot highlight the critical necessity of advancing model capabilities to understand narrativedriven series guiding the future development of mllms seriesbench is publicly available at,0
conventional bagofwords approaches for topic modeling like latent dirichlet allocation lda struggle with literary text literature challenges lexical methods because narrative language focuses on immersive sensory details instead of abstractive description or exposition writers are advised to show dont tell we propose retell a simple accessible topic modeling approach for literature here we prompt resourceefficient generative language models lms to tell what passages show thereby translating narratives surface forms into higherlevel concepts and themes by running lda on lms retellings of passages we can obtain more precise and informative topics than by running lda alone or by directly asking lms to list topics to investigate the potential of our method for cultural analytics we compare our methods outputs to expertguided annotations in a case study on racialcultural identity in high school english language arts books,0
novels are often adapted into feature films but the differences between the two media usually require dropping sections of the source text from the movie script here we study this screen adaptation process by constructing narrative alignments using the smithwaterman local alignment algorithm coupled with sbert embedding distance to quantify text similarity between scenes and book units we use these alignments to perform an automated analysis of 40 adaptations revealing insights into the screenwriting process concerning i faithfulness of adaptation ii importance of dialog iii preservation of narrative order and iv gender representation issues reflective of the bechdel test,0
we present a pipeline for a statistical textual exploration offering a stylometrybased explanation and statistical validation of a hypothesized partition of a text given a parameterization of the text our pipeline 1 detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions 2 performs a hypothesistesting analysis to quantify the statistical significance of the optimal overlap while conserving implicit correlations between units of text that are more likely to be grouped and 3 extracts and quantifies the importance of features most responsible for the classification estimates their statistical stability and clusterwise abundance we apply our pipeline to the first two books in the bible where one stylistic component stands out in the eyes of biblical scholars namely the priestly component we identify and explore statistically significant stylistic differences between the priestly and nonpriestly components,0
narratives are key interpretative devices by which humans make sense of political reality as the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident there is a growing demand for methods that support their empirical analysis to this end we propose a graphbased formalism and machineguided method for extracting representing and analyzing selected narrative signals from digital textual corpora based on abstract meaning representation amr the formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches social media posts transcripts of parliamentary debates and political manifestos on party websites we approach the study of such political narratives as a problem of information retrieval starting from a textual corpus we first extract a graphlike representation of the meaning of each sentence in the corpus using amr drawing on transferable concepts from narratology we then apply a set of heuristics to filter these graphs for representations of 1 actors and their relationships 2 the events in which these actors figure and 3 traces of the perspectivization of these events we approach these references to actors events and instances of perspectivization as core narrative signals that allude to larger political narratives by systematically analyzing and reassembling these signals into networks that guide the researcher to the relevant parts of the text the underlying narratives can be reconstructed through a combination of distant and close reading a case study of state of the european union addresses 2010 2023 demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse,0
automated discourse analysis tools based on natural language processing nlp aiming at the diagnosis of languageimpairing dementias generally extract several textual metrics of narrative transcripts however the absence of sentence boundary segmentation in the transcripts prevents the direct application of nlp methods which rely on these marks to function properly such as taggers and parsers we present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis presenting a new automatic sentence segmentation method for impaired speech our model uses recurrent convolutional neural networks with prosodic part of speech pos features and word embeddings it was evaluated intrinsically on impaired spontaneous speech as well as normal prepared speech and presents better results for healthy elderly ctl f1 074 and mild cognitive impairment mci patients f1 070 than the conditional random fields method f1 055 and 053 respectively used in the same context of our study the results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by mci and ctl,0
this paper presents an exploration of long shortterm memory lstm networks in the realm of text generation focusing on the utilization of historical datasets for shakespeare and nietzsche lstms known for their effectiveness in handling sequential data are applied here to model complex language patterns and structures inherent in historical texts the study demonstrates that lstmbased models when trained on historical datasets can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time the finding presents models that are highly accurate and efficient in predicting text from works of nietzsche with low loss values and a training time of 100 iterations the accuracy of the model is 09521 indicating high accuracy the loss of the model is 02518 indicating its effectiveness the accuracy of the model in predicting text from the work of shakespeare is 09125 indicating a low error rate the training time of the model is 100 mirroring the efficiency of the nietzsche dataset this efficiency demonstrates the effectiveness of the model design and training methodology especially when handling complex literary texts this research contributes to the field of natural language processing by showcasing the versatility of lstm networks in text generation and offering a pathway for future explorations in historical linguistics and beyond,0
we study the relationship between vocabulary size and text length in a corpus of 75 literary works in english authored by six writers distinguishing between the contributions of three grammatical classes or tags namely it nouns it verbs and it others and analyze the progressive appearance of new words of each tag along each individual text while the powerlaw relation prescribed by heaps law is satisfactorily fulfilled by total vocabulary sizes and text lengths the appearance of new words in each text is on the whole well described by the average of random shufflings of the text which does not obey a power law deviations from this average however are statistically significant and show a systematic trend across the corpus specifically they reveal that the appearance of new words along each text is predominantly retarded with respect to the average of random shufflings moreover different tags are shown to add systematically distinct contributions to this tendency with it verbs and it others being respectively more and less retarded than the mean trend and it nouns following instead this overall mean these statistical systematicities are likely to point to the existence of linguistically relevant information stored in the different variants of heaps law a feature that is still in need of extensive assessment,0
serialized tv shows are built on complex storylines that can be hard to track and evolve in ways that defy straightforward analysis this paper introduces a multiagent system designed to extract and analyze these narrative arcs tested on the first season of greys anatomy abc 2005 the system identifies three types of arcs anthology selfcontained soap relationshipfocused and genrespecific strictly related to the series genre episodic progressions of these arcs are stored in both relational and semantic vectorial databases enabling structured analysis and comparison to bridge the gap between automation and critical interpretation the system is paired with a graphical interface that allows for human refinement using tools to enhance and visualize the data the system performed strongly in identifying anthology arcs and character entities but its reliance on textual paratexts such as episode summaries revealed limitations in recognizing overlapping arcs and subtler dynamics this approach highlights the potential of combining computational and human expertise in narrative analysis beyond television it offers promise for serialized written formats where the narrative resides entirely in the text future work will explore the integration of multimodal inputs such as dialogue and visuals and expand testing across a wider range of genres to refine the system further,0
children efficiently acquire language not just by listening but by interacting with others in their social environment conversely large language models are typically trained with nextword prediction on massive amounts of text motivated by this contrast we investigate whether language models can be trained with less data by learning not only from nextword prediction but also from highlevel cognitively inspired feedback we train a student model to generate stories which a teacher model rates on readability narrative coherence and creativity by varying the amount of pretraining before the feedback loop we assess the impact of this interactive learning on formal and functional linguistic competence we find that the highlevel feedback is highly data efficient with just 1 m words of input in interactive learning storytelling skills can improve as much as with 410 m words of nextword prediction,0
this paper introduces the concept of an education tool that utilizes generative artificial intelligence genai to enhance storytelling we evaluate genaidriven narrative cocreation texttospeech conversion texttomusic and texttovideo generation to produce an engaging experience for learners we describe the cocreation process the adaptation of narratives into spoken words using texttospeech models and the transformation of these narratives into contextually relevant visuals through texttovideo technology our evaluation covers the linguistics of the generated stories the texttospeech conversion quality and the accuracy of the generated visuals,0
narratives are key interpretative devices by which humans make sense of political reality in this work we show how the analysis of conflicting narratives ie conflicting interpretive lenses through which political reality is experienced and told provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere building upon previous work that has identified ideologically polarized issues in the german twittersphere between 2021 and 2023 we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups focusing on a selection of salient issues and events the war in ukraine covid climate change we show evidence for conflicting narratives along two dimensions i different attributions of actantial roles to the same set of actants eg diverging interpretations of the role of nato in the war in ukraine and ii emplotment of different actants for the same event eg bill gates in the rightleaning covid narrative furthermore we provide first evidence for patterns of narrative alignment a discursive strategy that political actors employ to align opinions across issues these findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization,0
visual storytellingvist is a task to tell a narrative story about a certain topic according to the given photo stream the existing studies focus on designing complex models which rely on a huge amount of humanannotated data however the annotation of vist is extremely costly and many topics cannot be covered in the training dataset due to the longtail topic distribution in this paper we focus on enhancing the generalization ability of the vist model by considering the fewshot setting inspired by the way humans tell a story we propose a topic adaptive storyteller to model the ability of intertopic generalization in practice we apply the gradientbased metalearning algorithm on multimodal seq2seq models to endow the model the ability to adapt quickly from topic to topic besides we further propose a prototype encoding structure to model the ability of intratopic derivation specifically we encode and restore the few training story text to serve as a reference to guide the generation at inference time experimental results show that topic adaptation and prototype encoding structure mutually bring benefit to the fewshot model on bleu and meteor metric the further case study shows that the stories generated after fewshot adaptation are more relative and expressive,0
this paper presents categorization of croatian texts using nonstandard words nsw as features nonstandard words are numbers dates acronyms abbreviations currency etc nsws in croatian language are determined according to croatian nsw taxonomy for the purpose of this research 390 text documents were collected and formed the skipez collection with 6 classes official literary informative popular educational and scientific text categorization experiment was conducted on three different representations of the skipez collection in the first representation the frequencies of nsws are used as features in the second representation the statistic measures of nsws variance coefficient of variation standard deviation etc are used as features while the third representation combines the first two feature sets naive bayes cn2 c45 knn classification trees and random forest algorithms were used in text categorization experiments the best categorization results are achieved using the first feature set nsw frequencies with the categorization accuracy of 87 this suggests that the nsws should be considered as features in highly inflectional languages such as croatian nsw based features reduce the dimensionality of the feature space without standard lemmatization procedures and therefore the bagofnsws should be considered for further croatian texts categorization experiments,0
it is an open question to what extent perceptions of literary quality are derived from textintrinsic versus social factors while supervised models can predict literary quality ratings from textual factors quite successfully as shown in the riddle of literary quality project koolen et al 2020 this does not prove that social factors are not important nor can we assume that readers make judgments on literary quality in the same way and based on the same information as machine learning models we report the results of a pilot study to gauge the effect of textual features on literary ratings of dutchlanguage novels by participants in a controlled experiment with 48 participants in an exploratory analysis we compare the ratings to those from the large reader survey of the riddle in which social factors were not excluded and to machine learning predictions of those literary ratings we find moderate to strong correlations of questionnaire ratings with the survey ratings but the predictions are closer to the survey ratings code and data,0
dream narratives provide a unique window into human cognition and emotion yet their systematic analysis using artificial intelligence has been underexplored we introduce dreamnet a novel deep learning framework that decodes semantic themes and emotional states from textual dream reports optionally enhanced with remstage eeg data leveraging a transformerbased architecture with multimodal attention dreamnet achieves 921 accuracy and 884 f1score in textonly mode dnett on a curated dataset of 1500 anonymized dream narratives improving to 990 accuracy and 952 f1score with eeg integration dnetm strong dreamemotion correlations eg fallinganxiety r 091 p 001 highlight its potential for mental health diagnostics cognitive science and personalized therapy this work provides a scalable tool a publicly available enriched dataset and a rigorous methodology bridging ai and psychological research,0
writing compelling fiction is a multifaceted process combining elements such as crafting a plot developing interesting characters and using evocative language while large language models llms show promise for story writing they currently rely heavily on intricate prompting which limits their use we propose agents room a generation framework inspired by narrative theory that decomposes narrative writing into subtasks tackled by specialized agents to illustrate our method we introduce tell me a story a highquality dataset of complex writing prompts and humanwritten stories and a novel evaluation framework designed specifically for assessing long narratives we show that agents room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components we provide extensive analysis with automated and humanbased metrics of the generated output,0
the current prevalence of conspiracy theories on the internet is a significant issue tackled by many computational approaches however these approaches fail to recognize the relevance of distinguishing between texts which contain a conspiracy theory and texts which are simply critical and oppose mainstream narratives furthermore little attention is usually paid to the role of intergroup conflict in oppositional narratives we contribute by proposing a novel topicagnostic annotation scheme that differentiates between conspiracies and critical texts and that defines spanlevel categories of intergroup conflict we also contribute with the multilingual xaidisinfodemics corpus english and spanish which contains a highquality annotation of telegram messages related to covid19 5000 messages per language we also demonstrate the feasibility of an nlpbased automatization by performing a range of experiments that yield strong baseline solutions finally we perform an analysis which demonstrates that the promotion of intergroup conflict and the presence of violence and anger are key aspects to distinguish between the two types of oppositional narratives ie conspiracy vs critical,0
question answering qa as a research field has primarily focused on either knowledge bases kbs or free text as a source of knowledge these two sources have historically shaped the kinds of questions that are asked over these sources and the methods developed to answer them in this work we look towards a practical usecase of qa over userinstructed knowledge that uniquely combines elements of both structured qa over knowledge bases and unstructured qa over narrative introducing the task of multirelational qa over personal narrative as a first step towards this goal we make three key contributions i we generate and release textworldsqa a set of five diverse datasets where each dataset contains dynamic narrative that describes entities and relations in a simulated world paired with variably compositional questions over that knowledge ii we perform a thorough evaluation and analysis of several stateoftheart qa models and their variants at this task and iii we release a lightweight pythonbased framework we call textworlds for easily generating arbitrary additional worlds and narrative with the goal of allowing the community to create and share a growing collection of diverse worlds as a testbed for this task,0
visual storytelling is the task of generating stories based on a sequence of images inspired by the recent works in neural generation focusing on controlling the form of text this paper explores the idea of generating these stories in different personas however one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas having said that there are independent datasets for both visual storytelling and annotated sentences for various persona in this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation we inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction to this end we propose five models which are incremental extensions to the baseline model to perform the task at hand in our experiments we use five different personas to guide the generation process we find that the models based on our hypotheses perform better at capturing words while generating stories in the target persona,0
with the development of generative models like gpt3 it is increasingly more challenging to differentiate generated texts from humanwritten ones there is a large number of studies that have demonstrated good results in bot identification however the majority of such works depend on supervised learning methods that require labelled data andor prior knowledge about the botmodel architecture in this work we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data by combining findings in semantic analysis by clustering crisp and fuzzy and information techniques we construct a robust model that detects a generated text for different types of bot we find that the generated texts tend to be more chaotic while literary works are more complex we also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and wellseparated clusters of botgenerated texts,0
we present realitytalk a system that augments realtime live presentations with speechdriven interactive virtual elements augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling however existing tools for live presentations often lack interactivity and improvisation while creating such effects in video editing tools require significant time and expertise realitytalk enables users to create live augmented presentations with realtime speechdriven interactions the user can interactively prompt move and manipulate graphical elements through realtime speech and supporting modalities based on our analysis of 177 existing videoedited augmented presentations we propose a novel set of interaction techniques and then incorporated them into realitytalk we evaluate our tool from a presenters perspective to demonstrate the effectiveness of our system,0
visual storytelling is a task of generating relevant and interesting stories for given image sequences in this work we aim at increasing the diversity of the generated stories while preserving the informative content from the images we propose to foster the diversity and informativeness of a generated story by using a concept selection module that suggests a set of concept candidates then we utilize a large scale pretrained model to convert concepts and images into full stories to enrich the candidate concepts a commonsense knowledge graph is created for each image sequence from which the concept candidates are proposed to obtain appropriate concepts from the graph we propose two novel modules that consider the correlation among candidate concepts and the imageconcept correlation extensive automatic and human evaluation results demonstrate that our model can produce reasonable concepts this enables our model to outperform the previous models by a large margin on the diversity and informativeness of the story while retaining the relevance of the story to the image sequence,0
this paper investigates the application of translation alignment algorithms in the creation of a multilingual digital edition mde of alessandro manzonis italian novel i promessi sposi the betrothed with translations in eight languages english spanish french german dutch polish russian and chinese from the 19th and 20th centuries we identify key requirements for the mde to improve both the reader experience and support for translation studies our research highlights the limitations of current stateoftheart algorithms when applied to the translation of literary texts and outlines an automated pipeline for mde creation this pipeline transforms raw texts into webbased sidebyside representations of original and translated texts with different rendering options in addition we propose new metrics for evaluating the alignment of literary translations and suggest visualization techniques for future analysis,0
literary translation is a culturally significant task but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world machine translation mt holds potential to complement the work of human translators by improving both training procedures and their overall efficiency literary translation is less constrained than more traditional mt settings since translators must balance meaning equivalence readability and critical interpretability in the target language this property along with the complex discourselevel context present in literary texts also makes literary mt more challenging to computationally model and evaluate to explore this task we collect a dataset par3 of nonenglish language novels in the public domain each aligned at the paragraph level to both human and automatic english translations using par3 we discover that expert literary translators prefer reference human translations over machinetranslated paragraphs at a rate of 84 while stateoftheart automatic mt metrics do not correlate with those preferences the experts note that mt outputs contain not only mistranslations but also discoursedisrupting errors and stylistic inconsistencies to address these problems we train a postediting model whose output is preferred over normal mt output at a rate of 69 by experts we publicly release par3 at to spur future research into literary mt,0
events in a narrative differ in salience some are more important to the story than others estimating event salience is useful for tasks such as story generation and as a tool for text analysis in narratology and folkloristics to compute event salience without any annotations we adopt barthes definition of event salience and propose several unsupervised methods that require only a pretrained language model evaluating the proposed methods on folktales with event salience annotation we show that the proposed methods outperform baseline methods and find finetuning a language model on narrative texts is a key factor in improving the proposed methods,0
for more than forty years now modern theories of literature compagnon 1979 insist on the role of paraphrases rewritings citations reciprocal borrowings and mutual contributions of any kinds the notions of intertextuality transtextuality hypertextualityhypotextuality were introduced in the seventies and eighties to approach these phenomena the careful analysis of these references is of particular interest in evaluating the distance that the creator voluntarily introduces with hisher masters phoebus is collaborative project that makes computer scientists from the university pierre and marie curie lip6upmc collaborate with the literary teams of parissorbonne university with the aim to develop efficient tools for literary studies that take advantage of modern computer science techniques in this context we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature this paper describes the principles on which is based this program the significant results that have already been obtained and the perspectives for the near future,0
the collection of narrative spontaneous reports is an irreplaceable source for the prompt detection of suspected adverse drug reactions adrs qualified domain experts manually revise a huge amount of narrative descriptions and then encode texts according to meddra standard terminology the manual annotation of narrative documents with medical terminology is a subtle and expensive task since the number of reports is growing up daybyday magicoder a natural language processing algorithm is proposed for the automatic encoding of freetext descriptions into meddra terms magicoder procedure is efficient in terms of computational complexity in particular it is linear in the size of the narrative input and the terminology we tested it on a large dataset of about 4500 manually revised reports by performing an automated comparison between human and magicoder revisions for the current base version of magicoder we measured on short descriptions an average recall of 86 and an average precision of 88 on mediumlong descriptions up to 255 characters an average recall of 64 and an average precision of 63 from a practical point of view magicoder reduces the time required for encoding adr reports pharmacologists have simply to review and validate the magicoder terms proposed by the application instead of choosing the right terms among the 70k low level terms of meddra such improvement in the efficiency of pharmacologists work has a relevant impact also on the quality of the subsequent data analysis we developed magicoder for the italian pharmacovigilance language however our proposal is based on a general approach not depending on the considered language nor the term dictionary,0
we analyzed historical and literary documents in chinese to gain insights into research issues and overview our studies which utilized four different sources of text materials in this paper we investigated the history of concepts and transliterated words in china with the database for the study of modern china thought and literature which contains historical documents about china between 1830 and 1930 we also attempted to disambiguate names that were shared by multiple government officers who served between 618 and 1912 and were recorded in chinese local gazetteers to showcase the potentials and challenges of computerassisted analysis of chinese literatures we explored some interesting yet nontrivial questions about two of the four great classical novels of china 1 which monsters attempted to consume the buddhist monk xuanzang in the journey to the west jttw which was published in the 16th century 2 which was the most powerful monster in jttw and 3 which major role smiled the most in the dream of the red chamber which was published in the 18th century similar approaches can be applied to the analysis and study of modern documents such as the newspaper articles published about the 228 incident that occurred in 1947 in taiwan,0
chiasmus a debated literary device in biblical texts has captivated mystics while sparking ongoing scholarly discussion in this paper we introduce the first computational approach to systematically detect chiasmus within biblical passages our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus applied at multiple levels of textual granularity halfverses verses we also involve expert annotators to review a subset of the detected patterns despite its computational efficiency our method achieves robust results with high interannotator agreement and system precisionk of 080 at the verse level and 060 at the halfverse level we further provide a qualitative analysis of the distribution of detected chiasmi along with selected examples that highlight the effectiveness of our approach,0
in order to tell stories in different voices for different audiences interactive story systems require 1 a semantic representation of story structure and 2 the ability to automatically generate story and dialogue from this semantic representation using some form of natural language generation nlg however there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events in this paper we present an automatic method for converting from scheherazades story intention graph a semantic representation to the input required by the personage nlg engine using 36 aesop fables distributed in dramabank a collection of story encodings we train translation rules on one story and then test these rules by generating text for the remaining 35 the results are measured in terms of the string similarity metrics levenshtein distance and bleu score the results show that we can generate the 35 stories with correct content the test set stories on average are close to the output of the scheherazade realizer which was customized to this semantic representation we provide some examples of story variations generated by personage in future work we will experiment with measuring the quality of the same stories generated in different voices and with techniques for making storytelling interactive,0
how can we detect when global events fundamentally reshape public discourse this study introduces a topological framework for identifying structural change in media narratives using persistent homology drawing on international news articles surrounding major events including the russian invasion of ukraine feb 2022 the murder of george floyd may 2020 the us capitol insurrection jan 2021 and the hamasled invasion of israel oct 2023 we construct daily cooccurrence graphs of noun phrases to trace evolving discourse each graph is embedded and transformed into a persistence diagram via a vietorisrips filtration we then compute wasserstein distances and persistence entropies across homological dimensions to capture semantic disruption and narrative volatility over time our results show that major geopolitical and social events align with sharp spikes in both h0 connected components and h1 loops indicating sudden reorganization in narrative structure and coherence crosscorrelation analyses reveal a typical lag pattern in which changes to componentlevel structure h0 precede higherorder motif shifts h1 suggesting a bottomup cascade of semantic change an exception occurs during the russian invasion of ukraine where h1 entropy leads h0 possibly reflecting topdown narrative framing before local discourse adjusts persistence entropy further distinguishes tightly focused from diffuse narrative regimes these findings demonstrate that persistent homology offers a mathematically principled unsupervised method for detecting inflection points and directional shifts in public attention without requiring prior knowledge of specific events this topological approach advances computational social science by enabling realtime detection of semantic restructuring during crises protests and information shocks,0
we present and make available medlatinepi and medlatinlit two datasets of medieval latin texts to be used in research on computational authorship analysis medlatinepi and medlatinlit consist of 294 and 30 curated texts respectively labelled by author medlatinepi texts are of epistolary nature while medlatinlit texts consist of literary comments and treatises about various subjects as such these two datasets lend themselves to supporting research in authorship analysis tasks such as authorship attribution authorship verification or sameauthor verification along with the datasets we provide experimental results obtained on these datasets for the authorship verification task ie the task of predicting whether a text of unknown authorship was written by a candidate author or not we also make available the source code of the authorship verification system we have used thus allowing our experiments to be reproduced and to be used as baselines by other researchers we also describe the application of the above authorship verification system using these datasets as training data for investigating the authorship of two medieval epistles whose authorship has been disputed by scholars,0
every time an interactive storytelling is system gets a player input it is facing the worldupdate problem classical approaches to this problem consist in mapping that input to known preprogrammed actions what can severely constrain the free will of the player when the expected experience has a strong focus on improvisation like in roleplaying games rpgs this problem is critical in this paper we present payador a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves to implement this approach we ground a large language model to a minimal representation of the fictional world obtaining promising results we make this contribution opensource so it can be adapted and used for other related research on unleashing the cocreativity power of rpgs,0
postediting machine translation mt for creative texts such as literature requires balancing efficiency with the preservation of creativity and style while neural mt systems struggle with these challenges large language models llms offer improved capabilities for contextaware and creative translation this study evaluates the feasibility of postediting literary translations generated by llms using a custom research tool we collaborated with professional literary translators to analyze editing time quality and creativity our results indicate that postediting llmgenerated translations significantly reduces editing time compared to human translation while maintaining a similar level of creativity the minimal difference in creativity between pe and mt combined with substantial productivity gains suggests that llms may effectively support literary translators working with highresource languages,0
the gutenberg literary english corpus glec provides a rich source of textual data for research in digital humanities computational linguistics or neurocognitive poetics however so far only a small subcorpus the gutenberg english poetry corpus has been submitted to quantitative text analyses providing predictions for scientific studies of literature here we show that in the entire glec quasi errorfree text classification and authorship recognition is possible with a method using the same set of five style and five content features computed via style and sentiment analysis in both tasks our results identify two standard and two novel features ie typetoken ratio frequency sonority score surprise as most diagnostic in these tasks by providing a simple tool applicable to both short poems and long novels generating quantitative predictions about features that codeterme the cognitive and affective processing of specific text categories or authors our data pave the way for many future computational and empirical studies of literature or experiments in reading psychology,0
this work presents a computational approach to analyze character development along the narrative timeline the analysis characterizes the inner and outer changes the protagonist undergoes within a narrative and the interplay between them we consider transcripts of holocaust survivor testimonies as a test case each telling the story of an individual in firstperson terms we focus on the survivors religious trajectory examining the evolution of their disposition toward religious belief and practice along the testimony clustering the resulting trajectories in the dataset we identify common sequences in the data our findings highlight multiple common structures of religiosity across the narratives in terms of belief most present a constant disposition while for practice most present an oscillating structure serving as valuable material for historical and sociological research this work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives,0
language models increasingly rely on massive web dumps for diverse text data however these sources are rife with undesirable content as such resources like wikipedia books and newswire often serve as anchors for automatically selecting web text most suitable for language modeling a process typically referred to as quality filtering using a new dataset of us high school newspaper articles written by students from across the country we investigate whose language is preferred by the quality filter used for gpt3 we find that newspapers from larger schools located in wealthier educated and urban zip codes are more likely to be classified as high quality we then demonstrate that the filters measurement of quality is unaligned with other sensible metrics such as factuality or literary acclaim we argue that privileging any corpus as high quality entails a language ideology and more care is needed to construct training corpora for language models with better transparency and justification for the inclusion or exclusion of various texts,0
with a growing interest in modeling inherent subjectivity in natural language we present a linguisticallymotivated process to understand and analyze the writing style of individuals from three perspectives lexical syntactic and semantic we discuss the stylistically expressive elements within each of these levels and use existing methods to quantify the linguistic intuitions related to some of these elements we show that such a multilevel analysis is useful for developing a wellknit understanding of style which is independent of the natural language task at hand and also demonstrate its value in solving three downstream tasks authors style analysis authorship attribution and emotion prediction we conduct experiments on a variety of datasets comprising texts from social networking sites user reviews legal documents literary books and newswire the results on the aforementioned tasks and datasets illustrate that such a multilevel understanding of style which has been largely ignored in recent works models stylerelated subjectivity in text and can be leveraged to improve performance on multiple downstream tasks both qualitatively and quantitatively,0
we introduce toyteller an aipowered storytelling system where users generate a mix of story text and visuals by directly manipulating character symbols like they are toyplaying anthropomorphized symbol motions can convey rich and nuanced social interactions toyteller leverages these motions 1 to let users steer story text generation and 2 as a visual output format that accompanies story text we enabled motionsteered text generation and textsteered motion generation by mapping motions and text onto a shared semantic space so that large language models and motion generation models can use it as a translational layer technical evaluations showed that toyteller outperforms a competitive baseline gpt4o our user study identified that toyplaying helps express intentions difficult to verbalize however only motions could not express all user intentions suggesting combining it with other modalities like language we discuss the design space of toyplaying interactions and implications for technical hci research on humanai interaction,0
large language models llms have grown more powerful in language generation producing fluent text and even imitating personal style yet this ability also heightens the risk of identity impersonation to the best of our knowledge no prior work has examined personalized machinegenerated text mgt detection in this paper we introduce dataset the first benchmark for evaluating detector robustness in personalized settings built from literary and blog texts paired with their llmgenerated imitations our experimental results demonstrate large performance gaps across detectors in personalized settings some stateoftheart models suffer significant drops we attribute this limitation to the textitfeatureinversion trap where features that are discriminative in general domains become inverted and misleading when applied to personalized text based on this finding we propose method a simple and reliable way to predict detector performance changes in personalized settings method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence our experiments show that method can accurately predict both the direction and the magnitude of posttransfer changes showing 85 correlation with the actual performance gaps we hope that this work will encourage further research on personalized text detection,0
social biases and stereotypes are embedded in our culture in part through their presence in our stories as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories because these analyses are often conducted manually and at a small scale such investigations can benefit from the use of more recent natural language processing methods that examine social bias in models and data corpora our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories we propose a computational pipeline that automatically extracts a storys temporal narrative verbbased event chain for each of its characters as well as character attributes such as gender we also present a verbbased event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes through a case study analyzing gender bias in fairy tales we demonstrate that our framework can reveal bias in not only the unigram verbbased events in which female and male characters participate but also in the temporal narrative order of such event participation,0
artificial intelligence ai is rapidly transforming healthcare enabling fast development of tools like stress monitors wellness trackers and mental health chatbots however rapid and lowbarrier development can introduce risks of bias privacy violations and unequal access especially when systems ignore realworld contexts and diverse user needs many recent methods use ai to detect risks automatically but this can reduce human engagement in understanding how harms arise and who they affect we present a humancentered framework that generates user stories and supports multiagent discussions to help people think creatively about potential benefits and harms before deployment in a user study participants who read stories recognized a broader range of harms distributing their responses more evenly across all 13 harm types in contrast those who did not read stories focused primarily on privacy and wellbeing 583 our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about ais impact on users,0
human emotions are essentially molded by lived experiences from which we construct personalised meaning the engagement in such meaningmaking process has been practiced as an intervention in various psychotherapies to promote wellness nevertheless to support recollecting and recounting lived experiences in everyday life remains under explored in hci it also remains unknown how technologies such as generative ai models can facilitate the meaning making process and ultimately support affective mindfulness in this paper we present metamorpheus an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams metamorpheus arranges the storyline based on a dreams emotional arc and provokes selfreflection through the creation of metaphorical images and text depictions the system provides metaphor suggestions and generates visual metaphors and text depictions using generative ai models while users can apply generations to recolour and rearrange the interface to be visually affective our experiencecentred evaluation manifests that by interacting with metamorpheus users can recall their dreams in vivid detail through which they relive and reflect upon their experiences in a meaningful way,0
identifying cultural capital cc themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms however themes such as aspirational goals or family support are often woven into narratives rather than appearing as direct keywords this makes them difficult to detect for standard nlp models that process sentences in isolation the core challenge stems from a lack of awareness as standard models are pretrained on general corpora leaving them blind to the domainspecific language and narrative context inherent to the data to address this we introduce aware a framework that systematically attempts to improve a transformer models awareness for this nuanced task aware has three core components 1 domain awareness adapting the models vocabulary to the linguistic style of student reflections 2 context awareness generating sentence embeddings that are aware of the full essay context and 3 class overlap awareness employing a multilabel strategy to recognize the coexistence of themes in a single sentence our results show that by making the model explicitly aware of the properties of the input aware outperforms a strong baseline by 21 percentage points in macrof1 and shows considerable improvements across all themes this work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative,0
finding previously debunked narratives involves identifying claims that have already undergone factchecking the issue intensifies when similar false claims persist in multiple languages despite the availability of debunks for several months in another language hence automatically finding debunks or factchecks in multiple languages is crucial to make the best use of scarce factcheckers resources mainly due to the lack of readily available data this is an understudied problem particularly when considering the crosslingual scenario ie the retrieval of debunks in a language different from the language of the online post being checked this study introduces crosslingual debunked narrative retrieval and addresses this research gap by i creating multilingual misinformation tweets mmtweets a dataset that stands out featuring crosslingual pairs images human annotations and finegrained labels making it a comprehensive resource compared to its counterparts ii conducting an extensive experiment to benchmark stateoftheart crosslingual retrieval models and introducing multistage retrieval methods tailored for the task and iii comprehensively evaluating retrieval models for their crosslingual and crossdataset transfer capabilities within mmtweets and conducting a retrieval latency analysis we find that mmtweets presents challenges for crosslingual debunked narrative retrieval highlighting areas for improvement in retrieval models nonetheless the study provides valuable insights for creating mmtweets datasets and optimising debunked narrative retrieval models to empower factchecking endeavours the dataset and annotation codebook are publicly available at,0
we introduce a new dataset for multiclass emotion analysis from longform narratives in english the dataset for emotions of narrative sequences dens was collected from both classic literature available on project gutenberg and modern online narratives available on wattpad annotated using amazon mechanical turk a number of statistics and baseline benchmarks are provided for the dataset of the tested techniques we find that the finetuning of a pretrained bert model achieves the best results with an average microf1 score of 604 our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentencelevel techniques,0
a character network is a graph extracted from a narrative in which vertices represent characters and edges correspond to interactions between them a number of narrativerelated problems can be addressed automatically through the analysis of character networks such as summarization classification or role detection character networks are particularly relevant when considering works of fictions eg novels plays movies tv series as their exploitation allows developing information retrieval and recommendation systems however works of fiction possess specific properties making these tasks harder this survey aims at presenting and organizing the scientific literature related to the extraction of character networks from works of fiction as well as their analysis we first describe the extraction process in a generic way and explain how its constituting steps are implemented in practice depending on the medium of the narrative the goal of the network analysis and other factors we then review the descriptive tools used to characterize character networks with a focus on the way they are interpreted in this context we illustrate the relevance of character networks by also providing a review of applications derived from their analysis finally we identify the limitations of the existing approaches and the most promising perspectives,0
authorship attribution mainly deals with undecided authorship of literary texts authorship attribution is useful in resolving issues like uncertain authorship recognize authorship of unknown texts spot plagiarism so on statistical methods can be used to set apart the approach of an author numerically the basic methodologies that are made use in computational stylometry are word length sentence length vocabulary affluence frequencies etc each author has an inborn style of writing which is particular to himself statistical quantitative techniques can be used to differentiate the approach of an author in a numerical way the problem can be broken down into three sub problems as author identification author characterization and similarity detection the steps involved are preprocessing extracting features classification and author identification for this different classifiers can be used here fuzzy learning classifier and svm are used after author identification the svm was found to have more accuracy than fuzzy classifier later combined the classifiers to obtain a better accuracy when compared to individual svm and fuzzy classifier,0
the greek fictional narratives often termed love novels or romances ranging from the first century ce to the middle of the 15th century have long been considered as similar in many ways not least in the use of particular literary motifs by applying the use of finetuned large language models this study aims to investigate which motifs exactly that the texts in this corpus have in common and in which ways they differ from each other the results show that while some motifs persist throughout the corpus others fluctuate in frequency indicating certain trends or external influences conclusively the method proves to adequately extract literary motifs according to a set definition providing data for both quantitative and qualitative analyses,0
large multimodal models lmms have achieved remarkable success across various visuallanguage tasks however existing benchmarks predominantly focus on singleimage understanding leaving the analysis of image sequences largely unexplored to address this limitation we introduce stripcipher a comprehensive benchmark designed to evaluate capabilities of lmms to comprehend and reason over sequential images stripcipher comprises a humanannotated dataset and three challenging subtasks visual narrative comprehension contextual frame prediction and temporal narrative reordering our evaluation of 16 stateoftheart lmms including gpt4o and qwen25vl reveals a significant performance gap compared to human capabilities particularly in tasks that require reordering shuffled sequential images for instance gpt4o achieves only 2393 accuracy in the reordering subtask which is 5607 lower than human performance further quantitative analysis discuss several factors such as input format of images affecting the performance of llms in sequential understanding underscoring the fundamental challenges that remain in the development of lmms,0
the vast collection of holocaust survivor testimonies presents invaluable historical insights but poses challenges for manual analysis this paper leverages advanced natural language processing nlp techniques to explore the usc shoah foundation holocaust testimony corpus by treating testimonies as structured questionandanswer sections we apply topic modeling to identify key themes we experiment with bertopic which leverages recent advances in language modeling technology we align testimony sections into fixed parts revealing the evolution of topics across the corpus of testimonies this highlights both a common narrative schema and divergences between subgroups based on age and gender we introduce a novel method to identify testimonies within groups that exhibit atypical topic distributions resembling those of other groups this study offers unique insights into the complex narratives of holocaust survivors demonstrating the power of nlp to illuminate historical discourse and identify potential deviations in survivor experiences,0
the performance of video question answering videoqa models is fundamentally constrained by the nature of their supervision which typically consists of isolated factual questionanswer pairs this bagoffacts approach fails to capture the underlying narrative and causal structure of events limiting models to a shallow understanding of video content to move beyond this paradigm we introduce a framework to synthesize richer supervisory signals we propose two complementary strategies questionbased paraphrasing qbp which synthesizes the diverse inquiries what how why from a videos existing set of questionanswer pairs into a holistic narrative paragraph that reconstructs the videos event structure and questionbased captioning qbc which generates finegrained visual rationales grounding the answer to each question in specific relevant evidence leveraging powerful generative models we use this synthetic data to train videoqa models under a unified nexttoken prediction objective extensive experiments on star and nextqa validate our approach demonstrating significant accuracy gains and establishing new stateoftheart results such as improving a 3b model to 725 on star 49 and a 7b model to 808 on nextqa beyond accuracy our analysis reveals that both qbp and qbc substantially enhance crossdataset generalization with qbp additionally accelerating model convergence by over 25x these results demonstrate that shifting data synthesis from isolated facts to narrative coherence and grounded rationales yields a more accurate efficient and generalizable training paradigm,0
chatgpt is a publicly available chatbot that can quickly generate texts on given topics but it is unknown whether the chatbot is really superior to human writers in all aspects of writing and whether its writing quality can be prominently improved on the basis of updating commands consequently this study compared the writing performance on a narrative topic by chatgpt and chinese intermediate english cie learners so as to reveal the chatbots advantage and disadvantage in writing the data were analyzed in terms of five discourse components using cohmetrix a special instrument for analyzing language discourses and the results revealed that chatgpt performed better than human writers in narrativity word concreteness and referential cohesion but worse in syntactic simplicity and deep cohesion in its initial version after more revision commands were updated while the resulting version was facilitated in syntactic simplicity yet it is still lagged far behind cie learners writing in deep cohesion in addition the correlation analysis of the discourse components suggests that narrativity was correlated with referential cohesion in both chatgpt and human writers but the correlations varied within each group,0
in this paper we use topological data analysis tda tools such as persistent homology persistent entropy and bottleneck distance to provide a it tdabased summary of any given set of texts and a general method for computing a distance between any two literary styles authors or periods to this aim deeplearning wordembedding techniques are combined with these tools in order to study the topological properties of texts embedded in a metric space as a case of study we use the written texts of three poets of the spanish golden age francisco de quevedo luis de gngora and lope de vega as far as we know this is the first time that word embedding bottleneck distance persistent homology and persistent entropy are used together to characterize texts and to compare different literary styles,0
visual narrative is often a combination of explicit information and judicious omissions relying on the viewer to supply missing details in comics most movements in time and space are hidden in the gutters between panels to follow the story readers logically connect panels together by inferring unseen actions through a process called closure while computers can now describe what is explicitly depicted in natural images in this paper we examine whether they can understand the closuredriven narratives conveyed by stylized artwork and dialogue in comic book panels we construct a dataset comics that consists of over 12 million panels 120 gb paired with automatic textbox transcriptions an indepth analysis of comics demonstrates that neither text nor image alone can tell a comic book story so a computer must understand both modalities to keep up with the plot we introduce three clozestyle tasks that ask models to predict narrative and charactercentric aspects of a panel given n preceding panels as context various deep neural architectures underperform human baselines on these tasks suggesting that comics contains fundamental challenges for both vision and language,0
recent advancements in nlp have spurred significant interest in analyzing social media text data for identifying linguistic features indicative of mental health issues however the domain of expressive narrative stories ensdeeply personal and emotionally charged narratives that offer rich psychological insightsremains underexplored this study bridges this gap by utilizing a dataset sourced from reddit focusing on ens from individuals with and without selfdeclared depression our research evaluates the utility of advanced language models bert and mentalbert against traditional models we find that traditional models are sensitive to the absence of explicit topicrelated words which could risk their potential to extend applications to ens that lack clear mental health terminology despite mentalbert is design to better handle psychiatric contexts it demonstrated a dependency on specific topic words for classification accuracy raising concerns about its application when explicit mental health terms are sparse pvalue005 in contrast bert exhibited minimal sensitivity to the absence of topic words in ens suggesting its superior capability to understand deeper linguistic features making it more effective for realworld applications both bert and mentalbert excel at recognizing linguistic nuances and maintaining classification accuracy even when narrative order is disrupted this resilience is statistically significant with sentence shuffling showing substantial impacts on model performance pvalue005 especially evident in ens comparisons between individuals with and without mental health declarations these findings underscore the importance of exploring ens for deeper insights into mental healthrelated narratives advocating for a nuanced approach to mental health text analysis that moves beyond mere keyword detection,0
natural language processing and machine learning have considerably advanced computational literary studies similarly the construction of cooccurrence networks of literary characters and their analysis using methods from social network analysis and network science have provided insights into the micro and macrolevel structure of literary texts combining these perspectives in this work we study character networks extracted from a text corpus of jrr tolkiens legendarium we show that this perspective helps us to analyse and visualise the narrative style that characterises tolkiens works addressing character classification embedding and cooccurrence prediction we further investigate the advantages of stateoftheart graph neural networks over a popular word embedding method our results highlight the large potential of graph learning in computational literary studies,0
this study investigates the relationship between deep learning dl model accuracy and expert agreement in classifying crash narratives we evaluate five dl models including bert variants use and a zeroshot classifier against expert labels and narratives and extend the analysis to four large language models llms gpt4 llama 3 qwen and claude our findings reveal an inverse relationship models with higher technical accuracy often show lower agreement with human experts while llms demonstrate stronger expert alignment despite lower accuracy we use cohens kappa and principal component analysis pca to quantify and visualize modelexpert agreement and employ shap analysis to explain misclassifications results show that expertaligned models rely more on contextual and temporal cues than locationspecific keywords these findings suggest that accuracy alone is insufficient for safetycritical nlp tasks we argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of llms as interpretable tools in crash analysis pipelines,0
this research investigates the efficacy of machine learning ml and deep learning dl methods in detecting misclassified intersectionrelated crashes in policereported narratives using 2019 crash data from the iowa department of transportation we implemented and compared a comprehensive set of models including support vector machine svm xgboost bert sentence embeddings bert word embeddings and albert model model performance was systematically validated against expert reviews of potentially misclassified narratives providing a rigorous assessment of classification accuracy results demonstrated that while traditional ml methods exhibited superior overall performance compared to some dl approaches the albert model achieved the highest agreement with expert classifications 73 with expert 1 and original tabular data 58 statistical analysis revealed that the albert model maintained performance levels similar to interexpert consistency rates significantly outperforming other approaches particularly on ambiguous narratives this work addresses a critical gap in transportation safety research through multimodal integration analysis which achieved a 542 reduction in error rates by combining narrative text with structured crash data we conclude that hybrid approaches combining automated classification with targeted expert review offer a practical methodology for improving crash data quality with substantial implications for transportation safety management and policy development,0
large language models llms and texttoimage t2i models have demonstrated the ability to generate compelling text and visual stories however their outputs are predominantly aligned with the sensibilities of the global north often resulting in an outsiders gaze on other cultures as a result nonwestern communities have to put extra effort into generating culturally specific stories to address this challenge we developed a visual storytelling tool called kahani that generates culturally grounded visual stories for nonwestern cultures our tool leverages offtheshelf models gpt4 turbo and stable diffusion xl sdxl by using chain of thought cot and t2i prompting techniques we capture the cultural context from users prompt and generate vivid descriptions of the characters and scene compositions to evaluate the effectiveness of kahani we conducted a comparative user study with chatgpt4 with dalle3 in which participants from different regions of india compared the cultural relevance of stories generated by the two tools the results of the qualitative and quantitative analysis performed in the user study show that kahanis visual stories are more culturally nuanced than those generated by chatgpt4 in 27 out of 36 comparisons kahani outperformed or was on par with chatgpt4 effectively capturing cultural nuances and incorporating more culturally specific items csi validating its ability to generate culturally grounded visual stories,0
large language models llms equipped with chainofthoughts cot prompting have shown significant multistep reasoning capabilities in factual content like mathematics commonsense and logic however their performance in narrative reasoning which demands greater abstraction capabilities remains unexplored this study utilizes tropes in movie synopses to assess the abstract reasoning abilities of stateoftheart llms and uncovers their low performance we introduce a tropewise querying approach to address these challenges and boost the f1 score by 118 points moreover while prior studies suggest that cot enhances multistep reasoning this study shows cot can cause hallucinations in narrative content reducing gpt4s performance we also introduce an adversarial injection method to embed troperelated text tokens into movie synopses without explicit tropes revealing cots heightened sensitivity to such injections our comprehensive analysis provides insights for future research directions,0
high lexical variation ambiguous references and longrange dependencies make entity resolution in literary texts particularly challenging we present mahnma the first largescale dataset for endtoend entity discovery and linking edl in sanskrit a morphologically rich and underresourced language derived from the mahbhrata the worlds longest epic the dataset comprises over 109k named entity mentions mapped to 55k unique entities and is aligned with an english knowledge base to support crosslingual linking the complex narrative structure of mahnma coupled with extensive name variation and ambiguity poses significant challenges to resolution systems our evaluation reveals that current coreference and entity linking models struggle when evaluated on the global context of the test set these results highlight the limitations of current approaches in resolving entities within such complex discourse mahnma thus provides a unique benchmark for advancing entity resolution especially in literary domains,0
narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled often evolving relations among characters and entities given the llms diminished reasoning over extended context and its high computational cost retrievalbased approaches remain a pivotal role in practice however traditional rag methods could fall short due to their stateless singlestep retrieval process which often overlooks the dynamic nature of capturing interconnected relations within longrange context in this work we propose comorag holding the principle that narrative reasoning is not a oneshot process but a dynamic evolving interplay between new evidence acquisition and past knowledge consolidation analogous to human cognition on reasoning with memoryrelated signals in the brain specifically when encountering a reasoning impasse comorag undergoes iterative reasoning cycles while interacting with a dynamic memory workspace in each cycle it generates probing queries to devise new exploratory paths then integrates the retrieved evidence of new aspects into a global memory pool thereby supporting the emergence of a coherent context for the query resolution across four challenging longcontext narrative benchmarks 200k tokens comorag outperforms strong rag baselines with consistent relative gains up to 11 compared to the strongest baseline further analysis reveals that comorag is particularly advantageous for complex queries requiring global context comprehension offering a principled cognitively motivated paradigm towards retrievalbased stateful reasoning our framework is made publicly available at,0
the increasing adoption of texttospeech technologies has led to a growing demand for natural and emotive voices that adapt to a conversations context and emotional tone the emotive narrative storytelling emns corpus is a unique speech dataset created to enhance conversations expressiveness and emotive quality in interactive narrativedriven systems the corpus consists of a 23hour recording featuring a female speaker delivering labelled utterances it encompasses eight acted emotional states evenly distributed with a variance of 068 along with expressiveness levels and natural language descriptions with word emphasis labels the evaluation of audio samples from different datasets revealed that the emns corpus achieved the highest average scores in accurately conveying emotions and demonstrating expressiveness it outperformed other datasets in conveying shared emotions and achieved comparable levels of genuineness a classification task confirmed the accurate representation of intended emotions in the corpus with participants recognising the recordings as genuine and expressive additionally the availability of the dataset collection tool under the apache 20 license simplifies remote speech data collection for researchers,0
we present franx a framing and narratives explorer that automatically detects entity mentions and classifies their narrative roles directly from raw text franx comprises a twostage system that combines sequence labeling with finegrained role classification to reveal how entities are portrayed as protagonists antagonists or innocents using a unique taxonomy of 22 finegrained roles nested under these three main categories the system supports five languages bulgarian english hindi russian and portuguese and two domains the russiaukraine conflict and climate change it provides an interactive web interface for media analysts to explore and compare framing across different sources tackling the challenge of automatically detecting and labeling how entities are framed our system allows end users to focus on a single article as well as analyze up to four articles simultaneously we provide aggregate level analysis including an intuitive graph visualization that highlights the narrative a group of articles are pushing our system includes a search feature for users to look up entities of interest along with a timeline view that allows analysts to track an entitys role transitions across different contexts within the article the franx system and the trained models are licensed under an mit license franx is publicly accessible at and a video demonstration is available at,0
standard topic models often struggle to capture culturally specific nuances in text this study evaluates the effectiveness of contextual embeddings for identifying culturally resonant themes in an underrepresented linguistic context we compare the performance of kmeans clustering latent dirichlet allocation lda and bertopic on a corpus of nearly 25000 daily personal narratives written in belgiandutch flemish while lda achieves strong performance on automated coherence metrics subsequent human evaluation reveals that bertopic consistently identifies the most coherent and culturally relevant topics highlighting the limitations of purely statistical methods on this narrativerich data furthermore the diminished performance of kmeans compared to prior work on similar dutch corpora underscores the unique linguistic challenges posed by personal narrative analysis our findings demonstrate the critical role of contextual embeddings in robust topic modeling and emphasize the need for humancentered evaluation particularly when working with lowresource languages and culturally specific domains,0
the development of bilingual dictionaries to medieval translations presents diverse difficulties these result from two types of philological circumstances a the asymmetry between the source language and the target language and b the varying available sources of both the original and translated texts in particular the full critical edition of tihova of constantine of preslavs uchitelnoe evangelie didactic gospel gives a relatively good idea of the old church slavonic translation but not of its greek source text this is due to the fact that cramers edition of the catenae used as the parallel text in it is based on several codices whose text does not fully coincide with the slavonic this leads to the addition of the newlydiscovered parallels from byzantine manuscripts and john chrysostoms homilies our approach to these issues is a stepwise process with two main goals a to facilitate the philological annotation of input data and b to consider the manifestations of the mentioned challenges first separately in order to simplify their resolution and then in their combination we demonstrate how we model various types of asymmetric translation correlates and the variability resulting from the pluralism of sources we also demonstrate how all these constructions are being modelled and processed into the final indices our approach is designed with generalisation in mind and is intended to be applicable also for other translations from greek into old church slavonic,0
in the middle ages texts were learned by heart and spread using oral means of communication from generation to generation adaptation of the art of prose and poems allowed keeping particular descriptions and compositions characteristic for many literary genres taking into account such a specific construction of literature composed in latin we can search for and indicate the probability patterns of familiar sources of specific narrative texts consideration of natural language processing tools allowed us the transformation of textual objects into numerical ones and then application of machine learning algorithms to extract information from the dataset we carried out the task consisting of the practical use of those concepts and observation to create a tool for analyzing narrative texts basing on opensource databases the tool focused on creating specific search tools resources which could enable us detailed searching throughout the text the main objectives of the study take into account finding similarities between sentences and between documents next we applied machine learning algorithms on chosen texts to calculate specific features of them for instance authorship or centuries and to recognize sources of anonymous texts with a certain percentage,0
we present a largescale computational analysis of migrationrelated discourse in uk parliamentary debates spanning over 75 years and compare it with us congressional discourse using openweight llms we annotate each statement with highlevel stances toward migrants and track the net tone toward migrants across time and political parties for the uk we extend this with a semiautomated framework for extracting finegrained narrative frames to capture nuances of migration discourse our findings show that while us discourse has grown increasingly polarised uk parliamentary attitudes remain relatively aligned across parties with a persistent ideological gap between labour and the conservatives reaching its most negative level in 2025 the analysis of narrative frames in the uk parliamentary statements reveals a shift toward securitised narratives such as border control and illegal immigration while longerterm integrationoriented frames such as social integration have declined moreover discussions of national law about immigration have been replaced over time by international law and human rights revealing nuances in discourse trends taken together broadly our findings demonstrate how llms can support scalable finegrained discourse analysis in political and historical contexts,0
in natural language using short sentences is considered efficient for communication however a text composed exclusively of such sentences looks technical and reads boring a text composed of long ones on the other hand demands significantly more effort for comprehension studying characteristics of the sentence length variability slv in a large corpus of worldfamous literary texts shows that an appealing and aesthetic optimum appears somewhere in between and involves selfsimilar cascadelike alternation of various lengths sentences a related quantitative observation is that the power spectra sf of thus characterized slv universally develop a convincing 1fbeta scaling with the average exponent beta 12 close to what has been identified before in musical compositions or in the brain waves an overwhelming majority of the studied texts simply obeys such fractal attributes but especially spectacular in this respect are hypertextlike stream of consciousness novels in addition they appear to develop structures characteristic of irreducibly interwoven sets of fractals called multifractals scaling of sf in the present context implies existence of the longrange correlations in texts and appearance of multifractality indicates that they carry even a nonlinear component a distinct role of the full stops in inducing the longrange correlations in texts is evidenced by the fact that the above quantitative characteristics on the longrange correlations manifest themselves in variation of the full stops recurrence times along texts thus in slv but to a much lesser degree in the recurrence times of the most frequent words in this latter case the nonlinear correlations thus multifractality disappear even completely for all the texts considered treated as one extra word the full stops at the same time appear to obey the zipfian rankfrequency distribution however,0
specific lexical choices in narrative text reflect both the writers attitudes towards people in the narrative and influence the audiences reactions prior work has examined descriptions of people in english using contextual affective analysis a natural language processing nlp technique that seeks to analyze how people are portrayed along dimensions of power agency and sentiment our work presents an extension of this methodology to multilingual settings which is enabled by a new corpus that we collect and a new multilingual model we additionally show how word connotations differ across languages and cultures highlighting the difficulty of generalizing existing english datasets and methods we then demonstrate the usefulness of our method by analyzing wikipedia biography pages of members of the lgbt community across three languages english russian and spanish our results show systematic differences in how the lgbt community is portrayed across languages surfacing cultural differences in narratives and signs of social biases practically this model can be used to identify wikipedia articles for further manual analysis articles that might contain content gaps or an imbalanced representation of particular social groups,0
in this study we employ a classification approach to show that different categories of literary quality display unique linguistic profiles leveraging a corpus that encompasses titles from the norton anthology penguin classics series and the open syllabus project contrasted against contemporary bestsellers nobel prize winners and recipients of prestigious literary awards our analysis reveals that canonical and so called highbrow texts exhibit distinct textual features when compared to other quality categories such as bestsellers and popular titles as well as to control groups likely responding to distinct but not mutually exclusive models of quality we apply a classic machine learning approach namely random forest to distinguish quality novels from control groups achieving up to 77 f1 scores in differentiating between the categories we find that quality category tend to be easier to distinguish from control groups than from other quality categories suggesting than literary quality features might be distinguishable but shared through quality proxies,0
in recent years researchers in the area of computational creativity have studied the human creative process proposing different approaches to reproduce it with a formal procedure in this paper we introduce a model for the generation of literary rhymes in spanish combining structures of language and neural network models textitword2vec into a structure for semantic assimilation the results obtained with a manual evaluation of the texts generated by our algorithm are encouraging,0
this article presents the results of a study involving the translation of a short story by kurt vonnegut from english to catalan and dutch using three modalities machinetranslation mt postediting pe and translation without aid ht our aim is to explore creativity understood to involve novelty and acceptability from a quantitative perspective the results show that ht has the highest creativity score followed by pe and lastly mt and this is unanimous from all reviewers a neural mt system trained on literary data does not currently have the necessary capabilities for a creative translation it renders literal solutions to translation problems more importantly using mt to postedit raw output constrains the creativity of translators resulting in a poorer translation often not fit for publication according to experts,0
person names and location names are essential building blocks for identifying events and social networks in historical documents that were written in literary chinese we take the lead to explore the research on algorithmically recognizing named entities in literary chinese for historical studies with languagemodel based and conditionalrandomfield based methods and extend our work to mining the document structures in historical documents practical evaluations were conducted with texts that were extracted from more than 220 volumes of local gazetteers difangzhi difangzhi is a huge and the single most important collection that contains information about officers who served in local government in chinese history our methods performed very well on these realistic tests thousands of names and addresses were identified from the texts a good portion of the extracted names match the biographical information currently recorded in the china biographical database cbdb of harvard university and many others can be verified by historians and will become as new additions to cbdb,0
establishing authorship of online texts is fundamental to combat cybercrimes unfortunately text length is limited on some platforms making the challenge harder we aim at identifying the authorship of twitter messages limited to 140 characters we evaluate popular stylometric features widely used in literary analysis and specific twitter features like urls hashtags replies or quotes we use two databases with 93 and 3957 authors respectively we test varying sized author sets and varying amounts of trainingtest texts per author performance is further improved by feature combination via automatic selection with a large number of training tweets 500 a good accuracy rank580 is achievable with only a few dozens of test tweets even with several thousands of authors with smaller sample sizes 1020 training tweets the search space can be diminished by 915 while keeping a high chance that the correct author is retrieved among the candidates in such cases automatic attribution can provide significant time savings to experts in suspect search for completeness we report verification results with few trainingtest tweets the eer is above 2025 which is reduced to 15 if hundreds of training tweets are available we also quantify the computational complexity and time permanence of the employed features,0
cancer treatments are known to introduce cardiotoxicity negatively impacting outcomes and survivorship identifying cancer patients at risk of heart failure hf is critical to improving cancer treatment outcomes and safety this study examined machine learning ml models to identify cancer patients at risk of hf using electronic health records ehrs including traditional ml timeaware long shortterm memory tlstm and large language models llms using novel narrative features derived from the structured medical codes we identified a cancer cohort of 12806 patients from the university of florida health diagnosed with lung breast and colorectal cancers among which 1602 individuals developed hf after cancer the llm gatortron39b achieved the best f1 scores outperforming the traditional support vector machines by 39 the tlstm deep learning model by 7 and a widely used transformer model bert by 56 the analysis shows that the proposed narrative features remarkably increased feature density and improved performance,0
in the context of the rapid development of large language models we have meticulously trained and introduced the gujibert and gujigpt language models which are foundational models specifically designed for intelligent information processing of ancient texts these models have been trained on an extensive dataset that encompasses both simplified and traditional chinese characters allowing them to effectively handle various natural language processing tasks related to ancient books including but not limited to automatic sentence segmentation punctuation word segmentation partofspeech tagging entity recognition and automatic translation notably these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets our research findings highlight the efficacy of employing selfsupervised methods to further train the models using classical text corpora thus enhancing their capability to tackle downstream tasks moreover it is worth emphasizing that the choice of font the scale of the corpus and the initial model selection all exert significant influence over the ultimate experimental outcomes to cater to the diverse text processing preferences of researchers in digital humanities and linguistics we have developed three distinct categories comprising a total of nine model variations we believe that by sharing these foundational language models specialized in the domain of ancient texts we can facilitate the intelligent processing and scholarly exploration of ancient literary works and consequently contribute to the global dissemination of chinas rich and esteemed traditional culture in this new era,0
we introduce internlmxcomposer2 a cuttingedge visionlanguage model excelling in freeform textimage composition and comprehension this model goes beyond conventional visionlanguage understanding adeptly crafting interleaved textimage content from diverse inputs like outlines detailed textual specifications and reference images enabling highly customizable content creation internlmxcomposer2 proposes a partial lora plora approach that applies additional lora parameters exclusively to image tokens to preserve the integrity of pretrained language knowledge striking a balance between precise vision understanding and text composition with literary talent experimental results demonstrate the superiority of internlmxcomposer2 based on internlm27b in producing highquality longtext multimodal content and its exceptional visionlanguage understanding performance across various benchmarks where it not only significantly outperforms existing multimodal models but also matches or even surpasses gpt4v and gemini pro in certain assessments this highlights its remarkable proficiency in the realm of multimodal understanding the internlmxcomposer2 model series with 7b parameters are publicly available at,0
popular media reflects and reinforces societal biases through the use of tropes which are narrative elements such as archetypal characters and plot arcs that occur frequently across media in this paper we specifically investigate gender bias within a large collection of tropes to enable our study we crawl tvtropesorg an online usercreated repository that contains 30k tropes associated with 19m examples of their occurrences across film television and literature we automatically score the genderedness of each trope in our tvtropes dataset which enables an analysis of 1 highlygendered topics within tropes 2 the relationship between gender bias and popular reception and 3 how the gender of a works creator correlates with the types of tropes that they use,0
as digital technology advances the proliferation of connected devices poses significant challenges and opportunities in mobile crowdsourcing and edge computing this narrative review focuses on the need for privacy protection in these fields emphasizing the increasing importance of data security in a datadriven world through an analysis of contemporary academic literature this review provides an understanding of the current trends and privacy concerns in mobile crowdsourcing and edge computing we present insights and highlight advancements in privacypreserving techniques addressing identity data and location privacy this review also discusses the potential directions that can be useful resources for researchers industry professionals and policymakers,0
clinician burnout poses a substantial threat to patient safety particularly in highacuity intensive care units icus existing research predominantly relies on retrospective survey tools or broad electronic health record ehr metadata often overlooking the valuable narrative information embedded in clinical notes in this study we analyze 10000 icu discharge summaries from mimiciv a publicly available database derived from the electronic health records of beth israel deaconess medical center the dataset encompasses diverse patient data including vital signs medical orders diagnoses procedures treatments and deidentified freetext clinical notes we introduce a hybrid pipeline that combines biobert sentiment embeddings finetuned for clinical narratives a lexical stress lexicon tailored for clinician burnout surveillance and fivetopic latent dirichlet allocation lda with workload proxies a providerlevel logistic regression classifier achieves a precision of 080 a recall of 089 and an f1 score of 084 on a stratified holdout set surpassing metadataonly baselines by greater than or equal to 017 f1 score specialtyspecific analysis indicates elevated burnout risk among providers in radiology psychiatry and neurology our findings demonstrate that icu clinical narratives contain actionable signals for proactive wellbeing monitoring,0
we show that the laws of autocorrelations decay in texts are closely related to applicability limits of language models using distributional semantics we empirically demonstrate that autocorrelations of words in texts decay according to a power law we show that distributional semantics provides coherent autocorrelations decay exponents for texts translated to multiple languages the autocorrelations decay in generated texts is quantitatively and often qualitatively different from the literary texts we conclude that language models exhibiting markov behavior including large autoregressive language models may have limitations when applied to long texts whether analysis or generation,0
the automatic extraction of character networks from literary texts is generally carried out using natural language processing nlp cascading pipelines while this approach is widespread no study exists on the impact of lowlevel nlp tasks on their performance in this article we conduct such a study on a literary dataset focusing on the role of named entity recognition ner and coreference resolution when extracting cooccurrence networks to highlight the impact of these tasks performance we start with goldstandard annotations progressively add uniformly distributed errors and observe their impact in terms of character network quality we demonstrate that ner performance depends on the tested novel and strongly affects character detection we also show that nerdetected mentions alone miss a lot of character cooccurrences and that coreference resolution is needed to prevent this finally we present comparison points with 2 methods based on large language models llms including a fully endtoend one and show that these models are outperformed by traditional nlp pipelines in terms of recall,0
analyzing literature involves tracking interactions between characters locations and themes visualization has the potential to facilitate the mapping and analysis of these complex relationships but capturing structured information from unstructured story data remains a challenge as large language models llms continue to advance we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques toward this goal we introduce an llmdriven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts we then apply this pipeline to create story ribbons an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels through pipeline evaluations and user studies with story ribbons on 36 literary works we demonstrate the potential of llms to streamline narrative visualization creation and reveal new insights about familiar stories we also describe current limitations of aibased systems and interaction motifs designed to address these issues,0
we study temporal networks of characters in literature focusing on alices adventures in wonderland 1865 by lewis carroll and the anonymous la chanson de roland around 1100 the former one of the most influential pieces of nonsense literature ever written describes the adventures of alice in a fantasy world with logic plays interspersed along the narrative the latter a song of heroic deeds depicts the battle of roncevaux in 778 ad during charlemagnes campaign on the iberian peninsula we apply methods recently developed by taylor and coworkers citetaylor2015 to find timeaveraged eigenvector centralities freeman indices and vitalities of characters we show that temporal networks are more appropriate than static ones for studying stories as they capture features that the timeindependent approaches fail to yield,0
we investigate whether large language models llms can successfully perform financial statement analysis in a way similar to a professional human analyst we provide standardized and anonymous financial statements to gpt4 and instruct the model to analyze them to determine the direction of firms future earnings even without narrative or industryspecific information the llm outperforms financial analysts in its ability to predict earnings changes directionally the llm exhibits a relative advantage over human analysts in situations when the analysts tend to struggle furthermore we find that the prediction accuracy of the llm is on par with a narrowly trained stateoftheart ml model llm prediction does not stem from its training memory instead we find that the llm generates useful narrative insights about a companys future performance lastly our trading strategies based on gpts predictions yield a higher sharpe ratio and alphas than strategies based on other models our results suggest that llms may take a central role in analysis and decisionmaking,0
the rapid growth of online video content especially on short video platforms has created a growing demand for efficient video editing techniques that can condense longform videos into concise and engaging clips existing automatic editing methods predominantly rely on textual cues from asr transcripts and endtoend segment selection often neglecting the rich visual context and leading to incoherent outputs in this paper we propose a humaninspired automatic video editing framework hive that leverages multimodal narrative understanding to address these limitations our approach incorporates character extraction dialogue analysis and narrative summarization through multimodal large language models enabling a holistic understanding of the video content to further enhance coherence we apply scenelevel segmentation and decompose the editing process into three subtasks highlight detection openingending selection and pruning of irrelevant content to facilitate research in this area we introduce dramaad a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisementoriented editing tasks significantly narrowing the quality gap between automatic and humanedited videos,0
this paper describes the quantitative criticism lab a collaborative initiative between classicists quantitative biologists and computer scientists to apply ideas and methods drawn from the sciences to the study of literature a core goal of the project is the use of computational biology natural language processing and machine learning techniques to investigate authorial style intertextuality and related phenomena of literary significance as a case study in our approach here we review the use of sequence alignment a common technique in genomics and computational linguistics to detect intertextuality in latin literature sequence alignment is distinguished by its ability to find inexact verbal similarities which makes it ideal for identifying phonetic echoes in large corpora of latin texts although especially suited to latin sequence alignment in principle can be extended to many other languages,0
lowerandmiddle income countries are faced with challenges arising from a lack of data on cause of death cod which can limit decisions on population health and disease management a verbal autopsyva can provide information about a cod in areas without robust death registration systems a va consists of structured data combining numeric and binary features and unstructured data as part of an openended narrative text this study assesses the performance of various machine learning approaches when analyzing both the structured and unstructured components of the va report the algorithms were trained and tested via crossvalidation in the three settings of binary features text features and a combination of binary and text features derived from va reports from rural south africa the results obtained indicate narrative text features contain valuable information for determining cod and that a combination of binary and text features improves the automated cod classification task keywords diabetes mellitus verbal autopsy cause of death machine learning natural language processing,0
large language models llms have shown promising results in a variety of literary tasks often using complex memorized details of narration and fictional characters in this work we evaluate the ability of llama3 at attributing utterances of directspeech to their speaker in novels the llm shows impressive results on a corpus of 28 novels surpassing published results with chatgpt and encoderbased baselines by a large margin we then validate these results by assessing the impact of book memorization and annotation contamination we found that these types of memorization do not explain the large performance gain making llama3 the new stateoftheart for quotation attribution in english literature we release publicly our code and data,0
we suggest a new nlg task in the context of the discourse generation pipeline of computational storytelling systems this task textual embellishment is defined by taking a text as input and generating a semantically equivalent output with increased lexical and syntactic complexity ideally this would allow the authors of computational storytellers to implement just lightweight nlg systems and use a domainindependent embellishment module to translate its output into more literary text we present promising first results on this task using lstm encoderdecoder networks trained on the wikilarge dataset furthermore we introduce compiled computer tales a corpus of computationally generated stories that can be used to test the capabilities of embellishment algorithms,0
personal narratives are stories authors construct to make meaning of their experiences style the distinctive way authors use language to express themselves is fundamental to how these narratives convey subjective experiences yet there is a lack of a formal framework for systematically analyzing these stylistic choices we present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences our framework integrates three domains functional linguistics establishes language as a system of meaningful choices computer science provides methods for automatically extracting and analyzing sequential patterns and these patterns are linked to psychological observations using language models we automatically extract linguistic features such as processes participants and circumstances we apply our framework to hundreds of dream narratives including a case study on a war veteran with posttraumatic stress disorder analysis of his narratives uncovers distinctive patterns particularly how verbal processes dominate over mental ones illustrating the relationship between linguistic choices and psychological states,0
freetext crash narratives recorded in realworld crash databases have been shown to play a significant role in improving traffic safety however largescale analyses remain difficult to implement as there are no documented tools that can batch process the unstructured non standardized text content written by various authors with diverse experience and attention to detail in recent years transformerbased pretrained language models plms such as bidirectional encoder representations from transformers bert and large language models llms have demonstrated strong capabilities across various natural language processing tasks these models can extract explicit facts from crash narratives but their performance declines on inferenceheavy tasks in for example crash type identification which can involve nearly 100 categories moreover relying on closed llms through external apis raises privacy concerns for sensitive crash data additionally these blackbox tools often underperform due to limited domain knowledge motivated by these challenges we study whether compact opensource plms can support reasoningintensive extraction from crash narratives we target two challenging objectives 1 identifying the manner of collision for a crash and 2 crash type for each vehicle involved in the crash event from realworld crash narratives to bridge domain gaps we apply finetuning techniques to inject taskspecific knowledge to llms with lowrank adaption lora and bert experiments on the authoritative realworld dataset crash investigation sampling system ciss demonstrate that our finetuned compact models outperform strong closed llms such as gpt4o while requiring only minimal training resources further analysis reveals that the finetuned plms can capture richer narrative details and even correct some mislabeled annotations in the dataset,0
we analyze the ritimexes in temporally annotated corpora and propose two hypotheses regarding the normalization of ritimexes in the clinical narrative domain the anchor point hypothesis and the anchor relation hypothesis we annotate the ritimexes in three corpora to study the characteristics of ritmexes in different domains this informed the design of our ritimex normalization system for the clinical domain which consists of an anchor point classifier an anchor relation classifier and a rulebased ritimex text span parser we experiment with different feature sets and perform error analysis for each system component the annotation confirmed the hypotheses that we can simplify the ritimexes normalization task using two multilabel classifiers our system achieves anchor point classification anchor relation classification and rulebased parsing accuracy of 7468 8771 and 572 8209 under relaxed matching criteria respectively on the heldout test set of the 2012 i2b2 temporal relation challenge experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the ritimex error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect we formulate the ritimex normalization problem as a pair of multilabel classification problems considering only the ritimex extraction and normalization the system achieves statistically significant improvement over the ritimex results of the best systems in the 2012 i2b2 challenge,0
we consider the problem of embedding characterentity relationships from the reduced semantic space of narratives proposing and evaluating the assumption that these relationships hold under a reflection operation we analyze this assumption and compare the approach to a baseline stateoftheart model with a unique evaluation that simulates efficacy on a downstream clustering task with humancreated labels although our model creates clusters that achieve silhouette scores of 084 outperforming the baseline 227 our analysis reveals that the models approach the task much differently and perform well on very different examples we conclude that our assumption might be useful for specific types of data and should be evaluated on a wider range of tasks,0
large pretrained language models lms have demonstrated impressive capabilities in generating long fluent text however there is little to no analysis on their ability to maintain entity coherence and consistency in this work we focus on the end task of narrative generation and systematically analyse the longrange entity coherence and consistency in generated stories first we propose a set of automatic metrics for measuring model performance in terms of entity usage given these metrics we quantify the limitations of current lms next we propose augmenting a pretrained lm with a dynamic entity memory in an endtoend manner by using an auxiliary entityrelated loss for guiding the reads and writes to the memory we demonstrate that the dynamic entity memory increases entity coherence according to both automatic and human judgment and helps preserving entityrelated information especially in settings with a limited context window finally we also validate that our automatic metrics are correlated with human ratings and serve as a good indicator of the quality of generated stories,0
characters are at the heart of every story driving the plot and engaging readers in this study we explore the understanding of characters in fulllength books which contain complex narratives and numerous interacting characters we define two tasks character description which generates a brief factual profile and character analysis which offers an indepth interpretation including character development personality and social context we introduce the bookworm dataset pairing books from the gutenberg project with humanwritten descriptions and analyses using this dataset we evaluate stateoftheart longcontext models in zeroshot and finetuning settings utilizing both retrievalbased and hierarchical processing for booklength inputs our findings show that retrievalbased approaches outperform hierarchical ones in both tasks additionally finetuned models using coreferencebased retrieval produce the most factual descriptions as measured by fact and entailmentbased metrics we hope our dataset experiments and analysis will inspire further research in characterbased narrative understanding,0
since 2006 we have undertaken to describe the differences between 17th century english and contemporary english thanks to nlp software studying a corpus spanning the whole century tales of english travellers in the ottoman empire in the 17th century mary astells essay a serious proposal to the ladies and other literary texts has enabled us to highlight various lexical morphological or grammatical singularities thanks to the nooj linguistic platform we created dictionaries indexing the lexical variants and their transcription in ce the latter is often the result of the validation of forms recognized dynamically by morphological graphs we also built syntactical graphs aimed at transcribing certain archaic forms in contemporary english our previous research implied a succession of elementary steps alternating textual analysis and result validation we managed to provide examples of transcriptions but we have not created a global tool for automatic transcription therefore we need to focus on the results we have obtained so far study the conditions for creating such a tool and analyze possible difficulties in this paper we will be discussing the technical and linguistic aspects we have not yet covered in our previous work we are using the results of previous research and proposing a transcription method for words or sequences identified as archaic,0
we perform a mixedmethod frame semanticsbased analysis on a dataset of more than 49000 sentences collected from 5846 news articles that mention ai the dataset covers the twelvemonth period centred around the launch of openais chatbot chatgpt and is collected from the most visited openaccess englishlanguage news publishers our findings indicate that during the six months succeeding the launch media attention rose tenfoldunicodex2014from already historically high levels during this period discourse has become increasingly centred around experts and political leaders and ai has become more closely associated with dangers and risks a deeper review of the data also suggests a qualitative shift in the types of threat ai is thought to represent as well as the anthropomorphic qualities ascribed to it,0
the ability to transmit and receive complex information via language is unique to humans and is the basis of traditions culture and versatile social interactions through the disruptive introduction of transformer based large language models llms humans are not the only entity to understand and produce language any more in the present study we have performed the first steps to use llms as a model to understand fundamental mechanisms of language processing in neural networks in order to make predictions and generate hypotheses on how the human brain does language processing thus we have used chatgpt to generate seven different stylistic variations of ten different narratives aesops fables we used these stories as input for the open source llm bert and have analyzed the activation patterns of the hidden units of bert using multidimensional scaling and cluster analysis we found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of bert 1 than narrative content 45 despite the fact that bert consists of 12 identical building blocks that are stacked and trained on large text corpora the different layers perform different tasks this is a very useful model of the human brain where selfsimilar structures ie different areas of the cerebral cortex can have different functions and are therefore well suited to processing language in a very efficient way the proposed approach has the potential to open the black box of llms on the one hand and might be a further step to unravel the neural processes underlying human language processing and cognition in general,0
artistic pieces can be studied from several perspectives one example being their reception among readers over time in the present work we approach this interesting topic from the standpoint of literary works particularly assessing the task of predicting whether a book will become a best seller dissimilarly from previous approaches we focused on the full content of books and considered visualization and classification tasks we employed visualization for the preliminary exploration of the data structure and properties involving semaxis and linear discriminant analyses then to obtain quantitative and more objective results we employed various classifiers such approaches were used along with a dataset containing i books published from 1895 to 1924 and consecrated as best sellers by the publishers weekly bestseller lists and ii literary works published in the same period but not being mentioned in that list our comparison of methods revealed that the bestachieved result combining a bagofwords representation with a logistic regression classifier led to an average accuracy of 075 both for the leaveoneout and 10fold crossvalidations such an outcome suggests that it is unfeasible to predict the success of books with high accuracy using only the full content of the texts nevertheless our findings provide insights into the factors leading to the relative success of a literary work,0
conversations contain a wide spectrum of multimodal information that gives us hints about the emotions and moods of the speaker in this paper we developed a system that supports humans to analyze conversations our main contribution is the identification of appropriate multimodal features and the integration of such features into verbatim conversation transcripts we demonstrate the ability of our system to take in a wide range of multimodal information and automatically generated a prediction score for the depression state of the individual our experiments showed that this approach yielded better performance than the baseline model furthermore the multimodal narrative approach makes it easy to integrate learnings from other disciplines such as conversational analysis and psychology lastly this interdisciplinary and automated approach is a step towards emulating how practitioners record the course of treatment as well as emulating how conversational analysts have been analyzing conversations by hand,0
large language models llms are competitive with the state of the art on a wide range of sentencelevel translation datasets however their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult we show through a rigorous human evaluation that asking the gpt35 textdavinci003 llm to translate an entire literary paragraph eg from a novel at once results in higherquality translations than standard sentencebysentence translation across 18 linguisticallydiverse language pairs eg translating into and out of japanese polish and english our evaluation which took approximately 350 hours of effort for annotation and analysis is conducted by hiring translators fluent in both the source and target language and asking them to provide both spanlevel error annotations as well as preference judgments of which systems translations are better we observe that discourselevel llm translators commit fewer mistranslations grammar errors and stylistic inconsistencies than sentencelevel approaches with that said critical errors still abound including occasional content omissions and a human translators intervention remains necessary to ensure that the authors voice remains intact we publicly release our dataset and error annotations to spur future research on evaluation of documentlevel literary translation,0
while a source sentence can be translated in many ways most machine translation mt models are trained with only a single reference previous work has shown that using synthetic paraphrases can improve mt this paper investigates best practices for employing multiple references by analyzing the semantic similarity among different english translations of world literature in the par3 dataset we classify the semantic similarity between paraphrases into three levels low medium and high and finetune three different models mt5large llama27b and opusmt for literary mt tasks across different models holding the total training instances constant singlereference but more source texts only marginally outperforms multiplereference with half of the source texts moreover when finetuning an llm using paraphrases with medium and high semantic similarity outperforms an unfiltered dataset with improvements in bleu 0305 comet 0109 and chrf 017032 our code is publicly available on github,0
large neural language models trained on massive amounts of text have emerged as a formidable strategy for natural language understanding tasks however the strength of these models as natural language generators is less clear though anecdotal evidence suggests that these models generate better quality text there has been no detailed study characterizing their generation abilities in this work we compare the performance of an extensively pretrained model openai gpt2117 radford et al 2019 to a stateoftheart neural story generation model fan et al 2018 by evaluating the generated text across a wide variety of automatic metrics we characterize the ways in which pretrained models do and do not make better storytellers we find that although gpt2117 conditions more strongly on context is more sensitive to ordering of events and uses more unusual words it is just as likely to produce repetitive and underdiverse text when using likelihoodmaximizing decoding algorithms,0
road crashes claim over 13 million lives annually worldwide and incur global economic losses exceeding 18 trillion such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights conventional statistical models and tree ensemble approaches typically rely on structured crash data overlooking contextual nuances and struggling to capture complex relationships and underlying semantics moreover these approaches tend to incur significant information loss particularly in narrative elements related to multivehicle interactions crash progression and rare event characteristics this study presents crashsage a novel large language model llmcentered framework designed to advance crash analysis and modeling through four key innovations first we introduce a tabulartotext transformation strategy paired with relational data integration schema enabling the conversion of raw heterogeneous crash data into enriched structured textual narratives that retain essential structural and relational context second we apply contextaware data augmentation using a base llm model to improve narrative coherence while preserving factual integrity third we finetune the llama38b model for crash severity inference demonstrating superior performance over baseline approaches including zeroshot zeroshot with chainofthought prompting and fewshot learning with multiple models gpt4o gpt4omini llama370b finally we employ a gradientbased explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions this interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors,0
aviation safety is a global concern requiring detailed investigations into incidents to understand contributing factors comprehensively this study uses the national transportation safety board ntsb dataset it applies advanced natural language processing nlp techniques including latent dirichlet allocation lda nonnegative matrix factorization nmf latent semantic analysis lsa probabilistic latent semantic analysis plsa and kmeans clustering the main objectives are identifying latent themes exploring semantic relationships assessing probabilistic connections and cluster incidents based on shared characteristics this research contributes to aviation safety by providing insights into incident narratives and demonstrating the versatility of nlp and topic modelling techniques in extracting valuable information from complex datasets the results including topics identified from various techniques provide an understanding of recurring themes comparative analysis reveals that lda performed best with a coherence value of 0597 plsa of 0583 lsa of 0542 and nmf of 0437 kmeans clustering further reveals commonalities and unique insights into incident narratives in conclusion this study uncovers latent patterns and thematic structures within incident narratives offering a comparative analysis of multipletopic modelling techniques future research avenues include exploring temporal patterns incorporating additional datasets and developing predictive models for early identification of safety issues this research lays the groundwork for enhancing the understanding and improvement of aviation safety by utilising the wealth of information embedded in incident narratives,0
in this study we propose a methodology to extract index and visualize climate change narratives stories about the connection between causal and consequential events related to climate change we use two natural language processing methods bert bidirectional encoder representations from transformers and causal extraction to textually analyze newspaper articles on climate change to extract climate change narratives the novelty of the methodology could extract and quantify the causal relationships assumed by the newspapers writers looking at the extracted climate change narratives over time we find that since 2018 an increasing number of narratives suggest the impact of the development of climate change policy discussion and the implementation of climate changerelated policies on corporate behaviors macroeconomics and price dynamics we also observed the recent emergence of narratives focusing on the linkages between climate changerelated policies and monetary policy furthermore there is a growing awareness of the negative impacts of natural disasters eg abnormal weather and severe floods related to climate change on economic activities and this issue might be perceived as a new challenge for companies and governments the methodology of this study is expected to be applied to a wide range of fields as it can analyze causal relationships among various economic topics including analysis of inflation expectation or monetary policy communication strategy,0
this paper proposes a novel approach to evaluate counter narrative cn generation using a large language model llm as an evaluator we show that traditional automatic metrics correlate poorly with human judgements and fail to capture the nuanced relationship between generated cns and human perception to alleviate this we introduce a model ranking pipeline based on pairwise comparisons of generated cns from different models organized in a tournamentstyle format the proposed evaluation method achieves a high correlation with human preference with a  score of 088 as an additional contribution we leverage llms as zeroshot cn generators and provide a comparative analysis of chat instruct and base models exploring their respective strengths and limitations through meticulous evaluation including finetuning experiments we elucidate the differences in performance and responsiveness to domainspecific data we conclude that chataligned models in zeroshot are the best option for carrying out the task provided they do not refuse to generate an answer due to security concerns,0
electronic medical reports ehr contain a vast amount of information that can be leveraged for machine learning applications in healthcare however existing survival analysis methods often struggle to effectively handle the complexity of textual data particularly in its sequential form here we propose sigbert an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient sigbert processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings to capture temporal dynamics from the time series of sentence embedding coordinates we apply signature extraction from rough path theory to derive geometric features for each patient which significantly enhance survival model performance by capturing complex temporal dynamics these features are then integrated into a lassopenalized cox model to estimate patientspecific risk scores the model was trained and evaluated on a realworld oncology dataset from the lon brard center corpus with a cindex score of 075 sd 0014 on the independent test cohort sigbert integrates sequential medical data to enhance risk estimation advancing narrativebased survival analysis,0
current vision language models vlms demonstrate a critical gap between surfacelevel recognition and deep narrative reasoning when processing sequential visual storytelling through a comprehensive investigation of manga narrative understanding we reveal that while recent large multimodal models excel at individual panel interpretation they systematically fail at temporal causality and crosspanel cohesion core requirements for coherent story comprehension we introduce a novel evaluation framework that combines finegrained multimodal annotation crossmodal embedding analysis and retrievalaugmented assessment to systematically characterize these limitations our methodology includes i a rigorous annotation protocol linking visual elements to narrative structure through aligned light novel text ii comprehensive evaluation across multiple reasoning paradigms including direct inference and retrievalaugmented generation and iii crossmodal similarity analysis revealing fundamental misalignments in current vlms joint representations applying this framework to rezero manga across 11 chapters with 308 annotated panels we conduct the first systematic study of longform narrative understanding in vlms through three core evaluation axes generative storytelling contextual dialogue grounding and temporal reasoning our findings demonstrate that current models lack genuine storylevel intelligence struggling particularly with nonlinear narratives character consistency and causal inference across extended sequences this work establishes both the foundation and practical methodology for evaluating narrative intelligence while providing actionable insights into the capability of deep sequential understanding of discrete visual narratives beyond basic recognition in multimodal models project page,0
diplomatic events consistently prompt widespread public discussion and debate public sentiment plays a critical role in diplomacy as a good sentiment provides vital support for policy implementation helps resolve international issues and shapes a nations international image traditional methods for gauging public sentiment such as largescale surveys or manual content analysis of media are typically timeconsuming laborintensive and lack the capacity for forwardlooking analysis we propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive first we train a language model to predict public reaction towards diplomatic events to this end we construct a dataset comprising descriptions of diplomatic events and their associated public discussions second guided by communication theories and in collaboration with domain experts we predetermined several textual features for modification ensuring that any alterations changed the events narrative framing while preserving its core factswe develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text the results show that this framework successfully shifted public sentiment to a more favorable state with a 70 success rate this framework can therefore serve as a practical tool for diplomats policymakers and communication specialists offering datadriven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment,0
the present study is focused on the automatic identification and description of frozen similes in british and french novels written between the 19 th century and the beginning of the 20 th century two main patterns of frozen similes were considered adjectival ground simile marker nominal vehicle eg happy as a lark and eventuality simile marker nominal vehicle eg sleep like a top all potential similes and their components were first extracted using a rulebased algorithm then frozen similes were identified based on reference lists of existing similes and semantic distance between the tenor and the vehicle the results obtained tend to confirm the fact that frozen similes are not used haphazardly in literary texts in addition contrary to how they are often presented frozen similes often go beyond the ground or the eventuality and the vehicle to also include the tenor,0
large language models llms have been shown to encode clinical knowledge many evaluations however rely on structured questionanswer benchmarks overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in realworld settings using freetext clinical descriptions we present semiollm an evaluation framework that benchmarks 6 stateoftheart models gpt35 gpt4 mixtral8x7b qwen72b llama2 llama3 on a core diagnostic task in epilepsy leveraging a database of 1269 seizure descriptions we show that most llms are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain most models approach clinicianlevel performance after prompt engineering with expertguided chainofthought reasoning leading to the most consistent improvements performance was further strongly modulated by clinical incontext impersonation narrative length and language context 137 327 and 142 performance variation respectively however expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy underscoring the need to improve interpretability of llms in clinical use overall semiollm provides a scalable domainadaptable framework for evaluating llms in clinical disciplines where unstructured verbal descriptions encode diagnostic information by identifying both the strengths and limitations of stateoftheart models our work supports the development of clinically robust and globally applicable ai systems for healthcare,0
current large language models llms struggle to answer questions that span tens of thousands of tokens especially when multihop reasoning is involved while prior benchmarks explore longcontext comprehension or multihop reasoning in isolation none jointly vary context length and reasoning depth in natural narrative settings we introduce novelhopqa the first benchmark to evaluate 14 hop qa over 64k128ktoken excerpts from 83 fulllength publicdomain novels a keywordguided pipeline builds hopseparated chains grounded in coherent storylines we evaluate seven stateoftheart models and apply oraclecontext filtering to ensure all questions are genuinely answerable human annotators validate both alignment and hop depth we additionally present retrievalaugmented generation rag evaluations to test model performance when only selected passages are provided instead of the full context we noticed consistent accuracy drops with increased hops and context length increase even for frontier modelsrevealing that sheer scale does not guarantee robust reasoning failuremode analysis highlights common breakdowns such as missed finalhop integration and longrange drift novelhopqa offers a controlled diagnostic setting to test multihop reasoning at scale all code and datasets are available at,0
our daytoday life has always been influenced by what people think ideas and opinions of others have always affected our own opinions the explosion of web 20 has led to increased activity in podcasting blogging tagging contributing to rss social bookmarking and social networking as a result there has been an eruption of interest in people to mine these vast resources of data for opinions sentiment analysis or opinion mining is the computational treatment of opinions sentiments and subjectivity of text in this report we take a look at the various challenges and applications of sentiment analysis we will discuss in details various approaches to perform a computational treatment of sentiments and opinions various supervised or datadriven techniques to sa like nave byes maximum entropy svm and voted perceptrons will be discussed and their strengths and drawbacks will be touched upon we will also see a new dimension of analyzing sentiments by cognitive psychology mainly through the work of janyce wiebe where we will see ways to detect subjectivity perspective in narrative and understanding the discourse structure we will also study some specific topics in sentiment analysis and the contemporary works in those areas,0
understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts we consider the information retrieval problem of detecting and ranking the relevant words of a text by means of statistical information referring to the spatial use of the words shannons entropy of information is used as a tool for automatic keyword extraction by using the origin of species by charles darwin as a representative text sample we show the performance of our detector and compare it with another proposals in the literature the random shuffled text receives special attention as a tool for calibrating the ranking indices,0
robustly evaluating the longform storytelling capabilities of large language models llms remains a significant challenge as existing benchmarks often lack the necessary scale diversity or objective measures to address this we introduce webnovelbench a novel benchmark specifically designed for evaluating longform novel generation webnovelbench leverages a largescale dataset of over 4000 chinese web novels framing evaluation as a synopsistostory generation task we propose a multifaceted framework encompassing eight narrative quality dimensions assessed automatically via an llmasjudge approach scores are aggregated using principal component analysis and mapped to a percentile rank against humanauthored works our experiments demonstrate that webnovelbench effectively differentiates between humanwritten masterpieces popular web novels and llmgenerated content we provide a comprehensive analysis of 24 stateoftheart llms ranking their storytelling abilities and offering insights for future development this benchmark provides a scalable replicable and datadriven methodology for assessing and advancing llmdriven narrative generation,0
adversarial information operations can destabilize societies by undermining fair elections manipulating public opinions on policies and promoting scams despite their widespread occurrence and potential impacts our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior in this paper we explore whether these limitations can be mitigated with large language models llms using gpt35 as a casestudy for coordinated campaign annotation we first use gpt35 to scrutinize 126 identified information operations spanning over a decade we utilize a number of metrics to quantify the close if imperfect agreement between llm and ground truth descriptions we next extract coordinated campaigns from two large multilingual datasets from x formerly twitter that respectively discuss the 2022 french election and 2023 balikaran philippineus military exercise in 2023 for each coordinated campaign we use gpt35 to analyze posts related to a specific concern and extract goals tactics and narrative frames both before and after critical events such as the date of an election while the gpt35 sometimes disagrees with subjective interpretation its ability to summarize and interpret demonstrates llms potential to extract higherorder indicators from text to provide a more complete picture of the information campaigns compared to previous methods,0
detective fiction a genre defined by its complex narrative structures and characterdriven storytelling presents unique challenges for computational narratology a research field focused on integrating literary theory into automated narrative generation while traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods in this paper we present an aidriven approach for systematically characterizing the investigative methods of fictional detectives our multiphase workflow explores the capabilities of 15 large language models llms to extract synthesize and validate distinctive investigative traits of fictional detectives this approach was tested on a diverse set of seven iconic detectives hercule poirot sherlock holmes william murdoch columbo father brown miss marple and auguste dupin capturing the distinctive investigative styles that define each character the identified traits were validated against existing literary analyses and further tested in a reverse identification phase achieving an overall accuracy of 9143 demonstrating the methods effectiveness in capturing the distinctive investigative approaches of each detective this work contributes to the broader field of computational narratology by providing a scalable framework for character analysis with potential applications in aidriven interactive storytelling and automated narrative generation,0
this study investigates the spread of conspiracy theories in arabic digital spaces through computational analysis of online content by combining named entity recognition and topic modeling techniques specifically the top2vec algorithm we analyze data from arabic blogs and facebook to identify and classify conspiratorial narratives our analysis uncovers six distinct categories genderfeminist geopolitical government coverups apocalyptic judeomasonic and geoengineering the research highlights how these narratives are deeply embedded in arabic social media discourse shaped by regional historical cultural and sociopolitical contexts by applying advanced natural language processing methods to arabic content this study addresses a gap in conspiracy theory research which has traditionally focused on englishlanguage content or offline data the findings provide new insights into the manifestation and evolution of conspiracy theories in arabic digital spaces enhancing our understanding of their role in shaping public discourse in the arab world,0
large language models llms have been shown to demonstrate imbalanced biases against certain groups however the study of unprovoked targeted attacks by llms towards atrisk populations remains underexplored our paper presents three novel contributions 1 the explicit evaluation of llmgenerated attacks on highly vulnerable mental health groups 2 a networkbased framework to study the propagation of relative biases and 3 an assessment of the relative degree of stigmatization that emerges from these attacks our analysis of a recently released largescale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks as revealed by a significantly higher mean centrality of closeness pvalue 406e10 and dense clustering gini coefficient 07 drawing from sociological foundations of stigmatization theory our stigmatization analysis indicates increased labeling components for mental health disorderrelated targets relative to initial targets in generation chains taken together these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation,0
evaluating the creative capabilities of large language models llms in complex tasks often requires human assessments that are difficult to scale we introduce a novel scalable methodology for evaluating llm story generation by analyzing underlying social structures in narratives as signed character networks to demonstrate its effectiveness we conduct a largescale comparative analysis using networks from over 1200 stories generated by four leading llms gpt4o gpt4o mini gemini 15 pro and gemini 15 flash and a humanwritten corpus our findings based on network properties like density clustering and signed edge weights show that llmgenerated stories consistently exhibit a strong bias toward tightlyknit positive relationships which aligns with findings from prior research using human assessment our proposed approach provides a valuable tool for evaluating limitations and tendencies in the creative storytelling of current and future llms,0
in traffic safety research extracting information from crash narratives using text analysis is a common practice with recent advancements of large language models llm it would be useful to know how the popular llm interfaces perform in classifying or extracting information from crash narratives to explore this our study has used the three most popular publicly available llm interfaces chatgpt bard and gpt4 this study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from iowa and kansas during the investigation their capabilities and limitations were assessed and their responses to the queries were compared five questions were asked related to the narratives 1 who is atfault 2 what is the manner of collision 3 has the crash occurred in a workzone 4 did the crash involve pedestrians and 5 what are the sequence of harmful events in the crash for questions 1 through 4 the overall similarity among the llms were 70 35 96 and 89 respectively the similarities were higher while answering direct questions requiring binary responses and significantly lower for complex questions to compare the responses to question 5 network diagram and centrality measures were analyzed the network diagram from the three llms were not always similar although they sometimes have the same influencing events with high indegree outdegree and betweenness centrality this study suggests using multiple models to extract viable information from narratives also caution must be practiced while using these interfaces to obtain crucial safety related information,0
suspense is an important tool in storytelling to keep readers engaged and wanting to read more however it has so far not been studied extensively in computational literary studies in this paper we focus on one of the elements authors can use to build up suspense dangerous situations we introduce a corpus of texts annotated with dangerous situations distinguishing between 7 types of danger additionally we annotate parts of the text that describe fear experienced by a character regardless of the actual presence of danger we present experiments towards the automatic detection of these situations finding that unsupervised baseline methods can provide valuable signals for the detection but more complex methods are necessary for further analysis not unexpectedly the description of danger and fear often relies heavily on the context both local eg situations where danger is only mentioned but not actually present and global eg storm being used in a literal sense in an adventure novel but metaphorically in a romance novel,0
we collect 14 representative corpora for major periods in chinese history in this study these corpora include poetic works produced in several dynasties novels of the ming and qing dynasties and essays and news reports written in modern chinese the time span of these corpora ranges between 1046 bce and 2007 ce we analyze their character and word distributions from the viewpoint of the zipfs law and look for factors that affect the deviations and similarities between their zipfian curves genres and epochs demonstrated their influences in our analyses specifically the character distributions for poetic works of between 618 ce and 1644 ce exhibit striking similarity in addition although texts of the same dynasty may tend to use the same set of characters their character distributions still deviate from each other,0
scientific news reports serve as a bridge adeptly translating complex research articles into reports that resonate with the broader public the automated generation of such narratives enhances the accessibility of scholarly insights in this paper we present a new corpus to facilitate this paradigm development our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines to demonstrate the utility and reliability of our dataset we conduct an extensive analysis highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts we benchmark our dataset employing stateoftheart text generation models the evaluation process involves both automatic and human evaluation which lays the groundwork for future explorations into the automated generation of scientific news reports the dataset and code related to this work are available at,0
cardiovascular diseases are becoming increasingly prevalent in modern society with a profound impact on global health and wellbeing these cardiovascular disorders are complex and multifactorial influenced by genetic predispositions lifestyle choices and diverse socioeconomic and clinical factors information about these interrelated factors is dispersed across multiple types of textual data including patient narratives medical records and scientific literature natural language processing nlp has emerged as a powerful approach for analysing such unstructured data enabling healthcare professionals and researchers to gain deeper insights that may transform the diagnosis treatment and prevention of cardiac disorders this review provides a comprehensive overview of nlp research in cardiology from 2014 to 2025 we systematically searched six literature databases for studies describing nlp applications across a range of cardiovascular diseases after a rigorous screening process we identified 265 relevant articles each study was analysed across multiple dimensions including nlp paradigms cardiologyrelated tasks disease types and data sources our findings reveal substantial diversity within these dimensions reflecting the breadth and evolution of nlp research in cardiology a temporal analysis further highlights methodological trends showing a progression from rulebased systems to large language models finally we discuss key challenges and future directions such as developing interpretable llms and integrating multimodal data to the best of our knowledge this review represents the most comprehensive synthesis of nlp research in cardiology to date,0
accurately identifying distant recurrences in breast cancer from the electronic health records ehr is important for both clinical care and secondary analysis although multiple applications have been developed for computational phenotyping in breast cancer distant recurrence identification still relies heavily on manual chart review in this study we aim to develop a model that identifies distant recurrences in breast cancer using clinical narratives and structured data from ehr we apply metamap to extract features from clinical narratives and also retrieve structured clinical data from ehr using these features we train a support vector machine model to identify distant recurrences in breast cancer patients we train the model using 1396 doubleannotated subjects and validate the model using 599 doubleannotated subjects in addition we validate the model on a set of 4904 singleannotated subjects as a generalization test we obtained a high area under curve auc score of 092 sd001 in the crossvalidation using the training dataset then obtained auc scores of 095 and 093 in the heldout test and generalization test using 599 and 4904 samples respectively our model can accurately and efficiently identify distant recurrences in breast cancer by combining features extracted from unstructured clinical narratives and structured clinical data,0
over the past decades there has been an increase in the prevalence of abusive and violent content in hollywood movies in this study we use language models to explore the longitudinal abuse and sentiment analysis of hollywood oscar and blockbuster movie dialogues from 1950 to 2024 we provide an analysis of subtitles for over a thousand movies which are categorised into four genres we employ finetuned language models to examine the trends and shifts in emotional and abusive content over the past seven decades findings reveal significant temporal changes in movie dialogues which reflect broader social and cultural influences overall the emotional tendencies in the films are diverse and the detection of abusive content also exhibits significant fluctuations the results show a gradual rise in abusive content in recent decades reflecting social norms and regulatory policy changes genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict at the same time underlying positive emotions such as humour and optimism remain prevalent in most of the movies furthermore the gradual increase of abusive content in movie dialogues has been significant over the last two decades where oscarnominated movies overtook the top ten blockbusters,0
adult content detection still poses a great challenge for automation existing classifiers primarily focus on distinguishing between erotic and nonerotic texts however they often need more nuance in assessing the potential harm unfortunately the content of this nature falls beyond the reach of generative models due to its potentially harmful nature ethical restrictions prohibit large language models llms from analyzing and classifying harmful erotics let alone generating them to create synthetic datasets for other neural models in such instances where data is scarce and challenging a thorough analysis of the structure of such texts rather than a large model may offer a viable solution especially given that harmful erotic narratives despite appearing similar to harmless ones usually reveal their harmful nature first through contextual information hidden in the nonsexual parts of the narrative this paper introduces a hybrid neural and rulebased contextaware system that leverages coreference resolution to identify harmful contextual cues in erotic content collaborating with professional moderators we compiled a dataset and developed a classifier capable of distinguishing harmful from nonharmful erotic content our hybrid model tested on polish text demonstrates a promising accuracy of 84 and a recall of 80 models based on roberta and longformer without explicit usage of coreference chains achieved significantly weaker results underscoring the importance of coreference resolution in detecting such nuanced content as harmful erotics this approach also offers the potential for enhanced visual explainability supporting moderators in evaluating predictions and taking necessary actions to address harmful content,0
the world is currently experiencing an outbreak of mpox which has been declared a public health emergency of international concern by who no prior work related to social media mining has focused on the development of a dataset of instagram posts about the mpox outbreak the work presented in this paper aims to address this research gap and makes two scientific contributions to this field first it presents a multilingual dataset of 60127 instagram posts about mpox published between july 23 2022 and september 5 2024 the dataset available at contains instagram posts about mpox in 52 languages for each of these posts the post id post description date of publication language and translated version of the post translation to english was performed using the google translate api are presented as separate attributes in the dataset after developing this dataset sentiment analysis hate speech detection and anxiety or stress detection were performed this process included classifying each post into i one of the sentiment classes ie fear surprise joy sadness anger disgust or neutral ii hate or not hate and iii anxietystress detected or no anxietystress detected these results are presented as separate attributes in the dataset second this paper presents the results of performing sentiment analysis hate speech analysis and anxiety or stress analysis the variation of the sentiment classes fear surprise joy sadness anger disgust and neutral were observed to be 2795 257 869 594 269 153 and 5064 respectively in terms of hate speech detection 9575 of the posts did not contain hate and the remaining 425 of the posts contained hate finally 7205 of the posts did not indicate any anxietystress and the remaining 2795 of the posts represented some form of anxietystress,0
addressing online disinformation requires analysing narratives across languages to help factcheckers and journalists sift through large amounts of data the exu project focuses on developing aibased models for multilingual disinformation analysis addressing the tasks of rumour stance classification and claim retrieval we describe the exu project proposal and summarise the results of a user requirements survey regarding the design of tools to support factchecking,0
zipfs law is a fundamental paradigm in the statistics of written and spoken natural language as well as in other communication systems we raise the question of the elementary units for which zipfs law should hold in the most natural way studying its validity for plain word forms and for the corresponding lemma forms in order to have as homogeneous sources as possible we analyze some of the longest literary texts ever written comprising four different languages with different levels of morphological complexity in all cases zipfs law is fulfilled in the sense that a powerlaw distribution of word or lemma frequencies is valid for several orders of magnitude we investigate the extent to which the wordlemma transformation preserves two parameters of zipfs law the exponent and the lowfrequency cutoff we are not able to demonstrate a strict invariance of the tail as for a few texts both exponents deviate significantly but we conclude that the exponents are very similar despite the remarkable transformation that going from words to lemmas represents considerably affecting all ranges of frequencies in contrast the lowfrequency cutoffs are less stable,0
the recent advances in large language models generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems however effectively integrating recommender system knowledge into llms for natural language generation which is tailored towards recommendation tasks remains a challenge this paper addresses this challenge by making two key contributions first we introduce a new dataset regen for natural language generation tasks in conversational recommendations regen reviews enhanced with generative narratives extends the amazon product reviews dataset with rich user narratives including personalized explanations of product preferences product endorsements for recommended items and summaries of user purchase history regen is made publicly available to facilitate further research furthermore we establish benchmarks using wellknown generative metrics and perform an automated evaluation of the new dataset using a rater llm second the paper introduces a fusion architecture cf model with an llm which serves as a baseline for regen and to the best of our knowledge represents the first attempt to analyze the capabilities of llms in understanding recommender signals and generating rich narratives we demonstrate that llms can effectively learn from simple fusion architectures utilizing interactionbased cf embeddings and this can be further enhanced using the metadata and personalization data associated with items our experiments show that combining cf and content embeddings leads to improvements of 412 in key language metrics compared to using either type of embedding individually we also provide an analysis to interpret how cf and content embeddings contribute to this new generative task,0
the media attention to the personal sphere of famous and important individuals has become a key element of the gender narrative here we combine lexical syntactic and sentiment analysis to investigate the role of gender in the personalization of a wide range of political office holders in italy during the period 20172020 on the basis of a score for words that is introduced to account for gender unbalance in both representative and news coverage we show that the political personalization in italy is more detrimental for women than men with the persistence of entrenched stereotypes including a masculine connotation of leadership the resulting womens unsuitability to hold political functions and a greater deal of focus on their attractiveness and body parts in addition women politicians are covered with a more negative tone than their men counterpart when personal details are reported further the major contribution to the observed gender differences comes from online news rather than print news suggesting that the expression of certain stereotypes may be better conveyed when click baiting and personal targeting have a major impact,0
coordinated disinformation campaigns are used to influence social media users potentially leading to offline violence in this study we introduce a general methodology to uncover coordinated messaging through analysis of user parleys on parler the proposed method constructs a usertouser coordination network graph induced by a usertotext graph and a texttotext similarity graph the texttotext graph is constructed based on the textual similarity of parler posts we study three influential groups of users in the 6 january 2020 capitol riots and detect networks of coordinated user clusters that are all posting similar textual content in support of different disinformation narratives related to the us 2020 elections,0
peer review is a key component of the publishing process in most fields of science the increasing submission rates put a strain on reviewing quality and efficiency motivating the development of applications to support the reviewing and editorial work while existing nlp studies focus on the analysis of individual texts editorial assistance often requires modeling interactions between pairs of texts yet general frameworks and datasets to support this scenario are missing relationships between texts are the core object of the intertextuality theory a family of approaches in literary studies not yet operationalized in nlp inspired by prior theoretical work we propose the first intertextual model of textbased collaboration which encompasses three major phenomena that make up a full iteration of the reviewreviseandresubmit cycle pragmatic tagging linking and longdocument version alignment while peer review is used across the fields of science and publication formats existing datasets solely focus on conferencestyle review in computer science addressing this we instantiate our proposed model in the first annotated multidomain corpus in journalstyle postpublication open peer review and provide detailed insights into the practical aspects of intertextual annotation our resource is a major step towards multidomain finegrained applications of nlp in editorial support for peer review and our intertextual framework paves the path for generalpurpose modeling of textbased collaboration our corpus and accompanying code are publicly available,0
the paper considers the possibility of finetuning llama 2 large language model llm for the disinformation analysis and fake news detection for finetuning the peftlora based approach was used in the study the model was finetuned for the following tasks analysing a text on revealing disinformation and propaganda narratives fact checking fake news detection manipulation analytics extracting named entities with their sentiments the obtained results show that the finetuned llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives extracted sentiments for named entities can be considered as predictive features in supervised machine learning models,0
prominent applications of sentiment analysis are countless covering areas such as marketing customer service and communication the conventional bagofwords approach for measuring sentiment merely counts term frequencies however it neglects the position of the terms within the discourse as a remedy we develop a discourseaware method that builds upon the discourse structure of documents for this purpose we utilize rhetorical structure theory to label subclauses according to their hierarchical relationships and then assign polarity scores to individual leaves to learn from the resulting rhetorical structure we propose a tensorbased treestructured deep neural network named discourselstm in order to process the complete discourse tree the underlying tensors infer the salient passages of narrative materials in addition we suggest two algorithms for data augmentation node reordering and artificial leaf insertion that increase our training set and reduce overfitting our benchmarks demonstrate the superior performance of our approach moreover our tensor structure reveals the salient text passages and thereby provides explanatory insights,0
do bilingual russianfrench authors of the end of the twentieth century such as andre makine valry afanassiev vladimir fdorovski iegor gran luba jurgenson have common stylistic traits in the novels they wrote in french can we distinguish between them and nonbilingual french writers texts is the phenomenon of interference observable in french texts of russian authors this paper applies authorship attribution methods including support vector machine svm knearest neighbors knn ridge classification and neural network to answer these questions,0
the electoral programs of six german parties issued before the parliamentary elections of 2021 are analyzed using stateoftheart computational tools for quantitative narrative topic and sentiment analysis we compare different methods for computing the textual similarity of the programs jaccard bag similarity latent semantic analysis doc2vec and sbert the representational and computational complexity increasing from the 1st to the 4th method a new similarity measure for entire documents derived from the fowlkes mallows score is applied to kmeans clustering of sbert transformed sentences using novel indices of the readability and emotion potential of texts computed via sentiart jacobs 2019 our data shed light on the similarities and differences of the programs regarding their length main ideas comprehensibility likeability and semantic complexity among others they reveal that the programs of the spd and cdu have the best chances to be comprehensible and likeable all other things being equal and they raise the important issue of which similarity measure is optimal for comparing texts such as electoral programs which necessarily share a lot of words while such analyses can not replace qualitative analyses or a deep reading of the texts they offer predictions that can be verified in empirical studies and may serve as a motivation for changing aspects of future electoral programs potentially making them more comprehensible andor likeable,0
intertextuality is a key concept in literary theory that challenges traditional notions of text signification or authorship it views texts as part of a vast intertextual network that is constantly evolving and being reconfigured this paper argues that the field of computational literary studies is the ideal place to conduct a study of intertextuality since we have now the ability to systematically compare texts with each others specifically we present a work on a corpus of more than 12000 french fictions from the 18th 19th and early 20th century we focus on evaluating the underlying roles of two literary notions subgenres and the literary canon in the framing of textuality the article attempts to operationalize intertextuality using stateoftheart contextual language models to encode novels and capture features that go beyond simple lexical or thematic approaches previous research hughes 2012 supports the existence of a literary style of a time and our findings further reinforce this concept our findings also suggest that both subgenres and canonicity play a significant role in shaping textual similarities within french fiction these discoveries point to the importance of considering genre and canon as dynamic forces that influence the evolution and intertextual connections of literary works within specific historical contexts,0
during the covid19 pandemic the news media coverage encompassed a wide range of topics that includes viral transmission allocation of medical resources and government response measures there have been studies on sentiment analysis of social media platforms during covid19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic apart from social media newspapers have played a vital role in the dissemination of information including information from the government experts and also the public about various topics a study of sentiment analysis of newspaper sources during covid19 for selected countries can give an overview of how the media covered the pandemic in this study we select the guardian newspaper and provide a sentiment analysis during various stages of covid19 that includes initial transmission lockdowns and vaccination we employ novel large language models llms and refine them with expertlabelled sentiment analysis data we also provide an analysis of sentiments experienced prepandemic for comparison the results indicate that during the early pandemic stages public sentiment prioritised urgent crisis response later shifting focus to addressing the impact on health and the economy in comparison with related studies about social media sentiment analyses we found a discrepancy between the guardian with dominance of negative sentiments sad annoyed anxious and denial suggesting that social media offers a more diversified emotional reflection we found a grim narrative in the guardian with overall dominance of negative sentiments pre and during covid19 across news sections including australia uk world news and opinion,0
as large language models llms become increasingly deployed understanding the complexity and evolution of jailbreaking strategies is critical for ai safety we present a massscale empirical analysis of jailbreak complexity across over 2 million realworld conversations from diverse platforms including dedicated jailbreaking communities and generalpurpose chatbots using a range of complexity metrics spanning probabilistic measures lexical diversity compression ratios and cognitive load indicators we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations this pattern holds consistently across specialized jailbreaking communities and general user populations suggesting practical bounds on attack sophistication temporal analysis reveals that while user attack toxicity and complexity remains stable over time assistant response toxicity has decreased indicating improving safety mechanisms the absence of powerlaw scaling in complexity distributions further points to natural limits on jailbreak development our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders instead suggesting that llm safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing our results highlight critical information hazards in academic jailbreak disclosure as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation,0
discourse cohesion facilitates text comprehension and helps the reader form a coherent narrative in this study we aim to computationally analyze the discourse cohesion in scientific scholarly texts using multilayer network representation and quantify the writing quality of the document exploiting the hierarchical structure of scientific scholarly texts we design sectionlevel and documentlevel metrics to assess the extent of lexical cohesion in text we use a publicly available dataset along with a curated set of contrasting examples to validate the proposed metrics by comparing them against select indices computed using existing cohesion analysis tools we observe that the proposed metrics correlate as expected with the existing cohesion indices we also present an analytical framework chiaa check it again author to provide pointers to the author for potential improvements in the manuscript with the help of the sectionlevel and documentlevel metrics the proposed chiaa framework furnishes a clear and precise prescription to the author for improving writing by localizing regions in text with cohesion gaps we demonstrate the efficacy of chiaa framework using succinct examples from cohesiondeficient text excerpts in the experimental dataset,0
automated story writing has been a subject of study for over 60 years large language models can generate narratively consistent and linguistically coherent short fiction texts despite these advancements rigorous assessment of such outputs for literary merit especially concerning aesthetic qualities has received scant attention in this paper we address the challenge of evaluating aigenerated microfictions and argue that this task requires consideration of literary criteria across various aspects of the text such as thematic coherence textual clarity interpretive depth and aesthetic quality to facilitate this we present graimes an evaluation protocol grounded in literary theory specifically drawing from a literary perspective to offer an objective framework for assessing aigenerated microfiction furthermore we report the results of our validation of the evaluation protocol as answered by both literature experts and literary enthusiasts this protocol will serve as a foundation for evaluating automatically generated microfictions and assessing their literary value,0
congenital heart disease chd presents complex lifelong challenges often underrepresented in traditional clinical metrics while unstructured narratives offer rich insights into patient and caregiver experiences manual thematic analysis ta remains laborintensive and unscalable we propose a fully automated large language model llm pipeline that performs endtoend ta on clinical narratives which eliminates the need for manual coding or full transcript review our system employs a novel multiagent framework where specialized llm agents assume roles to enhance theme quality and alignment with human analysis to further improve thematic relevance we optionally integrate reinforcement learning from human feedback rlhf this supports scalable patientcentered analysis of large qualitative datasets and allows llms to be finetuned for specific clinical contexts,0
in this article we propose and apply a method to compare adaptations of the same story across different media we tackle this task by modelling such adaptations through character networks we compare them by leveraging two concepts at the core of storytelling the characters involved and the dynamics of the story we propose several methods to match characters between media and compare their position in the networks and perform narrative matching ie match the sequences of narrative units that constitute the plots we apply these methods to the novel series textita song of ice and fire by grr martin and its comics and tv show adaptations our results show that interactions between characters are not sufficient to properly match individual characters between adaptations but that using some additional information such as character affiliation or gender significantly improves the performance on the contrary character interactions convey enough information to perform narrative matching and allow us to detect the divergence between the original novels and its tv show adaptation,0
whenever human beings interact with each other they exchange or express opinions emotions and sentiments these opinions can be expressed in text speech or images analysis of these sentiments is one of the popular research areas of present day researchers sentiment analysis also known as opinion mining tries to identify or classify these sentiments or opinions into two broad categories positive and negative in recent years the scientific community has taken a lot of interest in analyzing sentiment in textual data available in various social media platforms much work has been done on social media conversations blog posts newspaper articles and various narrative texts however when it comes to identifying emotions from scientific papers researchers have faced some difficulties due to the implicit and hidden nature of opinion by default citation instances are considered inherently positive in emotion popular ranking and indexing paradigms often neglect the opinion present while citing in this paper we have tried to achieve three objectives first we try to identify the major sentiment in the citation text and assign a score to the instance we have used a statistical classifier for this purpose secondly we have proposed a new index we shall refer to it hereafter as mindex which takes into account both the quantitative and qualitative factors while scoring a paper thirdly we developed a ranking of research papers based on the mindex we also try to explain how the mindex impacts the ranking of scientific papers,0
entities like person location organization are important for literary text analysis the lack of annotated data hinders the progress of named entity recognition ner in literary domain to promote the research of literary ner we build the largest multigenre literary ner corpus containing 263135 entities in 105851 sentences from 260 online chinese novels spanning 13 different genres based on the corpus we investigate characteristics of entities from different genres we propose several baseline ner models and conduct crossgenre and crossdomain experiments experimental results show that genre difference significantly impact ner performance though not as much as domain difference like literary domain and news domain compared with ner in news domain literary ner still needs much improvement and the outofvocabulary oov problem is more challenging due to the high variety of entities in literary works our data and models are opensourced at,0
while quantitative methods have been used to examine changes in word usage in books studies have focused on overall trends such as the shapes of narratives which are independent of book length we instead look at how words change over the course of a book as a function of the number of words rather than the fraction of the book completed at any given point we define this measure as cumulative wordtime using ousiometrics a reinterpretation of the valencearousaldominance framework of meaning obtained from semantic differentials we convert text into time series of power and danger scores in cumulative wordtime each time series is then decomposed using empirical mode decomposition into a sum of constituent oscillatory modes and a nonoscillatory trend by comparing the decomposition of the original power and danger time series with those derived from shuffled text we find that shorter books exhibit only a general trend while longer books have fluctuations in addition to the general trend these fluctuations typically have a period of a few thousand words regardless of the book length or library classification code but vary depending on the content and structure of the book our findings suggest that in the ousiometric sense longer books are not expanded versions of shorter books but are more similar in structure to a concatenation of shorter texts further they are consistent with editorial practices that require longer texts to be broken down into sections such as chapters our method also provides a datadriven denoising approach that works for texts of various lengths in contrast to the more traditional approach of using large window sizes that may inadvertently smooth out relevant information especially for shorter texts these results open up avenues for future work in computational literary analysis particularly the measurement of a basic unit of narrative,0
emotion recognition capabilities in multimodal ai systems are crucial for developing culturally responsive educational technologies yet remain underexplored for arabic language contexts where culturally appropriate learning tools are critically needed this study evaluates the emotion recognition performance of two advanced multimodal large language models gpt4o and gemini 15 pro when processing arabic childrens storybook illustrations we assessed both models across three prompting strategies zeroshot fewshot and chainofthought using 75 images from seven arabic storybooks comparing model predictions with human annotations based on plutchiks emotional framework gpt4o consistently outperformed gemini across all conditions achieving the highest macro f1score of 59 with chainofthought prompting compared to geminis best performance of 43 error analysis revealed systematic misclassification patterns with valence inversions accounting for 607 of errors while both models struggled with culturally nuanced emotions and ambiguous narrative contexts these findings highlight fundamental limitations in current models cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotionaware educational technologies for arabicspeaking learners,0
correlation networks were used to detect characteristics which although fixed over time have an important influence on the evolution of prices over time potentially important features were identified using the websites and whitepapers of cryptocurrencies with the largest userbases these were assessed using two datasets to enhance robustness one with fourteen cryptocurrencies beginning from 9 november 2017 and a subset with nine cryptocurrencies starting 9 september 2016 both ending 6 march 2018 separately analysing the subset of cryptocurrencies raised the number of data points from 115 to 537 and improved robustness to changes in relationships over time excluding usd tether the results showed a positive association between different cryptocurrencies that was statistically significant robust strong positive associations were observed for six cryptocurrencies where one was a fork of the other bitcoin bitcoin cash was an exception there was evidence for the existence of a group of cryptocurrencies particularly associated with cardano and a separate group correlated with ethereum the data was not consistent with a tokens functionality or creation mechanism being the dominant determinants of the evolution of prices over time but did suggest that factors other than speculation contributed to the price,0
identifying the structural dependence between the cryptocurrencies and predicting market trend are fundamental for effective portfolio management in cryptocurrency trading in this paper we present a unified bayesian framework based on potential field theory and gaussian process to characterize the structural dependency of various cryptocurrencies using historic price information the following are our significant contributions i proposed a novel model for cryptocurrency price movements as a trajectory of a dynamical system governed by a timevarying nonlinear potential field ii validated the existence of the nonlinear potential function in cryptocurrency market through lyapunov stability analysis iii developed a bayesian framework for inferring the nonlinear potential function from observed cryptocurrency prices iv proposed that attractors and repellers inferred from the potential field are reliable cryptocurrency market indicators surpassing existing attributes such as mean open price or close price of an observation window in the literature v analysis of cryptocurrency market during various bitcoin crash durations from april 2017 to november 2021 shows that attractors captured the market trend volatility and correlation in addition attractors aids explainability and visualization vi the structural dependence inferred by the proposed approach was found to be consistent with results obtained using the popular wavelet coherence approach vii the proposed market indicators attractors and repellers can be used to improve the prediction performance of stateofart deep learning price prediction models as an example we show improvement in litecoin price prediction up to a horizon of 12 days,0
bitcoin is a peertopeer electronic payment system that has rapidly grown in popularity in recent years usually the complete history of bitcoin blockchain data must be queried to acquire variables with economic meaning this task has recently become increasingly difficult as there are over 16 billion historical transactions on the bitcoin blockchain it is thus important to query bitcoin transaction data in a way that is more efficient and provides economic insights we apply cohort analysis that interprets bitcoin blockchain data using methods developed for population data in the social sciences specifically we query and process the bitcoin transaction input and output data within each daily cohort this enables us to create datasets and visualizations for some key bitcoin transaction indicators including the daily lifespan distributions of spent transaction output stxo and the daily age distributions of the cumulative unspent transaction output utxo we provide a computationally feasible approach for characterizing bitcoin transactions that paves the way for future economic studies of bitcoin,0
cryptocurrency the most controversial and simultaneously the most interesting asset has attracted many investors and speculators in recent years the visibly significant market capitalization of cryptos also motivates modern financial instruments such as futures and options those will depend on the dynamics volatility or even the jumps of cryptos we provide a comprehensive investigation of the risk dynamics of the bitcoin market from a realized volatility perspective the bitcoin market is extremely risky in the sense of volatility entangled jumps and extensive consecutive jumps which reflect the major incidents worldwide empirical study shows that the lagged realized variance increases the future realized variance while the jumps especially positive ones significantly reduce future realized variance the outofsample forecasting model reveals that in terms of forecasting accuracy and utility gain investors interested in the longterm realized variance benefit from explicitly modelling the jumps and signed estimators which is unnecessary for the shortterm realized variance forecast,0
this research aims to demonstrate a dynamic cointegrationbased pairs trading strategy including an optimal lookback window framework in the cryptocurrency market and evaluate its return and risk by applying three different scenarios we employ the englegranger methodology the kapetaniossnellshin kss test and the johansen test as cointegration tests in different scenarios we calibrate the meanreversion speed of the ornsteinuhlenbeck process to obtain the halflife used for the asset selection phase and lookback window estimation by considering the main limitations in the market microstructure our strategy exceeds the naive buyandhold approach in the bitmex exchange another significant finding is that we implement a numerous collection of cryptocurrency coins to formulate the models spread which improves the riskadjusted profitability of the pairs trading strategy besides the strategys maximum drawdown level is reasonably low which makes it useful to be deployed the results also indicate that a class of coins has better potential arbitrage opportunities than others this research has some noticeable advantages making it stand out from similar studies in the cryptocurrency market first is the accuracy of data in which minutebinned data create the signals in the formation period besides to backtest the strategy during the trading period we simulate the trading signals using best bidask quotes and market trades we exclusively take the order execution into account when the asset size is already available at its quoted price with one or more period gaps after signal generation this action makes the backtesting much more realistic,0
we study to what extent the bitcoin blockchain security permanently depends on the underlying distribution of cryptocurrency market outcomes we use daily blockchain and bitcoin data for 20142019 and employ the ardl approach we test three equilibrium hypotheses i sensitivity of the bitcoin blockchain to mining reward ii security outcomes of the bitcoin blockchain and the proofofwork cost and iii the speed of adjustment of the bitcoin blockchain security to deviations from the equilibrium path our results suggest that the bitcoin price and mining rewards are intrinsically linked to bitcoin security outcomes the bitcoin blockchain securitys dependency on mining costs is geographically differenced it is more significant for the global mining leader china than for other world regions after input or output price shocks the bitcoin blockchain security reverts to its equilibrium security level,0
cryptocurrency markets present unique prediction challenges due to their extreme volatility 247 operation and hypersensitivity to news events with existing approaches suffering from key information extraction and poor sideways market detection critical for risk management we introduce a theoreticallygrounded multiagent cryptocurrency trend prediction framework that advances the stateoftheart through three key innovations 1 an informationpreserving news analysis system with formal theoretical guarantees that systematically quantifies market impact regulatory implications volume dynamics risk assessment technical correlation and temporal effects using large language models 2 an adaptive volatilityconditional fusion mechanism with proven optimal properties that dynamically combines news sentiment and technical indicators based on market regime detection 3 a distributed multiagent coordination architecture with low communication complexity enabling realtime processing of heterogeneous data streams comprehensive experimental evaluation on bitcoin across three prediction horizons demonstrates statistically significant improvements over stateoftheart natural language processing baseline establishing a new paradigm for financial machine learning with broad implications for quantitative trading and risk management systems,0
this paper contextualises the common queries of why is crypto crashing and why is crypto down the research transcends beyond the frequent market fluctuations to unravel how cryptocurrencies fundamentally work and the stepbystep process on how to create a cryptocurrency the study examines blockchain technologies and their pivotal role in the evolving metaverse shedding light on topics such as how to invest in cryptocurrency the mechanics behind crypto mining and strategies to effectively buy and trade cryptocurrencies through an interdisciplinary approach the research transitions from the fundamental principles of fintech investment strategies to the overarching implications of blockchain within the metaverse alongside exploring machine learning potentials in financial sectors and risk assessment methodologies the study critically assesses whether developed or developing nations are poised to reap greater benefits from these technologies moreover it probes into both enduring and dubious crypto projects drawing a distinct line between genuine blockchain applications and ponzilike schemes the conclusion resolutely affirms the continuing dominance of blockchain technologies underlined by a profound exploration of their intrinsic value and a reflective commentary by the author on the potential risks confronting individual investors,0
we take inspiration from statistical physics to develop a novel conceptual framework for the analysis of financial markets we model the order book dynamics as a motion of particles and define the momentum measure of the system as a way to summarise and assess the state of the market our approach proves useful in capturing salient financial market phenomena in particular it helps detect the market manipulation activities called spoofing and layering we apply our method to identify pathological order book behaviours during the flash crash of the luna cryptocurrency uncovering widespread instances of spoofing and layering in the market furthermore we establish that our technique outperforms the conventional zscorebased anomaly detection method in identifying market manipulations across both luna and bitcoin cryptocurrency markets,0
this paper explores neural networkbased approaches for algorithmic trading in cryptocurrency markets our approach combines multitimeframe trend analysis with highfrequency direction prediction networks achieving positive riskadjusted returns through statistical modeling and systematic market exploitation the system integrates diverse data sources including market data onchain metrics and orderbook dynamics translating these into unified buysell pressure signals we demonstrate how machine learning models can effectively capture crosstimeframe relationships enabling subsecond trading decisions with statistical confidence,0
in the distributed systems landscape blockchain has catalyzed the rise of cryptocurrencies merging enhanced security and decentralization with significant investment opportunities despite their potential current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences this paper presents a novel dual attention mechanism dam for forecasting cryptocurrency trends using multimodal timeseries data our approach which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through cryptobert addresses the inherent volatility and prediction challenges in cryptocurrency markets by combining elements of distributed systems natural language processing and financial forecasting our method outperforms conventional models like lstm and transformer by up to 20 in prediction accuracy this advancement deepens the understanding of distributed systems and has practical implications in financial markets benefiting stakeholders in cryptocurrency and blockchain technologies moreover our enhanced forecasting approach can significantly support decentralized science desci by facilitating strategic planning and the efficient adoption of blockchain technologies improving operational efficiency and financial risk management in the rapidly evolving digital asset domain thus ensuring optimal resource allocation,0
historically gold and silver have played distinct roles in traditional monetary systems while gold has primarily been revered as a superior store of value prompting individuals to hoard it silver has commonly been used as a medium of exchange as the financial world evolves the emergence of cryptocurrencies has introduced a new paradigm of value and exchange however the storeofvalue characteristic of these digital assets remains largely uncharted charlie lee the founder of litecoin once likened bitcoin to gold and litecoin to silver to validate this analogy our study employs several metrics including unspent transaction outputs utxo spent transaction outputs stxo weighted average lifespan wal coindaysdestroyed cdd and public onchain transaction data furthermore weve devised trading strategies centered around the pricetoutility pu ratio offering a fresh perspective on cryptoasset valuation beyond traditional utilities our backtesting results not only display trading indicators for both bitcoin and litecoin but also substantiate lees metaphor underscoring bitcoins superior storeofvalue proposition relative to litecoin we anticipate that our findings will drive further exploration into the valuation of crypto assets for enhanced transparency and to promote future research weve made our datasets available on harvard dataverse and shared our python code on github as open source,0
cryptocurrency refers to a type of digital asset that uses distributed ledger or blockchain technology to enable a secure transaction although the technology is widely misunderstood many central banks are considering launching their own national cryptocurrency in contrast to most data in financial economics detailed data on the history of every transaction in the cryptocurrency complex are freely available furthermore empiricallyoriented research is only now beginning presenting an extraordinary research opportunity for academia we provide some insights into the mechanics of cryptocurrencies describing summary statistics and focusing on potential future research avenues in financial economics,0
this letter revisits the informational efficiency of the bitcoin market in particular we analyze the timevarying behavior of long memory of returns on bitcoin and volatility 2011 until 2017 using the hurst exponent our results are twofold first rs method is prone to detect long memory whereas dfa method can discriminate more precisely variations in informational efficiency across time second daily returns exhibit persistent behavior in the first half of the period under study whereas its behavior is more informational efficient since 2014 finally price volatility measured as the logarithmic difference between intraday high and low prices exhibits long memory during all the period this reflects a different underlying dynamic process generating the prices and volatility,0
commonly used limit order book attributes are empirically considered based on nasdaq itch data it is shown that some of them have the properties drastically different from the ones assumed in many market dynamics study because of this difference we propose to make a transition from statistical type of order book study typical for academics to dynamical type of study typical for market practitioners based on market data analysis we conclude that most of market dynamics information is contained in attributes with spikes eg executed trades flow idvdt there is no any stationary case on the market and typical market dynamics is a fast excitation and then slow relaxation type of behavior with a wide distribution of excitation frequencies and relaxation times a computer code providing full depth order book information and recently executed trades is available from authors,0
in recent years cryptocurrency trading has captured the attention of practitioners and academics the volume of the exchange with standard currencies has known a dramatic increasing of late this paper addresses to the need of models describing a bitcoinus dollar exchange dynamic and their use to evaluate european option having bitcoin as underlying asset,0
a first attempt at obtaining marketdirectional information from a nonstationary solution of the dynamic equation future price tends to the value that maximizes the number of shares traded per unit time is presented we demonstrate that the concept of price impact is poorly applicable to market dynamics instead we consider the execution flow idvdt operator with the impact from the future term providing information about notyetexecuted trades the impact from the future on i can be directly estimated from the alreadyexecuted trades the directional information on price is then obtained from the experimentally observed fact that the i and p operators have the same eigenfunctions the exact result in the dynamic impact approximation ppi the condition for no information about the future is found and directional prediction quality is discussed this work makes a substantial contribution toward solving the ultimate market dynamics problem find evidence of existence or proof of nonexistence of an automated trading machine which consistently makes positive pl on a free market as an autonomous agent aka the existence of the market dynamics equation the software with a reference implementation of the theory is provided,0
this paper offers a thorough examination of the univariate predictability in cryptocurrency timeseries by exploiting a combination of complexity measure and model predictions we explore the cryptocurrencies timeseries forecasting task focusing on the exchange rate in usd of litecoin binance coin bitcoin ethereum and xrp on one hand to assess the complexity and the randomness of these timeseries a comparative analysis has been performed using brownian and colored noises as a benchmark the results obtained from the complexityentropy causality plane and power density spectrum analysis reveal that cryptocurrency timeseries exhibit characteristics closely resembling those of brownian noise when analyzed in a univariate context on the other hand the application of a wide range of statistical machine and deep learning models for timeseries forecasting demonstrates the low predictability of cryptocurrencies notably our analysis reveals that simpler models such as naive models consistently outperform the more complex machine and deep learning ones in terms of forecasting accuracy across different forecast horizons and time windows the combined study of complexity and forecasting accuracies highlights the difficulty of predicting the cryptocurrency market these findings provide valuable insights into the inherent characteristics of the cryptocurrency data and highlight the need to reassess the challenges associated with predicting cryptocurrencys price movements,0
this paper presents an agentbased artificial cryptocurrency market in which heterogeneous agents buy or sell cryptocurrencies in particular bitcoins in this market there are two typologies of agents random traders and chartists which interact with each other by trading bitcoins each agent is initially endowed with a finite amount of crypto andor fiat cash and issues buy and sell orders according to her strategy and resources the number of bitcoins increases over time with a rate proportional to the real one even if the mining process is not explicitly modelled the model proposed is able to reproduce some of the real statistical properties of the price absolute returns observed in the bitcoin real market in particular it is able to reproduce the autocorrelation of the absolute returns and their cumulative distribution function the simulator has been implemented using objectoriented technology and could be considered a valid starting point to study and analyse the cryptocurrency market and its future evolutions,0
currently there are no convincing proxies for the fundamentals of cryptocurrency assets we propose a new markettofundamental ratio the pricetoutility pu ratio utilizing unique blockchain accounting methods we then proxy various existing fundamentaltomarket ratios by bitcoin historical data and find they have little predictive power for shortterm bitcoin returns however pu ratio effectively predicts longterm bitcoin returns than alternative methods furthermore we verify the explainability of pu ratio using machine learning finally we present an automated trading strategy advised by the pu ratio that outperforms the conventional buyandhold and markettiming strategies our research contributes to explainable ai in finance from three facets first our markettofundamental ratio is based on classic monetary theory and the unique utxo model of bitcoin accounting rather than ad hoc second the empirical evidence testifies the buylow and sellhigh implications of the ratio finally we distribute the trading algorithms as opensource software via python package index for future research which is exceptional in finance research,0
hawkes process has been used to model limit order book lob dynamics in several ways in the literature however the focus has been limited to capturing the interevent times while the order size is usually assumed to be constant we propose a novel methodology of using compound hawkes process for the lob where each event has an order size sampled from a calibrated distribution the process is formulated in a novel way such that the spread of the process always remains positive further we condition the model parameters on time of day to support empirical observations we make use of an enhanced nonparametric method to calibrate the hawkes kernels and allow for inhibitory crossexcitation kernels we showcase the results and quality of fits for an equity stocks lob in the nasdaq exchange and compare them against several baselines finally we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated,0
the paper analyzes the cryptocurrency ecosystem at both the aggregate and individual levels to understand the factors that impact future volatility the study uses highfrequency panel data from 2020 to 2022 to examine the relationship between several market volatility drivers such as daily leverage signed volatility and jumps several known autoregressive model specifications are estimated over different market regimes and results are compared to equity data as a reference benchmark of a more mature asset class the panel estimations show that the positive market returns at the highfrequency level increase price volatility contrary to what is expected from the classical financial literature we attributed this effect to the price dynamics over the last year of the dataset 2022 by repeating the estimation on different time spans moreover the positive signed volatility and negative daily leverage positively impact the cryptocurrencies future volatility unlike what emerges from the same study on a crosssection of stocks this result signals a structural difference in a nascent cryptocurrency market that has to mature yet further individuallevel analysis confirms the findings of the panel analysis and highlights that these effects are statistically significant and commonly shared among many components in the selected universe,0
in this note we compare bitcoin trading performance using two machine learning modelslight gradient boosting machine lightgbm and long shortterm memory lstmand two technical analysisbased strategies exponential moving average ema crossover and a combination of moving average convergencedivergence with the average directional index macdadx the objective is to evaluate how trading signals can be used to maximize profits in the bitcoin market this comparison was motivated by the us securities and exchange commissions sec approval of the first spot bitcoin exchangetraded funds etfs on 20240110 our results show that the lstm model achieved a cumulative return of approximately 6523 in under a year significantly outperforming lightgbm the ema and macdadx strategies as well as the baseline buyandhold this study highlights the potential for deeper integration of machine learning and technical analysis in the rapidly evolving cryptocurrency landscape,0
this paper introduces new methods for analysing the extreme and erratic behaviour of time series to evaluate the impact of covid19 on cryptocurrency market dynamics across 51 cryptocurrencies we examine extreme behaviour through a study of distribution extremities and erratic behaviour through structural breaks first we analyse the structure of the market as a whole and observe a reduction in selfsimilarity as a result of covid19 particularly with respect to structural breaks in variance second we compare and contrast these two behaviours and identify individual anomalous cryptocurrencies tether usdt and trueusd tusd are consistent outliers with respect to their returns while holo hot nexo nexo maker mkr and nem xem are frequently observed as anomalous with respect to both behaviours and time even among a market known as consistently volatile this identifies individual cryptocurrencies that behave most irregularly in their extreme and erratic behaviour and shows these were more affected during the covid19 market crisis,0
this study presents an innovative approach for predicting cryptocurrency time series specifically focusing on bitcoin ethereum and litecoin the methodology integrates the use of technical indicators a performer neural network and bilstm bidirectional long shortterm memory to capture temporal dynamics and extract significant features from raw cryptocurrency data the application of technical indicators such facilitates the extraction of intricate patterns momentum volatility and trends the performer neural network employing fast attention via positive orthogonal random features favor has demonstrated superior computational efficiency and scalability compared to the traditional multihead attention mechanism in transformer models additionally the integration of bilstm in the feedforward network enhances the models capacity to capture temporal dynamics in the data processing it in both forward and backward directions this is particularly advantageous for time series data where past and future data points can influence the current state the proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature the results underscore the potential of the proposed method to outperform existing models marking a significant progression in the field of cryptocurrency price prediction,0
a new approach to obtaining marketdirectional information based on a nonstationary solution to the dynamic equation future price tends to the value that maximizes the number of shares traded per unit time is presented in our previous work we established that it is the share execution flow idvdt and not the share trading volume v that is the driving force of the market and that asset prices are much more sensitive to the execution flow i the dynamic impact than to the traded volume v the regular impact in this paper an important advancement is achieved we define the scalpprice cal p as the sum of only those price moves that are relevant to market dynamics the criterion of relevance is a high i thus only follow the market and not little bounce events are included in cal p changes in the scalpprice defined this way indicate a market trend change not a bear market rally or a bull market selloff the approach can be further extended to nonlocal price change the software calculating the scalpprice given market observations triples time execution price shares traded is available from the authors,0
in recent years the tendency of the number of financial institutions including cryptocurrencies in their portfolios has accelerated cryptocurrencies are the first pure digital assets to be included by asset managers although they have some commonalities with more traditional assets they have their own separate nature and their behaviour as an asset is still in the process of being understood it is therefore important to summarise existing research papers and results on cryptocurrency trading including available trading platforms trading signals trading strategy research and risk management this paper provides a comprehensive survey of cryptocurrency trading research by covering 146 research papers on various aspects of cryptocurrency trading eg cryptocurrency trading systems bubble and extreme conditions prediction of volatility and return cryptoassets portfolio construction and cryptoassets technical trading and others this paper also analyses datasets research trends and distribution among research objectscontentsproperties and technologies concluding with some promising opportunities that remain open in cryptocurrency trading,0
the growing attention on cryptocurrencies has led to increasing research on digital stock markets approaches and tools usually applied to characterize standard stocks have been applied to the digital ones among these tools is the identification of processes of market fluctuations being interesting stochastic processes the usual statistical methods are appropriate tools for their reconstruction there besides chance the description of a behavioural component shall be present whenever a deterministic pattern is ever found markov approaches are at the leading edge of this endeavour in this paper markov chains of orders one to eight are considered as a way to forecast the dynamics of three major cryptocurrencies it is accomplished using an empirical basis of intraday returns besides forecasting we investigate the existence of eventual longmemory components in each of those stochastic processes results show that predictions obtained from using the empirical probabilities are better than random choices,0
much significant research has been done to investigate various facets of the link between bitcoin price and its fundamental sources this study goes beyond by looking into least to most influential factorsacross the fundamental macroeconomic financial speculative and technical determinants as well as the 2016 eventswhich drove the value of bitcoin in times of economic and geopolitical chaos we use a bayesian quantile regression to inspect how the structure of dependence of bitcoin price and its determinants varies across the entire conditional distribution of bitcoin price movements in doing so three groups of determinants were derived the use of bitcoin in trade and the uncertainty surrounding chinas deepening slowdown brexit and indias demonetization were found to be the most potential contributors of bitcoin price when the market is improving the intense anxiety over donald trump being the president of united states was shown to be a positive determinant pushing up the price of bitcoin when the market is functioning around the normal mode the velocity of bitcoins in circulation the gold price the venezuelan currency demonetization and the hash rate were found to be the fundamentals influencing the bitcoin price when the market is heading into decline,0
this paper investigates how changes in investor base is related to idiosyncratic volatility in cryptocurrency markets for each cryptocurrency we set change in its subreddit followers as a proxy for the change in its investor base and find out that the latter can significantly increase cryptocurrencies idiosyncratic volatility this finding is not subsumed by effects of size momentum liquidity and volume and is robust to various measures of idiosyncratic volatility,0
in this study we perform some analysis for the probability distributions in the space of frequency and time variables however in the domain of high frequencies it behaves in such a way as the highly nonlinear dynamics the wavelet analysis is a powerful tool to perform such analysis in order to search for the characteristics of frequency variations over time for the prices of major cryptocurrencies in fact the wavelet analysis is found to be quite useful as it examine the validity of the efficient market hypothesis in the weak form especially for the presence of the cyclical persistence at different frequencies if we could find some cyclical persistence at different frequencies that means that there exist some intrinsic causal relationship for some given investment horizons defined by some chosen sampling scales this is one of the characteristic results of the wavelet analysis in the timefrequency domains,0
in this work we demonstrate experimentally that the execution flow i dvdt is the fundamental driving force of market dynamics we develop a numerical framework to calculate execution flow from sampled moments using the radonnikodym derivative a notable feature of this approach is its ability to automatically determine thresholds that can serve as actionable triggers the technique also determines the characteristic time scale directly from the corresponding eigenproblem the methodology has been validated on actual market data to support these findings additionally we introduce a framework based on the christoffel function spectrum which is invariant under arbitrary nondegenerate linear transformations of input attributes and offers an alternative to traditional principal component analysis pca which is limited to unitary invariance,0
the cryptocurrency market is highly volatile compared to traditional financial markets hence forecasting its volatility is crucial for risk management in this paper we investigate cryptoquant data eg onchain analytics exchange and miner data and whalealert tweets and explore their relationship to bitcoins nextday volatility with a focus on extreme volatility spikes we propose a deep learning synthesizer transformer model for forecasting volatility our results show that the model outperforms existing stateoftheart models when forecasting extreme volatility spikes for bitcoin using cryptoquant data as well as whalealert tweets we analysed our model with the captum xai library to investigate which features are most important we also backtested our prediction results with different baseline trading strategies and the results show that we are able to minimize drawdown while keeping steady profits our findings underscore that the proposed method is a useful tool for forecasting extreme volatility movements in the bitcoin market,0
we study the fluctuations particularly the inequality of fluctuations in cryptocurrency prices over the last ten years we calculate the inequality in the price fluctuations through different measures such as the gini and kolkata indices and also the q factor given by the ratio between the highest value and the average value of these fluctuations we compare the results with the equivalent quantities in some of the more prominent national currencies and see that while the fluctuations or inequalities in such fluctuations for cryptocurrencies were initially significantly higher than national currencies over time the fluctuation levels of cryptocurrencies tend towards the levels characteristic of national currencies we also compare similar quantities for a few prominent stock prices,0
blockchain technology shows significant results and huge potential for serving as an interweaving fabric that goes through every industry and market allowing decentralized and secure value exchange thus connecting our civilization like never before the standard approach for asset value predictions is based on market analysis with an lstm neural network blockchain technologies however give us access to vast amounts of public data such as the executed transactions and the account balance distribution we explore whether analyzing this data with modern deep leaning techniques results in higher accuracies than the standard approach during a series of experiments on the ethereum blockchain we achieved 4 times error reduction with blockchain data than an lstm approach with trade volume data by utilizing blockchain account distribution histograms spatial dataset modeling and a convolutional architecture the error was reduced further by 26 the proposed methodologies are implemented in an open source cryptocurrency prediction framework allowing them to be used in other analysis contexts,0
stylized facts can be regarded as constraints for any modeling attempt of price dynamics on a financial market in that an empirically reasonable model has to reproduce these stylized facts at least qualitatively the dynamics of market prices is modeled on a macrolevel as the result of the dynamic coupling of two dynamical components the degree of their dynamical decoupling is shown to have a significant impact on the stochastic properties of return trials such as the return distribution volatility clustering and the multifractal behavior of time scales of asset returns particularly we observe a cross over in the return distribution from a gaussianlike to a levylike shape when the degree of decoupling increases in parallel the larger the degree of decoupling is the more pronounced is volatility clustering these findings suggest that the considerations of time in an economic system in general and the coupling of constituting processes is essential for understanding the behavior of a financial market,0
we propose a model for the dynamics of a limit order book in a liquid market where buy and sell orders are submitted at high frequency we derive a functional central limit theorem for the joint dynamics of the bid and ask queues and show that when the frequency of order arrivals is large the intraday dynamics of the limit order book may be approximated by a markovian jumpdiffusion process in the positive orthant whose characteristics are explicitly described in terms of the statistical properties of the underlying order flow this result allows to obtain tractable analytical approximations for various quantities of interest such as the probability of a price increase or the distribution of the duration until the next price move conditional on the state of the order book our results allow for a wide range of distributional assumptions and temporal dependence in the order flow and apply to a wide class of stochastic models proposed for order book dynamics including models based on poisson point processes selfexciting point processes and models of the acdgarch family,0
the study examines whether famafrench equity factors can effectively explain the idiosyncratic risk and return characteristics of bitcoin by incorporating famafrench factors the explanatory power of these factors on bitcoins excess returns over various moving average periods is tested through applications of several statistical methods the analysis aims to determine if equity market factors are significant in explaining and modeling systemic risk in bitcoin,0
this paper presents a multi agent bitcoin trading system that utilizes large language models llms for alpha generation and portfolio management in the cryptocurrencies market unlike equities cryptocurrencies exhibit extreme volatility and are heavily influenced by rapidly shifting market sentiments and regulatory announcements making them difficult to model using static regression models or neural networks trained solely on historical data the proposed framework overcomes this by structuring llms into specialised agents for technical analysis sentiment evaluation decisionmaking and performance reflection the agents improve over time via a novel verbal feedback mechanism where a reflect agent provides daily and weekly naturallanguage critiques of trading decisions these textual evaluations are then injected into future prompts of the agents allowing them to adjust allocation logic without weight updates or finetuning backtesting on bitcoin price data from july 2024 to april 2025 shows consistent outperformance across market regimes the quantitative agent delivered over 30 higher returns in bullish phases and 15 overall gains versus buyandhold while the sentimentdriven agent turned sideways markets from a small loss into a gain of over 100 adding weekly feedback further improved total performance by 31 and reduced bearish losses by 10 the results demonstrate that verbal feedback represents a new scalable and lowcost approach of tuning llms for financial goals,0
the aim of this paper is to investigate the effect of a novel method called linear lawbased feature space transformation llt on the accuracy of intraday price movement prediction of cryptocurrencies to do this the 1minute interval price data of bitcoin ethereum binance coin and ripple between 1 january 2019 and 22 october 2022 were collected from the binance cryptocurrency exchange then 14hour nonoverlapping time windows were applied to sample the price data the classification was based on the first 12 hours and the two classes were determined based on whether the closing price rose or fell after the next 2 hours these price data were first transformed with the llt then they were classified by traditional machine learning algorithms with 10fold crossvalidation based on the results llt greatly increased the accuracy for all cryptocurrencies which emphasizes the potential of the llt algorithm in predicting price movements,0
covariance matrices estimated from short noisy and nongaussian financial time seriesparticularly cryptocurrenciesare notoriously unstable empirical evidence indicates that these covariance structures often exhibit powerlaw scaling reflecting complex and hierarchical interactions among assets building on this insight we propose a powerlaw covariance model to characterize the collective dynamics of cryptocurrencies and develop a hybrid estimator that integrates random matrix theory rmt with residual neural networks resnets the rmt component regularizes the eigenvalue spectrum under highdimensional noise while the resnet learns datadriven corrections to recover latent structural dependencies monte carlo simulations show that resnetbased estimators consistently minimize both frobenius and minimumvariance mv losses across diverse covariance models empirical experiments on 89 cryptocurrencies 20202025 using a training period ending at the local btc maximum in november 2021 and testing through the subsequent bear market demonstrate that a twostep estimator combining hierarchical filtering with resnet corrections yields the most profitable and balanced portfolios remaining robust under market regime shifts these findings highlight the potential of combining rmt deep learning and powerlaw modeling to capture the intrinsic complexity of financial systems and enhance portfolio optimization under realistic conditions,0
cryptocurrencies such as bitcoin are one of the most controversial and complex technological innovations in todays financial system this study aims to forecast the movements of bitcoin prices at a high degree of accuracy to this aim four different machine learning ml algorithms are applied namely the support vector machines svm the artificial neural network ann the naive bayes nb and the random forest rf besides the logistic regression lr as a benchmark model in order to test these algorithms besides existing continuous dataset discrete dataset was also created and used for the evaluations of algorithm performances the f statistic accuracy statistic the mean absolute error mae the root mean square error rmse and the root absolute error rae metrics were used the t test was used to compare the performances of the svm ann nb and rf with the performance of the lr empirical findings reveal that while the rf has the highest forecasting performance in the continuous dataset the nb has the lowest on the other hand while the ann has the highest and the nb the lowest performance in the discrete dataset furthermore the discrete dataset improves the overall forecasting performance in all algorithms models estimated,0
cryptocurrencies have emerged as a novel financial asset garnering significant attention in recent years a defining characteristic of these digital currencies is their pronounced shortterm market volatility primarily influenced by widespread sentiment polarization particularly on social media platforms such as twitter recent research has underscored the correlation between sentiment expressed in various networks and the price dynamics of cryptocurrencies this study delves into the 15minute impact of informative tweets disseminated through foundation channels on trader behavior with a focus on potential outcomes related to sentiment polarization the primary objective is to identify factors that can predict positive price movements and potentially be leveraged through a trading algorithm to accomplish this objective we conduct a conditional examination of return and excess return rates within the 15 minutes following tweet publication the empirical findings reveal statistically significant increases in return rates particularly within the initial three minutes following tweet publication notably adverse effects resulting from the messages were not observed surprisingly sentiments were found to have no discernible impact on cryptocurrency price movements our analysis further identifies that investors are primarily influenced by the quality of tweet content as reflected in the choice of words and tweet volume while the basic trading algorithm presented in this study does yield some benefits within the 15minute timeframe these benefits are not statistically significant nevertheless it serves as a foundational framework for potential enhancements and further investigations,0
in the burgeoning realm of cryptocurrency social media platforms like twitter have become pivotal in influencing market trends and investor sentiments in our study we leverage gpt4 and a finetuned transformerbased bert model for a multimodal sentiment analysis focusing on the impact of emoji sentiment on cryptocurrency markets by translating emojis into quantifiable sentiment data we correlate these insights with key market indicators like btc price and the vcrix index our architectures analysis of emoji sentiment demonstrated a distinct advantage over finberts pure text sentiment analysis in such predicting power this approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends crucially our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns this research underscores the practical benefits of integrating advanced aidriven analyses into financial strategies offering a nuanced perspective on the interplay between digital communication and market dynamics in an academic context,0
we propose the financial connectome a new scientific discipline that models financial markets through the lens of brain functional architecture inspired by the foundational work of group independent component analysis groupica in neuroscience we reimagine markets not as collections of assets but as highdimensional dynamic systems composed of latent market modules treating stocks as functional nodes and their cofluctuations as expressions of collective cognition we introduce dynamic market network connectivity dmnc the financial analogue of dynamic functional connectivity dfnc this biologically inspired framework reveals structurally persistent market subnetworks captures regime shifts and uncovers systemic early warning signals all without reliance on predictive labels our results suggest that markets like brains exhibit modular selforganizing and temporally evolving architectures this work inaugurates the field of financial connectomics a principled synthesis of systems neuroscience and quantitative finance aimed at uncovering the hidden logic of complex economies,0
this paper presents a novel model for simulating and analyzing sparse limit order books lobs with a specific application to the european intraday electricity market in illiquid markets characterized by significant gaps between order levels due to sparse trading volumes traditional lob models often fall short our approach utilizes an inhomogeneous poisson process to accurately capture the sporadic nature of order arrivals and cancellations on both the bid and ask sides of the book by applying this model to the intraday electricity market we gain insights into the unique microstructural behaviors and challenges of this dynamic trading environment the results offer valuable implications for market participants enhancing their understanding of lob dynamics in illiquid markets this work contributes to the broader field of market microstructure by providing a robust framework adaptable to various illiquid market settings beyond electricity trading,0
we present a model of price formation in an inelastic market whose dynamics are partially driven by both money flows and their impact on asset prices the money flow to the market is viewed as an investment policy of outside investors for the price impact effect we use an impact function that incorporates the phenomena of market inelasticity and saturation from new money the dumb money effect due to the dependence of market investors flows on market performance the model implies a feedback mechanism that gives rise to nonlinear dynamics consequently the market price dynamics are seen as a nonlinear diffusion of a particle the marketron in a twodimensional space formed by the logprice x and a memory variable y the latter stores information about past money flows so that the dynamics are nonmarkovian in the log price x alone but markovian in the pair xy bearing a strong resemblance to spiking neuron models in neuroscience in addition to market flows the model dynamics are partially driven by return predictors modeled as unobservable ornsteinuhlenbeck processes by using a new interpretation of predictive signals as selfpropulsion components of the price dynamics we treat the marketron as an active particle amenable to methods developed in the physics of active matter we show that depending on the choice of parameters our model can produce a rich variety of interesting dynamic scenarios in particular it predicts three distinct regimes of the market which we call the good the bad and the ugly markets the latter regime describes a scenario of a total market collapse or alternatively a corporate default event depending on whether our model is applied to the whole market or an individual stock,0
we study the price dynamics of cryptocurrencies using adaptive complementary ensemble empirical mode decomposition aceemd and hilbert spectral analysis this is a multiscale noiseassisted approach that decomposes any time series into a number of intrinsic mode functions along with the corresponding instantaneous amplitudes and instantaneous frequencies the decomposition is adaptive to the timevarying volatility of each cryptocurrency price evolution different combinations of modes allow us to reconstruct the time series using components of different timescales we then apply hilbert spectral analysis to define and compute the instantaneous energyfrequency spectrum of each cryptocurrency to illustrate the properties of various timescales embedded in the original time series,0
the longterm dependence of bitcoin btc manifesting itself through a hurst exponent h05 is exploited in order to predict future btcusd price a monte carlo simulation with 104 geometric fractional brownian motion realisations is performed as extensions of historical data the accuracy of statistical inferences is 10 the most probable bitcoin price at the beginning of 2018 is 6358 usd,0
we have embedded the classical theory of stochastic finance into a differential geometric framework called geometric arbitrage theory and show that it is possible to write arbitrage as curvature of a principal fibre bundle parameterize arbitrage strategies by its holonomy give the fundamental theorem of asset pricing a differential homotopic characterization characterize geometric arbitrage theory by five principles and show they they are consistent with the classical theory of stochastic finance derive for a closed market the equilibrium solution for market portfolio and dynamics in the cases where arbitrage is allowed but minimized arbitrage is not allowed prove that the nofreelunchwithvanishingrisk condition implies the zero curvature condition,0
we present a new bitcoin coin selection algorithm coin selection with leverage which aims to improve upon cost savings than that of standard knapsack like approaches parameters to the new algorithm are available to be tuned at the users discretion to address other goals of coin selection our approach naturally fits as a replacement for the standard knapsack ingredient of full coin selection procedures,0
this study investigates the relationship between narratives conveyed through microblogging platforms namely twitter and the value of crypto assets our study provides a unique technique to build narratives about cryptocurrency by combining topic modelling of short texts with sentiment analysis first we used an unsupervised machine learning algorithm to discover the latent topics within the massive and noisy textual data from twitter and then we revealed 45 cryptocurrencyrelated narratives including financial investment technological advancement related to crypto financial and political regulations crypto assets and media coverage in a number of situations we noticed a strong link between our narratives and crypto prices our work connects the most recent innovation in economics narrative economics to a new area of study that combines topic modelling and sentiment analysis to relate consumer behaviour to narratives,0
blockchain has empowered computer systems to be more secure using a distributed network however the current blockchain design suffers from fairness issues in transaction ordering miners are able to reorder transactions to generate profits the socalled miner extractable value mev existing research recognizes mev as a severe security issue and proposes potential solutions including prominent flashbots however previous studies have mostly analyzed blockchain data which might not capture the impacts of mev in a much broader ai society thus in this research we applied natural language processing nlp methods to comprehensively analyze topics in tweets on mev we collected more than 20000 tweets with mev and flashbots hashtags and analyzed their topics our results show that the tweets discussed profound topics of ethical concern including security equity emotional sentiments and the desire for solutions to mev we also identify the comovements of mev activities on blockchain and social media platforms our study contributes to the literature at the interface of blockchain security mev solutions and ai ethics,0
the dynamics of financial markets are driven by the interactions between participants as well as the trading mechanisms and regulatory frameworks that govern these interactions decisionmakers would rather not ignore the impact of other participants on these dynamics and should employ tools and models that take this into account to this end we demonstrate the efficacy of applying opponentmodeling in a number of simulated market settings while our simulations are simplified representations of actual market dynamics they provide an idealized playground in which our techniques can be demonstrated and tested we present this work with the aim that our techniques could be refined and with some effort scaled up to the full complexity of realworld market scenarios we hope that the results presented encourage practitioners to adopt opponentmodeling methods and apply them online systems in order to enable not only reactive but also proactive decisions to be made,0
recently the invention of quantum computers was so revolutionary that they bring transformative challenges in a variety of fields especially for the traditional cryptographic blockchain and it may become a real thread for most of the cryptocurrencies in the market that is it becomes inevitable to consider to implement a postquantum cryptography which is also referred to as quantumresistant cryptography for attaining quantum resistance in blockchains,0
in this paper the crosscorrelations of cryptocurrency returns are analysed the paper examines one years worth of data for 146 cryptocurrencies from the period january 1 2019 to december 31 2019 the crosscorrelations of these returns are firstly analysed by comparing eigenvalues and eigenvector components of the crosscorrelation matrix c with random matrix theory rmt assumptions results show that c deviates from these assumptions indicating that c contains genuine information about the correlations between the different cryptocurrencies from here louvain community detection method is applied as a clustering mechanism and 15 community groupings are detected finally pca is completed on the standardised returns of each of these clusters to create a portfolio of cryptocurrencies for investment this method selects a portfolio which contains a number of high value coins when compared back against their market ranking in the same year in the interest of assessing continuity of the initial results the method is also applied to a smaller dataset of the top 50 cryptocurrencies across three time periods of t 125 days which produces similar results the results obtained in this paper show that these methods could be useful for constructing a portfolio of optimally performing cryptocurrencies,0
in a financial exchange market impact is a measure of the price change of an asset following a transaction this is an important element of market microstructure which determines the behaviour of the market following a trade in this paper we first provide a discussion on the market impact observed in the btcusd futures market then we present a novel multiagent market simulation that can follow an underlying price series whilst maintaining the ability to reproduce the market impact observed in the market in an explainable manner this simulation of the financial exchange allows the model to interact realistically with market participants helping its users better estimate market slippage as well as the knockon consequences of their market actions in turn it allows various stakeholders such as industrial practitioners governments and regulators to test their market hypotheses without deploying capital or destabilising the system,0
we study the optimal market making problem in a limit order book lob market simulated using a highfidelity mutually exciting hawkes process departing from traditional browniandriven midprice models our setup captures key microstructural properties such as queue dynamics interarrival clustering and endogenous price impact recognizing the realistic constraint that market makers cannot update strategies at every lob event we formulate the control problem within an impulse control framework where interventions occur discretely via limit cancel or market orders this leads to a highdimensional nonlocal hamiltonjacobibellman quasivariational inequality hjbqvi whose solution is analytically intractable and computationally expensive due to the curse of dimensionality to address this we propose a novel reinforcement learning rl approximation inspired by auxiliary control formulations using a twonetwork ppobased architecture with selfimitation learning we demonstrate strong empirical performance with limited training achieving sharpe ratios above 30 in a realistic simulated lob in addition to that we solve the hjbqvi using a deep learning method inspired by sirignano and spiliopoulos 2018 and compare the performance with the rl agent our findings highlight the promise of combining impulse control theory with modern deep rl to tackle optimal execution problems in jumpdriven microstructural markets,0
models which postulate lognormal dynamics for interest rates which are compounded according to market conventions such as forward libor or forward swap rates can be constructed initially in a discrete tenor framework interpolating interest rates between maturities in the discrete tenor structure is equivalent to extending the model to continuous tenor the present paper sets forth an alternative way of performing this extension one which preserves the markovian properties of the discrete tenor models and guarantees the positivity of all interpolated rates,0
in the present work we introduce a novel multiagent model with the aim to reproduce the dynamics of a double auction market at microscopic time scale through a faithful simulation of the matching mechanics in the limit order book the agents follow a noise decision making process where their actions are related to a stochastic variable the market sentiment which we define as a mixture of public and private information the model despite making just few basic assumptions over the trading strategies of the agents is able to reproduce several empirical features of the highfrequency dynamics of the market microstructure not only related to the price movements but also to the deposition of the orders in the book,0
this paper describes simulations and analysis of flash crash scenarios in an agentbased modelling framework we design implement and assess a novel highfrequency agentbased financial market simulator that generates realistic millisecondlevel financial price time series for the emini sp 500 futures market specifically a microstructure model of a single security traded on a central limit order book is provided where different types of traders follow different behavioural rules the model is calibrated using the machine learning surrogate modelling approach statistical test and moment coverage ratio results show that the model has excellent capability of reproducing realistic stylised facts in financial markets by introducing an institutional trader that mimics the realworld sell algorithm on may 6th 2010 the proposed highfrequency agentbased financial market simulator is used to simulate the flash crash that took place that day we scrutinise the market dynamics during the simulated flash crash and show that the simulated dynamics are consistent with what happened in historical flash crash scenarios with the help of monte carlo simulations we discover functional relationships between the amplitude of the simulated 2010 flash crash and three conditions the percentage of volume of the sell algorithm the market maker inventory limit and the trading frequency of fundamental traders similar analyses are carried out for mini flash crash events an innovative spiking trader is introduced to the model aiming at precipitating mini flash crash events we analyse the market dynamics during the course of a typical simulated mini flash crash event and study the conditions affecting its characteristics the proposed model can be used for testing resiliency and robustness of trading algorithms and providing advice for policymakers,0
we develop from basic economic principles a continuoustime model for a large investor who trades with a finite number of market makers at their utility indifference prices in this model the market makers compete with their quotes for the investors orders and trade among themselves to attain pareto optimal allocations we first consider the case of simple strategies and then in analogy to the construction of stochastic integrals investigate the transition to general continuous dynamics as a result we show that the models evolution can be described by a nonlinear stochastic differential equation for the market makers expected utilities,0
we postulates and then show experimentally that liquidity deficit is the driving force of the markets in the first part of the paper a kinematic of liquidity deficit is developed the calculuslike approach which is based on radonnikodym derivatives and their generalization allows us to calculate important characteristics of observable market dynamics in the second part of the paper this calculus is used in an attempt to build a dynamic equation in the form future price tend to the value maximizing the number of shares traded per unit time to build a practical automated trading machine pl dynamics instead of price dynamics is considered this allows a trading automate resilient to catastrophic pl drains to be built the results are very promising yet when all the fees and trading commissions are taken into account are close to breakeven in the end of the paper important criteria for automated trading systems are presented we list the system types that can and cannot make money on the market these criteria can be successfully applied not only by automated trading machines but also by a human trader,0
bitcoin has emerged as a fascinating phenomenon of the financial markets without any central authority issuing the currency it has been associated with controversy ever since its popularity and public interest reached high levels here we contribute to the discussion by examining potential drivers of bitcoin prices ranging from fundamental to speculative and technical sources as well as a potential influence of the chinese market the evolution of the relationships is examined in both time and frequency domains utilizing the continuous wavelets framework so that we comment on development of the interconnections in time but we can also distinguish between shortterm and longterm connections,0
exploring complex adaptive financial trading environments through multiagent based simulation methods presents an innovative approach within the realm of quantitative finance despite the dominance of multiagent reinforcement learning approaches in financial markets with observable data there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability we therefore devise a multiagent simulation approach employing smallscale metaheuristic methods this approach aims to represent the opaque bilateral market for australian government bond trading capturing the bilateral nature of banktobank trading also referred to as overthecounter otc trading and commonly occurring between market makers the uniqueness of the bilateral market characterized by negotiated transactions and a limited number of agents yields valuable insights for agentbased modelling and quantitative finance the inherent rigidity of this market structure which is at odds with the global proliferation of multilateral platforms and the decentralization of finance underscores the unique insights offered by our agentbased model we explore the implications of market rigidity on market structure and consider the element of stability in market design this extends the ongoing discourse on complex financial trading environments providing an enhanced understanding of their dynamics and implications,0
bitcoin with its evergrowing popularity has demonstrated extreme price volatility since its origin this volatility together with its decentralised nature make bitcoin highly subjective to speculative trading as compared to more traditional assets in this paper we propose a multimodal model for predicting extreme price fluctuations this model takes as input a variety of correlated assets technical indicators as well as twitter content in an indepth study we explore whether social media discussions from the general public on bitcoin have predictive power for extreme price movements a dataset of 5000 tweets per day containing the keyword bitcoin was collected from 2015 to 2021 this dataset called prebit is made available online in our hybrid model we use sentencelevel finbert embeddings pretrained on financial lexicons so as to capture the full contents of the tweets and feed it to the model in an understandable way by combining these embeddings with a convolutional neural network we built a predictive model for significant market movements the final multimodal ensemble model includes this nlp model together with a model based on candlestick data technical indicators and correlated asset prices in an ablation study we explore the contribution of the individual modalities finally we propose and backtest a trading strategy based on the predictions of our models with varying prediction threshold and show that it can used to build a profitable trading strategy with a reduced risk over a hold or moving average strategy,0
we investigate logarithmic price returns crosscorrelations at different time horizons for a set of 25 liquid cryptocurrencies traded on the ftx digital currency exchange we study how the structure of the minimum spanning tree mst and the triangulated maximally filtered graph tmfg evolve from high 15 s to low 1 day frequency time resolutions for each horizon we test the stability statistical significance and economic meaningfulness of the networks results give a deep insight into the evolutionary process of the time dependent hierarchical organization of the system under analysis a decrease in correlation between pairs of cryptocurrencies is observed for finer time sampling resolutions a growing structure emerges for coarser ones highlighting multiple changes in the hierarchical reference role played by mainstream cryptocurrencies this effect is studied both in its pairwise realizations and intrasector ones,0
the ai traders in financial markets have sparked significant interest in their effects on price formation mechanisms and market volatility raising important questions for market stability and regulation despite this interest a comprehensive model to quantitatively assess the specific impacts of ai traders remains undeveloped this study aims to address this gap by modeling the influence of ai traders on market price formation and volatility within a multiagent framework leveraging the concept of microfoundations microfoundations involve understanding macroeconomic phenomena such as market price formation through the decisionmaking and interactions of individual economic agents while widely acknowledged in macroeconomics microfoundational approaches remain unexplored in empirical finance particularly for models like the garch model which captures key financial statistical properties such as volatility clustering and fat tails this study proposes a multiagent market model to derive the microfoundations of the garch model incorporating three types of agents noise traders fundamental traders and ai traders by mathematically aggregating the microstructure of these agents we establish the microfoundations of the garch model we validate this model through multiagent simulations confirming its ability to reproduce the stylized facts of financial markets finally we analyze the impact of ai traders using parameters derived from these microfoundations contributing to a deeper understanding of their role in market dynamics,0
in this paper we introduce a novel framework to model the exchange rate dynamics between two intrinsically linked cryptoassets such as stablecoins pegged to the same fiat currency or a liquid staking token and its associated native token our approach employs multilevel nested ornsteinuhlenbeck ou processes for which we derive key properties and develop calibration and filtering techniques then we design an automated market maker amm model specifically tailored for the swapping of closely related cryptoassets distinct from existing models our amm leverages the unique exchange rate dynamics provided by the multilevel nested ou processes enabling more precise risk management and enhanced liquidity provision we validate the model through numerical simulations using realworld data for the usdcusdt and wstethweth pairs demonstrating that it consistently yields efficient quotes this approach offers significant potential to improve liquidity in markets for pegged assets,0
this paper presents a realistic simulated stock market where large language models llms act as heterogeneous competing trading agents the opensource framework incorporates a persistent order book with market and limit orders partial fills dividends and equilibrium clearing alongside agents with varied strategies information sets and endowments agents submit standardized decisions using structured outputs and function calls while expressing their reasoning in natural language three findings emerge first llms demonstrate consistent strategy adherence and can function as value investors momentum traders or market makers per their instructions second market dynamics exhibit features of real financial markets including price discovery bubbles underreaction and strategic liquidity provision third the framework enables analysis of llms responses to varying market conditions similar to partial dependence plots in machinelearning interpretability the framework allows simulating financial theories without closedform solutions creating experimental designs that would be costly with human participants and establishing how prompts can generate correlated behaviors affecting market stability,0
urban housing markets along with markets of other assets universally exhibit periods of strong price increases followed by sharp corrections the mechanisms generating such nonlinearities are not yet well understood we develop an agentbased model populated by a large number of heterogeneous households the agents behavior is compatible with economic rationality with the trendfollowing behavior found to be essential in replicating market dynamics the model is calibrated using several large and distributed datasets of the greater sydney region demographic economic and financial across three specific and diverse periods since 2006 the model is not only capable of explaining price dynamics during these periods but also reproduces the novel behavior actually observed immediately prior to the market peak in 2017 namely a sharp increase in the variability of prices this novel behavior is related to a combination of trendfollowing aptitude of the household agents rational herding and their propensity to borrow,0
in this paper we provide an overview of the recent work in the quantum finance realm from various perspectives the applications in consideration are portfolio optimization fraud detection and monte carlo methods for derivative pricing and risk calculation furthermore we give a comprehensive overview of the applications of quantum computing in the field of blockchain technology which is a main concept in fintech in that sense we first introduce the general overview of blockchain with its main cryptographic primitives such as digital signature algorithms hash functions and random number generators as well as the security vulnerabilities of blockchain technologies after the merge of quantum computers considering shors quantum factoring and grovers quantum search algorithms we then discuss the privacy preserving quantumresistant blockchain systems via threshold signatures ring signatures and zeroknowledge proof systems ie zksnarks in quantum resistant blockchains after emphasizing the difference between the quantumresistant blockchain and quantumsafe blockchain we mention the security countermeasures to take against the possible quantumized attacks aiming these systems we finalize our discussion with quantum blockchain efficient quantum mining and necessary infrastructures for constructing such systems based on quantum computing this review has the intention to be a bridge to fill the gap between quantum computing and one of its most prominent application realms finance we provide the stateoftheart results in the intersection of finance and quantum technology for both industrial practitioners and academicians,0
in this study we introduce a physical model inspired by statistical physics for predicting price volatility and expected returns by leveraging level 3 order book data by drawing parallels between orders in the limit order book and particles in a physical system we establish unique measures for the systems kinetic energy and momentum as a way to comprehend and evaluate the state of limit order book our model goes beyond examining merely the top layers of the order book by introducing the concept of active depth a computationallyefficient approach for identifying order book levels that have impact on price dynamics we empirically demonstrate that our model outperforms the benchmarks of traditional approaches and machine learning algorithm our model provides a nuanced comprehension of market microstructure and produces more accurate forecasts on volatility and expected returns by incorporating principles of statistical physics this research offers valuable insights on understanding the behaviours of market participants and order book dynamics,0
temporal graph learning tgl is crucial for capturing the evolving nature of stock markets traditional methods often ignore the interplay between dynamic temporal changes and static relational structures between stocks to address this issue we propose the dynamic graph representation with contrastive learning dgrcl framework which integrates dynamic and static graph relations to improve the accuracy of stock trend prediction our framework introduces two key components the embedding enhancement ee module and the contrastive constrained training cct module the ee module focuses on dynamically capturing the temporal evolution of stock data while the cct module enforces static constraints based on stock relations refined within contrastive learning this dualrelation approach allows for a more comprehensive understanding of stock market dynamics our experiments on two major us stock market datasets nasdaq and nyse demonstrate that dgrcl significantly outperforms stateoftheart tgl baselines ablation studies indicate the importance of both modules overall dgrcl not only enhances prediction ability but also provides a robust framework for integrating temporal and relational data in dynamic graphs code and data are available for public access,0
we develop several innovations to bring the best practices of traditional investment funds to the blockchain landscape specifically we illustrate how 1 fund prices can be updated regularly like mutual funds 2 performance fees can be charged like hedge funds 3 mutually hedged blockchain investment funds can operate with investor protection schemes such as high water marks and 4 measures to offset trading related slippage costs when redemptions happen using our concepts and blockchain technology traditional funds can calculate performance fees in a simplified manner and alleviate several operational issues blockchain can solve many problems for traditional finance while tried and tested wealth management techniques can benefit decentralization speeding its adoption we provide detailed steps including mathematical formulations and instructive pointers to implement these ideas and discuss how our designs overcome several blockchain bottlenecks making smart contracts smarter we provide numerical illustrations of several scenarios related to our mechanisms,0
we present a simulationandregression method for solving dynamic portfolio allocation problems in the presence of general transaction costs liquidity costs and market impacts this method extends the classical least squares monte carlo algorithm to incorporate switching costs corresponding to transaction costs and transient liquidity costs as well as multiple endogenous state variables namely the portfolio value and the asset prices subject to permanent market impacts to do so we improve the accuracy of the control randomization approach in the case of discrete controls and propose a global iteration procedure to further improve the allocation estimates we validate our numerical method by solving a realistic cashandstock portfolio with a powerlaw liquidity model we quantify the certainty equivalent losses associated with ignoring liquidity effects and illustrate how our dynamic allocation protects the investors capital under illiquid market conditions lastly we analyze under different liquidity conditions the sensitivities of certainty equivalent returns and optimal allocations with respect to trading volume stock price volatility initial investment amount riskaversion level and investment horizon,0
the marketron model introduced by describes price formation in inelastic markets as the nonlinear diffusion of a quasiparticle the marketron in a multidimensional space comprising the logprice x a memory variable y encoding past money flows and unobservable return predictors z while the original work calibrated the model to sp 500 time series data this paper extends the framework to option markets a fundamentally distinct challenge due to market incompleteness stemming from nontradable state variables we develop a utilitybased pricing approach that constructs a riskadjusted measure via the dual solution of an optimal investment problem the resulting hamiltonjacobibellman hjb equation though computationally formidable is solved using a novel methodology enabling efficient calibration even on standard laptop hardware having done that we look at the additional question to answer whether the marketron model calibrated to market option prices can simultaneously reproduce the statistical properties of the underlying assets logreturns we discuss our results in view of the longstanding challenge in quantitative finance of developing an unified framework capable of jointly capturing equity returns option smile dynamics and potentially volatility index behavior,0
we propose an analytically tractable class of models for the dynamics of a limit order book described through a stochastic partial differential equation spde with multiplicative noise for the order book centered at the midprice along with stochastic dynamics for the midprice which is consistent with the order flow dynamics we provide conditions under which the model admits a finite dimensional realization driven by a lowdimensional markov process leading to efficient estimation and computation methods we study two examples of parsimonious models in this class a twofactor model and a model with meanreverting order book depth for each model we analyze in detail the role of different parameters the dynamics of the price order book depth volume and order imbalance provide an intuitive financial interpretation of the variables involved and show how the model reproduces statistical properties of price changes market depth and order flow in limit order markets,0
in the present paper we study the optimal execution problem under stochastic price recovery based on limit order book dynamics we model price recovery after execution of a large order by accelerating the arrival of the refilling order which is defined as a cox process whose intensity increases by the degree of the market impact we include not only the market order but also the limit order in our strategy in a restricted fashion we formulate the problem as a combined stochastic control problem over a finite time horizon the corresponding hamiltonjacobibellman quasivariational inequality is solved numerically the optimal strategy obtained consists of three components i the initial large trade ii the unscheduled small trades during the period iii the terminal large trade the size and timing of the trade is governed by the tolerance for market impact depending on the state at each time step and hence the strategy behaves dynamically we also provide competitive results due to inclusion of the limit order even though a limit order is allowed under conservative evaluation of the execution price,0
this research introduces a novel pairs trading strategy based on copulas for cointegrated pairs of cryptocurrencies to identify the most suitable pairs the study employs linear and nonlinear cointegration tests along with a correlation coefficient measure and fits different copula families to generate trading signals formulated from a reference asset for analyzing the mispricing index the strategys performance is then evaluated by conducting backtesting for various triggers of opening positions assessing its returns and risks the findings indicate that the proposed method outperforms buyandhold trading strategies in terms of both profitability and riskadjusted returns,0
leadlag relationships integral to market dynamics offer valuable insights into the trading behavior of highfrequency traders hfts and the flow of information at a granular level this paper investigates the leadlag relationships between stock index futures contracts of different maturities in the chinese financial futures market cffex using highfrequency tickbytick data we analyze how price movements in nearmonth futures contracts influence those in longerdated contracts such as nextmonth quarterly and semiannual contracts our findings reveal a consistent pattern of price discovery with the nearmonth contract leading the others by one tick driven primarily by liquidity additionally we identify a negative feedback effect of the leadlag spread on the leading asset which can predict returns of leading asset backtesting results demonstrate the profitability of trading based on the leadlag spread signal even after accounting for transaction costs altogether our analysis offers valuable insights to understand and capitalize on the evolving dynamics of futures markets,0
in this paper we analyze twitter signals as a medium for user sentiment to predict the price fluctuations of a smallcap alternative cryptocurrency called emphzclassic we extracted tweets on an hourly basis for a period of 35 weeks classifying each tweet as positive neutral or negative we then compiled these tweets into an hourly sentiment index creating an unweighted and weighted index with the latter giving larger weight to retweets these two indices alongside the raw summations of positive negative and neutral sentiment were juxtaposed to sim 400 data points of hourly pricing data to train an extreme gradient boosting regression tree model price predictions produced from this model were compared to historical price data with the resulting predictions having a 081 correlation with the testing data our models predictive data yielded statistical significance at the p 00001 level our model is the first academic proof of concept that social media platforms such as twitter can serve as powerful social signals for predicting price movements in the highly speculative alternative cryptocurrency or altcoin market,0
this paper presents an analytically tractable and practicallyoriented model of nonlinear dynamics of a multiasset market in the limit of a large number of assets the asset price dynamics are driven by money flows into the market from external investors and their price impact this leads to a model of a market as an ensemble of interacting nonlinear oscillators with the langevin dynamics in a homogeneous portfolio approximation the mean field treatment of the resulting langevin dynamics produces the mckeanvlasov equation as a dynamic equation for market returns due to the strong nonlinearity of the mckeanvlasov equation the resulting dynamics give rise to ergodicity breaking and first or secondorder phase transitions under variations of model parameters using a tractable potential of the nonequilibrium skew nes model previously suggested by the author for a singlestock case the new multiasset nes manes model enables an analytically tractable framework for a multiasset market the equilibrium expected market logreturn is obtained as a selfconsistent mean field of the mckeanvlasov equation and derived in closed form in terms of parameters that are inferred from market prices of sp 500 index options the model is able to accurately fit the market data for either a benign or distressed market environments while using only a single volatility parameter,0
blockchain technology is essential for the digital economy and metaverse supporting applications from decentralized finance to virtual assets however its potential is constrained by the blockchain trilemma which necessitates balancing decentralization security and scalability this study evaluates and compares two leading proofofstake pos systems algorand and ethereum 20 against these critical metrics our research interprets existing indices to measure decentralization evaluates scalability through transactional data and assesses security by identifying potential vulnerabilities utilizing realworld data we analyze each platforms strategies in a structured manner to understand their effectiveness in addressing trilemma challenges the findings highlight each platforms strengths and propose general methodologies for evaluating key blockchain characteristics applicable to other systems this research advances the understanding of blockchain technologies and their implications for the future digital economy data and code are available on github as open source,0
we present a simple model of a nonequilibrium selforganizing market where asset prices are partially driven by investment decisions of a boundedrational agent the agent acts in a stochastic market environment driven by various exogenous alpha signals agents own actions via market impact and noise unlike traditional agentbased models our agent aggregates all traders in the market rather than being a representative agent therefore it can be identified with a boundedrational component of the market itself providing a particular implementation of an invisible hand market mechanism in such setting market dynamics are modeled as a fictitious selfplay of such boundedrational marketagent in its adversarial stochastic environment as rewards obtained by such selfplaying market agent are not observed from market data we formulate and solve a simple model of such market dynamics based on a neuroscienceinspired bounded rational information theoretic inverse reinforcement learning britirl this results in effective asset price dynamics with a nonlinear mean reversion which in our model is generated dynamically rather than being postulated we argue that our model can be used in a similar way to the blacklitterman model in particular it represents in a simple modeling framework market views of common predictive signals market impacts and implied optimal dynamic portfolio allocations and can be used to assess values of private signals moreover it allows one to quantify a marketimplied optimal investment strategy along with a measure of market rationality our approach is numerically light and can be implemented using standard offtheshelf software such as tensorflow,0
the disbalance of supply and demand is typically considered as the driving force of the markets however the measurement or estimation of supply and demand at price different from the execution price is not possible even after the transaction an approach in which supply and demand are always matched but the rate idvdt number of units traded per unit time of their matching varies is proposed the state of the system is determined not by a price p but by a probability distribution defined as the square of a wavefunction p the equilibrium state  is postulated to be the one giving maximal i and obtained from maximizing the matching rate functional i2p2p ie solving the dynamic equation of the form future price tend to the value maximizing the number of shares traded per unit time an application of the theory in a quasistationary case is demonstrated this transition from supply and demand concept to liquidity deficit concept described by the matching rate i allows to operate only with observable variables and have a theory applicable to practical problems,0
financial markets are difficult to predict due to its complex systems dynamics although there have been some recent studies that use machine learning techniques for financial markets prediction they do not offer satisfactory performance on financial returns we propose a novel onedimensional convolutional neural networks cnn model to predict financial market movement the customized onedimensional convolutional layers scan financial trading data through time while different types of data such as prices and volume share parameters kernels with each other our model automatically extracts features instead of using traditional technical indicators and thus can avoid biases caused by selection of technical indicators and predefined coefficients in technical indicators we evaluate the performance of our prediction model with strictly backtesting on historical trading data of six futures from january 2010 to october 2017 the experiment results show that our cnn model can effectively extract more generalized and informative features than traditional technical indicators and achieves more robust and profitable financial performance than previous machine learning approaches,0
the dual crises of the subprime mortgage crisis and the global financial crisis has prompted a call for explanations of nonequilibrium market dynamics recently a promising approach has been the use of agent based models abms to simulate aggregate market dynamics a key aspect of these models is the endogenous emergence of critical transitions between equilibria ie market collapses caused by multiple equilibria and changing market parameters several research themes have developed microeconomic based models that include multiple equilibria social decision theory brock and durlauf quantal response models mckelvey and palfrey and strategic complementarities goldstein a gap that needs to be filled in the literature is a unified analysis of the relationship between these models and how aggregate criticality emerges from the individual agent level this article reviews the agentbased foundations of markets starting with the individual agent perspective of mcfadden and the aggregate perspective of catastrophe theory emphasising connections between the different approaches it is shown that changes in the uncertainty agents have in the value of their interactions with one another even if these changes are onesided plays a central role in systemic market risks such as market instability and the twin crises effect these interactions can endogenously cause crises that are an emergent phenomena of markets,0
market simulator tries to create highquality synthetic financial data that mimics realworld market dynamics which is crucial for model development and robust assessment despite continuous advancements in simulation methodologies market fluctuations vary in terms of scale and sources but existing frameworks often excel in only specific tasks to address this challenge we propose financial wind tunnel fwt a retrievalaugmented market simulator designed to generate controllable reasonable and adaptable market dynamics for model testing fwt offers a more comprehensive and systematic generative capability across different data frequencies by leveraging a retrieval method to discover crosssectional information as the augmented condition our diffusionbased simulator seamlessly integrates both macro and microlevel market patterns furthermore our framework allows the simulation to be controlled with wide applicability including causal generation through whatif prompts or unprecedented crossmarket trend synthesis additionally we develop an automated optimizer for downstream quantitative models using stress testing of simulated scenarios via fwt to enhance returns while controlling risks experimental results demonstrate that our approach enables the generalizable and reliable market simulation significantly improve the performance and adaptability of downstream models particularly in highly complex and volatile market conditions our code and data sample is available at,0
deep generative models are becoming increasingly used as tools for financial analysis however it is unclear how these models will influence financial markets especially when they infer financial value in a semiautonomous way in this work we explore the interplay between deep generative models and market dynamics we develop a form of virtual traders that use deep generative models to make buysell decisions which we term neurosymbolic traders and expose them to a virtual market under our framework neurosymbolic traders are agents that use visionlanguage models to discover a model of the fundamental value of an asset agents develop this model as a stochastic differential equation calibrated to market data using gradient descent we test our neurosymbolic traders on both synthetic data and real financial time series including an equity stock commodity and a foreign exchange pair we then expose several groups of neurosymbolic traders to a virtual market environment this market environment allows for feedback between the traders belief of the underlying value to the observed price dynamics we find that this leads to price suppression compared to the historical data highlighting a future risk to market stability our work is a first step towards quantifying the effect of deep generative agents on markets dynamics and sets out some of the potential risks and benefits of this approach in the future,0
gamification is an effective strategy for motivating and engaging users which is grounded in business marketing and management by designing games in nongame contexts gamifying education which consists of the design and study of educational games is an emerging trend however the existing classroom games for understanding macroeconomics have weak connections to the microfoundations of individual decisionmaking we design an educational game on cryptocurrency investment for understanding macroeconomic concepts in microeconomic decisions we contribute to the literature by designing gamebased learning that engages students in understanding macroeconomics in incentivized individual investment decisions our game can be widely implemented in online inperson and hybrid classrooms we also reflect on strategies for improving the user experience for future educational game implementations,0
in this paper we study the herding phenomena in financial markets arising from the combined effect of 1 noncoordinated collective interactions between the market players and 2 concurrent reactions of market players to dynamic market signals by interpreting the expected rate of return of an asset and the favorability on that asset as position and velocity in phase space we construct an agentbased particle model for herding behavior in finance we then define two types of herding functionals using this model and show that they satisfy a gronwall type estimate and a lasalle type invariance property respectively leading to the herding behavior of the market players various numerical tests are presented to numerically verify these results,0
i study the price dynamics of nonfungible tokens nfts and propose a deep learning framework for dynamic valuation of nfts i use data from the ethereum blockchain and opensea to train a deep learning model on historical trades market trends and traitsrarity features of bored ape yacht club nfts after hyperparameter tuning the model is able to predict the price of nfts with high accuracy i propose an application framework for this model using zeroknowledge machine learning zkml and discuss its potential use cases in the context of decentralized finance defi applications,0
this paper introduces a novel robust trading paradigm called textitmultidouble linear policies situated within a textitgeneralized lattice market distinctively our framework departs from most existing robust trading strategies which are predominantly limited to single or paired assets and typically embed asset correlation within the trading strategy itself rather than as an inherent characteristic of the market our generalized lattice market model incorporates both serially correlated returns and asset correlation through a conditional probabilistic model in the nominal case where the parameters of the model are known we demonstrate that the proposed policies ensure survivability and probabilistic positivity we then derive an analytic expression for the worstcase expected gainloss and prove sufficient conditions that the proposed policies can maintain a textitpositive expected profits even within a seemingly nonprofitable symmetric lattice market when the parameters are unknown and require estimation we show that the parameter space of the lattice model forms a convex polyhedron and we present an efficient estimation method using a constrained leastsquares method these theoretical findings are strengthened by extensive empirical studies using data from the top 30 companies within the sp 500 index substantiating the efficacy of the generalized model and the robustness of the proposed policies in sustaining the positive expected profit and providing downside risk protection,0
we propose in this paper to consider the stock market as a physical system assimilate to a fluid evolving in a macroscopic space subject to a force that influences its movement over time where this last is arising from the collision between the supply and the demand of financial agents in fluid mechanics this force also results from the collisions of fluid molecules led by its physical property such as density viscosity and surface tension the purpose of this article is to show that the dynamism of the stock market behavior can be explained qualitatively and quantitatively by considering the supply demand collision as the result of financial agents physical properties defined by stokes law the first objective of this article is to show theoretically that fluid mechanics equations can be used to describe stock market physical properties the second objective based on the knowledge of stock market physical properties is to propose an econophysics analog of the stock market viscosity and reynolds number to measure stock market conditions whether laminar transitory or turbulent the reynolds number defined in this way can be applied in research into the study and classification of stock market dynamics phases through for instance the creation of econophysics analog of moddy diagram this last could be seen as a physical way to quantify asset and stock index idiosyncratic risk the last objective is to present evidence from a computer simulation that the stock market behavior can be a priori and posteriori explained by physical properties viscosity density quantifiable by fluid mechanics law stokes law and measurable with the stock market reynolds number,0
we consider a dynamic market model where buyers and sellers submit limit orders if at a given moment in time the buyer is unable to complete his entire order due to the shortage of sell orders at the required limit price the unmatched part of the order is recorded in the order book subsequently these buy unmatched orders may be matched with new incoming sell orders the resulting demand curve constitutes the sole input to our model the clearing price is then mechanically calculated using the market clearing condition we use a brownian sheet to model the demand curve and provide some theoretical assumptions under which such a model is justified our main result is the proof that if there exists a unique equivalent martingale measure for the clearing price then under some mild assumptions there is no arbitrage we use the ito wentzell formula to obtain that result and also to characterize the dynamics of the demand curve and of the clearing price in the equivalent measure we find that the volatility of the clearing price is up to a stochastic factor inversely proportional to the sum of buy and sell order flow density evaluated at the clearing price which confirms the intuition that volatility is inversely proportional to volume we also demonstrate that our approach is implementable we use real order book data and simulate option prices under a particularly simple parameterization of our model the noarbitrage conditions we obtain are applicable to a wide class of models in the same way that the heathjarrowmorton conditions apply to a wide class of interest rate models,0
we present a stochasticlocal volatility model for derivative contracts on commodity futures able to describe forwardcurve and smile dynamics with a fast calibration to liquid market quotes a parsimonious parametrization is introduced to deal with the limited number of options quoted in the market cleared commodity markets for futures and options are analyzed to include in the pricing framework specific trading clauses and margining procedures numerical examples for calibration and pricing are provided for different commodity products,0
the efficient market hypothesis emh based on rational expectations and market equilibrium is the dominant perspective for modelling economic markets however the most notable critique of the emh is the inability to model periods of outofequilibrium behaviour in the absence of any significant external news when such dynamics emerge endogenously the traditional economic frameworks provide no explanation for such behaviour and the deviation from equilibrium this work offers an alternate perspective explaining the endogenous emergence of punctuated outofequilibrium dynamics based on bounded rational agents in a concise market entrance game we show how boundedly rational strategic reasoning can lead to endogenously emerging crises exhibiting fat tails in returns we also show how other common stylised facts of economic markets such as clustered volatility can be explained due to agent diversity or lack thereof and the varying learning updates across the agents this work explains various stylised facts and crisis emergence in economic markets in the absence of any external news based purely on agent interactions and bounded rational reasoning,0
we demonstrate the application of an algorithmic trading strategy based upon the recently developed dynamic mode decomposition dmd on portfolios of financial data the method is capable of characterizing complex dynamical systems in this case financial market dynamics in an equationfree manner by decomposing the state of the system into lowrank terms whose temporal coefficients in time are known by extracting key temporal coherent structures portfolios in its sampling window it provides a regression to a best fit linear dynamical system allowing for a predictive assessment of the market dynamics and informing an investment strategy the datadriven analytics capitalizes on stock market patterns either real or perceived to inform buysellhold investment decisions critical to the method is an associated learning algorithm that optimizes the sampling and prediction windows of the algorithm by discovering trading hotspots the underlying mathematical structure of the algorithms is rooted in methods from nonlinear dynamical systems and shows that the decomposition is an effective mathematical tool for datadriven discovery of market patterns,0
a spin model is used for simulations of financial markets to determine return volatility in the spin financial market we use the garch model often used for volatility estimation in empirical finance we apply the bayesian inference performed by the markov chain monte carlo method to the parameter estimation of the garch model it is found that volatility determined by the garch model exhibits volatility clustering also observed in the real financial markets using volatility determined by the garch model we examine the mixtureofdistribution hypothesis mdh suggested for the asset return dynamics we find that the returns standardized by volatility are approximately standard normal random variables moreover we find that the absolute standardized returns show no significant autocorrelation these findings are consistent with the view of the mdh for the return dynamics,0
multiresolution analysis has applications across many disciplines in the study of complex systems and their dynamics financial markets are among the most complex entities in our environment yet mainstream quantitative models operate at predetermined scale rely on linear correlation measures and struggle to recognize nonlinear or causal structures in this paper we combine neural networks known to capture nonlinear associations with a multiscale decomposition to facilitate a better understanding of financial market data substructures quantization keeps our decompositions calibrated to market at every scale we illustrate our approach in the context of seven use cases,0
we conduct modeling of the price dynamics following order flow imbalance in market microstructure and apply the model to the analysis of chinese csi 300 index futures there are three findings the first is that the order flow imbalance is analogous to a shock to the market unlike the common practice of using hawkes processes we model the impact of order flow imbalance as an ornsteinuhlenbeck process with memory and meanreverting characteristics driven by a jumptype lvy process motivated by the empirically stable correlation between order flow imbalance and contemporaneous price changes we propose a modified asset price model where the drift term of canonical geometric brownian motion is replaced by an ornsteinuhlenbeck process we establish stochastic differential equations and derive the logarithmic return process along with its mean and variance processes under initial boundary conditions and evolution of costeffectiveness ratio with order flow imbalance as the trading trigger point termed as the quasisharpe ratio or response ratio secondly our results demonstrate horizondependent heterogeneity in how conventional metrics interact with order flow imbalance this underscores the critical role of forecast horizon selection for strategies thirdly we identify regimedependent dynamics in the memory and forecasting power of order flow imbalance this taxonomy provides both a screening protocol for existing indicators and an exante evaluation paradigm for novel metrics,0
we calculate the realized volatility in the spin model of financial markets and examine the returns standardized by the realized volatility we find that moments of the standardized returns agree with the theoretical values of standard normal variables this is the first evidence that the return dynamics of the spin financial market is consistent with the view of the mixtureofdistribution hypothesis that also holds in the real financial markets,0
in this paper we explore the usage of deep reinforcement learning algorithms to automatically generate consistently profitable robust uncorrelated trading signals in any general financial market in order to do this we present a novel markov decision process mdp model to capture the financial trading markets we review and propose various modifications to existing approaches and explore different techniques like the usage of technical indicators to succinctly capture the market dynamics to model the markets we then go on to use deep reinforcement learning to enable the agent the algorithm to learn how to take profitable trades in any market on its own while suggesting various methodology changes and leveraging the unique representation of the fmdp financial mdp to tackle the primary challenges faced in similar works through our experimentation results we go on to show that our model could be easily extended to two very different financial markets and generates a positively robust performance in all conducted experiments,0
we engineer blockchain based risk managed portfolios by creating three funds with distinct risk and return profiles 1 alpha high risk portfolio 2 beta mimics the wider market and 3 gamma represents the risk free rate adjusted to beat inflation each of the subfunds alpha beta and gamma provides risk parity because the weight of each asset in the corresponding portfolio is set to be inversely proportional to the risk derived from investing in that asset this can be equivalently stated as equal risk contributions from each asset towards the overall portfolio risk we provide detailed mechanics of combining assets including mathematical formulations to obtain better risk managed portfolios the descriptions are intended to show how a risk parity based efficient frontier portfolio management engine that caters to different risk appetites of investors by letting each individual investor select their preferred riskreturn combination can be created seamlessly on blockchain any investor using decentralized ledger technology can select their desired level of risk or return and allocate their wealth accordingly among the sub funds which balance one another under different market conditions this evolution of the risk parity principle resulting in a mechanism that is geared to do well under all market cycles brings more robust performance and can be termed as conceptual parity we have given several numerical examples that illustrate the various scenarios that arise when combining alpha beta and gamma to obtain parity the final investment frontier is now possible a modification to the efficient frontier thus becoming more than a mere theoretical construct on blockchain since anyone from anywhere can participate at anytime to obtain wealth appreciation based on their financial goals,0
leadlag relationships among assets represent a useful tool for analyzing high frequency financial data however research on these relationships predominantly focuses on correlation analyses for the dynamics of stock prices spots and futures on market indexes whereas foreign exchange data have been less explored to provide a valuable insight on the nature of the leadlag relationships in foreign exchange markets here we perform a detailed study for the oneminute log returns on exchange rates through three different approaches i lagged correlations ii lagged partial correlations and iii granger causality in all studies we find that even though for most pairs of exchange rates lagged effects are absent there are many pairs which pass statistical significance tests out of the statistically significant relationships we construct directed networks and investigate the influence of individual exchange rates through the pagerank algorithm the algorithm in general ranks stock market indexes quoted in their respective currencies as most influential in contrast to the claims of the efficient market hypothesis these findings suggest that all market information does not spread instantaneously,0
passive liquidity providers lps in automated market makers amms face losses due to adverse selection lvr which static trading fees often fail to offset in practice we study the key determinants of lp profitability in a dynamic reducedform model where an amm operates in parallel with a centralized exchange cex traders route their orders optimally to the venue offering the better price and arbitrageurs exploit price discrepancies using largescale simulations and real market data we analyze how lp profits vary with market conditions such as volatility and trading volume and characterize the optimal amm fee as a function of these conditions we highlight the mechanisms driving these relationships through extensive comparative statics and confirm the models relevance through market data calibration a key tradeoff emerges fees must be low enough to attract volume yet high enough to earn sufficient revenues and mitigate arbitrage losses we find that under normal market conditions the optimal amm fee is competitive with the trading cost on the cex and remarkably stable whereas in periods of very high volatility a high fee protects passive lps from severe losses these findings suggest that a thresholdtype dynamic fee schedule is both robust enough to market conditions and improves lp outcomes,0
in this research we focus on the ordersplitting behavior the order splitting is a trading strategy to execute their large potential metaorder into small pieces to reduce transaction cost this strategic behavior is believed to be important because it is a promising candidate for the microscopic origin of the longrange correlation lrc in the persistent order flow indeed in 2005 lillo mike and farmer lmf introduced a microscopic model of the ordersplitting traders to predict the asymptotic behavior of the lrc from the microscopic dynamics even quantitatively the plausibility of this scenario has been qualitatively investigated by toth et al 2015 however no solid support has been presented yet on the quantitative prediction by the lmf model in the lack of large microscopic datasets in this report we have provided the first quantitative statistical analysis of the ordersplitting behavior at the level of each trading account we analyse a large dataset of the tokyo stock exchange tse market over nine years including the account data of traders called virtual servers the virtual server is a unit of trading accounts in the tse market and we can effectively define the trader ids by an appropriate preprocessing we apply a strategy clustering to individual traders to identify the ordersplitting traders and the random traders for most of the stocks we find that the metaorder length distribution obeys power laws with exponent  such that plpropto l1 with the metaorder length l by analysing the sign correlation cpropto  we directly confirmed the lmf prediction approx 1 furthermore we discuss how to estimate the total number of the splitting traders only from public data via the acf prefactor formula in the lmf model our work provides the first quantitative evidence of the lmf model,0
as renewable energy integration increases supply variability battery energy storage systems bess present a viable solution for balancing supply and demand this paper proposes a novel approach for optimizing battery bess participation in multiple electricity markets we develop a joint bidding strategy that combines participation in the primary frequency reserve market with continuous trading in the intraday market addressing a gap in the extant literature which typically considers these markets in isolation or simplifies the continuous nature of intraday trading our approach utilizes a mixed integer linear programming implementation of the rolling intrinsic algorithm for intraday decisions and state of charge recovery alongside a learned classifier strategy lcs that determines optimal capacity allocation between markets a comprehensive outofsample backtest over more than one year of historical german market data validates our approach the lcs increases overall profits by over 4 compared to the bestperforming static strategy and by more than 3 over a naive dynamic benchmark crucially our method closes the gap to a theoretical perfect foresight strategy to just 4 demonstrating the effectiveness of dynamic learningbased allocation in a complex multimarket environment,0
machine learning is critical for innovation and efficiency in financial markets offering predictive models and datadriven decisionmaking however challenges such as missing data lack of transparency untimely updates insecurity and incompatible data sources limit its effectiveness blockchain technology with its transparency immutability and realtime updates addresses these challenges we present a framework for integrating highfrequency onchain data with lowfrequency offchain data providing a benchmark for addressing novel research questions in economic mechanism design this framework generates modular extensible datasets for analyzing economic mechanisms such as the transaction fee mechanism enabling multimodal insights and fairnessdriven evaluations using four machine learning techniques including linear regression deep neural networks xgboost and lstm models we demonstrate the frameworks ability to produce datasets that advance financial research and improve understanding of blockchaindriven systems our contributions include 1 proposing a research scenario for the transaction fee mechanism and demonstrating how the framework addresses previously unexplored questions in economic mechanism design 2 providing a benchmark for financial machine learning by opensourcing a sample dataset generated by the framework and the code for the pipeline enabling continuous dataset expansion and 3 promoting reproducibility transparency and collaboration by fully opensourcing the framework and its outputs this initiative supports researchers in extending our work and developing innovative financial machinelearning models fostering advancements at the intersection of machine learning blockchain and economics,0
i unravel the basic long run dynamics of the broker call money market which is the pile of cash that funds margin loans to retail clients read continuous time kelly gamblers call money is assumed to supply itself perfectly inelastically and to continuously reinvest all principal and interest i show that the relative size of the money market that is relative to the kelly bankroll is a martingale that nonetheless converges in probability to zero the margin loan interest rate is a submartingale that converges in mean square to the choke price rinfty22 where  is the asymptotic compound growth rate of the stock market and  is its annual volatility in this environment the gambler no longer beats the market asymptotically as by an exponential factor as he would under perfectly elastic supply rather he beats the market asymptotically with very high probability think 98 by a factor say 187 or 87 more final wealth whose mean cannot exceed what the leverage ratio was at the start of the model say 21 although the ratio of the gamblers wealth to that of an equivalent buyandhold investor is a submartingale always expected to increase his realized compound growth rate converges in mean square to  this happens because the equilibrium leverage ratio converges to 11 in lockstep with the gradual rise of margin loan interest rates,0
we propose a framework for studying optimal market making policies in a limit order book lob the bidask spread of the lob is modelled by a markov chain with finite values multiple of the tick size and subordinated by the poisson process of the ticktime clock we consider a small agent who continuously submits limit buysell orders and submits market orders at discrete dates the objective of the market maker is to maximize her expected utility from revenue over a short term horizon by a tradeoff between limit and market orders while controlling her inventory position this is formulated as a mixed regime switching regular impulse control problem that we characterize in terms of quasivariational system by dynamic programming methods in the case of a meanvariance criterion with martingale reference price or when the asset price follows a levy process and with exponential utility criterion the dynamic programming system can be reduced to a system of simple equations involving only the inventory and spread variables calibration procedures are derived for estimating the transition matrix and intensity parameters for the spread and for cox processes modelling the execution of limit orders several computational tests are performed both on simulated and real data and illustrate the impact and profit when considering execution priority in limit orders and market orders,0
this paper explores a timevarying version of weakform market efficiency that is a key component of the socalled adaptive market hypothesis amh one of the most common methodologies used for modeling and estimating a degree of market efficiency lies in an analysis of the serial autocorrelation in observed return series under the amh a timevarying market efficiency level is modeled by timevarying autoregressive ar process and traditionally estimated by the kalman filter kf being a linear estimator the kf is hardly capable to track the hidden nonlinear dynamics that is an essential feature of the models under investigation the contribution of this paper is threefold we first provide a brief overview of timevarying ar models and estimation methods utilized for testing a weakform market efficiency in econometrics literature secondly we propose novel accurate estimation approach for recovering the hidden process of evolving market efficiency level by the extended kalman filter ekf thirdly our empirical study concerns an examination of the standard and poors 500 composite stock index and the dow jones industrial average index monthly data covers the period from november 1927 to june 2020 which includes the us great depression the 20082009 global financial crisis and the first wave of recent covid19 recession the results reveal that the us market was affected during all these periods but generally remained weakform efficient since the mid of 1946 as detected by the estimator,0
we propose a highly efficient and accurate methodology for generating synthetic financial market data using a diffusion model approach the synthetic data produced by our methodology align closely with observed market data in several key aspects i they pass the twosample cramer von mises test for portfolios of assets and ii q q plots demonstrate consistency across quantiles including in the tails between observed and generated market data moreover the covariance matrices derived from a large set of synthetic market data exhibit significantly lower condition numbers compared to the estimated covariance matrices of the observed data this property makes them suitable for use as regularized versions of the latter for model training we develop an efficient and fast algorithm based on numerical integration rather than monte carlo simulations the methodology is tested on a large set of equity data,0
we construct realistic spot and equity option market simulators for a single underlying on the basis of normalizing flows we address the highdimensionality of market observed call prices through an arbitragefree autoencoder that approximates efficient lowdimensional representations of the prices while maintaining no static arbitrage in the reconstructed surface given a multiasset universe we leverage the conditional invertibility property of normalizing flows and introduce a scalable method to calibrate the joint distribution of a set of independent simulators while preserving the dynamics of each simulator empirical results highlight the goodness of the calibrated simulators and their fidelity,0
gold and currency markets form a unique pair with specific interactions and dynamics we focus on the efficiency ranking of gold markets with respect to the currency of purchase by utilizing the efficiency index ei based on fractal dimension approximate entropy and longterm memory on a wide portfolio of 142 gold price series for different currencies we construct the efficiency ranking based on the extended ei methodology we provide rather unexpected results are uncovered as the gold prices in major currencies lay among the least efficient ones whereas very minor currencies are among the most efficient ones we argue that such counterintuitive results can be partly attributed to a unique period of examination 20112014 characteristic by quantitative easing and rather unorthodox monetary policies together with the investigated illegal collusion of major foreign exchange market participants as well as some other factors discussed in some detail,0
this paper presents a tractable model of nonlinear dynamics of market returns using a langevin approach due to nonlinearity of an interaction potential the model admits regimes of both small and large return fluctuations langevin dynamics are mapped onto an equivalent quantum mechanical qm system borrowing ideas from supersymmetric quantum mechanics susy qm a parameterized ground state wave function wf of this qm system is used as a direct input to the model which also fixes a nonlinear langevin potential using a twocomponent gaussian mixture as a ground state wf with an asymmetric double well potential produces a tractable lowparametric model with interpretable parameters referred to as the nes nonequilibrium skew model supersymmetry susy is then used to find timedependent solutions of the model in an analytically tractable way additional approximations give rise to a final practical version of the nes model where realmeasure and riskneutral return distributions are given by three component gaussian mixtures this produces a closedform approximation for option pricing in the nes model by a mixture of three blackscholes prices providing accurate calibration to option prices for either benign or distressed market environments while using only a single volatility parameter these results stand in stark contrast to the most of other option pricing models such as local stochastic or rough volatility models that need more complex specifications of noise to fit the market data,0
in this article we apply the forward variance modeling approach by lbergomi to the coterminal swap market model we build an interest rate model for which all the market price changes of hedging instruments interest rate swaps and european swaptions are interpreted as the state variable variations and no diffusion parameter calibration procedure is required the model provides quite simple profit and loss pnl formula with which we can easily understand where a material pnl trend comes from when it appears and consider how we should modify the model parameters the model has high flexibility to control the model dynamics because parameter calibration is unnecessary and the model parameters can be used solely for the purpose of the model dynamics control with the model the position management of the exotic interest rate products eg bermudan swaptions can be carried out in a more sophisticated and systematic manner a numerical experiment is performed to show the effectiveness of the approach for a canary swaption which is a special form of a bermudan swaption,0
neural network based datadriven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics though in this sense generative market simulation is modelfree the concrete modelling choices are nevertheless decisive for the features of the simulated paths we give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series and address some of the challenges to achieve good results in the latter we also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach while most generative models tend to rely on large amounts of training data we present here a generative model that works reliably in environments where the amount of available training data is notoriously small furthermore we show how a rough paths perspective combined with a parsimonious variational autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce finally we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our market generator to deep hedging,0
despite being described as a medium of exchange cryptocurrencies do not have the typical attributes of a medium of exchange consequently cryptocurrencies are more appropriately described as crypto assets a common investment attribute shared by the more than 2500 crypto assets is that they are highly volatile an investor interested in reducing price volatility of a portfolio of crypto assets can do so by constructing an optimal portfolio through standard optimization techniques that minimize tail risk because crypto assets are not backed by any real assets forming a hedge to reduce the risk contribution of a single crypto asset can only be done with another set of similar assets ie a set of other crypto assets a major finding of this paper is that crypto portfolios constructed via optimizations that minimize variance and conditional value at risk outperform a major stock market index the sp 500 as of this writing options in which the underlying is a crypto asset index are not traded one of the reasons being that the academic literature has not formulated an acceptable fair pricing model we offer a fair valuation model for crypto asset options based on a dynamic pricing model for the underlying crypto assets the model was carefully backtested and therefore offers a reliable model for the underlying crypto assets in the natural world we then obtain the valuation of crypto options by passing the natural world to the equivalent martingale measure via the esscher transform because of the absence of traded crypto options we could not compare the prices obtained from our valuation model to market prices yet we can claim that if such options on crypto assets are introduced they should follow closely our theoretical prices after adjusting for market frictions and design feature nuances,0
online social networks offer a new way to investigate financial markets dynamics by enabling the largescale analysis of investors collective behavior we provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship we take advantage of an extensive data set composed of social media messages related to djia index components by using informationtheoretic measures to cope for possible nonlinear causal coupling between social media and stock markets systems we point out stunning differences in the results with respect to linear coupling two main conclusions are drawn first social media significant causality on stocks returns are purely nonlinear in most cases second social media dominates the directional coupling with stock market an effect not observable within linear modeling results also serve as empirical guidance on model adequacy in the investigation of sociotechnical and financial systems,0
market competition has a role which is directly or indirectly associated with influential effects of individual sectors on other sectors of the economy the present work studies the relative position of a product in the market through the identification of influential spreaders and its corresponding effect on the other sectors of the market using complex network analysis during the pre in and postcrisis induced lockdown periods using daily data of nse from december 2019 to june 2021 the existing approaches using different centrality measures failed to distinguish between the positive and negative influences of the different sectors in the market which act as spreaders to obviate this problem this paper presents an effective measure called liest local influential effects for specific target that can examine the positive and negative influences separately with respect to any crisis period liest considers the combined impact of all possible nodes which are at most three steps away from the specific targets for the networks the essence of nonlinearity in the network dynamics without considering single node effect becomes visible particularly in the proposed network,0
we consider the problem of simultaneously approximating the conditional distribution of market prices and their log returns with a single machine learning model we show that an instance of the gdn model of kratsios and papon 2022 solves this problem without having prior assumptions on the markets clipped log returns other than that they follow a generalized ornsteinuhlenbeck process with a priori unknown dynamics we provide universal approximation guarantees for these conditional distributions and contingent claims with a lipschitz payoff function,0
highfrequency trading hft is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds traditional hft approaches fit models with historical data and assume that future market states follow similar patterns this limits the effectiveness of any single model to the specific conditions it was trained for additionally these models achieve optimal solutions only under specific market conditions such as assumptions about stock prices stochastic process stable order flow and the absence of sudden volatility realworld markets however are dynamic diverse and frequently volatile to address these challenges we propose the flowhft a novel imitation learning framework based on flow matching policy flowhft simultaneously learns strategies from numerous expert models each proficient in particular market scenarios as a result our framework can adaptively adjust investment decisions according to the prevailing market state furthermore flowhft incorporates a gridsearch finetuning mechanism this allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal we test flowhft in multiple market environments we first show that flow matching policy is applicable in stochastic market environments thus enabling flowhft to learn trading strategies under different market conditions notably our single framework consistently achieves performance superior to the best expert for each market condition,0
the market impact mi of volume weighted average price vwap orders is a convex function of a trading rate but most empirical estimates of transaction cost are concave functions how is this possible we show that isochronic constant trading time mi is slightly convex and isochoric constant trading volume mi is concave we suggest a model that fits all trading regimes and guarantees nodynamicarbitrage,0
blockchain technology has revolutionized financial markets by enabling decentralized exchanges dexs that operate without intermediaries uniswap v2 a leading dex facilitates the rapid creation and trading of new tokens offering high return potential but exposing investors to significant risks in this work we analyze the financial impact of newly created tokens assessing their market dynamics profitability and liquidity manipulations our findings reveal that a significant portion of market liquidity is trapped in honeypots reducing market efficiency and misleading investors applying a simple buyandhold strategy we are able to uncover some major risks associated with investing in newly created tokens including the widespread presence of rug pulls and sandwich attacks we extract the optimal sandwich amount revealing that their proliferation in new tokens stems from higher profitability in lowliquidity pools furthermore we analyze the fundamental differences between token price evolution in swap time and physical time using clustering techniques we highlight these differences and identify typical patterns of honeypot and sellable tokens our study provides insights into the risks and financial dynamics of decentralized markets and their challenges for investors,0
we propose a machine learningbased extension of the classical binomial option pricing model that incorporates key market microstructure effects traditional models assume frictionless markets overlooking empirical features such as bidask spreads discrete price movements and serial return correlations our framework augments the binomial tree with pathdependent transition probabilities estimated via random forest classifiers trained on highfrequency market data this approach preserves noarbitrage conditions while embedding realworld trading dynamics into the pricing model using 46655 minutelevel observations of spy from january to june 2025 we achieve an auc of 8825 in forecasting onestep price movements order flow imbalance is identified as the most influential predictor contributing 432 to feature importance after resolving timescaling inconsistencies in tree construction our model yields option prices that deviate by 1379 from blackscholes benchmarks highlighting the impact of microstructure on fair value estimation while computational limitations restrict the model to shortterm derivatives our results offer a robust datadriven alternative to classical pricing methods grounded in empirical market behavior,0
the graph protocol indexes historical blockchain transaction data and makes it available for querying as the protocol is decentralized there are many independent indexers that index and compete with each other for serving queries to the consumers one dimension along which indexers compete is pricing in this paper we propose a banditbased algorithm for maximization of indexers revenue via consumer budget discovery we present the design and the considerations we had to make for a dynamic pricing algorithm being used by multiple agents simultaneously we discuss the results achieved by our dynamic pricing bandits both in simulation and deployed into production on one of the indexers operating on ethereum we have opensourced both the simulation framework and tools we created which other indexers have since started to adapt into their own workflows,0
we explore a simple lattice field model intended to describe statistical properties of high frequency financial markets the model is relevant in the crossdisciplinary area of econophysics its signature feature is the emergence of a selforganized critical state this implies scale invariance of the model without tuning parameters prominent results of our simulation are time series of gains prices volatility and gains frequency distributions which all compare favorably to features of historical market data applying a standard garch11 fit to the lattice model gives results that are almost indistinguishable from historical nasdaq data,0
we examine the dynamics of informational efficiency in a market with asymmetrically informed boundedly rational traders who adaptively learn optimal strategies using simple multiarmed bandit mab algorithms the strategies available to the traders have two dimensions on the one hand the traders must endogenously choose whether to acquire a costly information signal on the other they must determine how aggressively they trade by choosing the share of their wealth to be invested in the risky asset our study contributes to two strands of literature the literature comparing the effects of competitive and strategic behavior on asset price efficiency under costly information as well as the actively growing literature on algorithmic tacit collusion and pseudocollusion in financial markets we find that for certain market environments with low information costs our model reproduces the results of kyle in that the ability of traders to trade strategically leads to worse price efficiency compared to the purely competitive case for other environments with high information costs on the other hand our results show that a market with strategically acting traders can be more efficient than a purely competitive one furthermore we obtain novel results on the ability of independently learning traders to coordinate on a pseudocollusive behavior leading to noncompetitive pricing contrary to some recent contributions see eg we find that the pseudocollusive behavior in our model is robust to a large number of agents demonstrating that even in the setting of financial markets with a large number of independently learning traders noncompetitive pricing and pseudocollusive behavior can frequently arise,0
econophysics has developed as a research field that applies the formalism of statistical mechanics and quantum mechanics to address economics and finance problems the branch of econophysics that applies of quantum theory to economics and finance is called quantum econophysics in finance quantum econophysics contributions have ranged from option pricing to market dynamics modeling behavioral finance and applications of game theory integrating the empirical finding from human decision analysis that shows that nonlinear update rules in probabilities leading to nonadditive decision weights can be computationally approached from quantum computation with resulting quantum interference terms explaining the nonadditive probabilities the current work draws on these results to introduce new tools from quantum artificial intelligence namely quantum artificial neural networks as a way to build and simulate financial market models with adaptive selection of trading rules leading to turbulence and excess kurtosis in the returns distributions for a wide range of parameters,0
bond markets respond differently to macroeconomic news compared to equity markets yet most sentiment models including finbert are trained primarily on general financial or equity news data this mismatch is important because bond prices often move in the opposite direction to economic optimism making general or equitybased sentiment tools potentially misleading in this paper we introduce bondbert a transformerbased language model finetuned on bondspecific news bondbert can act as the perception and reasoning component of a financial decisionsupport agent providing sentiment signals that integrate with forecasting models it is a generalisable framework for adapting transformers to lowvolatility domaininverse sentiment tasks by compiling and cleaning 30000 uk bond market articles 20182025 for training validation and testing we compare bondberts sentiment predictions against finbert fingpt and instructfingpt using eventbased correlation updown accuracy analyses and lstm forecasting across ten uk sovereign bonds we find that bondbert consistently produces positive correlations with bond returns achieves higher alignment and forecasting accuracy than the three baseline models with lower normalised rmse and higher information coefficient these results demonstrate that domainspecific sentiment adaptation better captures fixed income dynamics bridging a gap between nlp advances and bond market analytics,0
financial markets are a complex dynamical system the complexity comes from the interaction between a market and its participants in other words the integrated outcome of activities of the entire participants determines the markets trend while the markets trend affects activities of participants these interwoven interactions make financial markets keep evolving inspired by stochastic recurrent models that successfully capture variability observed in natural sequential data such as speech and video we propose clvsa a hybrid model that consists of stochastic recurrent networks the sequencetosequence architecture the self and interattention mechanism and convolutional lstm units to capture variationally underlying features in raw financial trading data our model outperforms basic models such as convolutional neural network vanilla lstm network and sequencetosequence model with attention based on backtesting results of six futures from january 2010 to december 2017 our experimental results show that by introducing an approximate posterior clvsa takes advantage of an extra regularizer based on the kullbackleibler divergence to prevent itself from overfitting traps,0
we present a systematic trading framework that forecasts shorthorizon market risk identifies its underlying drivers and generates alpha using a hybrid machine learning ensemble built to trade on the resulting signal the framework integrates neural networks with treebased voting models to predict fiveday drawdowns in the sp 500 etf leveraging a crossasset feature set spanning equities fixed income foreign exchange commodities and volatility markets interpretable feature attribution methods reveal the key macroeconomic and microstructural factors that differentiate highrisk crash from benign noncrash weekly regimes empirical results show a sharpe ratio of 251 and an annualized capm alpha of 028 with a market beta of 051 indicating that the model delivers substantial systematic alpha with limited directional exposure during the 20052025 backtest period overall the findings underscore the effectiveness of hybrid ensemble architectures in capturing nonlinear risk dynamics and identifying interpretable potentially causal drivers providing a robust blueprint for machine learningdriven alpha generation in systematic trading,0
we present a detailed analysis of interest rate derivatives valuation under credit risk and collateral modeling we show how the credit and collateral extended valuation framework in pallavicini et al 2011 and the related collateralized valuation measure can be helpful in defining the key market rates underlying the multiple interest rate curves that characterize current interest rate markets a key point is that spot libor rates are to be treated as market primitives rather than being defined by noarbitrage relationships we formulate a consistent realistic dynamics for the different rates emerging from our analysis and compare the resulting model performances to simpler models used in the industry we include the often neglected margin period of risk showing how this feature may increase the impact of different rates dynamics on valuation we point out limitations of multiple curve models with deterministic basis considering valuation of particularly sensitive products such as basis swaps we stress that a proper wrong way risk analysis for such products requires a model with a stochastic basis and we show numerical results confirming this fact,0
in this paper we examine the capacity of an arbitragefree neuralsde market model to produce realistic scenarios for the joint dynamics of multiple european options on a single underlying we subsequently demonstrate its use as a risk simulation engine for option portfolios through backtesting analysis we show that our models are more computationally efficient and accurate for evaluating the valueatrisk var of option portfolios with better coverage performance and less procyclicality than standard filtered historical simulation approaches,0
energy markets exhibit complex causal relationships between weather patterns generation technologies and price formation with regime changes occurring continuously rather than at discrete break points current approaches model electricity prices without explicit causal interpretation or counterfactual reasoning capabilities we introduce augmented time series causal models atscm for energy markets extending counterfactual reasoning frameworks to multivariate temporal data with learned causal structure our approach models energy systems through interpretable factors weather generation mix demand patterns rich grid dynamics and observable market variables we integrate neural causal discovery to learn timevarying causal graphs without requiring ground truth dags applied to realworld electricity price data atscm enables novel counterfactual queries such as what would prices be under different renewable generation scenarios,0
optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the tradeoff between market impact costs and timing or volatility risks traditional optimal execution strategies such as static almgrenchriss models often prove suboptimal in dynamic financial markets this paper propose flowoe a novel imitation learning framework based on flow matching models to address these limitations flowoe learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions a key innovation is the incorporation of a refining loss function during the imitation process enabling flowoe not only to mimic but also to improve upon the learned expert actions to the best of our knowledge this work is the first to apply flow matching models in a stochastic optimal execution problem empirical evaluations across various market conditions demonstrate that flowoe significantly outperforms both the specifically calibrated expert models and other traditional benchmarks achieving higher profits with reduced risk these results underscore the practical applicability and potential of flowoe to enhance adaptive optimal execution,0
in this paper we introduce a novel reinforcement learning framework for optimal trade execution in a limit order book we formulate the trade execution problem as a dynamic allocation task whose objective is the optimal placement of market and limit orders to maximize expected revenue by employing multivariate logisticnormal distributions to model random allocations the framework enables efficient training of the reinforcement learning algorithm numerical experiments show that the proposed method outperforms traditional benchmark strategies in simulated limit order book environments featuring noise traders submitting random orders tactical traders responding to order book imbalances and a strategic trader seeking to acquire or liquidate an asset position,0
this paper explores using a deep learning long shortterm memory lstm model for accurate stock price prediction and its implications for portfolio design despite the efficient market hypothesis suggesting that predicting stock prices is impossible recent research has shown the potential of advanced algorithms and predictive models the study builds upon existing literature on stock price prediction methods emphasizing the shift toward machine learning and deep learning approaches using historical stock prices of 180 stocks across 18 sectors listed on the nse india the lstm model predicts future prices these predictions guide buysell decisions for each stock and analyze sector profitability the studys main contributions are threefold introducing an optimized lstm model for robust portfolio design utilizing lstm predictions for buysell transactions and insights into sector profitability and volatility results demonstrate the efficacy of the lstm model in accurately predicting stock prices and informing investment decisions by comparing sector profitability and prediction accuracy the work provides valuable insights into the dynamics of the current financial markets in india,0
an attempt to obtain market directional information from nonstationary solution of the dynamic equation future price tends to the value maximizing the number of shares traded per unit time is presented a remarkable feature of the approach is an automatic time scale selection it is determined from the state of maximal execution flow calculated on past transactions both lagging and advancing prices are calculated,0
we present a numerically efficient approach for learning a riskneutral measure for paths of simulated spot and option prices up to a finite horizon under convex transaction costs and convex trading constraints this approach can then be used to implement a stochastic implied volatility model in the following two steps 1 train a market simulator for option prices as discussed for example in our recent 2 find a riskneutral density specifically the minimal entropy martingale measure the resulting model can be used for riskneutral pricing or for deep hedging in the case of transaction costs or trading constraints to motivate the proposed approach we also show that market dynamics are free from statistical arbitrage in the absence of transaction costs if and only if they follow a riskneutral measure we additionally provide a more general characterization in the presence of convex transaction costs and trading constraints these results can be seen as an analogue of the fundamental theorem of asset pricing for statistical arbitrage under trading frictions and are of independent interest,0
identifying macroeconomic events that are responsible for dramatic changes of economy is of particular relevance to understand the overall economic dynamics we introduce an opensource available efficient python implementation of a bayesian multitrend change point analysis which solves significant memory and computing time limitations to extract crisis information from a correlation metric therefore we focus on the recently investigated sp500 mean market correlation in a period of roughly 20 years that includes the dotcom bubble the global financial crisis and the euro crisis the analysis is performed twofold first in retrospect on the whole dataset and second in an online adaptive manner in precrisis segments the online sensitivity horizon is roughly determined to be 80 up to 100 trading days after a crisis onset a detailed comparison to global economic events supports the interpretation of the mean market correlation as an informative macroeconomic measure by a rather good agreement of change point distributions and major crisis events furthermore the results hint to the importance of the us housing bubble as trigger of the global financial crisis provide new evidence for the general reasoning of locally metastable economic states and could work as a comparative impact rating of specific economic events,0
we propose a model that forecasts market correlation structure from link and nodebased financial network features using machine learning for such market structure is modeled as a dynamic asset network by quantifying timedependent comovement of asset price returns across company constituents of major global market indices we provide empirical evidence using three different network filtering methods to estimate market structure namely dynamic asset graph dag dynamic minimal spanning tree dmst and dynamic threshold networks dtn experimental results show that the proposed model can forecast market structure with high predictive performance with up to 40 improvement over a timeinvariant correlationbased benchmark nonpairwise correlation features showed to be important compared to traditionally used pairwise correlation measures for all markets studied particularly in the longterm forecasting of stock market structure evidence is provided for stock constituents of the dax30 eurostoxx50 ftse100 hangseng50 nasdaq100 and nifty50 market indices findings can be useful to improve portfolio selection and risk management methods which commonly rely on a backwardlooking covariance matrix to estimate portfolio risk,0
we introduce a new approach for generating sequences of implied volatility iv surfaces across multiple assets that is faithful to historical prices we do so using a combination of functional data analysis and neural stochastic differential equations sdes combined with a probability integral transform penalty to reduce model misspecification we demonstrate that learning the joint dynamics of iv surfaces and prices produces market scenarios that are consistent with historical features and lie within the submanifold of surfaces that are essentially free of static arbitrage finally we demonstrate that delta hedging using the simulated surfaces generates profit and loss pl distributions that are consistent with realised pls,0
this paper introduces the concept of a global financial market for environmental indices addressing sustainability concerns and aiming to attract institutional investors risk mitigation measures are implemented to manage inherent risks associated with investments in this new financial market we monetize the environmental indices using quantitative measures and construct countryspecific environmental indices enabling them to be viewed as dollardenominated assets our primary goal is to encourage the active engagement of institutional investors in portfolio analysis and trading within this emerging financial market to evaluate and manage investment risks our approach incorporates financial econometric theory and dynamic asset pricing tools we provide an econometric analysis that reveals the relationships between environmental and economic indicators in this market additionally we derive financial put options as insurance instruments that can be employed to manage investment risks our factor analysis identifies key drivers in the global financial market for environmental indices to further evaluate the markets performance we employ pricing options efficient frontier analysis and regression analysis these tools help us assess the efficiency and effectiveness of the market overall our research contributes to the understanding and development of the global financial market for environmental indices,0
the ability to construct a realistic simulator of financial exchanges including reproducing the dynamics of the limit order book can give insight into many counterfactual scenarios such as a flash crash a margin call or changes in macroeconomic outlook in recent years agentbased models have been developed that reproduce many features of an exchange as summarised by a set of stylised facts and statistics however the ability to calibrate simulators to a specific period of trading remains an open challenge in this work we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning specifically using neural density estimators and embedding networks we demonstrate that our approach is able to correctly identify high probability parameter sets both when applied to synthetic and historical data and without reliance on manually selected or weighted ensembles of stylised facts,0
generative modeling of highfrequency limit order book lob dynamics is a critical yet unsolved challenge in quantitative finance essential for robust market simulation and strategy backtesting existing approaches are often constrained by simplifying stochastic assumptions or in the case of modern deep learning models like transformers rely on tokenization schemes that affect the highprecision numerical nature of financial data through discretization and binning to address these limitations we introduce bytegen a novel generative model that operates directly on the raw byte streams of lob events our approach treats the problem as an autoregressive nextbyte prediction task for which we design a compact and efficient 32byte packed binary format to represent market messages without information loss the core novelty of our work is the complete elimination of feature engineering and tokenization enabling the model to learn market dynamics from its most fundamental representation we achieve this by adapting the hnet architecture a hybrid mambatransformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules our primary contributions are 1 the first endtoend bytelevel framework for lob modeling 2 an efficient packed data representation and 3 a comprehensive evaluation on highfrequency data trained on over 34 million events from cme bitcoin futures bytegen successfully reproduces key stylized facts of financial markets generating realistic price distributions heavytailed returns and bursty event timing our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems achieving competitive performance on standard market quality metrics without the biases of tokenization,0
this paper explores the implications of using machine learning models in the pricing of catastrophe cat bonds by integrating advanced machine learning techniques our approach uncovers nonlinear relationships and complex interactions between key risk factors and cat bond spreads dynamics that are often overlooked by traditional linear regression models using primary market cat bond transaction records between january 1999 and march 2021 our findings demonstrate that machine learning models not only enhance the accuracy of cat bond pricing but also provide a deeper understanding of how various risk factors interact and influence bond prices in a nonlinear way these findings suggest that investors and issuers can benefit from incorporating machine learning to better capture the intricate interplay between risk factors when pricing cat bonds the results also highlight the potential for machine learning models to refine our understanding of asset pricing in markets characterized by complex risk structures,0
in todays increasingly international economy return and volatility spillover effects across international equity markets are major macroeconomic drivers of stock dynamics thus information regarding foreign markets is one of the most important factors in forecasting domestic stock prices however the crosscorrelation between domestic and foreign markets is highly complex hence it is extremely difficult to explicitly express this crosscorrelation with a dynamical equation in this study we develop stock return prediction models that can jointly consider international markets using multimodal deep learning our contributions are threefold 1 we visualize the transfer information between south korea and us stock markets by using scatter plots 2 we incorporate the information into the stock prediction models with the help of multimodal deep learning 3 we conclusively demonstrate that the early and intermediate fusion models achieve a significant performance boost in comparison with the late fusion and single modality models our study indicates that jointly considering international stock markets can improve the prediction accuracy and deep neural networks are highly effective for such tasks,0
dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability dynamic hedging optimization can be framed as a sequential decision problem thus reinforcement learning rl models were recently proposed to tackle this task however existing rl works for hedging do not consider market impact caused by the finite liquidity of traded instruments integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity in this paper we propose a novel general market impact dynamic hedging model based on deep reinforcement learning drl that considers several realistic features such as convex market impacts and impact persistence through time the optimal policy obtained from the drl model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging results show our drl model behaves better in contexts of low liquidity by among others 1 learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs 2 factoring in the impact of features not considered by conventional approaches such as previous hedging errors through the portfolio value and the underlying assets drift ie the magnitude of its expected return,0
recent advances in artificial intelligence ai have made algorithmic trading play a central role in finance however current research and applications are disconnected information islands we propose a generally applicable pipeline for designing programming and evaluating the algorithmic trading of stock and crypto assets moreover we demonstrate how our data science pipeline works with respect to four conventional algorithms the moving average crossover volumeweighted average price sentiment analysis and statistical arbitrage algorithms our study offers a systematic way to program evaluate and compare different trading strategies furthermore we implement our algorithms through objectoriented programming in python3 which serves as opensource software for future academic research and applications,0
with the increasing maturity and expansion of the cryptocurrency market understanding and predicting its price fluctuations has become an important issue in the field of financial engineering this article introduces an innovative genetic algorithmgenerated alpha sentiment gas blending ensemble model specifically designed to predict bitcoin market trends the model integrates advanced ensemble learning methods feature selection algorithms and indepth sentiment analysis to effectively capture the complexity and variability of daily bitcoin trading data the gas framework combines 34 alpha factors with 8 news economic sentiment factors to provide deep insights into bitcoin price fluctuations by accurately analyzing market sentiment and technical indicators the core of this study is using a stacked model including lightgbm xgboost and random forest classifier for trend prediction which demonstrates excellent performance in traditional buyandhold strategies in addition this article also explores the effectiveness of using genetic algorithms to automate alpha factor construction as well as enhancing predictive models through sentiment analysis experimental results show that the gas model performs competitively in daily bitcoin trend prediction especially when analyzing highly volatile financial assets with rich data,0
in blockchain bribery is an inevitable problem since users with various goals can bribe miners by transferring cryptoassets to alleviate the negative effects of such collusion ethereum blockchain implemented new transaction fee mechanism in the london fork which was deployed on august 5th 2021 in this paper we first filter potential bribery by scanning ethereum transactions and the potential bribers and bribees are centralized in a small group then we construct bribing proxies to measure the active level of bribery and then investigate the effects of bribery consequently bribery can influence both ethereum and other mainstream blockchains in aspects of underlying cryptocurrency transaction statistics and network adoption moreover the london fork shows complicated effects on relationship between bribery and blockchain factors besides bribery in ethereum relates to stock markets eg sp 500 and nasdaq implying implicit interlinks between blockchain and traditional finance,0
crossmarket portfolio optimization has become increasingly complex with the globalization of financial markets and the growth of highfrequency multidimensional datasets traditional artificial neural networks while effective in certain portfolio management tasks often incur substantial computational overhead and lack the temporal processing capabilities required for largescale multimarket data this study investigates the application of spiking neural networks snns for crossmarket portfolio optimization leveraging neuromorphic computing principles to process equity data from both the indian nifty 500 and us sp 500 markets a fiveyear dataset comprising approximately 1250 trading days of daily stock prices was systematically collected via the yahoo finance api the proposed framework integrates leaky integrateandfire neuron dynamics with adaptive thresholding spiketimingdependent plasticity and lateral inhibition to enable eventdriven processing of financial time series dimensionality reduction is achieved through hierarchical clustering while populationbased spike encoding and multiple decoding strategies support robust portfolio construction under realistic trading constraints including cardinality limits transaction costs and adaptive risk aversion experimental evaluation demonstrates that the snnbased framework delivers superior riskadjusted returns and reduced volatility compared to ann benchmarks while substantially improving computational efficiency these findings highlight the promise of neuromorphic computation for scalable efficient and robust portfolio optimization across global financial markets,0
everlasting options a relatively new class of perpetual financial derivatives have emerged to tackle the challenges of rolling contracts and liquidity fragmentation in decentralized finance markets this paper offers an indepth analysis of markets for everlasting options modeled using a dynamic proactive market maker we examine the behavior of funding fees and transaction costs across varying liquidity conditions using simulations and modeling we demonstrate that liquidity providers can aim to achieve a net positive pnl by employing effective hedging strategies even in challenging environments characterized by low liquidity and high transaction costs additionally we provide insights into the incentives that drive liquidity providers to support the growth of everlasting option markets and highlight the significant benefits these instruments offer to traders as a reliable and efficient financial tool,0
machinelearning technologies are seeing increased deployment in realworld market scenarios in this work we explore the strategic behaviors of large language models llms when deployed as autonomous agents in multicommodity markets specifically within cournot competition frameworks we examine whether llms can independently engage in anticompetitive practices such as collusion or more specifically market division our findings demonstrate that llms can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies thereby maximizing profitability without direct human input or explicit collusion commands these results pose unique challenges and opportunities for businesses looking to integrate ai into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets the study provides a foundation for further exploration into the ramifications of deferring highstakes decisions to llmbased agents,0
decentralized finance defi is a nascent set of financial services using tokens smart contracts and blockchain technology as financial instruments we investigate four possible drivers of defi returns exposure to cryptocurrency market the network effect the investors attention and the valuation ratio as defi tokens are distinct from classical cryptocurrencies we design a new dedicated market index denoted defix first we show that defi tokens returns are driven by the investors attention on technical terms such as decentralized finance or defi and are exposed to their own network variables and cryptocurrency market we construct a valuation ratio for the defi market by dividing the total value locked tvl by the market capitalization mc our findings do not support the tvlmc predictive power assumption overall our empirical study shows that the impact of the cryptocurrency market on defi returns is stronger than any other considered driver and provides superior explanatory power,0
we address microscopic agent based and macroscopic stochastic modeling of the financial markets combining it with the exogenous noise the interplay between the endogenous dynamics of agents and the exogenous noise is the primary mechanism responsible for the observed longrange dependence and statistical properties of high volatility return intervals by exogenous noise we mean information flow orand order flow fluctuations numerical results based on the proposed model reveal that the exogenous fluctuations have to be considered as indispensable part of comprehensive modeling of the financial markets,0
in a market with transaction costs the price of a derivative can be expressed in terms of preconsistent price systems after kusuoka 1995 in this paper we consider a market with binomial model for stock price and discuss how to generate the price systems from this the price formula of a derivative can be reformulated as a stochastic control problem then the dynamic programming approach can be used to calculate the price we also discuss optimization of expected utility using price systems,0
this study presents a comprehensive empirical investigation of the presence of longrange dependence lrd in the dynamics of major us stock market indexessp 500 dow jones and nasdaqat daily weekly and monthly frequencies we employ three distinct methods the classical rescaled range rs analysis the more robust detrended fluctuation analysis dfa and a sophisticated arfimafigarch model with students tdistributed innovations our results confirm the presence of lrd primarily driven by long memory in volatility rather than in the mean returns building on these findings we explore the capability of a modern deep learning approach quant generative adversarial networks gans to learn and replicate the lrd observed in the empirical data while quant gans effectively capture heavytailed distributions and some aspects of volatility clustering they suffer from significant limitations in reproducing the lrd particularly at higher frequencies this work highlights the challenges and opportunities in using datadriven models for generating realistic financial time series that preserve complex temporal dependencies,0
in power markets green power purchase agreements have become an important contractual tool of the energy transition from fossil fuels to renewable sources such as wind or solar radiation trading green ppas exposes agents to price risks and weather risks also developed electricity markets feature the socalled cannibalisation effect large infeeds induce low prices and vice versa as weather is a nontradable entity the question arises how to hedge and riskmanage in this highly incomplete setting we propose a deep hedging framework utilising machine learning methods to construct hedging strategies the resulting strategies outperform static and dynamic benchmark strategies with respect to different risk measures,0
it is widely acknowledged that extracting market sentiments from news data benefits market predictions however existing methods of using financial sentiments remain simplistic relying on equalweight and static aggregation to manage sentiments from multiple news items this leads to a critical issue termed aggregated sentiment homogenization which has been explored through our analysis of a large financial news dataset from industry practice this phenomenon occurs when aggregating numerous sentiments causing representations to converge towards the mean values of sentiment distributions and thereby smoothing out unique and important information consequently the aggregated sentiment representations lose much predictive value of news data to address this problem we introduce the market attentionweighted news aggregation network mananet a novel method that leverages a dynamic marketnews attention mechanism to aggregate news sentiments for market prediction mananet learns the relevance of news sentiments to price changes and assigns varying weights to individual news items by integrating the news aggregation step into the networks for market prediction mananet allows for trainable sentiment representations that are optimized directly for prediction we evaluate mananet using the sp 500 and nasdaq 100 indices along with financial news spanning from 2003 to 2018 experimental results demonstrate that mananet outperforms various recent market prediction methods enhancing profit loss by 11 and the daily sharpe ratio by 0252,0
this study analyzes historical data from five agricultural commodities in the chinese futures market to explore the correlation cointegration and granger causality between peanut futures and related futures multivariate linear regression models are constructed for prices and logarithmic returns while dynamic relationships are examined using var and dccegarch models the results reveal a significant dynamic linkage between peanut and soybean oil futures through dccegarch whereas the var model suggests limited influence from other futures additionally the application of mlp cnn and lstm neural networks for price prediction highlights the critical role of time step configurations in forecasting accuracy these findings provide valuable insights into the interconnectedness of agricultural futures markets and the efficacy of advanced modeling techniques in financial analysis,0
in this paper we propose an eventdriven limit order book lob model that captures twelve of the most observed lob events in exchangebased financial markets to model these events we propose using the stateoftheart neural hawkes process a more robust alternative to traditional hawkes process models more specifically this model captures the dynamic relationships between different event types particularly their long and shortterm interactions using a long shortterm memory neural network using this framework we construct a midprice process that captures the eventdriven behavior of the lob by simulating highfrequency dynamics like how they appear in real financial markets the empirical results show that our model captures many of the broader characteristics of the price fluctuations particularly in terms of their overall volatility we apply this lob simulation model within a deep reinforcement learning marketmaking framework where the trading agent can now complete trade order fills in a manner that closely resembles realmarket trade execution here we also compare the results of the simulated model with those from real data highlighting how the overall performance and the distribution of trade order fills closely align with the same analysis on real data,0
we consider an optimal investment and consumption problem for a blackscholes financial market with stochastic coefficients driven by a diffusion process we assume that an agent makes consumption and investment decisions based on crra utility functions the dynamical programming approach leads to an investigation of the hamilton jacobi bellman hjb equation which is a highly non linear partial differential equation pde of the second oder by using the feynman kac representation we prove uniqueness and smoothness of the solution moreover we study the optimal convergence rate of the iterative numerical schemes for both the value function and the optimal portfolio we show that in this case the optimal convergence rate is super geometrical ie is more rapid than any geometrical one we apply our results to a stochastic volatility financial market,0
financial portfolio management is the process of constant redistribution of a fund into different financial products this paper presents a financialmodelfree reinforcement learning framework to provide a deep machine learning solution to the portfolio management problem the framework consists of the ensemble of identical independent evaluators eiie topology a portfoliovector memory pvm an online stochastic batch learning osbl scheme and a fully exploiting and explicit reward function this framework is realized in three instants in this work with a convolutional neural network cnn a basic recurrent neural network rnn and a long shortterm memory lstm they are along with a number of recently reviewed or published portfolioselection strategies examined in three backtest experiments with a trading period of 30 minutes in a cryptocurrency market cryptocurrencies are electronic and decentralized alternatives to governmentissued money with bitcoin as the bestknown example of a cryptocurrency all three instances of the framework monopolize the top three positions in all experiments outdistancing other compared trading algorithms although with a high commission rate of 025 in the backtests the framework is able to achieve at least 4fold returns in 50 days,0
the programmable and composable nature of smart contract protocols has enabled the emergence of novel market structures and asset classes that are architecturally frictional to implement in traditional financial paradigms this fluidity has produced an understudied class of market dynamics particularly in coupled markets where one market serves as an oracle for the other in such market structures purchases or liquidations through the intermediate asset create coupled price action between the intermediate and final assets leading to basket inflation or deflation when denominated in the riskless asset this paper examines the microstructure of this inflationary dynamic given two constant function market makers cfmms as the intermediate market structures attempting to quantify their contributions to the former relative to familiar pool metrics such as price drift trade size and market depth further a concrete case study is developed where both markets are constant product markets the intention is to shed light on the market design process within such coupled environments,0
we adress the maximization problem of expected utility from terminal wealth the special feature of this paper is that we consider a financial market where the price process of risky assets can have a default time using dynamic programming we characterize the value function with a backward stochastic differential equation and the optimal portfolio policies we separately treat the cases of exponential power and logarithmic utility,0
the purpose of this article is to introduce a new lvy process termed variance gamma process to model the dynamic of assets in illiquid markets such a process has the mathematical tractability of the variance gamma process and is obtained applying the selfdecomposability of the gamma law compared to the variance gamma model it has an additional parameter representing the measure of the trading activity we give a full characterization of the variance gamma process in terms of its characteristic triplet characteristic function and transition density in addition we provide efficient path simulation algorithms both forward and backward in time we also obtain an efficient integralfree explicit pricing formula for european options these results are instrumental to apply fourierbased option pricing and maximum likelihood techniques for the parameter estimation finally we apply our model to illiquid markets namely to the calibration of european power future market data we accordingly evaluate exotic derivatives using the monte carlo method and compare these values to those obtained using the variance gamma process and give an economic interpretation of the obtained results finally we illustrate an extension to the multivariate framework,0
this article explores dynamic factor allocation by analyzing the cyclical performance of factors through regime analysis the authors focus on a us equity investment universe comprising seven longonly indices representing the market and six style factors value size momentum quality low volatility and growth their approach integrates factorspecific regime inferences of each factor indexs active performance relative to the market into the blacklitterman model to construct a fullyinvested longonly multifactor portfolio first the authors apply the sparse jump model sjm to identify bull and bear market regimes for individual factors using a feature set based on risk and return measures from historical factor active returns as well as variables reflecting the broader market environment the regimes identified by the sjm exhibit enhanced stability and interpretability compared to traditional methods a hypothetical singlefactor longshort strategy is then used to assess these regime inferences and finetune hyperparameters resulting in a positive sharpe ratio of this strategy across all factors with low correlation among them these regime inferences are then incorporated into the blacklitterman framework to dynamically adjust allocations among the seven indices with an equally weighted ew portfolio serving as the benchmark empirical results show that the constructed multifactor portfolio significantly improves the information ratio ir relative to the market raising it from just 005 for the ew benchmark to approximately 04 when measured relative to the ew benchmark itself the dynamic allocation achieves an ir of around 04 to 05 the strategy also enhances absolute portfolio performance across key metrics such as the sharpe ratio and maximum drawdown,0
we extend our studies of a quantum field model defined on a lattice having the dilation group as a local gauge symmetry the model is relevant in the crossdisciplinary area of econophysics a corresponding proposal by ilinski aimed at gauge modeling in nonequilibrium pricing is realized as a numerical simulation of the oneasset version the gauge field background enforces minimal arbitrage yet allows for statistical fluctuations the new feature added to the model is an updating prescription for the simulation that drives the model market into a selforganized critical state taking advantage of some flexibility of the updating prescription stylized features and dynamical behaviors of realworld markets are reproduced in some detail,0
we develop a deep reinforcement learning rl framework for an optimal marketmaking mm trading problem specifically focusing on price processes with semimarkov and hawkes jumpdiffusion dynamics we begin by discussing the basics of rl and the deep rl framework used where we deployed the stateoftheart soft actorcritic sac algorithm for the deep learning part the sac algorithm is an offpolicy entropy maximization algorithm more suitable for tackling complex highdimensional problems with continuous state and action spaces like in optimal marketmaking mm we introduce the optimal mm problem considered where we detail all the deterministic and stochastic processes that go into setting up an environment for simulating this strategy here we also give an indepth overview of the jumpdiffusion pricing dynamics used our method for dealing with adverse selection within the limit order book and we highlight the working parts of our optimization problem next we discuss training and testing results where we give visuals of how important deterministic and stochastic processes such as the bidask trade executions inventory and the reward function evolved we include a discussion on the limitations of these results which are important points to note for most diffusion models in this setting,0
the increasing adoption of digital assets das such as bitcoin btc rises the need for accurate option pricing models yet existing methodologies fail to cope with the volatile nature of the emerging das many models have been proposed to address the unorthodox market dynamics and frequent disruptions in the microstructure caused by the nonstationarity and peculiar statistics in da markets however they are either prone to the curse of dimensionality as additional complexity is required to employ traditional theories or they overfit historical patterns that may never repeat instead we leverage recent advances in market regime mr clustering with the implied stochastic volatility model isvm timeregime clustering is a temporal clustering method that clusters the historic evolution of a market into different volatility periods accounting for nonstationarity isvm can incorporate investor expectations in each of the sentimentdriven periods by using implied volatility iv data in this paper we applied this integrated timeregime clustering and isvm method termed mrisvm to highfrequency data on btc options at the popular trading platform deribit we demonstrate that mrisvm contributes to overcome the burden of complex adaption to jumps in higher order characteristics of option pricing models this allows us to price the market based on the expectations of its participants in an adaptive fashion,0
applying a network analysis to stock return correlations we study the dynamical properties of the network and how they correlate with the market return finding meaningful variables that partially capture the complex dynamical processes of stock interactions and the market structure we then use the individual properties of stocks within the network along with the global ones to find correlations with the future returns of individual sp 500 stocks applying these properties as input variables for forecasting we find a 50 improvement on the r2score in the prediction of stock returns on long time scales per year and 3 on short time scales 2 days relative to baseline models without network variables,0
blockchain technology and decentralized finance defi are reshaping global financial systems despite their impact the spatial distribution of public sentiment and its economic and geopolitical determinants are often overlooked this study analyzes over 150 million geotagged defirelated tweets from 2012 to 2022 sourced from a larger dataset of 74 billion tweets using sentiment scores from a bertbased multilingual classification model we integrated these tweets with economic and geopolitical data to create a multimodal dataset employing techniques like sentiment analysis spatial econometrics clustering and topic modeling we uncovered significant global variations in defi engagement and sentiment our findings indicate that economic development significantly influences defi engagement particularly after 2015 geographically weighted regression analysis revealed gdp per capita as a key predictor of defi tweet proportions with its impact growing following major increases in cryptocurrency values such as bitcoin while wealthier nations are more actively engaged in defi discourse the lowestincome countries often discuss defi in terms of financial security and sudden wealth conversely middleincome countries relate defi to social and religious themes whereas highincome countries view it mainly as a speculative instrument or entertainment this research advances interdisciplinary studies in computational social science and finance and supports open science by making our dataset and code available on github and providing a noncode workflow on the knime platform these contributions enable a broad range of scholars to explore defi adoption and sentiment aiding policymakers regulators and developers in promoting financial inclusion and responsible defi engagement globally,0
we propose a novel twostage framework to detect leadlag relationships in the chinese ashare market first longterm coupling between stocks is measured via daily data using correlation dynamic time warping and rankbased metrics then highfrequency data 1 5 and 15minute is used to detect statistically significant leadlag patterns via crosscorrelation granger causality and regression models our lowcoupling modular system supports scalable data processing and improves reproducibility results show that strongly coupled stock pairs often exhibit leadlag effects especially at finer time scales these findings provide insights into market microstructure and quantitative trading opportunities,0
learning customer preferences from an observed behaviour is an important topic in the marketing literature structural models typically model forwardlooking customers or firms as utilitymaximizing agents whose utility is estimated using methods of stochastic optimal control we suggest an alternative approach to study dynamic consumer demand based on inverse reinforcement learning irl we develop a version of the maximum entropy irl that leads to a highly tractable model formulation that amounts to lowdimensional convex optimization in the search for optimal model parameters using simulations of consumer demand we show that observational noise for identical customers can be easily confused with an apparent consumer heterogeneity,0
we propose a mathematical model for the wordofmouth communications among stock investors through social networks and explore how the changes of the investors social networks influence the stock price dynamics and vice versa an investor is modeled as a gaussian fuzzy set a fuzzy opinion with the center and standard deviation as inputs and the fuzzy set itself as output investors are connected in the following fashion the center input of an investor is taken as the average of the neighbors outputs where two investors are neighbors if their fuzzy opinions are close enough to each other and the standard deviation uncertainty input is taken with local global or external reference schemes to model different scenarios of how investors define uncertainties the centers and standard deviations of the fuzzy opinions are the expected prices and their uncertainties respectively that are used as inputs to the price dynamic equation we prove that with the local reference scheme the investors converge to different groups in finite time while with the global or external reference schemes all investors converge to a consensus within finite time and the consensus may change with time in the external reference case we show how to model trend followers contrarians and manipulators within this mathematical framework and prove that the biggest enemy of a manipulator is the other manipulators we perform monte carlo simulations to show how the model parameters influence the price dynamics and we apply a modified version of the model to the daily closing prices of fifteen top banking and real estate stocks in hong kong for the recent two years from dec 5 2013 to dec 4 2015 and discover that a sharp increase of the combined uncertainty is a reliable signal to predict the reversal of the current price trend,0
we propose a heterogeneous simultaneous graphical dynamic linear model hsgdlm which extends the standard sgdlm framework to incorporate a heterogeneous autoregressive realised volatility harrv model this novel approach creates a gpuscalable multivariate volatility estimator which decomposes multiple time series into economicallymeaningful variables to explain the endogenous and exogenous factors driving the underlying variability this unique decomposition goes beyond the classic one step ahead prediction indeed we investigate inferences up to one month into the future using stocks fx futures and etf futures demonstrating its superior performance according to accuracy of large moves longerterm prediction and consistency over time,0
in this work we introduce a monte carlo method for the dynamic hedging of general europeantype contingent claims in a multidimensional brownian arbitragefree market based on bounded variation martingale approximations for galtchoukkunitawatanabe decompositions we propose a feasible and constructive methodology which allows us to compute pure hedging strategies wrt arbitrary squareintegrable claims in incomplete markets in particular the methodology can be applied to quadratic hedgingtype strategies for fully pathdependent options with stochastic volatility and discontinuous payoffs we illustrate the method with numerical examples based on generalized follmerschweizer decompositions locallyrisk minimizing and meanvariance hedging strategies for vanilla and pathdependent options written on local volatility and stochastic volatility models,0
we use an adversarial expert based online learning algorithm to learn the optimal parameters required to maximise wealth trading zerocost portfolio strategies the learning algorithm is used to determine the relative population dynamics of technical trading strategies that can survive historical backtesting as well as form an overall aggregated portfolio trading strategy from the set of underlying trading strategies implemented on daily and intraday johannesburg stock exchange data the resulting population timeseries are investigated using unsupervised learning for dimensionality reduction and visualisation a key contribution is that the overall aggregated trading strategies are tested for statistical arbitrage using a novel hypothesis test proposed by jarrow et al 2012 on both daily sampled and intraday timescales the low frequency daily sampled strategies fail the arbitrage tests after costs while the high frequency intraday sampled strategies are not falsified as statistical arbitrages after costs the estimates of trading strategy success cost of trading and slippage are considered along with an online benchmark portfolio algorithm for performance comparison in addition the algorithms generalisation error is analysed by recovering a probability of backtest overfitting estimate using a nonparametric procedure introduced by bailey et al 2016 the work aims to explore and better understand the interplay between different technical trading strategies from a datainformed perspective,0
modern financial exchanges use an electronic limit order book lob to store bid and ask orders for a specific financial asset as the most finegrained information depicting the demand and supply of an asset lob data is essential in understanding market dynamics therefore realistic lob simulations offer a valuable methodology for explaining empirical properties of markets mainstream simulation models include agentbased models abms and stochastic models sms however abms tend not to be grounded on real historical data while sms tend not to enable dynamic agentinteraction to overcome these limitations we propose a novel hybrid lob simulation paradigm characterised by 1 representing the aggregation of market events logic by a neural stochastic background trader that is pretrained on historical lob data through a neural point process model and 2 embedding the background trader in a multiagent simulation with other trading agents we instantiate this hybrid nsabm model using the abides platform we first run the background trader in isolation and show that the simulated lob can recreate a comprehensive list of stylised facts that demonstrate realistic market behaviour we then introduce a population of trend and value trading agents which interact with the background trader we show that the stylised facts remain and we demonstrate order flow impact and financial herding behaviours that are in accordance with empirical observations of real markets,0
this study explores the use of transformerbased models to predict both covariance and semicovariance matrices for etf portfolio optimization traditional portfolio optimization techniques often rely on static covariance estimates or impose strict model assumptions which may fail to capture the dynamic and nonlinear nature of market fluctuations our approach leverages the power of transformer models to generate adaptive realtime predictions of asset covariances with a focus on the semicovariance matrix to account for downside risk the semicovariance matrix emphasizes negative correlations between assets offering a more nuanced approach to risk management compared to traditional methods that treat all volatility equally through a series of experiments we demonstrate that transformerbased predictions of both covariance and semicovariance significantly enhance portfolio performance our results show that portfolios optimized using the semicovariance matrix outperform those optimized with the standard covariance matrix particularly in volatile market conditions moreover the use of the sortino ratio a riskadjusted performance metric that focuses on downside risk further validates the effectiveness of our approach in managing risk while maximizing returns these findings have important implications for asset managers and investors offering a dynamic datadriven framework for portfolio construction that adapts more effectively to shifting market conditions by integrating transformerbased models with the semicovariance matrix for improved risk management this research contributes to the growing field of machine learning in finance and provides valuable insights for optimizing etf portfolios,0
a nonfungible token nft market is a new trading invention based on the blockchain technology which parallels the cryptocurrency market in the present work we study capitalization floor price the number of transactions the intertransaction times and the transaction volume value of a few selected popular token collections the results show that the fluctuations of all these quantities are characterized by heavytailed probability distribution functions in most cases well described by the stretched exponentials with a trace of powerlaw scaling at times longrange memory and in several cases even the fractal organization of fluctuations mostly restricted to the larger fluctuations however we conclude that the nft market even though young and governed by a somewhat different mechanisms of trading shares several statistical properties with the regular financial markets however some differences are visible in the specific quantitative indicators,0
we replicate the contested calibration of the farmer and joshi agent based model of financial markets using a genetic algorithm and a neldermead with threshold accepting algorithm following fabretti the novelty of the farmerjoshi model is that the dynamics are driven by trade entry and exit thresholds alone we recover the known claim that some important stylized facts observed in financial markets cannot be easily found under calibration in particular those relating to the autocorrelations in the absolute values of the price fluctuations and sufficient kurtosis however rather than concerns relating to the calibration method what is novel here is that we extended the farmerjoshi model to include agent adaptation using an brock and hommes approach to strategy fitness based on trading strategy profitability we call this an adaptive farmerjoshi model the model allows trading agents to switch between strategies by favouring strategies that have been more profitable over some period of time determined by a freeparameter fixing the profit monitoring timehorizon in the adaptive model we are able to calibrate and recover additional stylized facts despite apparent degeneracys this is achieved by combining the interactions of trade entry levels with trade strategy switching we use this to argue that for lowfrequency trading across days as calibrated to daily sampled data feedbacks can be accounted for by strategy dieout based on intermediate term profitability we find that the average trade monitoring horizon is approximately two to three months or 40 to 60 days of trading,0
i describe the rationale for and design of an agentbased simulation model of a contemporary online sportsbetting exchange such exchanges closely related to the exchange mechanisms at the heart of major financial markets have revolutionized the gambling industry in the past 20 years but gathering sufficiently large quantities of rich and temporally highresolution data from real exchanges ie the sort of data that is needed in large quantities for deep learning is often very expensive and sometimes simply impossible this creates a need for a plausibly realistic synthetic data generator which is what this simulation now provides the simulator named the bristol betting exchange bbe is intended as a common platform a datasource and experimental testbed for researchers studying the application of ai and machine learning ml techniques to issues arising in betting exchanges and as far as i have been able to determine bbe is the first of its kind a free opensource agentbased simulation model consisting not only of a sportsbetting exchange but also a minimal simulation model of racetrack sporting events eg horseraces or carraces about which bets may be made and a population of simulated bettors who each form their own private evaluation of odds and place bets on the exchange before and crucially during the race itself ie socalled inplay betting and whose betting opinions change secondbysecond as each race event unfolds bbe is offered as a proofofconcept system that enables the generation of large highresolution datasets for automated discovery or improvement of profitable strategies for betting on sporting events via the application of aiml and advanced data analytics techniques this paper offers an extensive survey of relevant literature and explains the motivation and design of bbe and presents brief illustrative results,0
we describe the innovations in finances introduced over the recent decades and analyze most of the business and regulatory challenges faced by the financial industry because of the present disruptive changes in the global capital markets we use the integrative thinking approach to formulate the new central bank strategy and propose that the new strategy has to be focused on the constant management of the monetary and financial instabilities using the knowledge base in the field of econophysics we propose the new theoretical model of economics which is called the nonlinear dynamic stochastic general equilibrium ndsge which takes to the account the nonlinearities appearing during the interaction between the business cycles we show that the central banks which will apply the knowledge gained from the econophysical analysis to understand the complex processes in the national financial systems in the time of high volatility in global capital markets will be able to govern the national financial systems successfully,0
systematic financial trading strategies account for over 80 of trade volume in equities and a large chunk of the foreign exchange market in spite of the availability of data from multiple markets current approaches in trading rely mainly on learning trading strategies per individual market in this paper we take a step towards developing fully endtoend global trading strategies that leverage systematic trends to produce superior marketspecific trading strategies we introduce quantnet an architecture that learns marketagnostic trends and use these to learn superior marketspecific trading strategies each marketspecific model is composed of an encoderdecoder pair the encoder transforms marketspecific data into an abstract latent representation that is processed by a global model shared by all markets while the decoder learns a marketspecific trading strategy based on both local and global information from the marketspecific encoder and the global model quantnet uses recent advances in transfer and metalearning where marketspecific parameters are free to specialize on the problem at hand whilst marketagnostic parameters are driven to capture signals from all markets by integrating over idiosyncratic market data we can learn general transferable dynamics avoiding the problem of overfitting to produce strategies with superior returns we evaluate quantnet on historical data across 3103 assets in 58 global equity markets against the top performing baseline quantnet yielded 51 higher sharpe and 69 calmar ratios in addition we show the benefits of our approach over the nontransfer learning variant with improvements of 15 and 41 in sharpe and calmar ratios code available in appendix,0
designing dynamic portfolio insurance strategies under market conditions switching between two or more regimes is a challenging task in financial economics recently a promising approach employing the valueatrisk var measure to assign weights to risky and riskless assets has been proposed in in their study the risky asset follows a geometric brownian motion with constant drift and diffusion coefficients in this paper we first extend their idea to a regimeswitching framework in which the expected return of the risky asset and its volatility depend on an unobservable markovian term which describes the cyclical nature of asset returns in modern financial markets we then analyze and compare the resulting varbased portfolio insurance vbpi strategy with the wellknown constant proportion portfolio insurance cppi strategy in this respect we employ a variety of performance evaluation criteria such as sharpe omega and kappa ratios to compare the two methods our results indicate that the cppi strategy has a better riskreturn tradeoff in most of the scenarios analyzed and maintains a relatively stable return profile for the resulting portfolio at the maturity,0
we complement the theory of tickbytick dynamics of financial markets based on a continuoustime random walk ctrw model recently proposed by scalas et al and we point out its consistency with the behaviour observed in the waitingtime distribution for bund future prices traded at liffe london,0
in this study we consider the pricing of energy derivatives when the evolution of spot prices follows a tempered stable or a cgmy driven ornstein uhlenbeck process to this end we first calculate the characteristic function of the transition law of such processes in closed form this result is instrumental for the derivation of nonarbitrage conditions such that the spot dynamics is consistent with the forward curve moreover based on the results of cufaro petroni and sabino 2020 we also conceive efficient algorithms for the exact simulation of the skeleton of such processes and propose a novel procedure when they coincide with compound poisson processes of ornsteinuhlenbeck type we illustrate the applicability of the theoretical findings and the simulation algorithms in the context of the pricing different contracts namely strips of daily call options asian options with european style and swing options finally we present an extension to future markets,0
the present work addresses the challenge of training neural networks for dynamic initial margin dim computation in counterparty credit risk a task traditionally burdened by the high costs associated with generating training datasets through nested monte carlo mc simulations by condensing the initial market state variables into an input vector determined through an interest rate model and a parsimonious parameterization of the current interest rate term structure we construct a training dataset where labels are noisy but unbiased dim samples derived from single mc paths a multioutput neural network structure is employed to handle dim as a timedependent function facilitating training across a mesh of monitoring times the methodology offers significant advantages it reduces the dataset generation cost to a single mc execution and parameterizes the neural network by initial market state variables obviating the need for repeated training experimental results demonstrate the approachs convergence properties and robustness across different interest rate models vasicek and hullwhite and portfolio complexities validating its general applicability and efficiency in more realistic scenarios,0
this paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies ais construction we propose the mean absolute directional loss madl function solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buysell signals in algorithmic investment strategies finally based on the data from two different asset classes cryptocurrencies bitcoin and commodities crude oil we show that the new loss function enables us to select better hyperparameters for the lstm model and obtain more efficient investment strategies with regard to riskadjusted return metrics on the outofsample data,0
price feeds of securities is a critical component for many financial services allowing for collateral liquidation margin trading derivative pricing and more with the advent of blockchain technology value in reporting accurate prices without a third party has become apparent there have been many attempts at trying to calculate prices without a third party in which each of these attempts have resulted in being exploited by an exploiter artificially inflating the price the industry has then shifted to a more centralized design fetching price data from multiple centralized sources and then applying statistical methods to reach a consensus price even though this strategy is secure compared to reading from a single source enough number of sources need to report to be able to apply statistical methods as more sources participate in reporting the price the feed gets more secure with the slowest feed becoming the bottleneck for query response time introducing a tradeoff between security and speed this paper provides the design and implementation details of a novel method to algorithmically compute security prices in a way that artificially inflating targeted pools has no effect on the reported price of the queried asset we hypothesize that the proposed algorithm can report accurate prices given a set of possibly dishonest sources,0
this paper proposes tipsearch a timepredictable inference scheduling framework for realtime market prediction under uncertain workloads motivated by the strict latency demands in highfrequency financial systems tipsearch dynamically selects a deep learning model from a heterogeneous pool aiming to maximize predictive accuracy while satisfying pertask deadline constraints our approach profiles latency and generalization performance offline then performs online taskaware selection without relying on explicit input domain labels we evaluate tipsearch on three realworld limit order book datasets fi2010 binance btcusdt lobster aapl and demonstrate that it outperforms static baselines with up to 85 improvement in accuracy and 100 deadline satisfaction our results highlight the effectiveness of tipsearch in robust lowlatency financial inference under uncertainty,0
recent advancements in large language models llms have the potential to transform financial analytics by integrating numerical and textual data however challenges such as insufficient context when fusing multimodal information and the difficulty in measuring the utility of qualitative outputs which llms generate as text have limited their effectiveness in tasks such as financial forecasting this study addresses these challenges by leveraging daily reports from securities firms to create highquality contextual information the reports are segmented into textbased key factors and combined with numerical data such as price information to form context sets by dynamically updating fewshot examples based on the query time the sets incorporate the latest information forming a highly relevant set closely aligned with the query point additionally a crafted prompt is designed to assign scores to the key factors converting qualitative insights into quantitative results the derived scores undergo a scaling process transforming them into realworld values that are used for prediction our experiments demonstrate that llms outperform timeseries models in market forecasting though challenges such as imperfect reproducibility and limited explainability remain,0
liquidity withdrawal is a critical indicator of market fragility in this project i test a framework for forecasting liquidity withdrawal at the individualstock level ranging from less liquid stocks to highly liquid largecap tickers and evaluate the relative performance of competing model classes in predicting shorthorizon order book stress we introduce the liquidity withdrawal index lwi defined as the ratio of order cancellations to the sum of standing depth and new additions at the best quotes as a bounded interpretable measure of transient liquidity removal using nasdaq marketbyorder mbo data we compare a spectrum of approaches linear benchmarks ar har and nonlinear tree ensembles xgboost across horizons ranging from 250ms to 5s beyond predictive accuracy our results provide insights into order placement and cancellation dynamics identify regimes where linear versus nonlinear signals dominate and highlight how earlywarning indicators of liquidity withdrawal can inform both market surveillance and execution,0
we introduce a new software toolbox for agentbased simulation facilitating rapid prototyping by offering a userfriendly python api its core rests on an efficient c implementation to support simulation of largescale multiagent systems our software environment benefits from a versatile messagedriven architecture originally developed to support research on financial markets it offers the flexibility to simulate a widerange of different easily customisable market rules and to study the effect of auxiliary factors such as delays on the market dynamics as a simple illustration we employ our toolbox to investigate the role of the order processing delay in normal trading and for the scenario of a significant price change owing to its general architecture our toolbox can also be employed as a generic multiagent system simulator we provide an example of such a nonfinancial application by simulating a mechanism for the coordination of noregret learning agents in a multiagent network routing scenario previously proposed in the literature,0
in the present paper given an evolving mixture of probability densities we define a candidate diffusion process whose marginal law follows the same evolution we derive as a particular case a stochastic differential equation sde admitting a unique strong solution and whose density evolves as a mixture of gaussian densities we present an interesting result on the comparison between the instantaneous and the terminal correlation between the obtained process and its squared diffusion coefficient as an application to mathematical finance we construct diffusion processes whose marginal densities are mixtures of lognormal densities we explain how such processes can be used to model the market smile phenomenon we show that the lognormal mixture dynamics is the onedimensional diffusion version of a suitable uncertain volatility model and suitably reinterpret the earlier correlation result we explore numerically the relationship between the future smile structures of both the diffusion and the uncertain volatility versions,0
a trading system is said to be robust if it generates a robust return regardless of market direction to this end a consistently positive expected trading gain is often used as a robustness metric for a trading system in this paper we propose a new class of trading policies called the double linear policy in an asset trading scenario when the transaction costs are involved unlike many existing papers we first show that the desired robust positive expected gain may disappear when transaction costs are involved then we quantify under what conditions the desired positivity can still be preserved in addition we conduct heavy montecarlo simulations for an underlying asset whose prices are governed by a geometric brownian motion with jumps to validate our theory a more realistic backtesting example involving historical data for cryptocurrency bitcoinusd is also studied,0
cryptocurrency is a cryptographybased digital asset with extremely volatile prices around usd 70 billion worth of cryptocurrency is traded daily on exchanges trading cryptocurrency is difficult due to the inherent volatility of the crypto market this study investigates whether reinforcement learning rl can enhance decisionmaking in cryptocurrency algorithmic trading compared to traditional methods in order to address this question we combined reinforcement learning with a statistical arbitrage trading technique pair trading which exploits the price difference between statistically correlated assets we constructed rl environments and trained rl agents to determine when and how to trade pairs of cryptocurrencies we developed new reward shaping and observationaction spaces for reinforcement learning we performed experiments with the developed reinforcement learner on pairs of btcgbp and btceur data separated by 1 min intervals n263520 the traditional nonrl pair trading technique achieved an annualized profit of 833 while the proposed rlbased pair trading technique achieved annualized profits from 994 to 3153 depending upon the rl learner our results show that rl can significantly outperform manual and traditional pair trading techniques when applied to volatile markets such ascryptocurrencies,0
we introduce a novel approach to options trading strategies using a highly scalable and datadriven machine learning algorithm in contrast to traditional approaches that often require specifications of underlying market dynamics or assumptions on an option pricing model our models depart fundamentally from the need for these prerequisites directly learning nontrivial mappings from market data to optimal trading signals backtesting on more than a decade of option contracts for equities listed on the sp 100 we demonstrate that deep learning models trained according to our endtoend approach exhibit significant improvements in riskadjusted performance over existing rulesbased trading strategies we find that incorporating turnover regularization into the models leads to further performance enhancements at prohibitively high levels of transaction costs,0
dynamic knowledge graphs dkgs are popular structures to express different types of connections between objects over time they can also serve as an efficient mathematical tool to represent information extracted from complex unstructured data sources such as text or images within financial applications dkgs could be used to detect trends for strategic thematic investing based on information obtained from financial news articles in this work we explore the properties of large language models llms as dynamic knowledge graph generators proposing a novel opensource finetuned llm for this purpose called the integrated contextual knowledge graph generator ickg we use ickg to produce a novel opensource dkg from a corpus of financial news articles called findkg and we propose an attentionbased gnn architecture for analysing it called kgtransformer we test the performance of the proposed model on benchmark datasets and findkg demonstrating superior performance on link prediction tasks additionally we evaluate the performance of the kgtransformer on findkg for thematic investing showing it can outperform existing thematic etfs,0
when modelling stock market dynamics the price formation is often based on an equilbrium mechanism in real stock exchanges however the price formation is goverend by the order book it is thus interesting to check if the resulting stylized facts of a model with equilibrium pricing change remain the same or more generally are compatible with the order book environment we tackle this issue in the framework of a case study by embedding the bornholdtkaizojifujiwara spin model into the order book dynamics to this end we use a recently developed agent based model that realistically incorporates the order book we find realistic stylized facts we conclude for the studied case that equilibrium pricing is not needed and that the corresponding assumption of a fundamental price may be abandoned,0
we extend a discrete time random walk dtrw numerical scheme to simulate the anomalous diffusion of financial market orders in a simulated order book here using random walks with sibuya waiting times to include a timedependent stochastic forcing function with nonuniformly sampled times between order book events in the setting of fractional diffusion this models the fluid limit of an order book by modelling the continuous arrival cancellation and diffusion of orders in the presence of information shocks we study the impulse response and stylised facts of orders undergoing anomalous diffusion for different forcing functions and model parameters concretely we demonstrate the price impact for flash limitorders and market orders and show how the numerical method generate kinks in the price impact we use cubic spline interpolation to generate smoothed price impact curves the work promotes the use of nonuniform sampling in the presence of diffusive dynamics as the preferred simulation method,0
this work addresses the problem of optimal pricing and hedging of a european option on an illiquid asset z using two proxies a liquid asset s and a liquid european option on another liquid asset y we assume that the shedge is dynamic while the yhedge is static using the indifference pricing approach we derive a hjb equation for the value function and solve it analytically in quadratures using an asymptotic expansion around the limit of the perfect correlation between assets y and z while in this paper we apply our framework to an incomplete market version of the creditequity mertons model the same approach can be used for other asset classes equity commodity fx etc eg for pricing and hedging options with illiquid strikes or illiquid exotic options,0
lending protocols lps as blockchainbased lending systems allow any agents to borrow and lend cryptocurrencies however liquidity risks could occur especially when salient loans are initiated by a particular group of borrowers this paper proposes measurements of liquidity risks focusing on both available liquidity and market concentration in lps by using aave as a case study we find that liquidity risks are highly volatile and show complex effects on aave and liquidity in aave may affect across onchain lending market compared to new users regular users that repeatedly borrow cryptocurrencies may negatively affect aave protocol implying that user loyalty is a doubleedged sword for lps,0
this paper introduces large execution models lems a novel deep learning framework that extends transformerbased architectures to address complex execution problems with flexible time boundaries and multiple execution constraints building upon recent advances in neural vwap execution strategies lems generalize the approach from fixedduration orders to scenarios where execution duration is bounded between minimum and maximum time horizons similar to share buyback contract structures the proposed architecture decouples market information processing from execution allocation decisions a common feature extraction pipeline using temporal kolmogorovarnold networks tkans variable selection networks vsns and multihead attention mechanisms processes market data to create informational context while independent allocation networks handle the specific execution logic for different scenarios fixed quantity vs fixed notional buy vs sell orders this architectural separation enables a unified model to handle diverse execution objectives while leveraging shared market understanding across scenarios through comprehensive empirical evaluation on intraday cryptocurrency markets and multiday equity trading using dow jones constituents we demonstrate that lems achieve superior execution performance compared to traditional benchmarks by dynamically optimizing execution paths within flexible time constraints the unified model architecture enables deployment across different execution scenarios buysell orders varying duration boundaries volumenotional targets through a single framework providing significant operational advantages over assetspecific approaches,0
this paper focuses on computing the fill probabilities for limit orders positioned at various price levels within the limit order book which play a crucial role in optimizing executions we adopt a generic stochastic model to capture the dynamics of the order book as a series of queueing systems this generic model is statedependent and also incorporates stylized factors we subsequently derive semianalytical expressions to compute the relevant probabilities within the context of statedependent stochastic order flows these probabilities cover various scenarios including the probability of a change in the midprice the fill probabilities of orders posted at the best quotes and those posted at a price level deeper than the best quotes in the book before the opposite best quote moves these expressions can be further generalized to accommodate orders posted even deeper in the order book although the associated probabilities are typically very small in such cases lastly we conduct extensive numerical experiments using real order book data from the foreign exchange spot market our findings suggest that the model is tractable and possesses the capability to effectively capture the dynamics of the limit order book moreover the derived formulas and numerical methods demonstrate reasonably good accuracy in estimating the fill probabilities,0
forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits a small improvement in forecasting performance can lead to increased profitability therefore obtaining a realistic forecast is very important for investors successful forecasting provides traders with effective buyorhold strategies allowing them to make more profits the most important thing in this process is to produce accurate forecasts suitable for reallife applications bitcoin frequently mentioned recently due to its volatility and chaotic behavior has begun to pay great attention and has become an investment tool especially during and after the covid19 pandemic this study provided a comprehensive methodology including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning ml algorithms to forecast bitcoin price movement a binarization procedure was applied using continuous data to construct the trend data representing each input feature trend following the related literature the input features are determined as technical indicators google trends and the number of tweets random forest rf knearest neighbor knn extreme gradient boosting xgboostxgb support vector machine svm naive bayes nb artificial neural networks ann and longshortterm memory lstm networks were applied on the selected features for prediction purposes this work investigates two main research questions i how does the sample size affect the prediction performance of ml algorithms ii how does the data type affect the prediction performance of ml algorithms accuracy and area under the roc curve auc values were used to compare the model performance a ttest was performed to test the statistical significance of the prediction results,0
the complexity of financial data characterized by its variability and low signaltonoise ratio necessitates advanced methods in quantitative investment that prioritize both performance and interpretabilitytransitioning from early manual extraction to genetic programming the most advanced approach in the alpha factor mining domain currently employs reinforcement learning to mine a set of combination factors with fixed weights however the performance of resultant alpha factors exhibits inconsistency and the inflexibility of fixed factor weights proves insufficient in adapting to the dynamic nature of financial markets to address this issue this paper proposes a twostage formulaic alpha generating framework alphaforge for alpha factor mining and factor combination this framework employs a generativepredictive neural network to generate factors leveraging the robust spatial exploration capabilities inherent in deep learning while concurrently preserving diversity the combination model within the framework incorporates the temporal performance of factors for selection and dynamically adjusts the weights assigned to each component alpha factor experiments conducted on realworld datasets demonstrate that our proposed model outperforms contemporary benchmarks in formulaic alpha factor mining furthermore our model exhibits a notable enhancement in portfolio returns within the realm of quantitative investment and real money investment,0
in the aftermath of the financial crisis supervisory authorities have considerably altered the mode of operation of financial stress testing despite these efforts significant concerns and extensive criticism have been raised by market participants regarding the considered unrealistic methodological assumptions and simplifications current stress testing methodologies attempt to simulate the risks underlying a financial institutions balance sheet by using several satellite models this renders their integration a really challenging task leading to significant estimation errors moreover advanced statistical techniques that could potentially capture the nonlinear nature of adverse shocks are still ignored this work aims to address these criticisms and shortcomings by proposing a novel approach based on recent advances in deep learning towards a principled method for dynamic balance sheet stress testing experimental results on a newly collected financialsupervisory dataset provide strong empirical evidence that our paradigm significantly outperforms traditional approaches thus it is capable of more accurately and efficiently simulating real world scenarios,0
we deploy and demonstrate the cointossx lowlatency highthroughput opensource matching engine with orders sent using the julia and python languages we show how this can be deployed for smallscale local desktop testing and discuss a larger scale but local hosting with multiple traded instruments managed concurrently and managed by multiple clients we then demonstrate a cloud based deployment using microsoft azure with largescale industrial and simulation research use cases in mind the system is exposed and interacted with via sockets using udp sbe message protocols and can be monitored using a simple web browser interface using we give examples showing how orders can be be sent to the system and market data feeds monitored using the julia and python languages the system is developed in java with orders submitted as binary encodings sbe via udp protocols using the aeron media driver as the lowlatency high throughput message transport the system separates the ordergeneration and simulation environments eg agentbased model simulation from the matching of orders datafeeds and various modularised components of the orderbook system this ensures a more natural and realistic asynchronicity between events generating orders and the events associated with orderbook dynamics and market datafeeds we promote the use of julia as the preferred order submission and simulation environment,0
modelling joint dynamics of liquid vanilla options is crucial for arbitragefree pricing of illiquid derivatives and managing risks of option trade books this paper develops a nonparametric model for the european options book respecting underlying financial constraints and while being practically implementable we derive a state space for prices which are free from static or modelindependent arbitrage and study the inference problem where a model is learnt from discrete time series data of stock and option prices we use neural networks as function approximators for the drift and diffusion of the modelled sde system and impose constraints on the neural nets such that noarbitrage conditions are preserved in particular we give methods to calibrate textitneural sde models which are guaranteed to satisfy a set of linear inequalities we validate our approach with numerical experiments using data generated from a heston stochastic local volatility model,0
in high frequency trading accurate prediction of order flow imbalance ofi is crucial for understanding market dynamics and maintaining liquidity this paper introduces a hybrid predictive model that combines vector auto regression var with a simple feedforward neural network fnn to forecast ofi and assess trading intensity the var component captures linear dependencies while residuals are fed into the fnn to model nonlinear patterns enabling a comprehensive approach to ofi prediction additionally the model calculates the intensity on the buy or sell side providing insights into which side holds greater trading pressure these insights facilitate the development of trading strategies by identifying periods of high buy or sell intensity using both synthetic and real trading data from binance we demonstrate that the hybrid model offers significant improvements in predictive accuracy and enhances strategic decisionmaking based on ofi dynamics furthermore we compare the hybrid models performance with standalone fnn and var models showing that the hybrid approach achieves superior forecasting accuracy across both synthetic and real datasets making it the most effective model for ofi prediction in high frequency trading,0
implied volatility iv is a key metric in financial markets reflecting market expectations of future price fluctuations research has explored ivs relationship with moneyness focusing on its connection to the implied hurst exponent h our study reveals that h approaches 12 when moneyness equals 1 marking a critical point in market efficiency expectations we developed an iv model that integrates h to capture these dynamics more effectively this model considers the interaction between h and the underlyingtostrike price ratio sk crucial for capturing iv variations based on moneyness using optuna optimization across multiple indexes the model outperformed sabr and fsabr in accuracy this approach provides a more detailed representation of market expectations and ivh dynamics improving options pricing and volatility forecasting while enhancing theoretical and pratcical financial analysis,0
we study the problem of optimal pricing and hedging of a european option written on an illiquid asset z using a set of proxies a liquid asset s and n liquid european options pi each written on a liquid asset yi i1n we assume that the shedge is dynamic while the multiname yhedge is static using the indifference pricing approach with an exponential utility we derive a hjb equation for the value function and build an efficient numerical algorithm the latter is based on several changes of variables a splitting scheme and a set of fast gauss transforms fgt which turns out to be more efficient in terms of complexity and lower local space error than a finitedifference method while in this paper we apply our framework to an incomplete market version of the creditequity mertons model the same approach can be used for other asset classes equity commodity fx etc eg for pricing and hedging options with illiquid strikes or illiquid exotic options,0
nonequilibrium phenomena occur not only in physical world but also in finance in this work stochastic relaxational dynamics together with path integrals is applied to option pricing theory a recently proposed model by ilinski et al considers fluctuations around this equilibrium state by introducing a relaxational dynamics with random noise for intermediate deviations called virtual arbitrage returns in this work the model is incorporated within a martingale pricing method for derivatives on securities eg stocks in incomplete markets using a mapping to option pricing theory with stochastic interest rates using a famous result by merton and with some help from the path integral method exact pricing formulas for european call and put options under the influence of virtual arbitrage returns or intermediate deviations from economic equilibrium are derived where only the final integration over initial arbitrage returns needs to be performed numerically this result is complemented by a discussion of the hedging strategy associated to a derivative which replicates the final payoff but turns out to be not selffinancing in the real world but selffinancing it when summed over the derivatives remaining life time numerical examples are given which underline the fact that an additional positive risk premium with respect to the blackscholes values is found reflecting extra hedging costs due to intermediate deviations from economic equilibrium,0
market dynamic is quantified in terms of the entropy sn of the clusters formed by the intersections between the series of the prices pt and the moving average widetildeptn the entropy sn is defined according to shannon as sum pnlog pn with pn the probability for the cluster to occur with duration  par the investigation is performed on highfrequency data of the nasdaq composite dow jones industrial avg and standard poor 500 indexes downloaded from the bloomberg terminal the cluster entropy sn is analysed in raw and sampled data over a broad range of temporal horizons m varying from one to twelve months over the year 2018 the cluster entropy sn is integrated over the cluster duration  to yield the market dynamic index imn a synthetic figure of price dynamics a systematic dependence of the cluster entropy sn and the market dynamic index imn on the temporal horizon m is evidenced par finally the market horizon dependence defined as hmnimni1n is compared with the horizon dependence of the pricing kernel with different representative agents obtained via a kullbackleibler entropy approach the market horizon dependence hmn of the three assets is compared against the values obtained by implementing the cluster entropy sn approach on artificially generated series fractional brownian motion,0
a market with defaultable bonds where the bond dynamics is in a heathjarrowmorton setting and the forward rates are driven by an infinite number of levy factors is considered the setting includes rating migrations driven by a markov chain all basic types of recovery are investigated we formulate necessary and sufficient conditions generalized hjm conditions under which the market is arbitrage free connections with consistency conditions are discussed,0
reinforcement learning rl applied to financial problems has been the subject of a lively area of research the use of rl for optimal trading strategies that exploit latent information in the market is to the best of our knowledge not widely tackled in this paper we study an optimal trading problem where a trading signal follows an ornsteinuhlenbeck process with regimeswitching dynamics we employ a blend of rl and recurrent neural networks rnn in order to make the most at extracting underlying information from the trading signal with latent parameters the latent parameters driving mean reversion speed and volatility are filtered from observations of the signal and trading strategies are derived via rl to address this problem we propose three deep deterministic policy gradient ddpgbased algorithms that integrate gated recurrent unit gru networks to capture temporal dependencies in the signal the first a one step approach hidddpg directly encodes hidden states from the gru into the rl trader the second and third are twostep methods one probddpg makes use of posterior regime probability estimates while the other regddpg relies on forecasts of the next signal value through extensive simulations with increasingly complex markovian regime dynamics for the trading signals parameters as well as an empirical application to equity pair trading we find that probddpg achieves superior cumulative rewards and exhibits more interpretable strategies by contrast regddpg provides limited benefits while hidddpg offers intermediate performance with less interpretable strategies our results show that the quality and structure of the information supplied to the agent are crucial embedding probabilistic insights into latent regimes substantially improves both profitability and robustness of reinforcement learningbased trading strategies,0
this paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils we introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies ais built based on the prices of these assets we employ four types of diverse theoretical models lstm long shortterm memory arimagarch autoregressive integrated moving average generalized autoregressive conditional heteroskedasticity momentum and contrarian to generate price forecasts which are then used to produce investment signals in single and complex ais in such a way we are able to verify the diversification potential of different types of investment strategies consisting of various assets energy commodities precious metals cryptocurrencies or soft commodities in hedging ensemble ais built for equity indices sp 500 index empirical data used in this study cover the period between 2004 and 2022 our main conclusion is that lstmbased strategies outperform the other models and that the best diversifier for the ais built for the sp 500 index is the ais built for bitcoin finally we test the lstm model for a higher frequency of data 1 hour we conclude that it outperforms the results obtained using daily data,0
in this research we develop a trading strategy for the discretetime optimal liquidation problem of large order trading with different market microstructures in an illiquid market in this framework the flow of orders can be viewed as a point process with stochastic intensity we model the price impact as a linear function of a selfexciting dynamic process we formulate the liquidation problem as a discretetime markov decision processes where the state process is a piecewise deterministic markov process pdmp the numerical results indicate that an optimal trading strategy is dependent on characteristics of the market microstructure when no orders above certain value come the optimal solution takes offers in the lower levels of the limit order book in order to prevent not filling of orders and facing final inventory costs,0
in an environment of increasingly volatile financial markets the accurate estimation of risk remains a major challenge traditional econometric models such as garch and its variants are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics to overcome these limitations we propose a hybrid framework for valueatrisk var estimation combining garch volatility models with deep reinforcement learning our approach incorporates directional market forecasting using the double deep qnetwork ddqn model treating the task as an imbalanced classification problem this architecture enables the dynamic adjustment of risklevel forecasts according to market conditions empirical validation on daily eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of var estimates as well as a reduction in the number of breaches and also in capital requirements while respecting regulatory risk thresholds the ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management,0
as financial markets grow increasingly complex in the big data era accurate stock prediction has become more critical traditional time series models such as grus have been widely used but often struggle to capture the intricate nonlinear dynamics of markets particularly in the flexible selection and effective utilization of key historical information recently methods like graph neural networks and reinforcement learning have shown promise in stock prediction but require high data quality and quantity and they tend to exhibit instability when dealing with data sparsity and noise moreover the training and inference processes for these models are typically complex and computationally expensive limiting their broad deployment in practical applications existing approaches also generally struggle to capture unobservable latent market states effectively such as market sentiment and expectations microstructural factors and participant behavior patterns leading to an inadequate understanding of market dynamics and subsequently impact prediction accuracy to address these challenges this paper proposes a stock prediction model mcigru based on a multihead crossattention mechanism and an improved gru first we enhance the gru model by replacing the reset gate with an attention mechanism thereby increasing the models flexibility in selecting and utilizing historical information second we design a multihead crossattention mechanism for learning unobservable latent market state representations which are further enriched through interactions with both temporal features and crosssectional features finally extensive experiments on four main stock markets show that the proposed method outperforms sota techniques across multiple metrics additionally its successful application in realworld fund management operations confirms its effectiveness and practicality,0
this paper presents a novel approach to pricing american options using piecewise diffusion markov processes pdifmps a type of generalised stochastic hybrid system that integrates continuous dynamics with discrete jump processes standard models often rely on constant drift and volatility assumptions which limits their ability to accurately capture the complex and erratic nature of financial markets by incorporating pdifmps our method accounts for sudden market fluctuations providing a more realistic model of asset price dynamics we benchmark our approach with the longstaffschwartz algorithm both in its original form and modified to include pdifmp asset price trajectories numerical simulations demonstrate that our pdifmpbased method not only provides a more accurate reflection of market behaviour but also offers practical advantages in terms of computational efficiency the results suggest that pdifmps can significantly improve the predictive accuracy of american options pricing by more closely aligning with the stochastic volatility and jumps observed in real financial markets,0
as global financial markets become increasingly interconnected financial contagion has developed into a major influencer of asset price dynamics motivated by this context our study explores financial contagion both within and between asset communities we contribute to the literature by examining the contagion phenomenon at the community level rather than among individual assets our experiments rely on highfrequency data comprising cryptocurrencies stocks and us etfs over the 4year period from april 2019 to may 2023 using the louvain community detection algorithm vector autoregression contagion detection model and tracywidom random matrix theory for noise removal from financial assets we present three main findings firstly while the magnitude of contagion remains relatively stable over time contagion density the percentage of asset pairs exhibiting contagion within a financial system increases this suggests that market uncertainty is better characterized by the transmission of shocks more broadly than by the strength of any single spillover secondly there is no significant difference between intra and intercommunity contagion indicating that contagion is a systemwide phenomenon rather than being confined to specific asset groups lastly certain communities themselves especially those dominated by information technology assets consistently appear to act as major contagion transmitters in the financial network over the examined period spreading shocks with high densities to many other communities our findings suggest that traditional risk management strategies such as portfolio diversification through investing in lowcorrelated assets or different types of investment vehicle might be insufficient due to widespread contagion,0
in this brief review we critically examine the recent work done on correlationbased networks in financial systems the structure of empirical correlation matrices constructed from the financial market data changes as the individual stock prices fluctuate with time showing interesting evolutionary patterns especially during critical events such as market crashes bubbles etc we show that the study of correlationbased networks and their evolution with time is useful for extracting important information of the underlying market dynamics we also present our perspective on the use of recently developed entropy measures such as structural entropy and eigenentropy for continuous monitoring of correlationbased networks,0
in the regime switching extension of blackscholesmerton model of asset price dynamics one assumes that the volatility coefficient evolves as a hidden pure jump process under the assumption of markov regime switching we have considered the locally risk minimizing price of european vanilla options by pretending these prices or their noisy versions as traded prices we have first computed the implied volatility iv of the underlying asset then by performing several numerical experiments we have investigated the dependence of iv on the time to maturity ttm and strike price of the vanilla options we have observed a clear dependence that is at par with the empirically observed stylized facts furthermore we have experimentally validated that iv time series obtained from contracts with moneyness and ttm varying in particular narrow ranges can recover the transition instances of the hidden markov chain such regime recovery has also been proved in a theoretical setting moreover the novel scheme for computing option price is shown to be stable,0
much research has been conducted arguing that tipping points at which complex systems experience phase transitions are difficult to identify to test the existence of tipping points in financial markets based on the alternating offer strategic model we propose a network of bargaining agents who mutually either cooperate or where the feedback mechanism between trading and price dynamics is driven by an external hidden variable r that quantifies the degree of market overpricing due to the feedback mechanism r fluctuates and oscillates over time and thus periods when the market is underpriced and overpriced occur repeatedly as the market becomes overpriced bubbles are created that ultimately burst in a market crash the probability that the index will drop in the next year exhibits a strong hysteresis behavior from which we calculate the tipping point the probability distribution function of r has a bimodal shape characteristic of small systems near the tipping point by examining the sp500 index we illustrate the applicability of the model and demonstate that the financial data exhibits a hysteresis and a tipping point that agree with the model predictions we report a cointegration between the returns of the sp 500 index and its intrinsic value,0
agentbased models particularly those applied to financial markets demonstrate the ability to produce realistic simulated system dynamics comparable to those observed in empirical investigations despite this they remain fairly difficult to calibrate due to their tendency to be computationally expensive even with recent advances in technology for this reason financial agentbased models are frequently validated by demonstrating an ability to reproduce wellknown log return time series and central limit order book stylized facts as opposed to being rigorously calibrated to transaction data we thus apply an established financial agentbased model calibration framework to a simple model of high and lowfrequency trader interaction and demonstrate possible inadequacies of a stylized factcentric approach to model validation we further argue for the centrality of calibration to the validation of financial agentbased models and possible pitfalls of current approaches to financial agentbased modeling,0
the pricing of derivatives tied to baskets of assets demands a sophisticated framework that aligns with the available market information to capture the intricate nonlinear dependency structure among the assets we describe the dynamics of the multivariate process of constituents with a copula model and propose an efficient method to extract the dependency structure from the market the proposed method generates coherent sets of samples of the constituents process through systematic sampling rearrangement these samples are then utilized to calibrate a local volatility model lvm of the basket process which is used to price basket derivatives we show that the method is capable of efficiently pricing basket options based on a large number of basket constituents accomplishing the calibration process within a matter of seconds and achieving nearperfect calibration to the index options of the market,0
we present a framework for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs market impact liquidity constraints or risk limits using modern deep reinforcement machine learning methods we discuss how standard reinforcement learning methods can be applied to nonlinear reward structures ie in our case convex risk measures as a general contribution to the use of deep learning for stochastic processes we also show that the set of constrained trading strategies used by our algorithm is large enough to approximate any optimal solution our algorithm can be implemented efficiently even in highdimensional situations using modern machine learning tools its structure does not depend on specific market dynamics and generalizes across hedging instruments including the use of liquid derivatives its computational performance is largely invariant in the size of the portfolio as it depends mainly on the number of hedging instruments available we illustrate our approach by showing the effect on hedging under transaction costs in a synthetic market driven by the heston model where we outperform the standard complete market solution,0
the inherent volatility and dynamic fluctuations within the financial stock market underscore the necessity for investors to employ a comprehensive and reliable approach that integrates risk management strategies market trends and the movement trends of individual securities by evaluating specific data investors can make more informed decisions however the current body of literature lacks substantial evidence supporting the practical efficacy of reinforcement learning rl agents as many models have only demonstrated success in back testing using historical data this highlights the urgent need for a more advanced methodology capable of addressing these challenges there is a significant disconnect in the effective utilization of financial indicators to better understand the potential market trends of individual securities the disclosure of successful trading strategies is often restricted within financial markets resulting in a scarcity of widely documented and published strategies leveraging rl furthermore current research frequently overlooks the identification of financial indicators correlated with various market trends and their potential advantages this research endeavors to address these complexities by enhancing the ability of rl agents to effectively differentiate between positive and negative buysell actions using financial indicators while we do not address all concerns this paper provides deeper insights and commentary on the utilization of technical indicators and their benefits within reinforcement learning this work establishes a foundational framework for further exploration and investigation of more complex scenarios,0
classical portfolio optimization often requires forecasting asset returns and their corresponding variances in spite of the low signaltonoise ratio provided in the financial markets modern deep reinforcement learning drl offers a framework for optimizing sequential trader decisions but lacks theoretical guarantees of convergence on the other hand the performances on real financial trading problems are strongly affected by the goodness of the signal used to predict returns to disentangle the effects coming from return unpredictability from those coming from algorithm untrainability we investigate the performance of modelfree drl traders in a market environment with different known meanreverting factors driving the dynamics when the framework admits an exact dynamic programming solution we can assess the limits and capabilities of different valuebased algorithms to retrieve meaningful trading signals in a datadriven manner we consider drl agents that leverage classical strategies to increase their performances and we show that this approach guarantees flexibility outperforming the benchmark strategy when the price dynamics is misspecified and some original assumptions on the market environment are violated with the presence of extreme events and volatility clustering,0
the recent work of horikawa and nakagawa 2024 claims that under a complete market admitting statistical arbitrage the difference between the hedging position provided by deep hedging and that of the replicating portfolio is a statistical arbitrage this raises concerns as it entails that deep hedging can include a speculative component aimed simply at exploiting the structure of the risk measure guiding the hedging optimisation problem we test whether such finding remains true in a garchbased market model which is an illustrative case departing from complete market dynamics we observe that the difference between deep hedging and delta hedging is a speculative overlay if the risk measure considered does not put sufficient relative weight on adverse outcomes nevertheless a suitable choice of risk measure can prevent the deep hedging agent from engaging in speculation,0
this paper applies deep reinforcement learning drl to optimize liquidity provisioning in uniswap v3 a decentralized finance defi protocol implementing an automated market maker amm model with concentrated liquidity we model the liquidity provision task as a markov decision process mdp and train an active liquidity provider lp agent using the proximal policy optimization ppo algorithm the agent dynamically adjusts liquidity positions by using information about price dynamics to balance fee maximization and impermanent loss mitigation we use a rolling window approach for training and testing reflecting realistic market conditions and regime shifts this study compares the datadriven performance of the drlbased strategy against common heuristics adopted by small retail lp actors that do not systematically modify their liquidity positions by promoting more efficient liquidity management this work aims to make defi markets more accessible and inclusive for a broader range of participants through a datadriven approach to liquidity management this work seeks to contribute to the ongoing development of more efficient and userfriendly defi markets,0
the aim of this paper is to propose a new methodology that allows forecasting through vasicek and cir models of future expected interest rates for each maturity based on rolling windows from observed financial market data the novelty apart from the use of those models not for pricing but for forecasting the expected rates at a given maturity consists in an appropriate partitioning of the data sample this allows capturing all the statistically significant time changes in volatility of interest rates thus giving an account of jumps in market dynamics the performance of the new approach is carried out for different term structures and is tested for both models it is shown how the proposed methodology overcomes both the usual challenges eg simulating regime switching volatility clustering skewed tails etc as well as the new ones added by the current market environment characterized by low to negative interest rates,0
in this research paper we investigate into a paper named a deep reinforcement learning framework for the financial portfolio management problem it is a portfolio management problem which is solved by deep learning techniques the original paper proposes a financialmodelfree reinforcement learning framework which consists of the ensemble of identical independent evaluators eiie topology a portfoliovector memory pvm an online stochastic batch learning osbl scheme and a fully exploiting and explicit reward function three different instants are used to realize this framework namely a convolutional neural network cnn a basic recurrent neural network rnn and a long shortterm memory lstm the performance is then examined by comparing to a number of recently reviewed or published portfolioselection strategies we have successfully replicated their implementations and evaluations besides we further apply this framework in the stock market instead of the cryptocurrency market that the original paper uses the experiment in the cryptocurrency market is consistent with the original paper which achieve superior returns but it doesnt perform as well when applied in the stock market,0
we use a deep neural network to generate controllers for optimal trading on high frequency data for the first time a neural network learns the mapping between the preferences of the trader ie risk aversion parameters and the optimal controls an important challenge in learning this mapping is that in intraday trading traders actions influence price dynamics in closed loop via the market impact the explorationexploitation tradeoff generated by the efficient execution is addressed by tuning the traders preferences to ensure long enough trajectories are produced during the learning phase the issue of scarcity of financial data is solved by transfer learning the neural network is first trained on trajectories generated thanks to a montecarlo scheme leading to a good initialization before training on historical trajectories moreover to answer to genuine requests of financial regulators on the explainability of machine learning generated controls we project the obtained blackbox controls on the space usually spanned by the closedform solution of the stylized optimal trading problem leading to a transparent structure for more realistic loss functions that have no closedform solution we show that the average distance between the generated controls and their explainable version remains small this opens the door to the acceptance of mlgenerated controls by financial regulators,0
in the highly volatile and uncertain global financial markets traditional quantitative trading models relying on statistical modeling or empirical rules often fail to adapt to dynamic market changes and black swan events due to rigid assumptions and limited generalization to address these issues this paper proposes qtmrl quantitative trading multiindicator reinforcement learning an intelligent trading agent combining multidimensional technical indicators with reinforcement learning rl for adaptive and stable portfolio management we first construct a comprehensive multiindicator dataset using 23 years of sp 500 daily ohlcv data 20002022 for 16 representative stocks across 5 sectors enriching raw data with trend volatility and momentum indicators to capture holistic market dynamics then we design a lightweight rl framework based on the advantage actorcritic a2c algorithm including data processing a2c algorithm and trading agent modules to support policy learning and actionable trading decisions extensive experiments compare qtmrl with 9 baselines eg arima lstm moving average strategies across diverse market regimes verifying its superiority in profitability risk adjustment and downside risk control the code of qtmrl is publicly available at,0
we map stock market interactions to spin models to recover their hierarchical structure using a simulated annealing based superparamagnetic clustering spc algorithm this is directly compared to a modified implementation of a maximum likelihood approach we call fast superparamagnetic clustering fspc the methods are first applied standard toy testcase problems and then to a dataset of 447 stocks traded on the new york stock exchange nyse over 1249 days the signal to noise ratio of stock market correlation matrices is briefly considered our result recover approximately clusters representative of standard economic sectors and mixed ones whose dynamics shine light on the adaptive nature of financial markets and raise concerns relating to the effectiveness of industry based static financial market classification in the world of realtime data analytics a key result is that we show that fspc maximum likelihood solutions converge to ones found within the superparamagnetic phase where the entropy is maximum and those solutions are qualitatively better for high dimensionality datasets,0
this study investigates the application of machine learning techniques specifically neural networks random forests and catboost for option pricing in comparison to traditional models such as blackscholes and heston model using both synthetically generated data and real market option data each model is evaluated in predicting the option price the results show that machine learning models can capture complex nonlinear relationships in option prices and in several cases outperform both blackscholes and heston models these findings highlight the potential of datadriven methods to improve pricing accuracy and better reflect market dynamics,0
the recent application of deep learning models to financial trading has heightened the need for high fidelity financial time series data this synthetic data can be used to supplement historical data to train large trading models the stateoftheart models for the generative application often rely on huge amounts of historical data and large complicated models these models range from autoregressive and diffusionbased models through to architecturally simpler models such as the temporalattention bilinear layer agentbased approaches to modelling limit order book dynamics can also recreate trading activity through mechanistic models of trader behaviours in this work we demonstrate how a popular agentbased framework for simulating intraday trading activity the chiarella model can be combined with one of the most performant deep learning models for forecasting multivariate time series the tabl model this forecasting model is coupled to a simulation of a matching engine with a novel method for simulating deleted order flow our simulator gives us the ability to test the generative abilities of the forecasting model using stylised facts our results show that this methodology generates realistic price dynamics however when analysing deeper parts of the markets microstructure are not accurately recreated highlighting the necessity for including more sophisticated agent behaviors into the modeling framework to help account for tail events,0
in financial trading large language model llmbased agents demonstrate significant potential however the high sensitivity to market noise undermines the performance of llmbased trading systems to address this limitation we propose a novel multiagent system featuring an internal competitive mechanism inspired by modern corporate management structures the system consists of two specialized teams 1 data team responsible for processing and condensing massive market data into diversified text factors ensuring they fit the models constrained context 2 research team tasked with making parallelized multipath trading decisions based on deep research methods the core innovation lies in implementing a realtime evaluation and ranking mechanism within each team driven by authentic market feedback each agents performance undergoes continuous scoring and ranking with only outputs from topperforming agents being adopted the design enables the system to adaptively adjust to dynamic environment enhances robustness against market noise and ultimately delivers superior trading performance experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics contesttrade is opensourced on github at,0
we propose a datadriven neural network nn optimization framework to determine the optimal multiperiod dynamic asset allocation strategy for outperforming a general stochastic target we formulate the problem as an optimal stochastic control with an asymmetric distribution shaping objective function the proposed framework is illustrated with the asset allocation problem in the accumulation phase of a defined contribution pension plan with the goal of achieving a higher terminal wealth than a stochastic benchmark we demonstrate that the datadriven approach is capable of learning an adaptive asset allocation strategy directly from historical market returns without assuming any parametric model of the financial market dynamics following the optimal adaptive strategy investors can make allocation decisions simply depending on the current state of the portfolio the optimal adaptive strategy outperforms the benchmark constant proportion strategy achieving a higher terminal wealth with a 90 probability a 46 higher median terminal wealth and a significantly more rightskewed terminal wealth distribution we further demonstrate the robustness of the optimal adaptive strategy by testing the performance of the strategy on bootstrap resampled market data which has different distributions compared to the training data,0
calculating true volatility is an essential task for option pricing and risk management however it is made difficult by market microstructure noise particle filtering has been proposed to solve this problem as it favorable statistical properties but relies on assumptions about underlying market dynamics machine learning methods have also been proposed but lack interpretability and often lag in performance in this paper we implement the svpfrnn a hybrid neural network and particle filter architecture our svpfrnn is designed specifically with stochastic volatility estimation in mind we then show that it can improve on the performance of a basic particle filter,0
we implement and test kernel averaging nonuniform fast fourier transform nufft methods to enhance the performance of correlation and covariance estimation on asynchronously sampled eventdata using the malliavinmancino fourier estimator the methods are benchmarked for dirichlet and fejr fourier basis kernels we consider test cases formed from geometric brownian motions to replicate synchronous and asynchronous data for benchmarking purposes we consider three standard averaging kernels to convolve the eventdata for synchronisation via oversampling for use with the fast fourier transform fft the gaussian kernel the kaiserbessel kernel and the exponential of semicircle kernel first this allows us to demonstrate the performance of the estimator with different combinations of basis kernels and averaging kernels second we investigate and compare the impact of the averaging scales explicit in each averaging kernel and its relationship between the timescale averaging implicit in the malliavinmancino estimator third we demonstrate the relationship between timescale averaging based on the number of fourier coefficients used in the estimator to a theoretical model of the epps effect we briefly demonstrate the methods on tradeandquote taq data from the johannesburg stock exchange to make an initial visualisation of the correlation dynamics for various timescales under market microstructure,0
in the everchanging and intricate landscape of financial markets portfolio optimisation remains a formidable challenge for investors and asset managers conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences to address this we propose an innovative framework termed diffusionaugmented reinforcement learning darl which synergistically integrates denoising diffusion probabilistic models ddpms with deep reinforcement learning drl for portfolio management by leveraging ddpms to generate synthetic market crash scenarios conditioned on varying stress intensities our approach significantly enhances the robustness of training data empirical evaluations demonstrate that darl outperforms traditional baselines delivering superior riskadjusted returns and resilience against unforeseen crises such as the 2025 tariff crisis this work offers a robust and practical methodology to bolster stress resilience in drldriven financial applications,0
with the increasing enrichment and development of the financial derivatives market the frequency of transactions is also faster and faster due to human limitations algorithms and automatic trading have recently become the focus of discussion in this paper we propose a bidirectional lstm neural network based on an attention mechanism which is based on two popular assets gold and bitcoin in terms of feature engineering on the one hand we add traditional technical factors and at the same time we combine time series models to develop factors in the selection of model parameters we finally chose a twolayer deep learning network according to auc measurement the accuracy of bitcoin and gold is 7194 and 7303 respectively using the forecast results we achieved a return of 108934 in two years at the same time we also compare the attention bilstm model proposed in this paper with the traditional model and the results show that our model has the best performance in this data set finally we discuss the significance of the model and the experimental results as well as the possible improvement direction in the future,0
it is well known that the coxingersollross cir stochastic model to study the term structure of interest rates as introduced in 1985 is inadequate for modelling the current market environment with negative short interest rates moreover the diffusion term in the rate dynamics goes to zero when short rates are small both volatility and longrun mean do not change with time they do not fit with the skewed fat tails distribution of the interest rates etc the aim of the present work is to suggest a new framework which we call the cir model that well fits the term structure of short interest rates so that the market volatility structure is preserved as well as the analytical tractability of the original cir model,0
the sabr model is a cornerstone of interest rate volatility modeling but its practical application relies heavily on the analytical approximation by hagan et al whose accuracy deteriorates for high volatility long maturities and outofthemoney options admitting arbitrage while machine learning approaches have been proposed to overcome these limitations they have often been limited by simplified sabr dynamics or a lack of systematic validation against the full spectrum of market conditions we develop a novel sabr dnn a specialized artificial deep neural network dnn architecture that learns the true sabr stochastic dynamics using an unprecedented large training dataset more than 200 million points of interest rate capfloor volatility surfaces including very long maturities 30y and extreme strikes consistently with market quotations our dataset is obtained via highprecision unbiased monte carlo simulation of a special scaled shiftedsabr stochastic dynamics which allows dimensional reduction without any loss of generality our sabr dnn provides arbitragefree calibration of real market volatility surfaces and capfloor prices for any maturity and strike with negligible computational effort and without retraining across business dates our results fully address the gaps in the previous machine learning sabr literature in a systematic and selfconsistent way and can be extended to cover any interest rate european options in different rate tenors and currencies thus establishing a comprehensive functional sabr framework that can be adopted for daily trading and risk management activities,0
affine diffusion dynamics are frequently used for valuation adjustments xva calculations due to their analytic tractability however these models cannot capture the marketimplied skew and smile which are relevant when computing xva metrics hence additional degrees of freedom are required to capture these market features in this paper we address this through an sde with statedependent coefficients the sde is consistent with the convex combination of a finite number of different ad dynamics we combine hullwhite onefactor models where one model parameter is varied we use the randomized ad rand technique to parameterize the combination of dynamics we refer to our sde with statedependent coefficients and the rand parametrization of the original models as the rhw model the rhw model allows for efficient semianalytic calibration to european swaptions through the analytic tractability of the hullwhite dynamics we use a regressionbased montecarlo simulation to calculate exposures in this setting we demonstrate the significant effect of skew and smile on exposures and xvas of linear and earlyexercise interest rate derivatives,0
in this paper we develop numerical pricing methodologies for european style exchange options written on a pair of correlated assets in a market with finite liquidity in contrast to the standard multiasset blackscholes framework trading in our market model has a direct impact on the assets price the price impact is incorporated into the dynamics of the first asset through a specific trading strategy as in large trader liquidity model twodimensional milstein scheme is implemented to simulate the pair of assets prices the option value is numerically estimated by monte carlo with the margrabe option as controlled variate time complexity of these numerical schemes are included finally we provide a deep learning framework to implement this model effectively in a production environment,0
this paper discusses a decentralized finance defi application called makerdao the maker protocol built on the ethereum blockchain enables users to create and hold currency current elements of the maker protocol are the dai stable coin maker vaults and voting makerdao governs the maker protocol by deciding on key parameters eg stability fees collateral types and rates etc through the voting power of maker mkr holders the maker protocol is one of the largest decentralized applications dapps on the ethereum blockchain and is the first decentralized finance defi application to earn significant adoption the objective of this paper is to analyze and discuss the significance uses and functions of this defi application,0
we investigate the potential of multiobjective deep reinforcement learning for stock and cryptocurrency singleasset trading in particular we consider a multiobjective algorithm which generalizes the reward functions and discount factor ie these components are not specified a priori but incorporated in the learning process firstly using several important assets cryptocurrency pairs btcusd ethusdt xrpusdt and stock indexes aapl spy nifty50 we verify the reward generalization property of the proposed multiobjective algorithm and provide preliminary statistical evidence showing increased predictive stability over the corresponding singleobjective strategy secondly we show that the multiobjective algorithm has a clear edge over the corresponding singleobjective strategy when the reward mechanism is sparse ie when nonnull feedback is infrequent over time finally we discuss the generalization properties with respect to the discount factor the entirety of our code is provided in open source format,0
this paper explores the effectiveness of highfrequency options trading strategies enhanced by advanced portfolio optimization techniques investigating their ability to consistently generate positive returns compared to traditional long or short positions on options utilizing spy options data recorded in fiveminute intervals over a onemonth period we calculate key metrics such as option greeks and implied volatility applying the binomial tree model for american options pricing and the newtonraphson algorithm for implied volatility calculation investment universes are constructed based on criteria like implied volatility and greeks followed by the application of various portfolio optimization models including standard meanvariance and robust methods our research finds that while basic longshort strategies centered on implied volatility and greeks generally underperform more sophisticated strategies incorporating advanced greeks such as vega and rho along with dynamic portfolio optimization show potential in effectively navigating the complexities of the options market the study highlights the importance of adaptability and responsiveness in dynamic portfolio strategies within the highfrequency trading environment particularly under volatile market conditions future research could refine strategy parameters and explore less frequently traded options offering new insights into highfrequency options trading and portfolio management,0
we develop a multifactor stochastic volatility libor model with displacement where each individual forward libor is driven by its own squareroot stochastic volatility process the main advantage of this approach is that maturitywise each squareroot process can be calibrated to the corresponding capletvolastrike panel at the market however since even after freezing the libors in the drift of this model the libor dynamics are not affine new affine approximations have to be developed in order to obtain fourier based approximate pricing procedures for caps and swaptions as a result we end up with a libor modeling package that allows for efficient calibration to a complete system of capswaption market quotes that performs well even in crises times where structural breaks in volastrikematurity panels are typically observed,0
in the ever evolving landscape of decentralized finance automated market makers amms play a key role they provide a market place for trading assets in a decentralized manner for socalled bluechip pairs arbitrage activity provides a major part of the revenue generation of amms but also a major source of loss due to the socalled informed orderflow finding ways to minimize those losses while still keeping uninformed trading activity alive is a major problem in the field in this paper we will investigate the mechanics of said arbitrage and try to understand how amms can maximize the revenue creation or in other words minimize the losses to that end we model the dynamics of arbitrage activity for a concrete implementation of a pool and study its sensitivity to the choice of fee aiming to maximize the revenue for the amm we identify dynamical fees that mimic the directionality of the price due to asymmetric fee choices as a promising avenue to mitigate losses to toxic flow this work is based on and extends a recent article by some of the authors,0
this dissertation develops and justifies a novel method for deriving approximate formulas to estimate two parameters in stochastic volatility diffusion models with exponentiallyaffine characteristic functions and single or twofactor variance these formulas aim to improve the accuracy of option pricing and enhance the calibration process by providing reliable initial values for local minimization algorithms the parameters relate to the volatility of the stochastic factor in instantaneous variance dynamics and the correlation between stochastic factors and asset price dynamics the study comprises five chapters chapter one outlines the currency option market pricing methods and the general structure of stochastic volatility models chapter two derives the replication strategy dynamics and introduces a new twofactor volatility model the ouou model chapter three analyzes the distribution and surface dynamics of implied volatilities using principal component and common factor analysis chapter four discusses calibration methods for stochastic volatility models particularly the heston model and presents the new implied central moments method to estimate parameters in the heston and schbelzhu models extensions to twofactor models bates and ouou are also explored chapter five evaluates the performance of the proposed formulas on the eurusd options market demonstrating the superior accuracy of the new method the dissertation successfully meets its research objectives expanding tools for derivative pricing and risk assessment key contributions include faster and more precise parameter estimation formulas and the introduction of the ouou model an extension of the schbelzhu model with a semianalytical valuation formula for european options previously unexamined in the literature,0
this paper studies the 28 time series of libor rates classified in seven maturities and four currencies during the last 14 years the analysis was performed using a novel technique in financial economics the complexityentropy causality plane this planar representation allows the discrimination of different stochastic and chaotic regimes using a temporal analysis based on moving windows this paper unveals an abnormal movement of libor time series arround the period of the 2007 financial crisis this alteration in the stochastic dynamics of libor is contemporary of what press called libor scandal ie the manipulation of interest rates carried out by several prime banks we argue that our methodology is suitable as a market watch mechanism as it makes visible the temporal redution in informational efficiency of the market,0
the new digital revolution of big data is deeply changing our capability of understanding society and forecasting the outcome of many social and economic systems unfortunately information can be very heterogeneous in the importance relevance and surprise it conveys affecting severely the predictive power of semantic and statistical methods here we show that the aggregation of web users behavior can be elicited to overcome this problem in a hard to predict complex system namely the financial market specifically our insample analysis shows that the combined use of sentiment analysis of news and browsing activity of users of yahoo finance greatly helps forecasting intraday and daily price changes of a set of 100 highly capitalized us stocks traded in the period 20122013 sentiment analysis or browsing activity when taken alone have very small or no predictive power conversely when considering a news signal where in a given time interval we compute the average sentiment of the clicked news weighted by the number of clicks we show that for nearly 50 of the companies such signal grangercauses hourly price returns our result indicates a wisdomofthecrowd effect that allows to exploit users activity to identify and weigh properly the relevant and surprising news enhancing considerably the forecasting power of the news sentiment,0
the santa fe model is an established econophysics model for describing stochastic dynamics of the limit order book from the viewpoint of the zerointelligence approach while its foundation was studied by combining a dimensional analysis and a meanfield theory by e smith et al in quantitative finance 2003 their arguments are rather heuristic and lack solid mathematical foundation indeed their meanfield equations were derived with heuristic arguments and their solutions were not explicitly obtained in this work we revisit the meanfield theory of the santa fe model from the viewpoint of kinetic theory a traditional mathematical program in statistical physics we study the exact master equation for the santa fe model and systematically derive the bogoliubovborngreenkirkwoodyvon bbgky hierarchical equation by applying the meanfield approximation we derive the meanfield equation for the orderbook density profile parallel to the boltzmann equation in conventional statistical physics furthermore we obtain explicit and closed expression of the meanfield solutions our solutions have several implications 1our scaling formulas are available for both to 0 and to infty asymptotics where  is the marketorder submission intensity particularly the meanfield theory works very well for small  while its validity is partially limited for large  2the method of image solution heuristically derived by bouchaudmzardpotters in quantitative finance 2002 is obtained for large  serving as a mathematical foundation for their heuristic arguments 3finally we point out an error in e smith et al 2003 in the scaling law for the diffusion constant due to a misspecification in their dimensional analysis,0
the background for the general mathematical link between utility and information theory investigated in this paper is a simple financial market model with two kinds of small traders less informed traders and insiders whose extra information is represented by an enlargement of the other agents filtration the expected logarithmic utility increment that is the difference of the insiders and the less informed traders expected logarithmic utility is described in terms of the information drift that is the drift one has to eliminate in order to perceive the price dynamics as a martingale from the insiders perspective on the one hand we describe the information drift in a very general setting by natural quantities expressing the probabilistic better informed view of the world this on the other hand allows us to identify the additional utility by entropy related quantities known from information theory in particular in a complete market in which the insider has some fixed additional information during the entire trading interval its utility increment can be represented by the shannon information of his extra knowledge for general markets and in some particular examples we provide estimates of maximal utility by information inequalities,0
predicting volatility is important for asset predicting option pricing and hedging strategies because it cannot be directly observed in the financial market the blackscholes option pricing model is one of the most widely used models by market participants notwithstanding the blackscholes model is based on heavily criticized theoretical premises one of which is the constant volatility assumption the dynamics of the volatility surface is difficult to estimate in this paper we establish a novel architecture based on physicsinformed neural networks and convolutional transformers the performance of the new architecture is directly compared to other wellknown deeplearning architectures such as standard physicsinformed neural networks convolutional longshort term memory convlstm and selfattention convlstm numerical evidence indicates that the proposed physicsinformed convolutional transformer network achieves a superior performance than other methods,0
most energy and commodity markets exhibit meanreversion and occasional distinctive price spikes which results in demand for derivative products which protect the holder against high prices to this end in this paper we present exact and fast methodologies for the simulation of the spot price dynamics modeled as the exponential of the sum of an ornsteinuhlenbeck and an independent pure jump process where the latter one is driven by a compound poisson process with bilateral exponentially distributed jumps these methodologies are finally applied to the pricing of asian options gas storages and swings under different combinations of jumpdiffusion market models and the apparent computational advantages of the proposed procedures are emphasized,0
coronavirus covid19 creates fear and uncertainty hitting the global economy and amplifying the financial markets volatility the oil price reaction to covid19 was gradually accommodated until march 09 2020 when 49 days after the release of the first coronavirus monitoring report by the world health organization who saudi arabia floods the market with oil as a result international prices drop with more than 20 in one single day against this background the purpose of this paper is to investigate the impact of covid19 numbers on crude oil prices while controlling for the impact of financial volatility and the united states us economic policy uncertainty our ardl estimation shows that the covid19 daily reported cases of new infections have a marginal negative impact on the crude oil prices in the long run nevertheless by amplifying the financial markets volatility covid19 also has an indirect effect on the recent dynamics of crude oil prices,0
the present article deals with intrahorizon risk in models with jumps our general understanding of intrahorizon risk is along the lines of the approach taken in boudoukh richardson stanton and whitelaw 2004 rossello 2008 bhattacharyya misra and kodase 2009 bakshi and panayotov 2010 and leippold and vasiljevi 2019 in particular we believe that quantifying market risk by strictly relying on pointintime measures cannot be deemed a satisfactory approach in general instead we argue that complementing this approach by studying measures of risk that capture the magnitude of losses potentially incurred at any time of a trading horizon is necessary when dealing with many financial positions to address this issue we propose an intrahorizon analogue of the expected shortfall for general profit and loss processes and discuss its key properties our intrahorizon expected shortfall is welldefined for many popular classes of lvy processes encountered when modeling market dynamics and constitutes a coherent measure of risk as introduced in cheridito delbaen and kupper 2004 on the computational side we provide a simple method to derive the intrahorizon risk inherent to popular lvy dynamics our general technique relies on results for maturityrandomized firstpassage probabilities and allows for a derivation of diffusion and single jump risk contributions these theoretical results are complemented with an empirical analysis where popular lvy dynamics are calibrated to sp 500 index data and an analysis of the resulting intrahorizon risk is presented,0
financial bond yield forecasting is challenging due to data scarcity nonlinear macroeconomic dependencies and evolving market conditions in this paper we propose a novel framework that leverages causal generative adversarial networks causalgans and soft actorcritic sac reinforcement learning rl to generate highfidelity synthetic bond yield data for four major bond categories aaa baa us10y junk by incorporating 12 key macroeconomic variables we ensure statistical fidelity by preserving essential market properties to transform this market dependent synthetic data into actionable insights we employ a finetuned large language model llm qwen257b that generates trading signals buyholdsell risk assessments and volatility projections we use automated human and llm evaluations all of which demonstrate that our framework improves forecasting performance over existing methods with statistical validation via predictive accuracy mae evaluation0103 profitloss evaluation 60 profit rate llm evaluation 3375 and expert assessments scoring 467 out of 5 the reinforcement learningenhanced synthetic data generation achieves the least mean absolute error of 0103 demonstrating its effectiveness in replicating realworld bond market dynamics we not only enhance datadriven trading strategies but also provides a scalable highfidelity synthetic financial data pipeline for risk volatility management and investment decisionmaking this work establishes a bridge between synthetic data generation llm driven financial forecasting and language model evaluation contributing to aidriven financial decisionmaking,0
for quantitative trading risk management purposes we present a novel idea the realized local volatility surface concisely it stands for the conditional expected volatility when sudden market behaviors of the underlying occur one is able to explore risk management usages by following the orthotical deltagamma dynamic hedging framework the realized local volatility surface is mathematically a generalized wiener measure from historical prices it is reconstructed via employing highfrequency trading market data a stickbreaking gaussian mixture model is fitted via hamiltonian monte carlo producing a local volatility surface with 95 credible intervals a practically validated bayesian nonparametric estimation workflow empirical results on tsla highfrequency data illustrate its ability to capture counterfactual volatility we also discuss its application in improving volatilitybased risk management,0
the rise of ethereum and other blockchains that support smart contracts has led to the creation of decentralized exchanges dexs such as uniswap balancer curve mstable and sushiswap which enable agents to trade cryptocurrencies without trusting a centralized authority while traditional exchanges use order books to match and execute trades dexs are typically organized as constant function market makers cfmms cfmms accept and reject proposed trades based on the evaluation of a function that depends on the proposed trade and the current reserves of the dex for trades that involve only two assets cfmms are easy to understand via two functions that give the quantity of one asset that must be tendered to receive a given quantity of the other and vice versa when more than two assets are being exchanged it is harder to understand the landscape of possible trades we observe that various problems of choosing a multiasset trade can be formulated as convex optimization problems and can therefore be reliably and efficiently solved,0
we study utility maximization problem for general utility functions using dynamic programming approach we consider an incomplete financial market model where the dynamics of asset prices are described by an rdvalued continuous semimartingale under some regularity assumptions we derive backward stochastic partial differential equation bspde related directly to the primal problem and show that the strategy is optimal if and only if the corresponding wealth process satisfies a certain forwardsde as examples the cases of power exponential and logarithmic utilities are considered,0
in this study we present models where participants strategically select their risk levels and earn corresponding rewards mirroring realworld competition across various sectors our analysis starts with a normal form game involving two players in a continuous action space confirming the existence and uniqueness of a nash equilibrium and providing an analytical solution we then extend this analysis to multiplayer scenarios introducing a new numerical algorithm for its calculation a key novelty of our work lies in using regret minimization algorithms to solve continuous games through discretization this groundbreaking approach enables us to incorporate additional realworld factors like market frictions and risk correlations among firms we also experimentally validate that the nash equilibrium in our model also serves as a correlated equilibrium our findings illuminate how market frictions and risk correlations affect strategic risktaking we also explore how policy measures can impact risktaking and its associated rewards with our model providing broader applicability than the diamonddybvig framework we make our methodology and opensource code available at finally we contribute methodologically by advocating the use of algorithms in economics shifting focus from finite games to games with continuous action sets our study provides a solid framework for analyzing strategic interactions in continuous action games emphasizing the importance of market frictions risk correlations and policy measures in strategic risktaking dynamics,0
this research paper introduces innovative approaches for multivariate time series forecasting based on different variations of the combined regression strategy we use specific data preprocessing techniques which makes a radical change in the behaviour of prediction we compare the performance of the model based on two types of hyperparameter tuning bayesian optimisation bo and usual grid search our proposed methodologies outperform all stateoftheart comparative models we illustrate the methodologies through eight time series datasets from three categories cryptocurrency stock index and shortterm load forecasting,0
we investigate optimal order execution problems in discrete time with instantaneous price impact and stochastic resilience first in the setting of linear transient price impact we derive a closedform recursion for the optimal strategy extending the deterministic results from obizhaeva and wang j financial markets 2013 second we develop a numerical algorithm based on dynamic programming and deep learning for the case of nonlinear transient price impact as proposed by bouchaud et al quant finance 2004 specifically we utilize an actorcritic framework that constructs two neuralnetwork nn surrogates for the value function and the feedback control the flexible scalability of nn functional approximators enables parametric learning ie incorporating several model or market parameters as part of the input space precise calibration of price impact resilience etc is known to be extremely challenging and hence it is critical to understand sensitivity of the execution policy to these parameters our nn learner organically scales across multiple input dimensions and is shown to accurately approximate optimal strategies across a wide range of parameter configurations we provide a fully reproducible jupyter notebook with our nn implementation which is of independent pedagogical interest demonstrating the ease of use of nn surrogates in parametric stochastic control problems,0
the proper design and architecture of testing machine learning models especially in their application to quantitative finance problems is crucial the most important aspect of this process is selecting an adequate loss function for training validation estimation purposes and hyperparameter tuning therefore in this research through empirical experiments on equity and cryptocurrency assets we apply the mean absolute directional loss madl function which is more adequate for optimizing forecastgenerating models used in algorithmic investment strategies the madl function results are compared between transformer and lstm models and we show that in almost every case transformer results are significantly better than those obtained with lstm,0
the lack of proper interoperability poses a significant challenge in leveraging use cases within the blockchain industry unlike typical solutions that rely on third parties such as oracles and witnesses the interpool design operates as a standalone solution that mints exchanges and burns meb within the same liquidity pool this meb approach ensures that minting is backed by the locked capital supplied by liquidity providers during the exchange process the order of transactions in the mempool is optimized to maximize returns effectively transforming the frontrunning issue into a solution that forges an external blockchain hash this forged hash enables a novel protocol listrack listen and track which ensures that ultimate liquidity is always enforced through a solid burning procedure strengthening a trustless design supported by listrack atomic swaps become feasible even outside the interpool thereby enhancing the current design into a comprehensive interoperability solution,0
stock trading strategies play a critical role in investment however it is challenging to design a profitable strategy in a complex and dynamic stock market in this paper we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return we train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actorcritic based algorithms proximal policy optimization ppo advantage actor critic a2c and deep deterministic policy gradient ddpg the ensemble strategy inherits and integrates the best features of the three algorithms thereby robustly adjusting to different market situations in order to avoid the large memory consumption in training networks with continuous action space we employ a loadondemand technique for processing very large data we test our algorithms on the 30 dow jones stocks that have adequate liquidity the performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the dow jones industrial average index and the traditional minvariance portfolio allocation strategy the proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the riskadjusted return measured by the sharpe ratio this work is fully opensourced at href,0
the paper explores the use of deep reinforcement learning drl in stock market trading focusing on two algorithms double deep qnetwork ddqn and proximal policy optimization ppo and compares them with buy and hold benchmark it evaluates these algorithms across three currency pairs the sp 500 index and bitcoin on the daily data in the period of 20192023 the results demonstrate drls effectiveness in trading and its ability to manage risk by strategically avoiding trades in unfavorable conditions providing a substantial edge over classical approaches based on supervised learning in terms of riskadjusted returns,0
we introduce a microscopic model of interacting financial agents where each agent is characterized by two portfolios money invested in bonds and money invested in stocks furthermore each agent is faced with an optimization problem in order to determine the optimal asset allocation the stock price evolution is driven by the aggregated investment decision of all agents in fact we are faced with a differential game since all agents aim to invest optimal mathematically such a problem is ill posed and we introduce the concept of nash equilibrium solutions to ensure the existence of a solution especially we denote an agent who solves this nash equilibrium exactly a rational agent as next step we use model predictive control to approximate the control problem this enables us to derive a precise mathematical characterization of the degree of rationality of a financial agent this is a novel concept in portfolio optimization and can be regarded as a general approach in a second step we consider the case of a fully myopic agent where we can solve the optimal investment decision of investors analytically we select the running cost to be the expected missed revenue of an agent and we assume quadratic transaction costs more precisely the expected revenues are determined by a combination of a fundamentalist or chartist strategy then we derive the mean field limit of the microscopic model in order to obtain a macroscopic portfolio model the novelty in comparison to existent macroeconomic models in literature is that our model is derived from microeconomic dynamics the resulting portfolio model is a three dimensional ode system which enables us to derive analytical results simulations reveal that our model is able to replicate the most prominent features of financial markets namely booms and crashes,0
in the rapidly evolving field of financial forecasting the application of neural networks presents a compelling advancement over traditional statistical models this research paper explores the effectiveness of two specific neural forecasting models nhits and nbeats in predicting financial market trends through a systematic comparison with conventional models this study demonstrates the superior predictive capabilities of neural approaches particularly in handling the nonlinear dynamics and complex patterns inherent in financial time series data the results indicate that nhits and nbeats not only enhance the accuracy of forecasts but also boost the robustness and adaptability of financial predictions offering substantial advantages in environments that require realtime decisionmaking the paper concludes with insights into the practical implications of neural forecasting in financial markets and recommendations for future research directions,0
modeling counterparty risk is computationally challenging because it requires the simultaneous evaluation of all the trades with each counterparty under both market and credit risk we present a multigaussian process regression approach which is well suited for otc derivative portfolio valuation involved in cva computation our approach avoids nested simulation or simulation and regression of cash flows by learning a gaussian metamodel for the marktomarket cube of a derivative portfolio we model the joint posterior of the derivatives as a gaussian process over function space with the spatial covariance structure imposed on the risk factors montecarlo simulation is then used to simulate the dynamics of the risk factors the uncertainty in portfolio valuation arising from the gaussian process approximation is quantified numerically numerical experiments demonstrate the accuracy and convergence properties of our approach for cva computations including a counterparty portfolio of interest rate swaps,0
this study enhances option pricing by presenting unique pricing model fractional order blackscholesmerton fobsm which is based on the blackscholesmerton bsm model the main goal is to improve the precision and authenticity of option pricing matching them more closely with the financial landscape the approach integrates the strengths of both the bsm and neural network nn with complex diffusion dynamics this study emphasizes the need to take fractional derivatives into account when analyzing financial market dynamics since fobsm captures memory characteristics in sequential data it is better at simulating realworld systems than integerorder models findings reveals that in complex diffusion dynamics this hybridization approach in option pricing improves the accuracy of price predictions the key contribution of this work lies in the development of a novel option pricing model fobsm that leverages fractional calculus and neural networks to enhance accuracy in capturing complex diffusion dynamics and memory effects in financial data,0
this paper proposes a model that enables permissionless and decentralized networks for complex computations we explore the integration and optimize load balancing in an open decentralized computational network our model leverages economic incentives and reputationbased mechanisms to dynamically allocate tasks between operators and coprocessors this approach eliminates the need for specialized hardware or software thereby reducing operational costs and complexities we present a mathematical model that enhances restaking processes in blockchain systems by enabling operators to delegate complex tasks to coprocessors the models effectiveness is demonstrated through experimental simulations showcasing its ability to optimize reward distribution enhance security and improve operational efficiency our approach facilitates a more flexible and scalable network through the use of economic commitments adaptable dynamic rating models and a coprocessor load incentivization system supported by experimental simulations the model demonstrates its capability to optimize resource allocation enhance system resilience and reduce operational risks this ensures significant improvements in both security and costefficiency for the blockchain ecosystem,0
we examine strategically incorporating broad stock market leveraged exchangetraded funds letfs into investment portfolios we demonstrate that easily understandable and implementable strategies can enhance the riskreturn profile of a portfolio containing letfs our analysis shows that seemingly reasonable investment strategies may result in undesirable omega ratios with these effects compounding across rebalancing periods by contrast relatively simple dynamic strategies that systematically derisk the portfolio once gains are observed can exploit this compounding effect taking advantage of favorable omega ratio dynamics our findings suggest that letfs represent a valuable tool for investors employing dynamic strategies while confirming their welldocumented unsuitability for passive or static approaches,0
this paper deals with numerical solutions to an impulse control problem arising from optimal portfolio liquidation with bidask spread and market price impact penalizing speedy execution trades the corresponding dynamic programming dp equation is a quasivariational inequality qvi with solvency constraint satisfied by the value function in the sense of constrained viscosity solutions by taking advantage of the lag variable tracking the time interval between trades we can provide an explicit backward numerical scheme for the time discretization of the dpqvi the convergence of this discretetime scheme is shown by viscosity solutions arguments an optimal quantization method is used for computing the conditional expectations arising in this scheme numerical results are presented by examining the behaviour of optimal liquidation strategies and comparative performance analysis with respect to some benchmark execution strategies we also illustrate our optimal liquidation algorithm on real data and observe various interesting patterns of order execution strategies finally we provide some numerical tests of sensitivity with respect to the bidask spread and market impact parameters,0
this study integrates realtime sentiment analysis from financial news gpt2 and finbert with technical indicators and timeseries models like arima and ets to optimize sp 500 trading strategies by merging sentiment data with momentum and trendbased metrics including a benchmark buyandhold and sentimentbased approach is evaluated through assets values and returns results show that combining sentimentdriven insights with traditional models improves trading performance offering a more dynamic approach to stock trading that adapts to market changes in volatile environments,0
technological advancement drives financial innovation reshaping the traditional finance landscape and redefining usermarket interactions the rise of blockchain and decentralized finance defi underscores this intertwined evolution of technology and finance while defi has introduced exciting opportunities it has also exposed the ecosystem to new forms of market misconduct this paper aims to bridge the academic and regulatory gaps by addressing key research questions about market misconduct in defi we begin by discussing how blockchain technology can potentially enable the emergence of novel forms of market misconduct we then offer a comprehensive definition and taxonomy for understanding defi market misconduct through comparative analysis and empirical measurements we examine the novel forms of misconduct in defi shedding light on their characteristics and social impact subsequently we investigate the challenges of building a tailored regulatory framework for defi we identify key areas where existing regulatory frameworks may need enhancement finally we discuss potential approaches that bring defi into the regulatory perimeter,0
we consider the problem of neural network training in a timevarying context machine learning algorithms have excelled in problems that do not change over time however problems encountered in financial markets are often timevarying we propose the online early stopping algorithm and show that a neural network trained using this algorithm can track a function changing with unknown dynamics we compare the proposed algorithm to current approaches on predicting monthly us stock returns and show its superiority we also show that prominent factors such as the size and momentum effects and industry indicators exhibit time varying stock return predictiveness we find that during market distress industry indicators experience an increase in importance at the expense of firm level features this indicates that industries play a role in explaining stock returns during periods of heightened risk,0
in this paper we develop a tractable structural model with analytical default probabilities depending on some dynamics parameters and we show how to calibrate the model using a chosen number of credit default swap cds market quotes we essentially show how to use structural models with a calibration capability that is typical of the much more tractable creditspread based intensity models we apply the structural model to a concrete calibration case and observe what happens to the calibrated dynamics when the cdsimplied credit quality deteriorates as the firm approaches default finally we provide a typical example of a case where the calibrated structural model can be used for credit pricing in a much more convenient way than a calibrated reduced form model the pricing of counterparty risk in an equity swap,0
we consider asset price models whose dynamics are described by linear functions of the time extended signature of a primary underlying process which can range from a marketinferred brownian motion to a general multidimensional continuous semimartingale the framework is universal in the sense that classical models can be approximated arbitrarily well and that the models parameters can be learned from all sources of available data by simple methods we provide conditions guaranteeing absence of arbitrage as well as tractable option pricing formulas for socalled sigpayoffs exploiting the polynomial nature of generic primary processes one of our main focus lies on calibration where we consider both timeseries and implied volatility surface data generated from classical stochastic volatility models and also from sp500 index market data for both tasks the linearity of the model turns out to be the crucial tractability feature which allows to get fast and accurate calibrations results,0
this paper presents a financial analysis over twitter sentiment analytics extracted from listed retail brands we investigate whether there is statisticallysignificant information between the twitter sentiment and volume and stock returns and volatility traditional newswires are also considered as a proxy for the market sentiment for comparative purpose the results suggest that social media is indeed a valuable source in the analysis of the financial dynamics in the retail sector even when compared to mainstream news such as the wall street journal and dow jones newswires,0
this paper extends the existing drawdown modulation control policy to include a novel restart mechanism for trading it is known that the drawdown modulation policy guarantees the maximum percentage drawdown no larger than a prespecified drawdown limit for all time with probability one however when the prespecified limit is approaching in practice such a modulation policy becomes a stoploss order which may miss the profitable followup opportunities if any motivated by this we add a datadriven restart mechanism into the drawdown modulation trading system to autotune the performance we find that with the restart mechanism our policy may achieve a superior trading performance to that without the restart even with a nonzero transaction costs setting to support our findings some empirical studies using equity etf and cryptocurrency with historical price data are provided,0
we model the dynamics of asset prices and associated derivatives by consideration of the dynamics of the conditional probability density process for the value of an asset at some specified time in the future in the case where the price process is driven by brownian motion an associated master equation for the dynamics of the conditional probability density is derived and expressed in integral form by a model for the conditional density process we mean a solution to the master equation along with the specification of a the initial density and b the volatility structure of the density the volatility structure is assumed at any time and for each value of the argument of the density to be a functional of the history of the density up to that time in practice one specifies the functional modulo sufficient parametric freedom to allow for the input of additional option data apart from that implicit in the initial density the scheme is sufficiently flexible to allow for the input of various types of data depending on the nature of the options market and the class of valuation problem being undertaken various examples are studied in detail with exact solutions provided in some cases,0
probabilistic forecasting of intraday electricity prices is essential to manage market uncertainties however current methods rely heavily on domain feature extraction which breaks the endtoend training pipeline and limits the models ability to learn expressive representations from the raw orderbook moreover these methods often require training separate models for different quantiles further violating the endtoend principle and introducing the quantile crossing issue recent advances in timeseries models have demonstrated promising performance in general forecasting tasks however these models lack inductive biases arising from buysell interactions and are thus overparameterized to address these challenges we propose an endtoend probabilistic model called orderfusion which produces interactionaware representations of buysell dynamics hierarchically estimates multiple quantiles and remains parameterefficient with only 4872 parameters we conduct extensive experiments and ablation studies on price indices id1 id2 and id3 using three years of orderbook in highliquidity german and lowliquidity austrian markets the experimental results demonstrate that orderfusion consistently outperforms multiple competitive baselines across markets and ablation studies highlight the contribution of its individual components the project page is at,0
the aim of this paper is to explain how parameters adjustments can be integrated in the design or the control of automates of trading typically we are interested by the online estimation of the market impacts generated by robots or single orders and how theythe controller should react in an optimal way to the informations generated by the observation of the realized impacts this can be formulated as an optimal impulse control problem with unknown parameters on which a prior is given we explain how a mix of the classical bayesian updating rule and of optimal control techniques allows one to derive the dynamic programming equation satisfied by the corresponding value function from which the optimal policy can be inferred we provide an example of convergent finite difference scheme and consider typical examples of applications,0
this paper examines systematic putwriting strategies applied to sp 500 index options with a focus on position sizing as a key determinant of longterm performance despite the welldocumented volatility risk premium where implied volatility exceeds realized volatility the practical implementation of shortdated volatilityselling strategies remains underdeveloped in the literature this study evaluates three position sizing approaches the kelly criterion vixbased volatility regime scaling and a novel hybrid method combining both using spxw options with expirations from 0 to 5 days the analysis explores a broad design space including moneyness levels volatility estimators and memory horizons results show that ultrashortdated far outofthemoney options deliver superior riskadjusted returns the hybrid sizing method consistently balances return generation with robust drawdown control particularly under lowvolatility conditions such as those seen in 2024 the study offers new insights into volatility harvesting introducing a dynamic sizing framework that adapts to shifting market regimes it also contributes practical guidance for constructing shortdated option strategies that are robust across market environments these findings have direct applications for institutional investors seeking to enhance portfolio efficiency through systematic exposure to volatility premia,0
we consider a collection of derivatives that depend on the price of an underlying asset at expiration or maturity the absence of arbitrage is equivalent to the existence of a riskneutral probability distribution on the price in particular any risk neutral distribution can be interpreted as a certificate establishing that no arbitrage exists we are interested in the case when there are multiple riskneutral probabilities we describe a number of convex optimization problems over the convex set of risk neutral price probabilities these include computation of bounds on the cumulative distribution var cvar and other quantities over the set of riskneutral probabilities after discretizing the underlying price these problems become finite dimensional convex or quasiconvex optimization problems and therefore are tractable we illustrate our approach using real options and futures pricing data for the sp 500 index and bitcoin,0
this study develops a financial word embedding using 15 years of business news our results show that this specialised language model produces more accurate results than general word embeddings based on a financial benchmark we established as an application we incorporate this word embedding into a simple machine learning model to enhance the har model for forecasting realised volatility this approach statistically and economically outperforms established econometric models using an explainable ai method we also identify key phrases in business news that contribute significantly to volatility offering insights into language patterns tied to market dynamics,0
this article studies families of curves with monotonic curvature function mccurves and their applications in geometric modelling and aesthetic design aesthetic analysis and assessment of the structure and plastic qualities of pseudospirals which are curves with monotonic curvature function are conducted for the first time in the field of geometric modelling from the position of technical aesthetics laws the example of car body surface modelling with the use of aesthetics splines is given,0
we present some notes on the definition of mathematical design as well as on the methods of mathematical modeling which are used in the process of the artistic design of the environment and its components for the first time in the field of geometric modeling we perform an aesthetic analysis of planar bernsteinbezier curves from the standpoint of the laws of technical aesthetics the shape features of the curve segments geometry were evaluated using the following criteria concisenessintegrity expressiveness proportional consistency compositional balance structural organization imagery rationality dynamism scale flexibility and harmony in the nonrussian literature bernsteinbezier curves using a monotonic curvature function ie a class a bezier curve are considered to be fair ie beautiful curves but their aesthetic analysis has never been performed the aesthetic analysis performed by the authors of this work means that this is no longer the case to confirm the conclusions of the authors research a survey of the aesthetic appropriateness of certain bernsteinbezier curve segments was conducted among 240 children aged 1417 the results of this survey have shown themselves to be in full accordance with the authors results,0
airborne particles are the medium for sarscov2 to invade the human body light also reflects through suspended particles in the air allowing people to see a colorful world impressionism is the most prominent art school that explores the spectrum of color created through color reflection of light we find similarities of color structure and color stacking in the impressionist paintings and the illustrations of the novel coronavirus by artists around the world with computerized data analysis through the main tones the way of color layout and the way of color stacking in the paintings of the impressionists we train computers to draw the novel coronavirus in an impressionist style using a generative adversarial network to create our artwork medium permeation this artwork is composed of 196 randomly generated viral pictures arranged in a 14 by 14 matrix to form a largescale painting in addition we have developed an extended work gradual change which is presented as video art we use graph neural network to present 196 paintings of the new coronavirus to the audience one by one in a gradual manner in front of led tv screen audience will find 196 virus paintings whose colors will change continuously this large video painting symbolizes that worldwide 196 countries have been invaded by the epidemic and every nation continuously pops up mutant viruses the speed of vaccine development cannot keep up with the speed of virus mutation this is also the first generative art in the world based on the common features and a metaphorical symbiosis between impressionist art and the novel coronavirus this work warns us of the unprecedented challenges posed by the sarscov2 implying that the world should not ignore the invisible enemy who uses air as a medium,0
generative art merges creativity with computation using algorithms to produce aesthetic works this paper introduces samila a pythonbased generative art library that employs mathematical functions and randomness to create visually compelling compositions the system allows users to control the generation process through random seeds function selections and projection modes enabling the exploration of randomness and artistic expression by adjusting these parameters artists can create diverse compositions that reflect intentionality and unpredictability we demonstrate that samilas outputs are uniquely determined by two random generation seeds making regeneration nearly impossible without both additionally altering the point generation functions while preserving the seed produces artworks with distinct graphical characteristics forming a visual family samila serves as both a creative tool for artists and an educational resource for teaching mathematical and programming concepts it also provides a platform for research in generative design and computational aesthetics future developments could include aidriven generation and aesthetic evaluation metrics to enhance creative control and accessibility,0
accurate evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research prior work has tended to focus on feature measures of the artefact such as symmetry complexity and coherence however research models from psychology suggest that human aesthetic experiences encapsulate factors beyond the artefact making accurate computational models very difficult to design the interactive genetic algorithm iga circumvents the problem through humanintheloop subjective evaluation of aesthetics but is limited due to user fatigue and small population sizes in this paper we look at how recent advances in deep learning can assist in automating personal aesthetic judgement using a leading artists computer art dataset we investigate the relationship between image measures such as complexity and human aesthetic evaluation we use dimension reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in a generative system convolutional neural networks trained on the artists prior aesthetic evaluations are used to suggest new possibilities similar or between known high quality genotypephenotype mappings we integrate this classification and discovery system into a software tool for evolving complex generative art and design,0
this paper explores visual indeterminacy as a description for artwork created with generative adversarial networks gans visual indeterminacy describes images which appear to depict real scenes but on closer examination defy coherent spatial interpretation gan models seem to be predisposed to producing indeterminate images and indeterminacy is a key feature of much modern representational art as well as most gan art it is hypothesized that indeterminacy is a consequence of a powerfulbutimperfect image synthesis model that must combine general classes of objects scenes and textures,0
while the problem of image aesthetics has been well explored the study of 3d shape aesthetics has focused on specific manually defined features in this paper we learn an aesthetics measure for 3d shapes autonomously from raw voxel data and without manuallycrafted features by leveraging the strength of deep learning we collect data from humans on their aesthetics preferences for various 3d shape classes we take a deep convolutional 3d shape ranking approach to compute a measure that gives an aesthetics score for a 3d shape we demonstrate our approach with various types of shapes and for applications such as aestheticsbased visualization search and scene composition,0
in this paper we extend welldesigned 3d face masks into ar face masks and demonstrate the possibility of transforming this into an nftcopyrighted ar face mask that helps authenticate the ownership of the ar mask user so as to improve creative control brand identification and id protection the output of this project will not only potentially validate the value of the nft technology but also explore how to combine the nft technology with ar technology so as to be applied to ecommerce and ebusiness aspects of the multimedia industry,0
achieving aesthetically pleasing photography necessitates attention to multiple factors including composition and capture conditions which pose challenges to novices prior research has explored the enhancement of photo aesthetics postcapture through 2d manipulation techniques however these approaches offer limited search space for aesthetics we introduce a pioneering method that employs 3d operations to simulate the conditions at the moment of capture retrospectively our approach extrapolates the input image and then reconstructs the 3d scene from the extrapolated image followed by an optimization to identify camera parameters and image aspect ratios that yield the best 3d view with enhanced aesthetics comparative qualitative and quantitative assessments reveal that our method surpasses traditional 2d editing techniques with superior aesthetics,0
participatory art allows for the spectator to be a participant or a viewer able to engage actively with interactive art realtime technologies offer new ways to create participative artworks we hereby investigate how to engage participation through movement in interactive digital art and what this engagement can awaken focusing on the ways to elicit improvisation and letting go we analyze two virtual reality installations interacte and eve dance is an unplaceable place involving body movement dance creativity and the presence of an observing audience we evaluate the premises the setup and the feedback of the spectators in the two installations we propose a model following three different perspectives of resonance 1 inter resonance between spectator and artwork which involves curiosity imitation playfulness and improvisation 2 inner resonance of spectator himherself where embodiment and creativity contribute to the sense of being present and letting go 3 collective resonance between spectatorartwork and audience which is stimulated by curiosity and triggers motor contagion engagement and gathering the two analyzed examples seek to awaken openminded communicative possibilities through the use of interactive digital artworks moreover the need to recognize and develop the idea of resonance becomes increasingly important in this time of urgency to communicate understand and support collectivity,0
we explore computational approaches for visual guidance to aid in creating aesthetically pleasing art and graphic design our work complements and builds on previous work that developed models for how humans look at images our approach comprises three steps first we collected a dataset of art masterpieces and labeled the visual fixations with stateofart vision models second we clustered the visual guidance templates of the art masterpieces with unsupervised learning third we developed a pipeline using generative adversarial networks to learn the principles of visual guidance and that can produce aesthetically pleasing layouts we show that the aesthetic visual guidance principles can be learned and integrated into a highdimensional model and can be queried by the features of graphic elements we evaluate our approach by generating layouts on various drawings and graphic designs moreover our model considers the color and structure of graphic elements when generating layouts consequently we believe our tool which generates multiple aesthetic layout options in seconds can help artists create beautiful art and graphic designs,0
in this work we present digital life project a framework utilizing language as the universal medium to build autonomous 3d characters who are capable of engaging in social interactions and expressing with articulated body motions thereby simulating life in a digital environment our framework comprises two primary components 1 sociomind a meticulously crafted digital brain that models personalities with systematic fewshot exemplars incorporates a reflection process based on psychology principles and emulates autonomy by initiating dialogue topics 2 momatmogen a textdriven motion synthesis paradigm for controlling the characters digital body it integrates motion matching a proven industry technique to ensure motion quality with cuttingedge advancements in motion generation for diversity extensive experiments demonstrate that each module achieves stateoftheart performance in its respective domain collectively they enable virtual characters to initiate and sustain dialogues autonomously while evolving their sociopsychological states concurrently these characters can perform contextually relevant bodily movements additionally a motion captioning module further allows the virtual character to recognize and appropriately respond to human players actions homepage,0
human aesthetic preferences for 3d shapes are central to industrial design virtual reality and consumer product development however most computational models of 3d aesthetics lack empirical grounding in largescale human judgments limiting their practical relevance we present a largescale study of human preferences we collected 22301 pairwise comparisons across five object categories chairs tables mugs lamps and dining chairs via amazon mechanical turk building on a previously published datasetcitedev2020learning we introduce new nonlinear modeling and crosscategory analysis to uncover the geometric drivers of aesthetic preference we apply the bradleyterry model to infer latent aesthetic scores and use random forests with shap analysis to identify and interpret the most influential geometric features eg symmetry curvature compactness our crosscategory analysis reveals both universal principles and domainspecific trends in aesthetic preferences we focus on human interpretable geometric features to ensure model transparency and actionable design insights rather than relying on blackbox deep learning approaches our findings bridge computational aesthetics and cognitive science providing practical guidance for designers and a publicly available dataset to support reproducibility this work advances the understanding of 3d shape aesthetics through a humancentric datadriven framework,0
we study creating and analyzing symmetry and broken symmetry in digital art our focus is not so much on computergenerating artistic images but rather on analyzing concepts and templates for incorporating symmetry and symmetry breaking into the creation process taking as a starting point patterns generated algorithmically by emulating the collective feeding behavior of sandbubbler crabs all four types of twodimensional symmetry are used as isometric maps apart from a geometric interpretation of symmetry we also consider color as an object of symmetric transformations color symmetry is realized as a color permutation consistent with the isometries moreover we analyze the abilities of computational aesthetic measures to serve as a metric that reflects design parameters ie the type of symmetry and the degree of symmetry breaking,0
we developed and validated a rating scale to assess the aesthetic pleasure or beauty of a visual data representation the beauvis scale with our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations unrelated to data or context of use our rating scale can for example be used to accompany results from controlled experiments or be used as informative data points during indepth qualitative studies given the lack of an aesthetic pleasure scale dedicated to visualizations researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations yet many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists to solve this problem we engaged in a multistep research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization osfiofxs76 our final beauvis scale consists of five items enjoyable likable pleasing nice and appealing beyond this scale itself we contribute a a systematic review of the terms used in past research to capture aesthetics b an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization and c a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations,0
this study investigates how artificial intelligence ai recognizes style through style transferan ai technique that generates a new image by applying the style of one image to another despite the considerable interest that style transfer has garnered among researchers most efforts have focused on enhancing the quality of output images through advanced ai algorithms in this paper we approach style transfer from an aesthetic perspective thereby bridging ai techniques and aesthetics we analyze two style transfer algorithms one based on convolutional neural networks cnns and the other utilizing recent transformer models by comparing the images produced by each we explore the elements that constitute the style of artworks through an aesthetic analysis of the style transfer results we then elucidate the limitations of current style transfer techniques based on these limitations we propose potential directions for future research on style transfer techniques,0
digital storytelling as an art form has struggled with costquality balance the emergence of aigenerated content aigc is considered as a potential solution for efficient digital storytelling production however the specific form effects and impacts of this fusion remain unclear leaving the boundaries of aigc combined with storytelling undefined this work explores the current integration state of aigc and digital storytelling investigates the artistic value of their fusion in a sample project and addresses common issues through interviews through our study we conclude that aigc while proficient in image creation voiceover production and music composition falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present especially in complex character animations facial expressions and sound effects the research objective is to increase public awareness of the current state limitations and challenges arising from combining aigc and digital storytelling,0
due to its unique computing principles quantum computing technology will profoundly change the spectacle of color art focusing on experimental exploration of color qubit representation color channel processing and color image generation via quantum computing this article proposes a new technical path for color computing in quantum computing environment by which digital color is represented operated and measured in quantum bits and then restored for classical computers as computing results this method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in qiskit and ibm q by building a bridge between classical chromatics and quantum graphics quantum computers can be used for information visualization image processing and more color computing tasks furthermore quantum computing can be expected to facilitate new color theories and artistic concepts,0
in parallel with the dissemination of information technology we note the persistence of frontiers within creative practices in particular between the digital arts and the performing arts crossings of these frontiers brought to light the need for a common appropriation of digital issues as a result of this appropriation the avatarstaging platform and its software dimension aknregie will be described in their use to direct avatars on a mixed theatre stage developed with the blueprint visual language within epic games unreal engine aknregie offers a user interface accessible to nonprogramming artists this feature will be used to describe two perspectives of appropriation of the tool the plugin perspective for these users and the blueprint perspective for programming artists who want to improve the tool these two perspectives are then completed by a c perspective that aligns aknregie with the language with which the engine itself is programmed the circulations between these three perspectives are finally studied by drawing on work on the ecology of collective intelligence,0
this paper presents a case study of the thematic pavilion null2 at expo 2025 osakakansai contrasting with the static jomon motifs of taro okamotos tower of the sun from expo 1970 the study discusses yayoiinspired mirror motifs and dynamically transforming interactive spatial configuration of null2 where visitors become integrated as experiential content the shift from static representation to a new ontological and aesthetic model characterized by the visitors body merging in realtime with architectural space at installation scale is analyzed referencing the philosophical context of expo 1970 theme progress and harmony for mankind this research reconsiders the worldview articulated by null2 in expo 2025 in which computation is naturalized and ubiquitous through its intersection with eastern philosophical traditions it investigates how immersive experiences within the pavilion grounded in the philosophical framework of digital nature reinterpret traditional spatial and structural motifs of the tea room positioning them within contemporary digital art discourse the aim is to contextualize and document null2 as an important contemporary case study from expo practices considering the historical and social background in japan from the 19th to 21st century during which world expositions served as pivotal points for the birth of modern japanese concept of fine art symbolic milestones of economic development and key moments in urban and media culture formation furthermore this paper academically organizes architectural techniques computer graphics methodologies media art practices and theoretical backgrounds utilized in null2 highlighting the scholarly significance of preserving these as an archival document for future generations,0
as per the objectives of project changes particularly its thematic subproject on the use of virtual technologies for museums and art collections our goal was to obtain a digital twin of the temporary exhibition on ulisse aldrovandi called the other renaissance and make it accessible to users online after a preliminary study of the exhibition focussing on acquisition constraints and related solutions we proceeded with the digital twin creation by acquiring processing modelling optimising exporting and metadating the exhibition we made hybrid use of two acquisition techniques to create new digital cultural heritage objects and environments and we used open technologies formats and protocols to make available the final digital product here we describe the process of collecting and curating bibliographical exhibition metadata and the beginning of the digital twin creation to foster its findability accessibility interoperability and reusability the creation of the digital twin is currently ongoing,0
iterating on creating pixel art character sprite sheets is essential to the game development process however it can take a lot of effort until the final versions containing different poses and animation clips are achieved this paper investigates using conditional generative adversarial networks to aid the designers in creating such sprite sheets we propose an architecture based on pix2pix to generate images of characters facing a target side eg right given sprites of them in a source pose eg front experiments with small pixel art datasets yielded promising results resulting in models with varying degrees of generalization sometimes capable of generating images very close to the ground truth we analyze the results through visual inspection and quantitatively with fid,0
the logaesthetic curves include the logarithmic equiangular spiral clothoid and involute curves although most of them are expressed only by an integral form of the tangent vector it is possible to interactively generate and deform them and they are expected to be utilized for practical use of industrial and graphical design the discrete logaesthetic filter based on the formulation of the logaesthetic curve has successfully been introduced not to impose strong constraints on the designers activity to let himher design freely and to embed the properties of the logaesthetic curves for complicated ones with both increasing and decreasing curvature in this paper in order to define the logaesthetic surface and develop surface filters based on its formulation at first we reformulate the logaesthetic curve with variational principle then we propose several new functionals to be minimized for freeform surfaces and define the logaesthetic surface furthermore we propose new discrete surface filters based on the logaesthetic surface formulation,0
in the field of aesthetic design logaesthetic curves have a significant role to meet the high industrial requirements in this paper we propose a new interactive g1 hermite interpolation method based on the algorithm of yoshida et al with a minor boundary condition in this novel approach we compute an extended logaesthetic curve segment that may include inflection point sshaped curve or cusp the curve segment is defined by its endpoints a tangent vector at the first point and a tangent direction at the second point the algorithm also determines the shape parameter of the logaesthetic curve based on the length of the first tangent that provides control over the curvature of the first point and makes the method capable of joining logaesthetic curve segments with g2 continuity,0
this paper introduces lav latent audiovisual a system that integrates encodecs neural audio compression with stylegan2s generative capabilities to produce visually dynamic outputs driven by prerecorded audio unlike previous works that rely on explicit feature mappings lav uses encodec embeddings as latent representations directly transformed into stylegan2s style latent space via randomly initialized linear mapping this approach preserves semantic richness in the transformation enabling nuanced and semantically coherent audiovisual translations the framework demonstrates the potential of using pretrained audio compression models for artistic and computational applications,0
generating structured ascii art using computational techniques demands a careful interplay between aesthetic representation and computational precision requiring models that can effectively translate visual information into symbolic text characters although convolutional neural networks cnns have shown promise in this domain the comparative performance of deep learning architectures and classical machine learning methods remains unexplored this paper explores the application of contemporary ml and dl methods to generate structured ascii art focusing on three key criteria fidelity character classification accuracy and output quality we investigate deep learning architectures including multilayer perceptrons mlps resnet and mobilenetv2 alongside classical approaches such as random forests support vector machines svms and knearest neighbors knn trained on an augmented synthetic dataset of ascii characters our results show that complex neural network architectures often fall short in producing highquality ascii art whereas classical machine learning classifiers despite their simplicity achieve performance similar to cnns our findings highlight the strength of classical methods in bridging model simplicity with output quality offering new insights into ascii art synthesis and machine learning on image data with low dimensionality,0
in this paper we first contribute a large scale online study n400 to better understand aesthetic perception of aerial video the results indicate that it is paramount to optimize smoothness of trajectories across all keyframes however for experts timing control remains an essential tool satisfying this dual goal is technically challenging because it requires giving up desirable properties in the optimization formulation second informed by this study we propose a method that optimizes positional and temporal reference fit jointly this allows to generate globally smooth trajectories while retaining user control over reference timings the formulation is posed as a variable infinite horizon contourfollowing algorithm finally a comparative lab study indicates that our optimization scheme outperforms the stateoftheart in terms of perceived usability and preference of resulting videos for novices our method produces smoother and better looking results and also experts benefit from generated timings,0
a sketch found in one of leonardo da vincis notebooks and covered by the written notes of this genius has been recently restored the restoration reveals a possible selfportrait of the artist drawn when he was young here we discuss the discovery of this selfportrait and the procedure used for restoration actually this is a restoration performed on the digital image of the sketch a procedure that can easily extended and applied to ancient documents for studies of art and palaeography,0
we present an algorithm to fair a given planar curve by a logaesthetic curve lac we show how a general lac segment can be uniquely characterized by seven parameters and present a method of parametric approximation based on this fact this work aims to provide tools to be used in reverse engineering for computer aided geometric design finally we show an example of usage by applying this algorithm to the point data obtained from 3d scanning a cars roof,0
we explore the intersection of human and machine creativity by generating sculptural objects through machine learning this research raises questions about both the technical details of automatic art generation and the interaction between ai and people as both artists and the audience of art we introduce two algorithms for generating 3d point clouds and then discuss their actualization as sculpture and incorporation into a holistic art installation specifically the amalgamated deepdream add algorithm solves the sparsity problem caused by the naive deepdreaminspired approach and generates creative and printable point clouds the partitioned deepdream pdd algorithm further allows us to explore more diverse 3d object creation by combining point cloud clustering algorithms and add,0
farin proposed a method for designing bezier curves with monotonic curvature and torsion such curves are relevant in design due to their aesthetic shape the method relies on applying a matrix m to the first edge of the control polygon of the curve in order to obtain by iteration the remaining edges with this method sufficient conditions on the matrix m are provided which lead to the definition of class a curves generalising a previous result by mineur et al for plane curves with m being the composition of a dilatation and a rotation however cao and wang have shown counterexamples for such conditions in this paper we revisit farins idea of using the subdivision algorithm to relate the curvature at every point of the curve to the curvature at the initial point in order to produce a closed formula for the curvature of planar curves in terms of the eigenvalues of the matrix m and the seed vector for the curve the first edge of the control polygon moreover we give new conditions in order to produce planar curves with monotonic curvature the main difference is that we do not require our conditions on the eigenvalues to be preserved under subdivision of the curve this facilitates giving a unified derivation of the existing results and obtain more general results in the planar case,0
automatic meshbased shape generation is of great interest across a wide range of disciplines from industrial design to gaming computer graphics and various other forms of digital art while most traditional methods focus on primitive based model generation advances in deep learning made it possible to learn 3dimensional geometric shape representations in an endtoend manner however most current deep learning based frameworks focus on the representation and generation of voxel and pointcloud based shapes making it not directly applicable to design and graphics communities this study addresses the needs for automatic generation of meshbased geometries and propose a novel framework that utilizes signed distance function representation that generates detail preserving threedimensional surface mesh by a deep learning based approach,0
pixel art is a popular artistic style adopted in the gaming industry and nowadays it is often accompanied by modern rendering techniques one example is dynamic lighting for the game sprites for which normal mapping defines how the light interacts with the material represented by each pixel although there are different methods to generate normal maps for 3d games applying them for pixel art may not yield correct results due to the style specificities therefore this work compiles different normal map generation methods and study their applicability for pixel art reducing the scarcity of existing material on the techniques and contributing to a qualitative analysis of the behavior of these methods in different case studies,0
avatarstaging framework consists in directing avatars on a mixed theatrical stage enabling a copresence between the materiality of the physical actor and the virtuality of avatars controlled in real time by motion capture or specific animation players it led to the implementation of the aknregie authoring tool programmed with the blueprint visual language as a plugin for the unreal engine ue video game engine the paper describes aknregie main functionalities as a tool for nonprogrammer theatrical people it gives insights of its implementation in the blueprint visual language specific to ue it details how the tool evolved along with its use in around ten theater productions a circulation process between a nonprogramming point of view on aknregie called plugin perspective and a programming acculturation to its development called blueprint perspective is discussed finally a c perspective is suggested to enhance the cultural appropriation of technological issues bridging the gap between performing arts deeply involved in human materiality and avatars inviting to discover new worlds,0
automatic animation line art colorization is a challenging computer vision problem since the information of the line art is highly sparse and abstracted and there exists a strict requirement for the color and style consistency between frames recently a lot of generative adversarial network gan based imagetoimage translation methods for single line art colorization have emerged they can generate perceptually appealing results conditioned on line art images however these methods can not be adopted for the purpose of animation colorization because there is a lack of consideration of the inbetween frame consistency existing methods simply input the previous colored frame as a reference to color the next line art which will mislead the colorization due to the spatial misalignment of the previous colored frame and the next line art especially at positions where apparent changes happen to address these challenges we design a kind of correlation matching feature transfer model called cmft to align the colored reference feature in a learnable way and integrate the model into an unet based generator in a coarsetofine manner this enables the generator to transfer the layerwise synchronized features from the deep semantic code to the content progressively extension evaluation shows that cmft model can effectively improve the inbetween consistency and the quality of colored frames especially when the motion is intense and diverse,0
over 60000 songs are released on spotify every day and the competition for the listeners attention is immense in that regard the importance of captivating and inviting cover art cannot be underestimated because it is deeply entangled with a songs character and the artists identity and remains one of the most important gateways to lead people to discover music however designing cover art is a highly creative lengthy and sometimes expensive process that can be daunting especially for nonprofessional artists for this reason we propose a novel deeplearning framework to generate cover art guided by audio features inspired by vqganclip our approach is highly flexible because individual components can easily be replaced without the need for any retraining this paper outlines the architectural details of our models and discusses the optimization challenges that emerge from them more specifically we will exploit genetic algorithms to overcome bad local minima and adversarial examples we find that our framework can generate suitable cover art for most genres and that the visual features adapt themselves to audio feature changes given these results we believe that our framework paves the road for extensions and more advanced applications in audioguided visual generation tasks,0
we propose progressive structureconditional generative adversarial networks psgan a new framework that can generate fullbody and highresolution character images based on structural information recent progress in generative adversarial networks with progressive training has made it possible to generate highresolution images however existing approaches have limitations in achieving both high image quality and structural consistency at the same time our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training in this paper we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024x1024 based on target pose sequences we also create a novel dataset containing fullbody 1024x1024 highresolution images and exact 2d pose keypoints using unity 3d avatar models,0
3d modeling holds significant importance in the realms of arvr and gaming allowing for both artistic creativity and practical applications however the process is often timeconsuming and demands a high level of skill in this paper we present a novel approach to create volumetric representations of 3d characters from consistent turnaround concept art which serves as the standard input in the 3d modeling industry while neural radiance field nerf has been a gamechanger in imagebased 3d reconstruction to the best of our knowledge there is no known research that optimizes the pipeline for concept art to harness the potential of concept art with its defined body poses and specific view angles we propose encoding it as priors for our model we train the network to make use of these priors for various 3d points through a learnable viewdirectionattended multihead selfattention layer additionally we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network our model is able to generate highquality 360degree views of characters subsequently we provide a simple guideline to better leverage our model to extract the 3d mesh it is important to note that our models inferencing capabilities are influenced by the training datas characteristics primarily focusing on characters with a single head two arms and two legs nevertheless our methodology remains versatile and adaptable to concept art from diverse subject matters without imposing any specific assumptions on the data,0
i consider how to influence cyclegan imagetoimage translation by using additional constraints from a neural network trained on art composition attributes i show how i trained the the art composition attributes network acan by incorporating domain knowledge based on the rules of art evaluation and the result of applying each art composition attribute to apple2orange image translation,0
color symmetry is an extension of symmetry imposed by isometric transformations and means that the colors of geometrical objects are assigned according to the symmetry properties of the objects a color symmetry permutes the coloring of the objects consistently with their symmetry group we apply this concept to bioinspired generative art therefore we interpret the geometrical objects as motifs that may repeat themselves with a symmetryconsistent coloring the motifs are obtained by design principles from stigmergy we discuss a design procedure and present visual results,0
fungal simulation and control are considered crucial techniques in bioart creation however coding algorithms for reliable fungal simulations have posed significant challenges for artists this study equates fungal morphology simulation to a twodimensional graphic timeseries generation problem we propose a zerocoding neural networkdriven cellular automaton fungal spread patterns are learned through an image segmentation model and a timeseries prediction model which then supervise the training of neural network cells enabling them to replicate realworld spreading behaviors we further implemented dynamic containment of fungal boundaries with lasers synchronized with the automaton the fungus successfully spreads into predesigned complex shapes in reality,0
this article describes the capabilities of a universal software platform for visualizing class f curves and developing specialized applications for cad systems based on microsoft excel vba the software complex faircurvemodeler and computer algebra systems additionally it demonstrates the use of a software platform for visualizing functional and logaesthetic curves integrated with cad fusion360 the value of the curves is evident in visualizing the qualitative geometry of the product shape in industrial design moreover the requirements for the characteristics of class f curves are emphasized to form a visual purity of shape in industrial design and to provide a positive emotional perception of the visual image of the product by a person,0
textguided synthesis of images has made a giant leap towards becoming a mainstream phenomenon with texttoimage generation systems anybody can create digital images and artworks this provokes the question of whether texttoimage generation is creative this paper expounds on the nature of human creativity involved in texttoimage art socalled ai art with a specific focus on the practice of prompt engineering the paper argues that the current productcentered view of creativity falls short in the context of texttoimage generation a case exemplifying this shortcoming is provided and the importance of online communities for the creative ecosystem of texttoimage art is highlighted the paper provides a highlevel summary of this online ecosystem drawing on rhodes conceptual four p model of creativity challenges for evaluating the creativity of texttoimage generation and opportunities for research on texttoimage generation in the field of humancomputer interaction hci are discussed,0
interactive digital storytelling becomes a popular choice for information presentation in many fields its application spans from media industry and business information visualization through digital cultural heritage serious games education to contemporary theater and visual arts the benefits of this form of multimedia presentation in education are generally recognized and several studies exploring and supporting the opinion are conducted in addition to discussing the benefits we wanted to address the challenges in introducing interactive digital storytelling and serious games in the classroom the challenge of inherent ambiguity of edutainment due to opposing features of education and entertainment is augmented with different viewpoints of multidisciplinary team members we specifically address the opposing views on artistic liberty at one side and technical constraints and historic facts on the other side in this paper we present the first findings related to these questions and to initiate furthering discussions in this area,0
collecting and labeling training data is one important step for learningbased methods because the process is timeconsuming and biased for face analysis tasks although some generative models can be used to generate face data they can only achieve a subset of generation diversity reconstruction accuracy 3d consistency highfidelity visual quality and easy editability one recent related work is the graphicsbased generative method but it can only render low realism head with high computation cost in this paper we propose metahead a unified and fullfeatured controllable digital head engine which consists of a controllable head radiance fieldmetaheadf to superrealistically generate or reconstruct viewconsistent 3d controllable digital heads and a generic topdown image generation framework labelhead to generate digital heads consistent with the given customizable feature labels experiments validate that our controllable digital head engine achieves the stateoftheart generation visual quality and reconstruction accuracy moreover the generated labeled data can assist real training data and significantly surpass the labeled data generated by graphicsbased methods in terms of training effect,0
much of the stateoftheart in image synthesis inspired by real artwork are either entirely generative by filtered random noise or inspired by the transfer of style this work explores the application of image inpainting to continue famous artworks and produce generative art with a conditional gan during the training stage of the process the borders of images are cropped leaving only the centre an inpainting gan is then tasked with learning to reconstruct the original image from the centre crop by way of minimising both adversarial and absolute difference losses which are analysed by both their frchet inception distances and manual observations which are presented once the network is trained images are then resized rather than cropped and presented as input to the generator following the learning process the generator then creates new images by continuing from the edges of the original piece three experiments are performed with datasets of 4766 landscape paintings impressionism and romanticism 1167 ukiyoe works from the japanese edo period and 4968 abstract artworks results show that geometry and texture including canvas and paint as well as scenery such as sky clouds water land including hills and mountains grass and flowers are implemented by the generator when extending real artworks in the ukiyoe experiments it was observed that features such as written text were generated even in cases where the original image did not have any due to the presence of an unpainted border within the input image,0
3d reconstruction of deformable or nonrigid scenes from a set of monocular 2d image observations is a longstanding and actively researched area of computer vision and graphics it is an illposed inverse problem since without additional prior assumptions it permits infinitely many solutions leading to accurate projection to the input 2d images nonrigid reconstruction is a foundational building block for downstream applications like robotics arvr or visual content creation the key advantage of using monocular cameras is their omnipresence and availability to the end users as well as their ease of use compared to more sophisticated camera setups such as stereo or multiview systems this survey focuses on stateoftheart methods for dense nonrigid 3d reconstruction of various deformable objects and composite scenes from monocular videos or sets of monocular views it reviews the fundamentals of 3d reconstruction and deformation modeling from 2d image observations we then start from general methods that handle arbitrary scenes and make only a few prior assumptions and proceed towards techniques making stronger assumptions about the observed objects and types of deformations eg human faces bodies hands and animals a significant part of this star is also devoted to classification and a highlevel comparison of the methods as well as an overview of the datasets for training and evaluation of the discussed techniques we conclude by discussing open challenges in the field and the social aspects associated with the usage of the reviewed methods,0
this article presents an innovative methodology for locating injection points in injectionmolded parts using intelligent models with geometric algorithms for discrete topologies the first algorithm calculates the center of mass of the discrete model based on the center of mass of each triangular facet in the system ensuring uniform molten plastic distribution during mold cavity filling two subalgorithms intelligently evaluate the geometry and optimal injection point location the first subalgorithm generates a geometric matrix based on a twodimensional nodal quadrature adapted to the parts bounding box the second subalgorithm projects the nodal matrix and associated circular areas orthogonally on the parts surface along the demolding direction the optimal injection point location is determined by minimizing the distance to the center of mass from the first algorithms result this novel methodology has been validated through rheological simulations in six case studies with complex geometries the results demonstrate uniform and homogeneous molten plastic distribution with minimal pressure loss during the filling phase importantly this methodology does not require expert intervention reducing time and costs associated with manual injection mold feed system design it is also adaptable to various design environments and virtual twin systems not tied to specific cad software the validated results surpass the state of the art offering an agile alternative for digital twin applications in new product design environments reducing dependence on experts facilitating designer training and ultimately cutting costs,0
this paper argues that generative art driven by conformance to a visual andor semantic corpus lacks the necessary criteria to be considered creative among several issues identified in the literature we focus on the fact that generative adversarial networks gans that create a single image in a vacuum lack a concept of novelty regarding how their product differs from previously created ones we envision that an algorithm that combines the novelty preservation mechanisms in evolutionary algorithms with the power of gans can deliberately guide its creative process towards output that is both good and novel in this paper we use recent advances in image generation based on semantic prompts using openais clip model interrupting the gans iterative process with short cycles of evolutionary divergent search the results of evolution are then used to continue the gans iterative process we hypothesise that this intervention will lead to more novel outputs testing our hypothesis using novelty search with local competition a qualitydiversity evolutionary algorithm that can increase visual diversity while maintaining quality in the form of adherence to the semantic prompt we explore how different notions of visual diversity can affect both the process and the product of the algorithm results show that even a simplistic measure of visual diversity can help counter a drift towards similar images caused by the gan this first experiment opens a new direction for introducing higher intentionality and a more nuanced drive for gans,0
efficient rendering of photorealistic virtual worlds is a long standing effort of computer graphics modern graphics techniques have succeeded in synthesizing photorealistic images from handcrafted scene representations however the automatic generation of shape materials lighting and other aspects of scenes remains a challenging problem that if solved would make photorealistic computer graphics more widely accessible concurrently progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing namely deep generative models neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics eg by the integration of differentiable rendering into network training with a plethora of applications in computer graphics and vision neural rendering is poised to become a new area in the graphics community yet no survey of this emerging field exists this stateoftheart report summarizes the recent trends and applications of neural rendering we focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs starting with an overview of the underlying computer graphics and machine learning concepts we discuss critical aspects of neural rendering approaches this stateoftheart report is focused on the many important use cases for the described algorithms such as novel view synthesis semantic photo manipulation facial and body reenactment relighting freeviewpoint video and the creation of photorealistic avatars for virtual and augmented reality telepresence finally we conclude with a discussion of the social implications of such technology and investigate open research problems,0
creating and updating pixel art character sprites with many frames spanning different animations and poses takes time and can quickly become repetitive however that can be partially automated to allow artists to focus on more creative tasks in this work we concentrate on creating pixel art character sprites in a target pose from images of them facing other three directions we present a novel approach to character generation by framing the problem as a missing data imputation task our proposed generative adversarial networks model receives the images of a character in all available domains and produces the image of the missing pose we evaluated our approach in the scenarios with one two and three missing images achieving similar or better results to the stateoftheart when more images are available we also evaluate the impact of the proposed changes to the base architecture,0
art has long been a medium for individuals to engage with the world scribble art a form of abstract visual expression features spontaneous gestural strokes made with pens or brushes these dynamic and expressive compositions created quickly and impulsively reveal intricate patterns and hidden meanings upon closer inspection while scribble art is often associated with spontaneous expression and experimentation it can also be planned and intentional some artists use scribble techniques as a starting point for their creative process exploring the possibilities of line shape and texture before refining their work into more polished compositions from ancient cave paintings to modern abstract sketches and doodles scribble art has evolved with civilizations reflecting diverse artistic movements and cultural influences this evolution highlights its universal appeal transcending language and cultural barriers and connecting people through the shared experience of creating art,0
procedural content generation pcg can be applied to a wide variety of tasks in games from narratives levels and sounds to trees and weapons a large amount of game content is comprised of graphical assets such as clouds buildings or vegetation that do not require gameplay function considerations there is also a breadth of literature examining the procedural generation of such elements for purposes outside of games the body of research focused on specific methods for generating specific assets provides a narrow view of the available possibilities hence it is difficult to have a clear picture of all approaches and possibilities with no guide for interested parties to discover possible methods and approaches for their needs and no facility to guide them through each technique or approach to map out the process of using them therefore a systematic literature review has been conducted yielding 200 accepted papers this paper explores stateoftheart approaches to graphical asset generation examining research from a wide range of applications inside and outside of games informed by the literature a conceptual framework has been derived to address the aforementioned gaps,0
reconstructing models of the real world including 3d geometry appearance and motion of real scenes is essential for computer graphics and computer vision it enables the synthesizing of photorealistic novel views useful for the movie industry and arvr applications it also facilitates the content creation necessary in computer games and arvr by avoiding laborious manual design processes further such models are fundamental for intelligent computing systems that need to interpret realworld scenes and actions to act and interact safely with the human world notably the world surrounding us is dynamic and reconstructing models of dynamic nonrigidly moving scenes is a severely underconstrained and challenging problem this stateoftheart report star offers the reader a comprehensive summary of stateoftheart techniques with monocular and multiview inputs such as data from rgb and rgbd sensors among others conveying an understanding of different approaches their potential applications and promising further research directions the report covers 3d reconstruction of general nonrigid scenes and further addresses the techniques for scene decomposition editing and controlling and generalizable and generative modeling more specifically we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the stateoftheart techniques by reviewing recent approaches that use traditional and machinelearningbased neural representations including a discussion on the newly enabled applications the star is concluded with a discussion of the remaining limitations and open challenges,0
fractal image generation algorithms exhibit extreme parallelizability using general purpose graphics processing unit gpu programming to implement escapetime algorithms for julia sets of functionsparallel methods generate visually attractive fractal images much faster than traditional methods vastly improved speeds are achieved using this method of computation which allow realtime generation and display of images a comparison is made between sequential and parallel implementations of the algorithm an application created by the authors demonstrates using the increased speed to create dynamic imaging of fractals where the user may explore paths of parameter values corresponding to a given functions mandelbrot set examples are given of artistic and mathematical insights gained by experiencing fractals interactively and from the ability to sample the parameter space quickly and comprehensively,0
the field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence ai which unlocks unprecedented capabilities for the generation editing and reconstruction of images videos and 3d scenes in these domains diffusion models are the generative ai architecture of choice within the last year alone the literature on diffusionbased tools and applications has seen exponential growth and relevant papers are published across the computer graphics computer vision and ai communities with new works appearing daily on arxiv this rapid growth of the field makes it difficult to keep up with all recent developments the goal of this stateoftheart report star is to introduce the basic mathematical concepts of diffusion models implementation details and design choices of the popular stable diffusion model as well as overview important aspects of these generative ai tools including personalization conditioning inversion among others moreover we give a comprehensive overview of the rapidly growing literature on diffusionbased generation and editing categorized by the type of generated medium including 2d images videos 3d objects locomotion and 4d scenes finally we discuss available datasets metrics open challenges and social implications this star provides an intuitive starting point to explore this exciting topic for researchers artists and practitioners alike,0
lowresolution quantized imagery such as pixel art is seeing a revival in modern applications ranging from video game graphics to digital design and fabrication where creativity is often bound by a limited palette of elemental units despite their growing popularity the automated generation of quantized images from raw inputs remains a significant challenge often necessitating intensive manual input we introduce sdxl an approach for producing quantized images that employs score distillation sampling in conjunction with a differentiable image generator our method enables users to input a prompt and optionally an image for spatial conditioning set any desired output size h times w and choose a palette of n colors or elements each color corresponds to a distinct class for our generator which operates on an h times w times n tensor we adopt a softmax approach computing a convex sum of elements thus rendering the process differentiable and amenable to backpropagation we show that employing gumbelsoftmax reparameterization allows for crisp pixel art effects unique to our method is the ability to transform input images into lowresolution quantized versions while retaining their key semantic features our experiments validate sdxls performance in creating visually pleasing and faithful representations consistently outperforming the current stateoftheart furthermore we showcase sdxls practical utility in fabrication through its applications in interlocking brick mosaic beading and embroidery design,0
in the burgeoning field of intelligent transportation systems the integration of generative artificial intelligence ai into vehicular networks presents a transformative potential for the automotive industry this paper explores the innovative applications of generative ai in enhancing communication protocols optimizing traffic management and bolstering security frameworks within vehicular networks by examining current technologies and recent advancements we identify key challenges such as scalability realtime data processing and security vulnerabilities that come with ai integration additionally we propose novel applications and methodologies that leverage generative ai to simulate complex network scenarios generate adaptive communication schemes and enhance predictive capabilities for traffic conditions this study not only reviews the state of the art but also highlights significant opportunities where generative ai can lead to groundbreaking improvements in vehicular network efficiency and safety through this comprehensive exploration our findings aim to guide future research directions and foster a deeper understanding of generative ais role in the next generation of vehicular technologies,0
we review human evaluation practices in automated speechdriven 3d gesture generation and find a lack of standardisation and frequent use of flawed experimental setups this leads to a situation where it is impossible to know how different methods compare or what the state of the art is in order to address common shortcomings of evaluation design and to standardise future user studies in gesturegeneration works we introduce a detailed human evaluation protocol for the widelyused beat2 motioncapture dataset using this protocol we conduct largescale crowdsourced evaluation to rank six recent gesturegeneration models each trained by its original authors across two key evaluation dimensions motion realism and speechgesture alignment our results provide strong evidence that 1 newer models do not consistently outperform earlier approaches 2 published claims of high motion realism or speechgesture alignment may not hold up under rigorous evaluation and 3 the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress finally in order to drive standardisation and enable new evaluation research we will release five hours of synthetic motion from the benchmarked models over 750 rendered video stimuli from the user studies enabling new evaluations without model reimplementation required alongside our opensource rendering script and the 16000 pairwise human preference votes collected for our benchmark,0
generative image modeling techniques such as gan demonstrate highly convincing image generation result however user interaction is often necessary to obtain the desired results existing attempts add interactivity but require either tailored architectures or extra data we present a humanintheoptimization method that allows users to directly explore and search the latent vector space of generative image modeling our system provides multiple candidates by sampling the latent vector space and the user selects the best blending weights within the subspace using multiple sliders in addition the user can express their intention through image editing tools the system samples latent vectors based on inputs and presents new candidates to the user iteratively an advantage of our formulation is that one can apply our method to arbitrary pretrained model without developing specialized architecture or data we demonstrate our method with various generative image modeling applications and show superior performance in a comparative user study with prior art igan,0
in this paper a simple technique for unmanned aerial vehicles uavs potential landing site detection using terrain information through identification of flat areas is presented the algorithm utilizes digital elevation models dem that represent the height distribution of an area flat areas which constitute appropriate landing zones for uavs in normal or emergency situations result by thresholding the image gradient magnitude of the digital surface model dsm the proposed technique also uses connected components evaluation on the thresholded gradient image in order to discover connected regions of sufficient size for landing moreover manmade structures and vegetation areas are detected and excluded from the potential landing sites quantitative performance evaluation of the proposed landing site detection algorithm in a number of areas on real world and synthetic datasets accompanied by a comparison with a stateoftheart algorithm proves its efficiency and superiority,0
large language model llmdriven digital humans have sparked a series of recent studies on cospeech gesture generation systems however existing approaches struggle with realtime synthesis and longtext comprehension this paper introduces transformerbased rich motion matching trimm a novel multimodal framework for realtime 3d gesture generation our method incorporates three modules 1 a crossmodal attention mechanism to achieve precise temporal alignment between speech and gestures 2 a longcontext autoregressive model with a sliding window mechanism for effective sequence modeling 3 a largescale gesture matching system that constructs an atomic action library and enables realtime retrieval additionally we develop a lightweight pipeline implemented in the unreal engine for experimentation our approach achieves realtime inference at 120 fps and maintains a persentence latency of 015 seconds on consumergrade gpus geforce rtx3060 extensive subjective and objective evaluations on the zeggs and beat datasets demonstrate that our model outperforms current stateoftheart methods trimm enhances the speed of cospeech gesture generation while ensuring gesture quality enabling llmdriven digital humans to respond to speech in real time and synthesize corresponding gestures our code is available at,0
this paper describes the bricklayer ecosystem a freelyavailable online educational ecosystem created for people of all ages and coding backgrounds bricklayer is designed in accordance with a lowthreshold infinite ceiling philosophy and has been successfully used to teach coding to primary school students middle school students university freshmen and inservice secondary math teachers bricklayer programs are written in the functional programming language sml and when executed create 2d and 3d artifacts these artifacts can be viewed using a variety of thirdparty tools such as lego digital designer ldd ldraw minecraft clients brickr as well as stereolithography viewers,0
we present a fully automatic system that can produce highfidelity photorealistic 3d digital human heads with a consumer rgbd selfie camera the system only needs the user to take a short selfie rgbd video while rotating hisher head and can produce a high quality head reconstruction in less than 30 seconds our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the stateoftheart specifically given the input video a twostage frame selection procedure is first employed to select a few highquality frames for reconstruction then a differentiable renderer based 3d morphable model 3dmm fitting algorithm is applied to recover facial geometries from multiview rgbd data which takes advantages of a powerful 3dmm basis constructed with extensive data generation and perturbation our 3dmm has much larger expressive capacities than conventional 3dmm allowing us to recover more accurate facial geometry using merely linear basis for reflectance synthesis we present a hybrid approach that combines parametric fitting and cnns to synthesize highresolution albedonormal maps with realistic hairporewrinkle details results show that our system can produce faithful 3d digital human faces with extremely realistic details the main code and the newly constructed 3dmm basis is publicly available,0
we introduce pixelaligned implicit function pifu a highly effective implicit representation that locally aligns pixels of 2d images with the global context of their corresponding 3d object using pifu we propose an endtoend deep learning method for digitizing highly detailed clothed humans that can infer both 3d surface and texture from a single image and optionally multiple input images highly intricate shapes such as hairstyles clothing as well as their variations and deformations can be digitized in a unified way compared to existing representations used for 3d deep learning pifu can produce highresolution surfaces including largely unseen regions such as the back of a person in particular it is memory efficient unlike the voxel representation can handle arbitrary topology and the resulting surface is spatially aligned with the input image furthermore while previous techniques are designed to process either a single image or multiple views pifu extends naturally to arbitrary number of views we demonstrate highresolution and robust reconstructions on real world images from the deepfashion dataset which contains a variety of challenging clothing types our method achieves stateoftheart performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image,0
we present deep illumination a novel machine learning technique for approximating global illumination gi in realtime applications using a conditional generative adversarial network our primary focus is on generating indirect illumination and soft shadows with offline rendering quality at interactive rates inspired from recent advancement in imagetoimage translation problems using deep generative convolutional networks we introduce a variant of this network that learns a mapping from gbuffers depth map normal map and diffuse map and direct illumination to any global illumination solution our primary contribution is showing that a generative model can be used to learn a density estimation from screen space buffers to an advanced illumination model for a 3d environment once trained our network can approximate global illumination for scene configurations it has never encountered before within the environment it was trained on we evaluate deep illumination through a comparison with both a state of the art realtime gi technique vxgi and an offline rendering gi technique path tracing we show that our method produces effective gi approximations and is also computationally cheaper than existing gi techniques our technique has the potential to replace existing precomputed and screenspace techniques for producing global illumination effects in dynamic scenes with physicallybased rendering quality,0
current 3d4d generation methods are usually optimized for photorealism efficiency and aesthetics however they often fail to preserve the semantic identity of the subject across different viewpoints adapting generation methods with one or few images of a specific subject also known as personalization or subjectdriven generation allows generating visual content that align with the identity of the subject however personalized 3d4d generation is still largely underexplored in this work we introduce tire track inpaint resplat a novel method for subjectdriven 3d4d generation it takes an initial 3d asset produced by an existing 3d generative model as input and uses video tracking to identify the regions that need to be modified then we adopt a subjectdriven 2d inpainting model for progressively infilling the identified regions finally we resplat the modified 2d multiview observations back to 3d while still maintaining consistency extensive experiments demonstrate that our approach significantly improves identity preservation in 3d4d generation compared to stateoftheart methods our project website is available at,0
we introduce synse a novel syntactically guided generative approach for zeroshot learning zsl our endtoend approach learns progressively refined generative embedding spaces constrained within and across the involved modalities visual language the intermodal constraints are defined between action sequence embedding and embeddings of parts of speech pos tagged words in the corresponding action description we deploy synse for the task of skeletonbased action sequence recognition our design choices enable synse to generalize compositionally ie recognize sequences whose action descriptions contain words not encountered during training we also extend our approach to the more challenging generalized zeroshot learning gzsl problem via a confidencebased gating mechanism we are the first to present zeroshot skeleton action recognition results on the largescale ntu60 and ntu120 skeleton action datasets with multiple splits our results demonstrate synses state of the art performance in both zsl and gzsl settings compared to strong baselines on the ntu60 and ntu120 datasets the code and pretrained models are available at,0
the recent developments in neural fields have brought phenomenal capabilities to the field of shape generation but they lack crucial properties such as incremental control a fundamental requirement for artistic work triangular meshes on the other hand are the representation of choice for most geometry related tasks offering efficiency and intuitive control but do not lend themselves to neural optimization to support downstream tasks previous art typically proposes a twostep approach where first a shape is generated using neural fields and then a mesh is extracted for further processing instead in this paper we introduce a hybrid approach that maintains both a mesh and a signed distance field sdf representations consistently using this representation we introduce magicclay an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization relying on the mesh representation we show how to render the sdf at higher resolutions and faster in addition we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required as indicated by the sdf using an implemented prototype we demonstrate superior generated geometry compared to the stateoftheart and novel consistent control allowing sequential promptbased edits to the same mesh for the first time,0
vector graphics are widely used in digital art and highly favored by designers due to their scalability and layerwise properties however the process of creating and editing vector graphics requires creativity and design expertise making it a timeconsuming task recent advancements in texttovector t2v generation have aimed to make this process more accessible however existing t2v methods directly optimize control points of vector graphics paths often resulting in intersecting or jagged paths due to the lack of geometry constraints to overcome these limitations we propose a novel neural path representation by designing a dualbranch variational autoencoder vae that learns the path latent space from both sequence and image modalities by optimizing the combination of neural paths we can incorporate geometric constraints while preserving expressivity in generated svgs furthermore we introduce a twostage path optimization method to improve the visual and topological quality of generated svgs in the first stage a pretrained texttoimage diffusion model guides the initial generation of complex vector graphics through the variational score distillation vsd process in the second stage we refine the graphics using a layerwise image vectorization strategy to achieve clearer elements and structure we demonstrate the effectiveness of our method through extensive experiments and showcase various applications the project page is,0
previous approaches to generate shapes in a 3d setting train a gan on the latent space of an autoencoder ae even though this produces convincing results it has two major shortcomings as the gan is limited to reproduce the dataset the ae was trained on we cannot reuse a trained ae for novel data furthermore it is difficult to add spatial supervision into the generation process as the ae only gives us a global representation to remedy these issues we propose to train the gan on grids ie each cell covers a part of a shape in this representation each cell is equipped with a latent vector provided by an ae this localized representation enables more expressiveness since the cellbased latent vectors can be combined in novel ways as well as spatial control of the generation process eg via bounding boxes our method outperforms the current state of the art on all established evaluation measures proposed for quantitatively evaluating the generative capabilities of gans we show limitations of these measures and propose the adaptation of a robust criterion from statistical analysis as an alternative,0
in this paper we examine the concept of complexity as it applies to generative art and design complexity has many different discipline specific definitions such as complexity in physical systems entropy algorithmic measures of information complexity and the field of complex systems we apply a series of different complexity measures to three different generative art datasets and look at the correlations between complexity and individual aesthetic judgement by the artist in the case of two datasets or the physically measured complexity of 3d forms our results show that the degree of correlation is different for each set and measure indicating that there is no overall better measure however specific measures do perform well on individual datasets indicating that careful choice can increase the value of using such measures we conclude by discussing the value of direct measures in generative and evolutionary art reinforcing recent findings from neuroimaging and psychology which suggest human aesthetic judgement is informed by many extrinsic factors beyond the measurable properties of the object being judged,0
the rise of nonlinear and interactive media such as video games has increased the need for automatic movement animation generation in this survey we review and analyze different aspects of building automatic movement generation systems using machine learning techniques and motion capture data we cover topics such as highlevel movement characterization training data features representation machine learning models and evaluation methods we conclude by presenting a discussion of the reviewed literature and outlining the research gaps and remaining challenges for future work,0
apparels significant role in human appearance underscores the importance of garment digitalization for digital human creation recent advances in 3d content creation are pivotal for digital human creation nonetheless garment generation from text guidance is still nascent we introduce a textdriven 3d garment generation framework dresscode which aims to democratize design for novices and offer immense potential in fashion design virtual tryon and digital human creation we first introduce sewinggpt a gptbased architecture integrating crossattention with textconditioned embedding to generate sewing patterns with text guidance we then tailor a pretrained stable diffusion to generate tilebased physicallybased rendering pbr textures for the garments by leveraging a large language model our framework generates cgfriendly garments through natural language interaction it also facilitates pattern completion and texture editing streamlining the design process through userfriendly interaction this framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work with comprehensive evaluations and comparisons with other stateoftheart methods our method showcases superior quality and alignment with input prompts user studies further validate our highquality rendering results highlighting its practical utility and potential in production settings our project page is,0
designing realistic digital humans is extremely complex most datadriven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes in this paper we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neuralnetworkbased generative models of 3d head and body meshes encouraging the latent variables of mesh variational autoencoders vaes or generative adversarial networks gans to follow the local eigenprojections of identity attributes we improve latent disentanglement and properly decouple the attribute creation experimental results show that our local eigenprojection disentangled led models not only offer improved disentanglement with respect to the stateoftheart but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models,0
dance as an art form fundamentally hinges on the precise synchronization with musical beats however achieving aesthetically pleasing dance sequences from music is challenging with existing methods often falling short in controllability and beat alignment to address these shortcomings this paper introduces beatit a novel framework for beatspecific key poseguided dance generation unlike prior approaches beatit uniquely integrates explicit beat awareness and key pose guidance effectively resolving two main issues the misalignment of generated dance motions with musical beats and the inability to map key poses to specific beats critical for practical choreography our approach disentangles beat conditions from music using a nearest beat distance representation and employs a hierarchical multicondition fusion mechanism this mechanism seamlessly integrates key poses beats and music features mitigating condition conflicts and offering rich multiconditioned guidance for dance generation additionally a specially designed beat alignment loss ensures the generated dance movements remain in sync with the designated beats extensive experiments confirm beatits superiority over existing stateoftheart methods in terms of beat alignment and motion controllability,0
in this work we propose a new computational framework based on generative deep models for synthesis of photorealistic food meal images from textual descriptions of its ingredients previous works on synthesis of images from text typically rely on pretrained text models to extract text features followed by a generative neural networks gans aimed to generate realistic images conditioned on the text features these works mainly focus on generating spatially compact and welldefined categories of objects such as birds or flowers in contrast meal images are significantly more complex consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods we propose a method that first builds an attentionbased ingredientsimage association model which is then used to condition a generative neural network tasked with synthesizing meal images furthermore a cycleconsistent constraint is added to further improve image quality and control appearance extensive experiments show our model is able to generate meal image corresponding to the ingredients which could be used to augment existing dataset for solving other computational food analysis problems,0
the use of digitally reconstructed radiographs drrs to solve inverse problems such as slicetovolume registration and 3d reconstruction is wellstudied in preoperative settings in intraoperative imaging the utility of drrs is limited by the challenges in generating them in realtime and supporting optimization procedures that rely on repeated drr synthesis while immense progress has been made in accelerating the generation of drrs through algorithmic refinements and gpu implementations drrbased optimization remains slow because most drr generators do not offer a straightforward way to obtain gradients with respect to the imaging parameters to make drrs interoperable with gradientbased optimization and deep learning frameworks we have reformulated siddons method the most popular raytracing algorithm used in drr generation as a series of vectorized tensor operations we implemented this vectorized version of siddons method in pytorch taking advantage of the librarys strong automatic differentiation engine to make this drr generator fully differentiable with respect to its parameters additionally using gpuaccelerated tensor computation enables our vectorized implementation to achieve rendering speeds equivalent to stateoftheart drr generators implemented in cuda and c we illustrate the resulting method in the context of slicetovolume registration moreover our simulations suggest that the loss landscapes for the slicetovolume registration problem are convex in the neighborhood of the optimal solution and gradientbased registration promises a much faster solution than prevailing gradientfree optimization strategies the proposed drr generator enables fast computer vision algorithms to support image guidance in minimally invasive procedures our implementation is publically available at,0
in 1960 in stuttgart max bense published the book programming the beautiful bense looks in cybernetics for scientific concepts and instigates the thought of programming in the field of literature his information aesthetics influences a whole generation of scientists and artists including the stuttgart circle which takes hold of the new aesthetics to carry out the first programmed artistic images is max bense a visionary how is he revolutionizing the world of images the article discusses the cybernetics that inspired bense a science of probability that contrasts with the principles of newtonian physics moreover in the sixties max bense together with elisabeth walther launched the experimental magazine rot which devoted its pages to the concrete poetry and the first computergenerated images of georg nees as frieder nake defends through his pioneering work and theory these images oppose the visible and the computable this dialectic opens to a critical thinking on the algorithmic image in art and science,0
threedimensional building generation is vital for applications in gaming virtual reality and digital twins yet current methods face challenges in producing diverse structured and hierarchically coherent buildings we propose buildingblock a hybrid approach that integrates generative models procedural content generation pcg and large language models llms to address these limitations specifically our method introduces a twophase pipeline the layout generation phase lgp and the building construction phase bcp lgp reframes boxbased layout generation as a pointcloud generation task utilizing a newly constructed architectural dataset and a transformerbased diffusion model to create globally consistent layouts with llms these layouts are extended into rulebased hierarchical designs seamlessly incorporating component styles and spatial structures the bcp leverages these layouts to guide pcg enabling localcustomizable highquality structured building generation experimental results demonstrate buildingblocks effectiveness in generating diverse and hierarchically structured buildings achieving stateoftheart results on multiple benchmarks and paving the way for scalable and intuitive architectural workflows,0
scene generation is crucial to many computer graphics applications recent advances in generative ai have streamlined sketchtoimage workflows easing the workload for artists and designers in creating scene concept art however these methods often struggle for complex scenes with multiple detailed objects sometimes missing small or uncommon instances in this paper we propose a trainingfree triplet tuning for sketchtoscene t3s2s generation after reviewing the entire crossattention mechanism this scheme revitalizes the existing controlnet model enabling effective handling of multiinstance generations involving prompt balance characteristics prominence and dense tuning specifically this approach enhances keyword representation via the prompt balance module reducing the risk of missing critical instances it also includes a characteristics prominence module that highlights topk indices in each channel ensuring essential features are better represented based on token sketches additionally it employs dense tuning to refine contour details in the attention map compensating for instancerelated regions experiments validate that our triplet tuning approach substantially improves the performance of existing sketchtoimage models it consistently generates detailed multiinstance 2d images closely adhering to the input prompts and enhancing visual quality in complex multiinstance scenes code is available at,0
recent advances in dance generation have enabled the automatic synthesis of 3d dance motions however existing methods still face significant challenges in simultaneously achieving high realism precise dancemusic synchronization diverse motion expression and physical plausibility to address these limitations we propose a novel approach that leverages a generative masked texttomotion model as a distribution prior to learn a probabilistic mapping from diverse guidance signals including music genre and pose into highquality dance motion sequences our framework also supports semantic motion editing such as motion inpainting and body part modification specifically we introduce a multitower masked motion model that integrates a textconditioned masked motion backbone with two parallel modalityspecific branches a musicguidance tower and a poseguidance tower the model is trained using synchronized and progressive masked training which allows effective infusion of the pretrained texttomotion prior into the dance synthesis process while enabling each guidance branch to optimize independently through its own loss function mitigating gradient interference during inference we introduce classifierfree logits guidance and poseguided token optimization to strengthen the influence of music genre and pose signals extensive experiments demonstrate that our method sets a new state of the art in dance generation significantly advancing both the quality and editability over existing approaches project page available at,0
chemists now routinely use software as part of their work for example virtual chemistry allows chemical reactions to be simulated in particular a selection of software is available for the visualisation of complex 3dimensional molecular structures many of these are very beautiful in their own right as well as being included as illustrations in academic papers such visualisations are often used on the covers of chemistry journals as artistically decorative and attractive motifs chemical images have also been used as the basis of artworks in exhibitions this paper explores the development of the relationship of chemistry art and it it covers some of the increasingly sophisticated software used to generate these projections eg ucsf chimera and their progressive use as a visual art form,0
efficient generation of 3d digital humans is important in several industries including virtual reality social media and cinematic production 3d generative adversarial networks gans have demonstrated stateoftheart sota quality and diversity for generated assets current 3d gan architectures however typically rely on volume representations which are slow to render thereby hampering the gan training and requiring multiviewinconsistent 2d upsamplers here we introduce gaussian shell maps gsms as a framework that connects sota generator network architectures with emerging 3d gaussian rendering primitives using an articulable multi shellbased scaffold in this setting a cnn generates a 3d texture stack with features that are mapped to the shells the latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose instead of rasterizing the shells directly we sample 3d gaussians on the shells whose attributes are encoded in the texture features these gaussians are efficiently and differentiably rendered the ability to articulate the shells is important during gan training and at inference time to deform a body into arbitrary userdefined poses our efficient rendering scheme bypasses the need for viewinconsistent upsamplers and achieves highquality multiview consistent renderings at a native resolution of 512 times 512 pixels we demonstrate that gsms successfully generate 3d humans when trained on singleview datasets including shhq and deepfashion,0
in search of the wave is a computergenerated film made in 2013 highlighting the computation of images through computer simulation and through text and voice originating from a screening of the film at the gustave eiffel university the article presents a reflection on researchcreation in and from algorithmic images fundamentally what is it in this researchcreation especially in research on algorithmic imagery that can be set in motion without fully distinguishing between what would be research on one hand and creation on the other we focus on characterizing forms aesthetics or theories that contribute to possible shifts the inventory of these possibilities is precisely the challenge of the text from mathematics to image and visualization from the birth of generative aesthetics to the coding related to pioneering works recoding or from indexing new aesthetics to new forms of critical production,0
in web data advertising images are crucial for capturing user attention and improving advertising effectiveness most existing methods generate background for products primarily focus on the aesthetic quality which may fail to achieve satisfactory online performance to address this limitation we explore the use of multimodal large language models mllms for generating advertising images by optimizing for clickthrough rate ctr as the primary objective firstly we build targeted pretraining tasks and leverage a largescale ecommerce multimodal dataset to equip mllms with initial capabilities for advertising image generation tasks to further improve the ctr of generated images we propose a novel reward model to finetune pretrained mllms through reinforcement learning rl which can jointly utilize multimodal features and accurately reflect user click preferences meanwhile a productcentric preference optimization strategy is developed to ensure that the generated background content aligns with the product characteristics after finetuning enhancing the overall relevance and effectiveness of the advertising images extensive experiments have demonstrated that our method achieves stateoftheart performance in both online and offline metrics our code and pretrained models are publicly available at,0
we propose diffsheg a diffusionbased approach for speechdriven holistic 3d expression and gesture generation with arbitrary length while previous works focused on cospeech gesture or expression generation individually the joint generation of synchronized expressions and gestures remains barely explored to address this our diffusionbased cospeech motion generation transformer enables unidirectional information flow from expression to gesture facilitating improved matching of joint expressiongesture distributions furthermore we introduce an outpaintingbased sampling strategy for arbitrary long sequence generation in diffusion models offering flexibility and computational efficiency our method provides a practical solution that produces highquality synchronized expression and gesture generation driven by speech evaluated on two public datasets our approach achieves stateoftheart performance both quantitatively and qualitatively additionally a user study confirms the superiority of diffsheg over prior approaches by enabling the realtime generation of expressive and synchronized motions diffsheg showcases its potential for various applications in the development of digital humans and embodied agents,0
we present a novel algorithm for implementing owenscrambling combining the generation and distribution of the scrambling bits in a single selfcontained compact process we employ a contextfree grammar to build a binary tree of symbols and equip each symbol with a scrambling code that affects all descendant nodes we nominate the grammar of adaptive regular tiles art derived from the repetitionavoiding thuemorse word and we discuss its potential advantages and shortcomings our algorithm has many advantages including random access to samples fixed time complexity gpu friendliness and scalability to any memory budget further it provides two unique features over known methods it admits optimization and it is invertible enabling screenspace scrambling of the highdimensional sobol sampler,0
islamic geometric patterns are a rich and venerable ornamental tradition many classic designs feature periodic arrangements of rosettes star shapes surrounded by rings of hexagonal petals we present a new technique for generating freeform compositions of rosettes finite designs that freely mix rosettes of unusual sizes while retaining the aesthetics of traditional patterns we use a circle packing as a scaffolding for developing a patch of polygons and fill each polygon with a motif based on established constructions from islamic art,0
automatically generating realistic musical performance motion can greatly enhance digital media production often involving collaboration between professionals and musicians however capturing the intricate body hand and finger movements required for accurate musical performances is challenging existing methods often fall short due to the complex mapping between audio and motion typically requiring additional inputs like scores or midi data in this work we present syncviolinist a multistage endtoend framework that generates synchronized violin performance motion solely from audio input our method overcomes the challenge of capturing both global and finegrained performance features through two key modules a bowingfingering module and a motion generation module the bowingfingering module extracts detailed playing information from the audio which the motion generation module uses to create precise coordinated body motions reflecting the temporal granularity and nature of the violin performance we demonstrate the effectiveness of syncviolinist with significantly improved qualitative and quantitative results from unseen violin performance audio outperforming stateoftheart methods extensive subjective evaluations involving professional violinists further validate our approach the code and dataset are available at,0
with the ongoing pandemic virtual concerts and live events using digitized performances of musicians are getting traction on massive multiplayer online worlds however well choreographed dance movements are extremely complex to animate and would involve an expensive and tedious production process in addition to the use of complex motion capture systems it typically requires a collaborative effort between animators dancers and choreographers we introduce a complete system for dance motion synthesis which can generate complex and highly diverse dance sequences given an input music sequence as motion capture data is limited for the range of dance motions and styles we introduce a massive dance motion data set that is created from youtube videos we also present a novel twostream motion transformer generative model which can generate motion sequences with high flexibility we also introduce new evaluation metrics for the quality of synthesized dance motions and demonstrate that our system can outperform stateoftheart methods our system provides highquality animations suitable for large crowds for virtual concerts and can also be used as reference for professional animation pipelines most importantly we show that vast online videos can be effective in training dance motion models,0
2d animation is a common factor in game development used for characters effects and background art it involves work that takes both skill and time but parts of which are repetitive and tedious automated animation approaches exist but are designed without animators in mind the focus is heavily on reallife video which follows strict laws of how objects move and does not account for the stylistic movement often present in 2d animation we propose a problem formulation that more closely adheres to the standard workflow of animation we also demonstrate a model sketchbetween which learns to map between keyframes and sketched inbetweens to rendered sprite animations we demonstrate that our problem formulation provides the required information for the task and that our model outperforms an existing method,0
color sequences ordered sets of colors for data visualization that balance aesthetics with accessibility considerations are presented in order to model aesthetic preference data were collected with an online survey and the results were used to train a machinelearning model to ensure accessibility this model was combined with minimumperceptualdistance constraints including for simulated colorvision deficiencies as well as with minimumlightnessdistance constraints for grayscale printing maximumlightness constraints for maintaining contrast with a white background and scores from a colorsaliency model for ease of use of the colors in verbal and written descriptions optimal color sequences containing six eight and ten colors were generated using the datadriven aestheticpreference model and accessibility constraints due to the balance of aesthetics and accessibility considerations the resulting color sequences can serve as reasonable defaults in dataplotting codes eg for use in scatter plots and line plots,0
this paper proposes 3dify a procedural 3d computer graphics 3dcg generation framework utilizing large language models llms the framework enables users to generate 3dcg content solely through natural language instructions 3dify is built upon dify an opensource platform for ai application development and incorporates several stateoftheart llmrelated technologies such as the model context protocol mcp and retrievalaugmented generation rag for 3dcg generation support 3dify automates the operation of various digital content creation dcc tools via mcp when dcc tools do not support mcpbased interaction the framework employs the computerusing agent cua method to automate graphical user interface gui operations moreover to enhance image generation quality 3dify allows users to provide feedback by selecting preferred images from multiple candidates the llm then learns variable patterns from these selections and applies them to subsequent generations furthermore 3dify supports the integration of locally deployed llms enabling users to utilize customdeveloped models and to reduce both time and monetary costs associated with external api calls by leveraging their own computational resources,0
image and shape editing are ubiquitous among digital artworks graphics algorithms facilitate artists and designers to achieve desired editing intents without going through manually tedious retouching in the recent advance of machine learning artists editing intents can even be driven by text using a variety of welltrained neural networks they have seen to be receiving an extensive success on such as generating photorealistic images artworks and human poses stylizing meshes from text or autocompletion given image and shape priors in this short survey we provide an overview over 50 papers on stateoftheart textguided imageandshape generation techniques we start with an overview on recent editing algorithms in the introduction then we provide a comprehensive review on textguided editing techniques for 2d and 3d independently where each of its subsection begins with a brief background introduction we also contextualize editing algorithms under recent implicit neural representations finally we conclude the survey with the discussion over existing methods and potential research ideas,0
creating and understanding art has long been a hallmark of human ability when presented with finished digital artwork professional graphic artists can intuitively deconstruct and replicate it using various drawing tools such as the line tool paint bucket and layer features including opacity and blending modes while most recent research in this field has focused on art generation proposing a range of methods these often rely on the concept of artwork being represented as a final image to bridge the gap between pixellevel results and the actual drawing process we present an approach that treats a set of drawing tools as executable programs this method predicts a sequence of steps to achieve the final image allowing for understandable and resolutionindependent reproductions under the usage of a set of drawing commands our experiments demonstrate that our program synthesizer art2prog can comprehensively understand complex input images and reproduce them using highquality executable programs the experimental results evidence the potential of machines to grasp higherlevel information from images and generate compact programlevel descriptions,0
an algorithm to generate the locus of a circle using the intersection points of straight lines is proposed the pixels on the circle are plotted independent of one another and the operations involved in finding the locus of the circle from the intersection of straight lines are parallelizable integer only arithmetic and algorithmic optimizations are used for speedup the proposed algorithm makes use of an envelope to form a parabolic arc which is consequent transformed into a circle the use of parabolic arcs for the transformation results in higher pixel errors as the radius of the circle to be drawn increases at its current state the algorithm presented may be suitable only for generating circles for string art,0
diffusion models have shown impressive results in texttoimage synthesis using massive datasets of captioned images diffusion models learn to generate raster images of highly diverse objects and scenes however designers frequently use vector representations of images like scalable vector graphics svgs for digital icons or art vector graphics can be scaled to any size and are compact we show that a textconditioned diffusion model trained on pixel representations of images can be used to generate svgexportable vector graphics we do so without access to large datasets of captioned svgs by optimizing a differentiable vector graphics rasterizer our method vectorfusion distills abstract semantic knowledge out of a pretrained diffusion model inspired by recent textto3d work we learn an svg consistent with a caption using score distillation sampling to accelerate generation and improve fidelity vectorfusion also initializes from an image sample experiments show greater quality than prior work and demonstrate a range of styles including pixel art and sketches see our project webpage at,0
shaded relief is an effective method for visualising terrain on topographic maps especially when the direction of illumination is adapted locally to emphasise individual terrain features however digital shading algorithms are unable to fully match the expressiveness of handcrafted masterpieces which are created through a laborious process by highly specialised cartographers we replicate handdrawn relief shading using unet neural networks the deep neural networks are trained with manual shaded relief images of the swiss topographic map series and terrain models of the same area the networks generate shaded relief that closely resemble handdrawn shaded relief art the networks learn essential design principles from manual relief shading such as removing unnecessary terrain details locally adjusting the illumination direction to accentuate individual terrain features and varying brightness to emphasise larger landforms neural network shadings are generated from digital elevation models in a few seconds and a study with 18 relief shading experts found that they are of high quality,0
polygonal meshes are ubiquitous in the digital 3d domain yet they have only played a minor role in the deep learning revolution leading methods for learning generative models of shapes rely on implicit functions and generate meshes only after expensive isosurfacing routines to overcome these challenges we are inspired by a classical spatial data structure from computer graphics binary space partitioning bsp to facilitate 3d learning the core ingredient of bsp is an operation for recursive subdivision of space to obtain convex sets by exploiting this property we devise bspnet a network that learns to represent a 3d shape via convex decomposition importantly bspnet is unsupervised since no convex shape decompositions are needed for training the network is trained to reconstruct a shape using a set of convexes obtained from a bsptree built on a set of planes the convexes inferred by bspnet can be easily extracted to form a polygon mesh without any need for isosurfacing the generated meshes are compact ie lowpoly and well suited to represent sharp geometry they are guaranteed to be watertight and can be easily parameterized we also show that the reconstruction quality by bspnet is competitive with stateoftheart methods while using much fewer primitives code is available at,0
the current stateoftheart video generative models can produce commercialgrade videos with highly realistic details however they still struggle to coherently present multiple sequential events in the stories specified by the prompts which is foreseeable an essential capability for future long video generation scenarios for example top t2v generative models still fail to generate a video of the short simple story how to put an elephant into a refrigerator while existing detailoriented benchmarks primarily focus on finegrained metrics like aesthetic quality and spatialtemporal consistency they fall short of evaluating models abilities to handle eventlevel story presentation to address this gap we introduce storyeval a storyoriented benchmark specifically designed to assess texttovideo t2v models storycompletion capabilities storyeval features 423 prompts spanning 7 classes each representing short stories composed of 24 consecutive events we employ advanced visionlanguage models such as gpt4v and llavaovchat72b to verify the completion of each event in the generated videos applying a unanimous voting method to enhance reliability our methods ensure high alignment with human evaluations and the evaluation of 11 models reveals its challenge with none exceeding an average storycompletion rate of 50 storyeval provides a new benchmark for advancing t2v models and highlights the challenges and opportunities in developing nextgeneration solutions for coherent storydriven video generation,0
generating fullbody human gestures based on speech signals remains challenges on quality and speed existing approaches model different body regions such as body legs and hands separately which fail to capture the spatial interactions between them and result in unnatural and disjointed movements additionally their autoregressivediffusionbased pipelines show slow generation speed due to dozens of inference steps to address these two challenges we propose gesturelsm a flowmatchingbased approach for cospeech gesture generation with spatialtemporal modeling our method i explicitly model the interaction of tokenized body regions through spatial and temporal attention for generating coherent fullbody gestures ii introduce the flow matching to enable more efficient sampling by explicitly modeling the latent velocity space to overcome the suboptimal performance of flow matching baseline we propose latent shortcut learning and beta distribution time stamp sampling during training to enhance gesture synthesis quality and accelerate inference combining the spatialtemporal modeling and improved flow matchingbased framework gesturelsm achieves stateoftheart performance on beat2 while significantly reducing inference time compared to existing methods highlighting its potential for enhancing digital humans and embodied agents in realworld applications project page,0
this paper proposes a fast and unsupervised scheme for the polygonal approximation of a closed digital curve it is demonstrated that the approximation scheme is faster than stateoftheart approximation and is competitive with rosins measure and aesthetic aspects the scheme comprises of three phases initial segmentation iterative vertex insertion iterative merging and vertex adjustment the initial segmentation is used to detect sharp turns that is vertices that seemingly have high curvature it is likely that some of the important vertices with low curvature might have been missed in the first phase therefore iterative vertex insertion is used to add vertices in a region where the curvature changes slowly but steadily the initial phase may pick up some undesirable vertices and thus merging is used to eliminate redundant vertices finally vertex adjustment was used to enhance the aesthetic appearance of the approximation the quality of the approximations was measured using the rosins method the robustness of the proposed scheme with respect to geometric transformation was observed,0
coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as digital twin city metaverse lods etc such piecewise planar models can be abstracted as twins from 3d dense reconstructions however these models typically lack realistic texture relative to the real building or scene making them unsuitable for vivid display or direct reference in this paper we present twintex the first automatic texture mapping framework to generate a photorealistic texture for a piecewise planar proxy our method addresses most challenges occurring in such twin texture generation specifically for each primitive plane we first select a small set of photos with greedy heuristics considering photometric quality perspective quality and facade texture completeness then different levels of line features lols are extracted from the set of selected photos to generate guidance for later steps with lols we employ optimization algorithms to align texture with geometry from local to global finally we finetune a diffusion model with a multimask initialization component and a new dataset to inpaint the missing region experimental results on many buildings indoor scenes and manmade objects of varying complexity demonstrate the generalization ability of our algorithm our approach surpasses stateoftheart texture mapping methods in terms of highfidelity quality and reaches a humanexpert production level with much less effort project page,0
weve built a webbased tool for the realtime interaction with loci of poncelet triangle families our initial goals were to facilitate exploratory detection of geometric properties of such families during frequent walks in my neighborhood it appeared to me poncelet loci shared a palette of motifs with those found in wrought iron gates at the entrance of many a residential building as a result i started to look at poncelet loci aesthetically a kind of generative art features were gradually added to the tool with the sole purpose of beautifying the output hundreds of interesting loci were subsequently collected into an online gallery with some further enhanced by a graphic designer we will tour some of these byproducts here an interesting question is if poncelet loci could serve as the basis for future metalwork andor architectural designs,0
this is a short technical report describing the winning entry of the physicsiq challenge presented at the perception test workshop at iccv 2025 stateoftheart video generative models exhibit severely limited physical understanding and often produce implausible videos the physics iq benchmark has shown that visual realism does not imply physics understanding yet intuitive physics understanding has shown to emerge from ssl pretraining on natural videos in this report we investigate whether we can leverage sslbased video world models to improve the physics plausibility of video generative models in particular we build ontop of the stateoftheart video generative model magi1 and couple it with the recently introduced video joint embedding predictive architecture 2 vjepa2 to guide the generation process we show that by leveraging vjepa2 as reward signal we can improve the physics plausibility of stateoftheart video generative models by 6,0
the art of instrument performance stands as a vivid manifestation of human creativity and emotion nonetheless generating instrument performance motions is a highly challenging task as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performerinstrument interaction while existing works primarily focus on modeling partial body motions we propose expressive cello performance motion generation for audio rendition elgar a stateoftheart diffusionbased framework for wholebody finegrained instrument performance motion generation solely from audio to emphasize the interactive nature of the instrument performance we introduce hand interactive contact loss hicl and bow interactive contact loss bicl which effectively guarantee the authenticity of the interplay moreover to better evaluate whether the generated motions align with the semantic context of the music audio we design novel metrics specifically for string instrument performance motion generation including fingercontact distance bowstring distance and bowing score extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods in addition we put forward a motion generation dataset spdgen collated and normalized from the mocap dataset spd as demonstrated elgar has shown great potential in generating instrument performance motions with complicated and fast interactions which will promote further development in areas such as animation music education interactive art creation etc,0
dance is an important human art form but creating new dances can be difficult and timeconsuming in this work we introduce editable dance generation edge a stateoftheart method for editable dance generation that is capable of creating realistic physicallyplausible dances while remaining faithful to the input music edge uses a transformerbased diffusion model paired with jukebox a strong music feature extractor and confers powerful editing capabilities wellsuited to dance including jointwise conditioning and inbetweening we introduce a new metric for physical plausibility and evaluate dance quality generated by our method extensively through 1 multiple quantitative metrics on physical plausibility beat alignment and diversity benchmarks and more importantly 2 a largescale user study demonstrating a significant improvement over previous stateoftheart methods qualitative samples from our model can be found at our website,0
mapping music to dance is a challenging problem that requires spatial and temporal coherence along with a continual synchronization with the musics progression taking inspiration from large language models we introduce a 2step approach for generating dance using a vector quantizedvariational autoencoder vqvae to distill motion into primitives and train a transformer decoder to learn the correct sequencing of these primitives we also evaluate the importance of music representations by comparing naive music feature extraction using librosa to deep audio representations generated by stateoftheart audio compression algorithms additionally we train variations of the motion generator using relative and absolute positional encodings to determine the effect on generated motion quality when generating arbitrarily long sequence lengths our proposed approach achieve stateoftheart results in musictomotion generation benchmarks and enables the realtime generation of considerably longer motion sequences the ability to chain multiple motion sequences seamlessly and easy customization of motion sequences to meet style requirements,0
generative adversarial networks gans have established themselves as a prevalent approach to image synthesis of these stylegan offers a fascinating case study owing to its remarkable visual quality and an ability to support a large array of downstream tasks this stateoftheart report covers the stylegan architecture and the ways it has been employed since its conception while also analyzing its severe limitations it aims to be of use for both newcomers who wish to get a grasp of the field and for more experienced readers that might benefit from seeing current research trends and existing tools laid out among stylegans most interesting aspects is its learned latent space despite being learned with no supervision it is surprisingly wellbehaved and remarkably disentangled combined with stylegans visual quality these properties gave rise to unparalleled editing capabilities however the control offered by stylegan is inherently limited to the generators learned distribution and can only be applied to images generated by stylegan itself seeking to bring stylegans latent control to realworld scenarios the study of gan inversion and latent space embedding has quickly gained in popularity meanwhile this same study has helped shed light on the inner workings and limitations of stylegan we map out stylegans impressive story through these investigations and discuss the details that have made stylegan the goto generator we further elaborate on the visual priors stylegan constructs and discuss their use in downstream discriminative tasks looking forward we point out stylegans limitations and speculate on current trends and promising directions for future research such as task and target specific finetuning,0
artificial intelligence ai models are prevalent today and provide a valuable tool for artists however a lesserknown artifact that comes with ai models that is not always discussed is the glitch glitches occur for various reasons sometimes they are known and sometimes they are a mystery artists who use ai models to generate art might not understand the reason for the glitch but often want to experiment and explore novel ways of augmenting the output of the glitch this paper discusses some of the questions artists have when leveraging the glitch in ai art production it explores the unexpected positive outcomes produced by glitches in the specific context of motion capture and performance art,0
data driven and learning based solutions for modeling dynamic garments have significantly advanced especially in the context of digital humans however existing approaches often focus on modeling garments with respect to a fixed parametric human body model and are limited to garment geometries that were seen during training in this work we take a different approach and model the dynamics of a garment by exploiting its local interactions with the underlying human body specifically as the body moves we detect local garmentbody collisions which drive the deformation of the garment at the core of our approach is a meshagnostic garment representation and a manifoldaware transformer network design which together enable our method to generalize to unseen garment and body geometries we evaluate our approach on a wide variety of garment types and motion sequences and provide competitive qualitative and quantitative results with respect to the state of the art,0
in this paper we introduce a music conditioned 3d dance generation model named midget based on dance motion vector quantised variational autoencoder vqvae model and motion generative pretraining gpt model to generate vibrant and highquality dances that match the music rhythm to tackle challenges in the field we introduce three new components 1 a pretrained memory codebook based on the motion vqvae model to store different human pose codes 2 employing motion gpt model to generate pose codes with music and motion encoders 3 a simple framework for music feature extraction we compare with existing stateoftheart models and perform ablation experiments on aist the largest publicly available musicdance dataset experiments demonstrate that our proposed framework achieves stateoftheart performance on motion quality and its alignment with the music,0
in various fields including medicine age distributions are crucial despite widespread media coverage of health topics there remains a need to enhance health communication narrative medical visualization is promising for improving information comprehension and retention this study explores the most effective ways to present age distributions of diseases through narrative visualizations we conducted a thorough analysis of existing visualizations held workshops with a broad audience and reviewed relevant literature from this we identified design choices focusing on comprehension aesthetics engagement and memorability we specifically tested three pictogram variants pictograms as bars stacked pictograms and annotations after evaluating 18 visualizations with 72 participants and three expert reviews we determined that annotations were most effective for comprehension and aesthetics however traditional bar charts were preferred for engagement and other variants were more memorable the study provides a set of design recommendations based on these insights,0
in this paper we propose a new method for mapping a 3d point cloud to the latent space of a 3d generative adversarial network our generative model for 3d point clouds is based on spgan a stateoftheart sphereguided 3d point cloud generator we derive an efficient way to encode an input 3d point cloud to the latent space of the spgan our point cloud encoder can resolve the point ordering issue during inversion and thus can determine the correspondences between points in the generated 3d point cloud and those in the canonical sphere used by the generator we show that our method outperforms previous gan inversion methods for 3d point clouds achieving stateoftheart results both quantitatively and qualitatively our code is available at,0
diffusion models have emerged as the stateoftheart for image generation among other tasks here we present an efficient diffusionbased model for 3daware generation of neural fields our approach preprocesses training data such as shapenet meshes by converting them to continuous occupancy fields and factoring them into a set of axisaligned triplane feature representations thus our 3d training scenes are all represented by 2d feature planes and we can directly train existing 2d diffusion models on these representations to generate 3d neural fields with high quality and diversity outperforming alternative approaches to 3daware generation our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model we demonstrate stateoftheart results on 3d generation on several object classes from shapenet,0
this paper takes on the problem of transferring the style of cartoon images to reallife photographic images by implementing previous work done by cartoongan we trained a generative adversial networkgan on over 60 000 images from works by hayao miyazaki at studio ghibli to evaluate our results we conducted a qualitative survey comparing our results with two stateoftheart methods 117 survey results indicated that our model on average outranked stateoftheart methods on cartoonlikeness,0
unsupervised generation of highquality multiviewconsistent images and 3d shapes using only collections of singleview 2d photographs has been a longstanding challenge existing 3d gans are either computeintensive or make approximations that are not 3dconsistent the former limits quality and resolution of the generated images and the latter adversely affects multiview consistency and shape quality in this work we improve the computational efficiency and image quality of 3d gans without overly relying on these approximations we introduce an expressive hybrid explicitimplicit network architecture that together with other design choices synthesizes not only highresolution multiviewconsistent images in real time but also produces highquality 3d geometry by decoupling feature generation and neural rendering our framework is able to leverage stateoftheart 2d cnn generators such as stylegan2 and inherit their efficiency and expressiveness we demonstrate stateoftheart 3daware synthesis with ffhq and afhq cats among other experiments,0
in this paper we introduce local logo generative adversarial network llgan that uses regional features extracted from faster rcnn for logo generation we demonstrate the strength of this approach by training the framework on a small stylerich dataset of real heavy metal logos to generate new ones llgan achieves inception score of 529 and frechet inception distance of 22394 improving on stateoftheart models stylegan2 and selfattention gan,0
in this paper we propose a technique for making humans in photographs protrude like reliefs unlike previous methods which mostly focus on the face and head our method aims to generate art works that describe the whole body activity of the character one challenge is that there is no groundtruth for supervised deep learning we introduce a sigmoid variant function to manipulate gradients tactfully and train our neural networks by equipping with a loss function defined in gradient domain the second challenge is that actual photographs often across different light conditions we used imagebased rendering technique to address this challenge and acquire rendering images and depth data under different lighting conditions to make a clear division of labor in network modules a twoscale architecture is proposed to create highquality relief from a single photograph extensive experimental results on a variety of scenes show that our method is a highly effective solution for generating digital 25d artwork from photographs,0
we introduce vertexregen a novel mesh generation framework that enables generation at a continuous level of detail existing autoregressive methods generate meshes in a partialtocomplete manner and thus intermediate steps of generation represent incomplete structures vertexregen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse ie vertex split learned through a generative model experimental results demonstrate that vertexregen produces meshes of comparable quality to stateoftheart methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail,0
for the classic aesthetic interpolation problem we propose an entirely new thought apply the golden section for how to apply the golden section to interpolation methods we present three examples the golden step interpolation the golden piecewise linear interpolation and the golden curve interpolation which respectively deal with the applications of golden section in the interpolation of degree 0 1 and 2 in the plane in each example we present our basic ideas the specific methods comparative examples and applications and relevant criteria and it is worth mentioning that for aesthetics we propose two novel concepts the golden cuspidal hill and the golden domed hill this paper aims to provide the reference for the combination of golden section and interpolation and stimulate more and better related researches,0
procedural models ie symbolic programs that output visual data are a historicallypopular method for representing graphics content vegetation buildings textures etc they offer many advantages interpretable design parameters stochastic variations highquality outputs compact representation and more but they also have some limitations such as the difficulty of authoring a procedural model from scratch more recently aibased methods and especially neural networks have become popular for creating graphic content these techniques allow users to directly specify desired properties of the artifact they want to create via examples constraints or objectives while a search optimization or learning algorithm takes care of the details however this ease of use comes at a cost as its often hard to interpret or manipulate these representations in this stateoftheart report we summarize research on neurosymbolic models in computer graphics methods that combine the strengths of both ai and symbolic programs to represent generate and manipulate visual data we survey recent work applying these techniques to represent 2d shapes 3d shapes and materials textures along the way we situate each prior work in a unified design space for neurosymbolic models which helps reveal underexplored areas and opportunities for future research,0
we introduce mggen a framework that generates motion graphics directly from a single raster image mggen decompose a single raster image into layered structures represented as html generate animation scripts for each layer and then render them into a video experiments confirm mggen generates dynamic motion graphics while preserving text readability and fidelity to the input conditions whereas stateoftheart imagetovideo generation methods struggle with them the code is available at,0
the diffusion model is a stateoftheart generative model that generates an image by applying a neural network iteratively moreover this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation based on the analysis of the truncation error of the diffusion ode and sde our study proposes a trainingfree algorithm that generates highquality 512 x 512 and 1024 x 1024 images in eight steps with flexible guidance scales to the best of my knowledge our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an fid performance comparable to that of the latest distillation model but without additional training meanwhile our algorithm can also generate a 512 x 512 image in 8 steps and its fid performance is better than the inference result using stateoftheart ode solver dpm 2m in 20 steps we validate our eightstep image generation algorithm using the coco 2014 coco 2017 and laion datasets and our best fid performance is 157 2235 and 1752 while the fid performance of dpm2m is 173 2375 and 1733 further it also outperforms the stateoftheart amedplugin solver whose fid performance is 1907 2550 and 1806 we also apply the algorithm in fivestep inference without additional training for which the best fid performance in the datasets mentioned above is 1918 2324 and 1961 respectively and is comparable to the performance of the stateoftheart amed pulgin solver in eight steps sdxlturbo in four steps and the stateoftheart diffusion distillation model flash diffusion in five steps we also validate our algorithm in synthesizing 1024 1024 images within 6 steps whose fid performance only has a limited distance to the latest distillation algorithm the code is in repo,0
we introduce a framework for the generation of gridshell structures that is based on voronoi diagrams and allows us to design tessellations that achieve excellent static performances we start from an analysis of stress on the input surface and we use the resulting tensor field to induce an anisotropic noneuclidean metric over it then we compute a centroidal voronoi tessellation under the same metric the resulting mesh is hexdominant and made of cells with a variable density which depends on the amount of stress and anisotropic shape which depends on the direction of maximum stress this mesh is further optimized taking into account symmetry and regularity of cells to improve aesthetics we demonstrate that our gridshells achieve better static performances with respect to quadbased grid shells while offering an innovative and aesthetically pleasing look,0
to bridge the gap between artists and nonspecialists we present a unified framework neuralpolyptych to facilitate the creation of expansive highresolution paintings by seamlessly incorporating interactive handdrawn sketches with fragments from original paintings we have designed a multiscale ganbased architecture to decompose the generation process into two parts each responsible for identifying global and local features to enhance the fidelity of semantic details generated from users sketched outlines we introduce a correspondence attention module utilizing our reference bank strategy this ensures the creation of highquality intricately detailed elements within the artwork the final result is achieved by carefully blending these local elements while preserving coherent global consistency consequently this methodology enables the production of digital paintings at megapixel scale accommodating diverse artistic expressions and enabling users to recreate content in a controlled manner we validate our approach to diverse genres of both eastern and western paintings applications such as large painting extension texture shuffling genre switching mural art restoration and recomposition can be successfully based on our framework,0
vector graphics are widely used in digital art and valued by designers for their scalability and layerwise topological properties however the creation and editing of vector graphics necessitate creativity and design expertise leading to a timeconsuming process in this paper we propose a novel pipeline that generates highquality customized vector graphics based on textual prompts while preserving the properties and layerwise information of a given exemplar svg our method harnesses the capabilities of large pretrained texttoimage models by finetuning the crossattention layers of the model we generate customized raster images guided by textual prompts to initialize the svg we introduce a semanticbased path alignment method that preserves and transforms crucial paths from the exemplar svg additionally we optimize path parameters using both imagelevel and vectorlevel losses ensuring smooth shape deformation while aligning with the customized raster image we extensively evaluate our method using multiple metrics from vectorlevel imagelevel and textlevel perspectives the evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality the project page is,0
we investigate the use of 2d blackandwhite textures for the visualization of categorical data and contribute a summary of texture attributes and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures blackandwhite textures are useful for instance as a visual channel for categorical data on lowcolor displays in 2d3d print to achieve the aesthetic of historic visualizations or to retain the color hue channel for other visual mappings we specifically study how to use what we call geometric and iconic textures geometric textures use patterns of repeated abstract geometric shapes while iconic textures use repeated icons that may stand for data categories we parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters 30 visualization experts used our tool and designed 66 textured bar charts pie charts and maps we then had 150 participants rate these designs for aesthetics finally with the toprated geometric and iconic textures our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as nontextured charts and that there are some differences depending on the type of chart,0
engineering sketches form the 2d basis of parametric computeraided design cad the foremost modeling paradigm for manufactured objects in this paper we tackle the problem of learning based engineering sketch generation as a first step towards synthesis and composition of parametric cad models we propose two generative models curvegen and turtlegen for engineering sketch generation both models generate curve primitives without the need for a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3d cad modeling operations we find in our perceptual evaluation using human subjects that both curvegen and turtlegen produce more realistic engineering sketches when compared with the current stateoftheart for engineering sketch generation,0
when obtaining interior 3d voxel data from triangular meshes most existing methods fail to handle low quality meshes which happens to take up a big portion on the internet in this work we present a robust voxelization method that is based on tetrahedral mesh generation within a user defined error bound comparing to other tetrahedral mesh generation methods our method produces much higher quality tetrahedral meshes as the intermediate outcome which allows us to utilize a faster voxelization algorithm that is based on a stronger assumption we show the results comparing to various methods including the stateoftheart our contribution includes a framework which takes triangular mesh as an input and produces voxelized data a proof to an unproved algorithm that performs better than the stateoftheart and various experiments including parallelization built on the gpu and cpu we further tested our method on various dataset including princeton modelnet and thingi10k to show the robustness of the framework where near 100 availability is achieved while others can only achieve around 50,0
the generative adversarial network gan has recently been applied to generate synthetic images from text despite significant advances most current stateoftheart algorithms are regulargrid region based when attention is used it is mainly applied between individual regulargrid regions and a word these approaches are sufficient to generate images that contain a single object in its foreground such as a bird or flower however natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image therefore the regulargrid based image attention weights may not necessarily concentrate on the intended foreground regions which in turn results in an unnatural looking image additionally individual words such as a blue and shirt do not necessarily provide a full visual context unless they are applied together for this reason in our paper we proposed a novel method in which we introduced an additional set of attentions between truegrid regions and word phrases the truegrid region is derived using a set of auxiliary bounding boxes these auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases word phrases are derived from analysing partofspeech pos results we perform experiments on this novel network architecture using the microsoft common objects in context mscoco dataset and the model generates 256 times 256 conditioned on a short sentence description our proposed approach is capable of generating more realistic images compared with the current stateoftheart algorithms,0
we introduce gaudi a generative model capable of capturing the distribution of complex and realistic 3d scenes that can be rendered immersively from a moving camera we tackle this challenging problem with a scalable yet powerful approach where we first optimize a latent representation that disentangles radiance fields and camera poses this latent representation is then used to learn a generative model that enables both unconditional and conditional generation of 3d scenes our model generalizes previous works that focus on single objects by removing the assumption that the camera pose distribution can be shared across samples we show that gaudi obtains stateoftheart performance in the unconditional generative setting across multiple datasets and allows for conditional generation of 3d scenes given conditioning variables like sparse image observations or text that describes the scene,0
we investigated the use of virtual technologies to digitise and enhance cultural heritage ch aligning with open science and fair principles through case studies in museums we developed reproducible workflows 3d models and tools fostering accessibility inclusivity and sustainability of ch applications include interdisciplinary research educational innovation and ch preservation,0
recent texttoimage models have achieved impressive results however since they require largescale datasets of textimage pairs it is impractical to train them on new domains where data is scarce or not labeled in this work we propose using largescale retrieval methods in particular efficient knearestneighbors knn which offers novel capabilities 1 training a substantially small and efficient texttoimage diffusion model without any text 2 generating outofdistribution images by simply swapping the retrieval database at inference time and 3 performing textdriven local semantic manipulations while preserving object identity to demonstrate the robustness of our method we apply our knn approach on two stateoftheart diffusion backbones and show results on several different datasets as evaluated by human studies and automatic metrics our method achieves stateoftheart results compared to existing approaches that train texttoimage generation models using images only without paired text data,0
we present neural head avatars a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in arvr or other applications in the movie or games industry that rely on a digital human our representation can be learned from a monocular rgb portrait video that features a range of different expressions and views specifically we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face and two feedforward networks predicting vertex offsets of the underlying mesh as well as a view and expressiondependent texture we demonstrate that this representation is able to accurately extrapolate to unseen poses and view points and generates natural expressions while providing sharp texture details compared to previous works on head avatars our method provides a disentangled shape and appearance model of the complete human head including hair that is compatible with the standard graphics pipeline moreover it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novelview synthesis,0
we propose a new generative model for layout generation we generate layouts in three steps first we generate the layout elements as nodes in a layout graph second we compute constraints between layout elements as edges in the layout graph third we solve for the final layout using constrained optimization for the first two steps we build on recent transformer architectures the layout optimization implements the constraints efficiently we show three practical contributions compared to the state of the art our work requires no user input produces higher quality layouts and enables many novel capabilities for conditional layout generation,0
advances in 3daware generative models have pushed the boundary of image synthesis with explicit camera control to achieve highresolution image synthesis several attempts have been made to design efficient generators such as hybrid architectures with both 3d and 2d components however such a design compromises multiview consistency and the design of a pure 3d generator with high resolution is still an open problem in this work we present generative volumetric primitives gvp the first pure 3d generative model that can sample and render 512resolution images in realtime gvp jointly models a number of volumetric primitives and their spatial information both of which can be efficiently generated via a 2d convolutional network the mixture of these primitives naturally captures the sparsity and correspondence in the 3d volume the training of such a generator with a high degree of freedom is made possible through a knowledge distillation technique experiments on several datasets demonstrate superior efficiency and 3d consistency of gvp over the stateoftheart,0
generating multiview images based on text or singleimage prompts is a critical capability for the creation of 3d content two fundamental questions on this topic are what data we use for training and how to ensure multiview consistency this paper introduces a novel framework that makes fundamental contributions to both questions unlike leveraging images from 2d diffusion models for training we propose a dense consistent multiview generation model that is finetuned from offtheshelf video generative models images from video generative models are more suitable for multiview generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency moreover the video data sets used to train these models are abundant and diverse leading to a reduced trainfinetuning domain gap to enhance multiview consistency we introduce a 3daware denoising sampling which first employs a feedforward reconstruction module to get an explicit global 3d model and then adopts a sampling strategy that effectively involves images rendered from the global 3d model into the denoising sampling loop to improve the multiview consistency of the final images as a byproduct this module also provides a fast way to create 3d assets represented by 3d gaussians within a few seconds our approach can generate 24 dense views and converges much faster in training than stateoftheart approaches 4 gpu hours versus many thousand gpu hours with comparable visual quality and consistency by further finetuning our approach outperforms existing stateoftheart methods in both quantitative metrics and visual effects our project page is aigc3dgithubiovideomv,0
automatic 3d content creation has gained increasing attention recently due to its potential in various applications such as video games film industry and arvr recent advancements in diffusion models and multimodal models have notably improved the quality and efficiency of 3d object generation given a single rgb image however 3d objects generated even by stateoftheart methods are still unsatisfactory compared to humancreated assets considering only textures instead of materials makes these methods encounter challenges in photorealistic rendering relighting and flexible appearance editing and they also suffer from severe misalignment between geometry and highfrequency texture details in this work we propose a novel approach to boost the quality of generated 3d objects from the perspective of physicsbased rendering pbr materials by analyzing the components of pbr materials we choose to consider albedo roughness metalness and bump maps for albedo and bump maps we leverage stable diffusion finetuned on synthetic data to extract these values with novel usages of these finetuned models to obtain 3d consistent albedo uv and bump uv for generated objects in terms of roughness and metalness maps we adopt a semiautomatic process to provide room for interactive adjustment which we believe is more practical extensive experiments demonstrate that our model is generally beneficial for various stateoftheart generation methods significantly boosting the quality and realism of their generated 3d objects with natural relighting effects and substantially improved geometry,0
recent texttoimage diffusion models are able to generate convincing results of unprecedented quality however it is nearly impossible to control the shapes of different regionsobjects or their layout in a finegrained fashion previous attempts to provide such controls were hindered by their reliance on a fixed set of labels to this end we present spatext a new method for texttoimage generation using openvocabulary scene control in addition to a global text prompt that describes the entire scene the user provides a segmentation map where each region of interest is annotated by a freeform natural language description due to lack of largescale datasets that have a detailed textual description for each region in the image we choose to leverage the current largescale texttoimage datasets and base our approach on a novel clipbased spatiotextual representation and show its effectiveness on two stateoftheart diffusion models pixelbased and latentbased in addition we show how to extend the classifierfree guidance method in diffusion models to the multiconditional case and present an alternative accelerated inference algorithm finally we offer several automatic evaluation metrics and use them in addition to fid scores and a user study to evaluate our method and show that it achieves stateoftheart results on image generation with freeform textual scene control,0
since 2014 we have been conducting experiments based on a multidisciplinary collaboration between specialists in theatrical staging and researchers in virtual reality digital art and video games this team focused its work on the similarities and differencesthat exist between real physical actors actorperformers and virtual digital actors avatars from this multidisciplinary approach experimental researchcreation projects have emerged and rely on a physical actor playing with the image of an avatar controlled by another physical actor via the intermediary of a lowcost motioncapture system in the first part of the paper we will introduce the scenographic design on which our presentation is based and the modifications we have made in relation to our previous work next in the second section we will discuss in detail the impact of augmenting the players game using an avatar compared to the scenic limitations of the theatrical stage in part three of the paper we will discuss the softwarerelated aspects of the project focusing on exchanges between the different components of our design and describing the algorithms enabling us to utilize the realtime movement of a player via various capture devices to conclude we will examine in detail how our experimental system linking physical actors and avatars profoundly alters the nature of collaboration between directors actors and digital artists in terms of actoravatar direction,0
transforming casually captured monocular videos into fully immersive dynamic experiences is a highly illposed task and comes with significant challenges eg reconstructing unseen regions and dealing with the ambiguity in monocular depth estimation in this work we introduce bulletgen an approach that takes advantage of generative models to correct errors and complete missing information in a gaussianbased dynamic scene representation this is done by aligning the output of a diffusionbased video generation model with the 4d reconstruction at a single frozen bullettime step the generated frames are then used to supervise the optimization of the 4d gaussian model our method seamlessly blends generative content with both static and dynamic scene components achieving stateoftheart results on both novelview synthesis and 2d3d tracking tasks,0
diffusion models have been popular for point cloud generation tasks existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution however the reverse diffusion process can produce samples with nonsmooth points on the surface because of the ignorance of the point cloud geometric properties we propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds outperforming multiple stateoftheart methods,0
with virtual reality digital painting on 2d canvases is now being extended to 3d spaces tilt brush and oculus quill are widely accepted among artists as tools that pave the way to a new form of art 3d emmersive painting current 3d painting systems are only a start emitting textured triangular geometries in this paper we advance this new art of 3d painting to 3d volumetric painting that enables an artist to draw a huge scene with full control of spatial color fields inspired by the fact that 2d paintings often use vast space to paint background and small but detailed space for foreground we claim that supporting a large canvas in varying detail is essential for 3d painting in order to help artists focus and audiences to navigate the large canvas space we provide small artistdefined areas called rooms that serve as beacons for artistsuggested scales spaces locations for intended appreciation view of the painting artists and audiences can easily transport themselves between different rooms technically our canvas is represented as an array of deep octrees of depth 24 or higher built on cpu for volume painting and on gpu for volume rendering using accurate ray casting in cpu side we design an efficient iterative algorithm to refine or coarsen octree as a result of volumetric painting strokes at highly interactive rates and update the corresponding gpu textures then we use gpubased ray casting algorithms to render the volumetric painting result we explore precision issues stemming from raycasting the octree of high depth and provide a new analysis and verification from our experimental results as well as the positive feedback from the participating artists we strongly believe that our new 3d volume painting system can open up a new possibility for vrdriven digital art medium to professional artists as well as to novice users,0
the novel volumeenclosing surface extraction algorithm vesta generates triangular isosurfaces from computed tomography volumetric images andor threedimensional 3d simulation data here we present various benchmarks for gpubased code implementations of both vesta and the current stateoftheart marching cubes algorithm mca one major result of this study is that vesta runs significantly faster than the mca,0
this article discusses the study of 3d graphic volume primitive computer system generation 3d segments based on general purpose graphics processing unit gpgpu technology for 3d volume visualization systems it is based on the general method of volume 3d primitive generation and an algorithm for the voxelization of 3d lines previously proposed and studied by the authors we considered the compute unified device architect cuda implementation of a parametric method for generating 3d line segments and characteristics of generation on modern graphics processing units experiments on the test bench showed the relative inefficiency of generating a single 3d line segment and the efficiency of generating both fixed and arbitrary length of 3d segments on a graphics processing unit gpu experimental studies have proven the effectiveness and the quality of produced solutions by our method when compared to existing stateoftheart approaches,0
we present a new fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models our method operates on a topdown imagebased representation and inserts objects iteratively into the scene by predicting their category location orientation and size with separate neural network modules our pipeline naturally supports automatic completion of partial scenes as well as synthesis of complete scenes our method is significantly faster than the previous imagebased method and generates result that outperforms it and other stateoftheart deep generative scene models in terms of faithfulness to training data and perceived visual quality,0
recent research works have focused on generating human models and garments from their 2d images however stateoftheart researches focus either on only a single layer of the garment on a human model or on generating multiple garment layers without any guarantee of the intersectionfree geometric relationship between them in reality people wear multiple layers of garments in their daily life where an inner layer of garment could be partially covered by an outer one in this paper we try to address this multilayer modeling problem and propose the layeredgarment net lgn that is capable of generating intersectionfree multiple layers of garments defined by implicit function fields over the body surface given the persons near frontview image with a special design of garment indication fields gif we can enforce an implicit covering relationship between the signed distance fields sdf of different layers to avoid selfintersections among different garment surfaces and the human body experiments demonstrate the strength of our proposed lgn framework in generating multilayer garments as compared to stateoftheart methods to the best of our knowledge lgn is the first research work to generate intersectionfree multiple layers of garments on the human body from a single image,0
a deep generative model such as a gan learns to model a rich set of semantic and physical rules about the target distribution but up to now it has been obscure how such rules are encoded in the network or how a rule could be changed in this paper we introduce a new problem setting manipulation of specific rules encoded by a deep generative model to address the problem we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory we derive an algorithm for modifying one entry of the associative memory and we demonstrate that several interesting structural rules can be located and modified within the layers of stateoftheart generative models we present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects and we show several proofofconcept applications finally results on multiple datasets demonstrate the advantage of our method against standard finetuning methods and edit transfer algorithms,0
3d shape generation aims to produce innovative 3d content adhering to specific conditions and constraints existing methods often decompose 3d shapes into a sequence of localized components treating each element in isolation without considering spatial consistency as a result these approaches exhibit limited versatility in 3d data representation and shape generation hindering their ability to generate highly diverse 3d shapes that comply with the specified constraints in this paper we introduce a novel spatialaware 3d shape generation framework that leverages 2d plane representations for enhanced 3d shape modeling to ensure spatial coherence and reduce memory usage we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3d shape using orthogonal 2d planes additionally we meticulously enforce spatial correspondences across distinct planes using a transformerbased autoencoder structure promoting the preservation of spatial relationships in the generated 3d shapes this yields an algorithm that consistently outperforms stateoftheart 3d shape generation methods on various tasks including unconditional shape generation multimodal shape completion singleview reconstruction and texttoshape synthesis our project page is available at,0
digital preservation of the cultural heritages is one of the major applications of various computer graphics and vision algorithms the advancement in the arvr technologies is giving the cultural heritage preservation an interesting spin due to its immense visualization ability the use of these technologies to digitally recreate heritage sites and art is becoming a popular trend a project called indian digital heritage idh for recreating the heritage site of hampi karnataka during vijaynagara empire 1336 1646 ce has been initiated by the department of science and technology dst few years back immense work on surveying the site collecting geographical and historic information about the life in hampi creating 3d models for buildings and people of hampi and many other related tasks has been undertaken by various participants of this project a major part of this project is to make tourists visiting hampi visualize the life of people in ancient hampi through any handy device with such a requirement the mobile ar based platform becomes a natural choice for developing any application for this purpose we contributed to the project by developing an ar based mobile application to recreate a scene from virupaksha bazaar of hampi with two components author scene with augmented virtual contents at any scale and visualize the same scene by reaching the physical location of augmentation we develop an interactive application for the purpose of digitally recreating ancient hampi though the focus of this work is not creating any static or dynamic content from scratch it shows an interesting application of the content created in a real world scenario,0
we present a ganbased transformer for general actionconditioned 3d human motion generation including not only singleperson actions but also multiperson interactive actions our approach consists of a powerful actionconditioned motion transformer actformer under a gan training scheme equipped with a gaussian process latent prior such a design combines the strong spatiotemporal representation capacity of transformer superiority in generative modeling of gan and inherent temporal correlations from the latent prior furthermore actformer can be naturally extended to multiperson motions by alternately modeling temporal correlations and human interactions with transformer encoders to further facilitate research on multiperson motion generation we introduce a new synthetic dataset of complex multiperson combat behaviors extensive experiments on ntu13 ntu rgbd 120 babel and the proposed combat dataset show that our method can adapt to various human motion representations and achieve superior performance over the stateoftheart methods on both singleperson and multiperson motion generation tasks demonstrating a promising step towards a general human motion generator,0
in recent years video generation has become a prominent generative tool and has drawn significant attention however there is little consideration in audiotovideo generation though audio contains unique qualities like temporal semantics and magnitude hence we propose the power of sound tpos model to incorporate audio input that includes both changeable temporal semantics and magnitude to generate video frames tpos utilizes a latent stable diffusion model with textual semantic information which is then guided by the sequential audio embedding from our pretrained audio encoder as a result this method produces audio reactive video contents we demonstrate the effectiveness of tpos across various tasks and compare its results with current stateoftheart techniques in the field of audiotovideo generation more examples are available at,0
face portrait line drawing is a unique style of art which is highly abstract and expressive however due to its high semantic constraints many existing methods learn to generate portrait drawings using paired training data which is costly and timeconsuming to obtain in this paper we propose a novel method to automatically transform face photos to portrait drawings using unpaired training data with two new features ie our method can 1 learn to generate high quality portrait drawings in multiple styles using a single network and 2 generate portrait drawings in a new style unseen in the training data to achieve these benefits we 1 propose a novel quality metric for portrait drawings which is learned from human perception and 2 introduce a quality loss to guide the network toward generating better looking portrait drawings we observe that existing unpaired translation methods such as cyclegan tend to embed invisible reconstruction information indiscriminately in the whole drawings due to significant information imbalance between the photo and portrait drawing domains which leads to important facial features missing to address this problem we propose a novel asymmetric cycle mapping that enforces the reconstruction information to be visible and only embedded in the selected facial regions along with localized discriminators for important facial regions our method well preserves all important facial features in the generated drawings generator dissection further explains that our model learns to incorporate face semantic information during drawing generation extensive experiments including a user study show that our model outperforms stateoftheart methods,0
human mesh recovery hmr is crucial in many computer vision applications from health to arts and entertainment hmr from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2d image however hmr from a single image is an illposed problem due to depth ambiguity and occlusions probabilistic methods have attempted to address this by generating and fusing multiple plausible 3d reconstructions but their performance has often lagged behind deterministic approaches in this paper we introduce genhmr a novel generative framework that reformulates monocular hmr as an imageconditioned generative task explicitly modeling and mitigating uncertainties in the 2dto3d mapping process genhmr comprises two key components 1 a pose tokenizer to convert 3d human poses into a sequence of discrete tokens in a latent space and 2 an imageconditional masked transformer to learn the probabilistic distributions of the pose tokens conditioned on the input image prompt along with randomly masked token sequence during inference the model samples from the learned conditional distribution to iteratively decode highconfidence pose tokens thereby reducing 3d reconstruction uncertainties to further refine the reconstruction a 2d poseguided refinement technique is proposed to directly finetune the decoded pose tokens in the latent space which forces the projected 3d body mesh to align with the 2d pose clues experiments on benchmark datasets demonstrate that genhmr significantly outperforms stateoftheart methods project website can be found at,0
the listener head generation lhg task aims to generate natural nonverbal listener responses based on the speakers multimodal cues while prior work either rely on limited modalities eg audio and facial information or employ autoregressive approaches which have limitations such as accumulating prediction errors to address these limitations we propose difflistener a discrete diffusion based approach for nonautoregressive listener head generation our model takes the speakers facial information audio and text as inputs additionally incorporating facial differential information to represent the temporal dynamics of expressions and movements with this explicit modeling of facial dynamics difflistener can generate coherent reaction sequences in a nonautoregressive manner through comprehensive experiments difflistener demonstrates stateoftheart performance in both quantitative and qualitative evaluations the user study shows that difflistener generates natural contextaware listener reactions that are well synchronized with the speaker the code and demo videos are available in,0
generating a new font library is a very laborintensive and timeconsuming job for glyphrich scripts despite the remarkable success of existing font generation methods they have significant drawbacks they require a large number of reference images to generate a new font set or they fail to capture detailed styles with only a few samples in this paper we focus on compositional scripts a widely used letter system in the world where each glyph can be decomposed by several components by utilizing the compositionality of compositional scripts we propose a novel font generation framework named dual memoryaugmented font generation network dmfont which enables us to generate a highquality font library with only a few samples we employ memory components and globalcontext awareness in the generator to take advantage of the compositionality in the experiments on koreanhandwriting fonts and thaiprinting fonts we observe that our method generates a significantly better quality of samples with faithful stylization compared to the stateoftheart generation methods quantitatively and qualitatively source code is available at,0
this paper presents a new approach for 3d shape generation enabling direct generative modeling on a continuous implicit representation in wavelet domain specifically we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3d shapes via truncated signed distance functions and multiscale biorthogonal wavelets and formulate a pair of neural networks a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and highquality shapes with complex topology and structures clean surfaces and fine details exceeding the 3d generation capabilities of the stateoftheart models,0
the ability to map descriptions of scenes to 3d geometric representations has many applications in areas such as art education and robotics however prior work on the text to 3d scene generation task has used manually specified object categories and language that identifies them we introduce a dataset of 3d scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects our method successfully grounds a variety of lexical terms to concrete referents and we show quantitatively that our method improves 3d scene generation over previous work using purely rulebased methods we evaluate the fidelity and plausibility of 3d scenes generated with our grounding approach through human judgments to ease evaluation on this task we also introduce an automated metric that strongly correlates with human judgments,0
numerous methods have been proposed to transform color and grayscale images to their single bitperpixel binary counterparts commonly the goal is to enhance specific attributes of the original image to make it more amenable for analysis however when the resulting binarized image is intended for human viewing aesthetics must also be considered binarization techniques such as halftoning stippling and hatching have been widely used for modeling the original images intensity profile we present an automated method to transform an image to a set of binary textures that represent not only the intensities but also the colors of the original the foundation of our method is information preservation creating a set of textures that allows for the reconstruction of the original images colors solely from the binarized representation we present techniques to ensure that the textures created are not visually distracting preserve the intensity profile of the images and are natural in that they map sets of colors that are perceptually similar to patterns that are similar the approach uses deepneural networks and is entirely selfsupervised no examples of good vs bad binarizations are required the system yields aesthetically pleasing binary images when tested on a variety of image sources,0
when people deliver a speech they naturally move heads and this rhythmic head motion conveys prosodic information however generating a lipsynced video while moving head naturally is challenging while remarkably successful existing works either generate still talkingface videos or rely on landmarkvideo frames as sparsedense mapping guidance to generate head movements which leads to unrealistic or uncontrollable video synthesis to overcome the limitations we propose a 3daware generative network along with a hybrid embedding module and a nonlinear composition module through modeling the head motion and facial expressions1 explicitly manipulating 3d animation carefully and embedding reference images dynamically our approach achieves controllable photorealistic and temporally coherent talkinghead videos with natural head movements thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the stateoftheart methods in both quantitative and qualitative comparisons the code is available on lelechen63talkingheadgenerationwithrhythmicheadmotion,0
we introduce a high resolution 3dconsistent image and shape generation technique which we call stylesdf our method is trained on singleview rgb data only and stands on the shoulders of stylegan2 for image generation while solving two main challenges in 3daware gans 1 highresolution viewconsistent generation of the rgb images and 2 detailed 3d shape we achieve this by merging a sdfbased 3d representation with a stylebased 2d generator our 3d implicit network renders lowresolution feature maps from which the stylebased network generates viewconsistent 1024x1024 images notably our sdfbased 3d modeling defines detailed 3d surfaces leading to consistent volume rendering our method shows higher quality results compared to state of the art in terms of visual and geometric quality,0
automatic generation of graphic designs has recently received considerable attention however the stateoftheart approaches are complex and rely on proprietary datasets which creates reproducibility barriers in this paper we propose an open framework for automatic graphic design called opencole where we build a modified version of the pioneering cole and train our model exclusively on publicly available datasets based on gpt4v evaluations our model shows promising performance comparable to the original cole we release the pipeline and training results to encourage open development,0
3d model reconstruction from a single image has achieved great progress with the recent deep generative models however the conventional reconstruction approaches with template mesh deformation and implicit fields have difficulty in reconstructing nonwatertight 3d mesh models such as garments in contrast to imagebased modeling the sketchbased approach can help users generate 3d models to meet the design intentions from handdrawn sketches in this study we propose sketch2cloth a sketchbased 3d garment generation system using the unsigned distance fields from the users sketch input sketch2cloth first estimates the unsigned distance function of the target 3d model from the sketch input and extracts the mesh from the estimated field with marching cubes we also provide the model editing function to modify the generated mesh we verified the proposed sketch2cloth with quantitative evaluations on garment generation and editing with a stateoftheart approach,0
efficient authoring of vast virtual environments hinges on algorithms that are able to automatically generate content while also being controllable we propose a method to automatically generate furniture layouts for indoor environments our method is simple efficient humaninterpretable and amenable to a wide variety of constraints we model the composition of rooms into classes of objects and learn joint cooccurrence statistics from a database of training layouts we generate new layouts by performing a sequence of conditional sampling steps exploiting the statistics learned from the database the generated layouts are specified as 3d object models along with their positions and orientations we show they are of equivalent perceived quality to the training layouts and compare favorably to a stateoftheart method we incorporate constraints using a general mechanism rejection sampling which provides great flexibility at the cost of extra computation we demonstrate the versatility of our method by applying a wide variety of constraints relevant to realworld applications,0
articulated 3d object generation is fundamental for creating realistic functional and interactable virtual assets which are not simply static we introduce meshart a hierarchical transformerbased approach to generate articulated 3d meshes with clean compact geometry reminiscent of humancrafted 3d models we approach articulated mesh generation in a partbypart fashion across two stages first we generate a highlevel articulationaware object structure then based on this structural information we synthesize each parts mesh faces key to our approach is modeling both articulation structures and part meshes as sequences of quantized triangle embeddings leading to a unified hierarchical framework with transformers for autoregressive generation object part structures are first generated as their bounding primitives and articulation modes a second transformer guided by these articulation structures then generates each parts mesh triangles to ensure coherency among generated parts we introduce structureguided conditioning that also incorporates local part mesh connectivity meshart shows significant improvements over state of the art with 571 improvement in structure coverage and a 209point improvement in mesh generation fid,0
recent studies show increasing demands and interests in automatically generating layouts while there is still much room for improving the plausibility and robustness in this paper we present a datadriven layout framework without model formulation and loss term optimization we achieve and organize priors directly based on samples from datasets instead of sampling probabilistic models therefore our method enables expressing and generating mathematically inexpressible relations among three or more objects subsequently a nonlearning geometric algorithm attempts arranging objects plausibly considering constraints such as walls windows etc experiments would show our generated layouts outperform the stateofart and our framework is competitive to human designers,0
we propose a twostage method for face hallucination first we generate facial components of the input image using cnns these components represent the basic facial structures second we synthesize finegrained facial structures from high resolution training images the details of these structures are transferred into facial components for enhancement therefore we generate facial components to approximate ground truth global appearance in the first stage and enhance them through recovering details in the second stage the experiments demonstrate that our method performs favorably against stateoftheart methods,0
despite advances in textto3d generation methods generation of multiobject arrangements remains challenging current methods exhibit failures in generating physically plausible arrangements that respect the provided text description we present scenemotifcoder smc an exampledriven framework for generating 3d object arrangements through visual program learning smc leverages large language models llms and program synthesis to overcome these challenges by learning visual programs from example arrangements these programs are generalized into compact editable metaprograms when combined with 3d object retrieval and geometryaware optimization they can be used to create object arrangements varying in arrangement structure and contained objects our experiments show that smc generates highquality arrangements using metaprograms learned from few examples evaluation results demonstrates that object arrangements generated by smc better conform to userspecified text descriptions and are more physically plausible when compared with stateoftheart textto3d generation and layout methods,0
in this paper we present treenhance an automatic method for lowlight image enhancement capable of improving the quality of digital images the method combines tree search theory and in particular the monte carlo tree search mcts algorithm with deep reinforcement learning given as input a lowlight image treenhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it during the training phase the method repeatedly alternates two main phases a generation phase where a modified version of mcts explores the space of image editing operations and selects the most promising sequence and an optimization phase where the parameters of a neural network implementing the enhancement policy are updated two different inference solutions are proposed for the enhancement of new images one is based on mcts and is more accurate but more time and memory consuming the other directly applies the learned policy and is faster but slightly less precise as a further contribution we propose a guided search strategy that reverses the enhancement procedure that a photo editor applied to a given input image unlike other methods from the state of the art treenhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning we tested the method on two datasets the lowlight dataset and the adobe fivek dataset obtaining good results from both a qualitative and a quantitative point of view,0
simulation of forest environments has applications from entertainment and art creation to commercial and scientific modelling due to the unique features and lighting in forests a forestspecific simulator is desirable however many current forest simulators are proprietary or highly tailored to a particular application here we review several areas of procedural generation and rendering specific to forest generation and utilise this to create a generalised opensource tool for generating and rendering interactive realistic forest scenes the system uses specialised lsystems to generate trees which are distributed using an ecosystem simulation algorithm the resulting scene is rendered using a deferred rendering pipeline a blinnphong lighting model with realtime leaf transparency and postprocessing lighting effects the result is a system that achieves a balance between high natural realism and visual appeal suitable for tasks including training computer vision algorithms for autonomous robots and visual media generation,0
this thesis presents a framework that integrates stateoftheart generative ai models for realtime creation of threedimensional 3d objects in augmented reality ar environments the primary goal is to convert diverse inputs such as images and speech into accurate 3d models enhancing user interaction and immersion key components include advanced object detection algorithms userfriendly interaction techniques and robust ai models like shape for 3d generation leveraging vision language models vlms and large language models llms the system captures spatial details from images and processes textual information to generate comprehensive 3d objects seamlessly integrating virtual objects into realworld environments the framework demonstrates applications across industries such as gaming education retail and interior design it allows players to create personalized ingame assets customers to see products in their environments before purchase and designers to convert realworld objects into 3d models for realtime visualization a significant contribution is democratizing 3d model creation making advanced ai tools accessible to a broader audience fostering creativity and innovation the framework addresses challenges like handling multilingual inputs diverse visual data and complex environments improving object detection and model generation accuracy as well as loading 3d models in ar space in realtime in conclusion this thesis integrates generative ai and ar for efficient 3d model generation enhancing accessibility and paving the way for innovative applications and improved user interactions in ar environments,0
texttoimage models are showcasing the impressive ability to create highquality and diverse generative images nevertheless the transition from freehand sketches to complex scene images remains challenging using diffusion models in this study we propose a novel sketchguided scene image generation framework decomposing the task of scene image scene generation from sketch inputs into objectlevel crossdomain generation and scenelevel image construction we employ pretrained diffusion models to convert each single object drawing into an image of the object inferring additional details while maintaining the sparse sketch structure in order to maintain the conceptual fidelity of the foreground during scene generation we invert the visual features of object images into identity embeddings for scene generation in scenelevel image construction we generate the latent representation of the scene image using the separated background prompts and then blend the generated foreground objects according to the layout of the sketch input to ensure the foreground objects details remain unchanged while naturally composing the scene image we infer the scene image on the blended latent representation using a global prompt that includes the trained identity tokens through qualitative and quantitative experiments we demonstrate the ability of the proposed approach to generate scene images from handdrawn sketches surpasses the stateoftheart approaches,0
achieving highfidelity and temporally smooth 3d human motion generation remains a challenge particularly within resourceconstrained environments we introduce flowmotion a novel method leveraging conditional flow matching cfm flowmotion incorporates a training objective within cfm that focuses on more accurately predicting target motion in 3d human motion generation resulting in enhanced generation fidelity and temporal smoothness while maintaining the fast synthesis times characteristic of flowmatchingbased methods flowmotion achieves stateoftheart jitter performance achieving the best jitter in the kit dataset and the secondbest jitter in the humanml3d dataset and a competitive fid value in both datasets this combination provides robust and natural motion sequences offering a promising equilibrium between generation quality and temporal naturalness,0
we propose a method that can generate cinemagraphs automatically from a still landscape image using a pretrained stylegan inspired by the success of recent unconditional video generation we leverage a powerful pretrained image generator to synthesize highquality cinemagraphs unlike previous approaches that mainly utilize the latent space of a pretrained stylegan our approach utilizes its deep feature space for both gan inversion and cinemagraph generation specifically we propose multiscale deep feature warping msdfw which warps the intermediate features of a pretrained stylegan at different resolutions by using msdfw the generated cinemagraphs are of high resolution and exhibit plausible looping animation we demonstrate the superiority of our method through user studies and quantitative comparisons with stateoftheart cinemagraph generation methods and a video generation method that uses a pretrained stylegan,0
over the past years deep generative models have achieved a new level of performance generated data has become difficult if not impossible to be distinguished from real data while there are plenty of use cases that benefit from this technology there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation at scale unfortunately current deep fake detection methods are not sustainable as the gap between real and fake continues to close in contrast our work enables a responsible disclosure of such stateoftheart generative models that allows model inventors to fingerprint their models so that the generated samples containing a fingerprint can be accurately detected and attributed to a source our technique achieves this by an efficient and scalable adhoc generation of a large population of models with distinct fingerprints our recommended operation point uses a 128bit fingerprint which in principle results in more than 1038 identifiable models experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution code and models are available at,0
image visual tryon aims at transferring a target clothing image onto a reference person and has become a hot topic in recent years prior arts usually focus on preserving the character of a clothing image eg texture logo embroidery when warping it to arbitrary human pose however it remains a big challenge to generate photorealistic tryon images when large occlusions and human poses are presented in the reference person to address this issue we propose a novel visual tryon network namely adaptive content generating and preserving network acgpn in particular acgpn first predicts semantic layout of the reference image that will be changed after tryon eg long sleeve shirtrightarrowarm armrightarrowjacket and then determines whether its image content needs to be generated or preserved according to the predicted semantic layout leading to photorealistic tryon and rich clothing details acgpn generally involves three major modules first a semantic layout generation module utilizes semantic segmentation of the reference image to progressively predict the desired semantic layout after tryon second a clothes warping module warps clothing images according to the generated semantic layout where a secondorder difference constraint is introduced to stabilize the warping process during training third an inpainting module for content fusion integrates all information eg reference image semantic layout warped clothes to adaptively produce each semantic part of human body in comparison to the stateoftheart methods acgpn can generate photorealistic images with much better perceptual quality and richer finedetails,0
autoregressive models have proven to be very powerful in nlp text generation tasks and lately have gained popularity for image generation as well however they have seen limited use for the synthesis of 3d shapes so far this is mainly due to the lack of a straightforward way to linearize 3d data as well as to scaling problems with the length of the resulting sequences when describing complex shapes in this work we address both of these problems we use octrees as a compact hierarchical shape representation that can be sequentialized by traversal ordering moreover we introduce an adaptive compression scheme that significantly reduces sequence lengths and thus enables their effective generation with a transformer while still allowing fully autoregressive sampling and parallel training we demonstrate the performance of our model by comparing against the stateoftheart in shape generation,0
we present a novel method for inserting objects specifically humans into existing images such that they blend in a photorealistic manner while respecting the semantic context of the scene our method involves three subnetworks the first generates the semantic map of the new person given the pose of the other persons in the scene and an optional bounding box specification the second network renders the pixels of the novel person and its blending mask based on specifications in the form of multiple appearance components a third network refines the generated face in order to match those of the target person our experiments present convincing highresolution outputs in this novel and challenging application domain in addition the three networks are evaluated individually demonstrating for example state of the art results in pose transfer benchmarks,0
we present a diffusionbased model for 3daware generative novel view synthesis from as few as a single input image our model samples from the distribution of possible renderings consistent with the input and even in the presence of ambiguity is capable of rendering diverse and plausible novel views to achieve this our method makes use of existing 2d diffusion backbones but crucially incorporates geometry priors in the form of a 3d feature volume this latent feature field captures the distribution over possible scene representations and improves our methods ability to generate viewconsistent novel renderings in addition to generating novel views our method has the ability to autoregressively synthesize 3dconsistent sequences we demonstrate stateoftheart results on synthetic renderings and roomscale scenes we also show compelling results for challenging realworld objects,0
the evolution of text to visual components facilitates peoples daily lives such as generating image videos from text and identifying the desired elements within the images computer vision models involving the multimodal abilities in the previous days are focused on image detection classification based on welldefined objects large language models llms introduces the transformation from nature language to visual objects which present the visual layout for text contexts openai gpt4 has emerged as the pinnacle in llms while the computer vision cv domain boasts a plethora of stateoftheart sota models and algorithms to convert 2d images to their 3d representations however the mismatching between the algorithms with the problem could lead to undesired results in response to this challenge we propose an unified visiongpt3d framework to consolidate the stateoftheart vision models thereby facilitating the development of visionoriented ai visiongpt3d provides a versatile multimodal framework building upon the strengths of multimodal foundation models it seamlessly integrates various sota vision models and brings the automation in the selection of sota vision models identifies the suitable 3d mesh creation algorithms corresponding to 2d depth maps analysis generates optimal results based on diverse multimodal inputs such as text prompts keywords visiongpt3d 3d vision understanding multimodal agent,0
while motion generation has made substantial progress its practical application remains constrained by dataset diversity and scale limiting its ability to handle outofdistribution scenarios to address this we propose a simple and effective baseline rmd which enhances the generalization of motion generation through retrievalaugmented techniques unlike previous retrievalbased methods rmd requires no additional training and offers three key advantages 1 the external retrieval database can be flexibly replaced 2 body parts from the motion database can be reused with an llm facilitating splitting and recombination and 3 a pretrained motion diffusion model serves as a prior to improve the quality of motions obtained through retrieval and direct combination without any training rmd achieves stateoftheart performance with notable advantages on outofdistribution data,0
seams distortions wasted uv space vertexduplication and varying resolution over the surface are the most prominent issues of the standard uvbased texturing of meshes these issues are particularly acute when automatic uvunwrapping techniques are used for this reason instead of generating textures in automatically generated uvplanes like most stateoftheart methods we propose to represent textures as coloured pointclouds whose colours are generated by a denoising diffusion probabilistic model constrained to operate on the surface of 3d objects our sampling and resolution agnostic generative model heavily relies on heat diffusion over the surface of the meshes for spatial communication between points to enable processing of arbitrarily sampled pointcloud textures and ensure longdistance texture consistency we introduce a fast resampling of the mesh spectral properties used during the heat diffusion and introduce a novel heatdiffusionbased selfattention mechanism our code and pretrained models are available at githubcomsimofotiuv3ted,0
we present quantum brush an opensource digital painting tool that harnesses quantum computing to generate novel artistic expressions the tool includes four different brushes that translate strokes into unique quantum algorithms each highlighting a different way in which quantum effects can produce novel aesthetics each brush is designed to be compatible with the current noisy intermediatescale quantum nisq devices as demonstrated by executing them on iqms sirius device,0
in poster design contentaware layout generation is crucial for automatically arranging visualtextual elements on the given image with limited training data existing work focused on imagecentric enhancement however this neglects the diversity of layouts and fails to cope with shapevariant elements or diverse design intents in generalized settings to this end we proposed a layoutcentric approach that leverages layout knowledge implicit in large language models llms to create posters for omnifarious purposes hence the name postero specifically it structures layouts from datasets as trees in svg language by universal shape design intent vectorization and hierarchical node representation then it applies llms during inference to predict new layout trees by incontext learning with intentaligned example selection after layout trees are generated we can seamlessly realize them into poster designs by editing the chat with llms extensive experimental results have demonstrated that postero can generate visually appealing layouts for given images achieving new stateoftheart performance across various benchmarks to further explore posteros abilities under the generalized settings we built pstylish7 the first dataset with multipurpose posters and variousshaped elements further offering a challenging test for advanced research,0
we present dynamic neural radiance fields for modeling the appearance and dynamics of a human face digitally modeling and reconstructing a talking human is a key buildingblock for a variety of applications especially for telepresence applications in ar or vr a faithful reproduction of the appearance including novel viewpoints or headposes is required in contrast to stateoftheart approaches that model the geometry and material properties explicitly or are purely imagebased we introduce an implicit representation of the head based on scene representation networks to handle the dynamics of the face we combine our scene representation network with a lowdimensional morphable model which provides explicit control over pose and expressions we use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only without the need of a specialized capture setup in our experiments we show that this learned volumetric representation allows for photorealistic image generation that surpasses the quality of stateoftheart videobased reenactment methods,0
chinese vector font generation is challenging due to the complex structure and huge amount of chinese characters recent advances remain limited to generating a small set of characters with simple structure in this work we first observe that most chinese characters can be disassembled into frequentlyreused components therefore we introduce the first efficient and scalable chinese vector font generation approach via component composition allowing generating numerous vector characters from a small set of components to achieve this we collect a largescale dataset that contains over textit90k chinese characters with their components and layout information upon the dataset we propose a simple yet effective framework based on spatial transformer networks stn and multiple losses tailored to font characteristics to learn the affine transformation of the components which can be directly applied to the bzier curves resulting in chinese characters in vector format our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the stateoftheart vector font generation methods in generating largescale complex chinese characters in both font generation and zeroshot font extension,0
we demonstrate generating hdr images using the concerted action of multiple blackbox pretrained ldr image diffusion models relying on a pretrained ldr generative diffusion models is vital as first there is no sufficiently large hdr image dataset available to retrain them and second even if it was retraining such models is impossible for most compute budgets instead we seek inspiration from the hdr image capture literature that traditionally fuses sets of ldr images called exposure brackets to produce a single hdr image we operate multiple denoising processes to generate multiple ldr brackets that together form a valid hdr result the key to making this work is to introduce a consistency term into the diffusion process to couple the brackets such that they agree across the exposure range they share while accounting for possible differences due to the quantization error we demonstrate stateoftheart unconditional and conditional or restorationtype ldr2hdr generative modeling results yet in hdr,0
image smoothing is a fundamental procedure in applications of both computer vision and graphics the required smoothing properties can be different or even contradictive among different tasks nevertheless the inherent smoothing nature of one smoothing operator is usually fixed and thus cannot meet the various requirements of different applications in this paper we first introduce the truncated huber penalty function which shows strong flexibility under different parameter settings a generalized framework is then proposed with the introduced truncated huber penalty function when combined with its strong flexibility our framework is able to achieve diverse smoothing natures where contradictive smoothing behaviors can even be achieved it can also yield the smoothing behavior that can seldom be achieved by previous methods and superior performance is thus achieved in challenging cases these together enable our framework capable of a range of applications and able to outperform the stateoftheart approaches in several tasks such as image detail enhancement clipart compression artifacts removal guided depth map restoration image texture removal etc in addition an efficient numerical solution is provided and its convergence is theoretically guaranteed even the optimization framework is nonconvex and nonsmooth a simple yet effective approach is further proposed to reduce the computational cost of our method while maintaining its performance the effectiveness and superior performance of our approach are validated through comprehensive experiments in a range of applications our code is available at,0
in recent years the role of image generative models in facial reenactment has been steadily increasing such models are usually subjectagnostic and trained on domainwide datasets the appearance of the reenacted individual is learned from a single image and hence the entire breadth of the individuals appearance is not entirely captured leading these methods to resort to unfaithful hallucination thanks to recent advancements it is now possible to train a personalized generative model tailored specifically to a given individual in this paper we propose a novel method for facial reenactment using a personalized generator we train the generator using frames from a short yet varied selfscan video captured using a simple commodity camera images synthesized by the personalized generator are guaranteed to preserve identity the premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions to this end we locate the desired frames in the latent space of the personalized generator using carefully designed latent optimization through extensive evaluation we demonstrate stateoftheart performance for facial reenactment furthermore we show that since our reenactment takes place in a semantic latent space it can be semantically edited and stylized in postprocessing,0
we introduce mesh silksong a compact and efficient mesh representation tailored to generate the polygon mesh in an autoregressive manner akin to silk weaving existing mesh tokenization methods always produce token sequences with repeated vertex tokens wasting the network capability therefore our approach tokenizes mesh vertices by accessing each mesh vertice only once reduces the token sequences redundancy by 50 and achieves a stateoftheart compression rate of approximately 22 furthermore mesh silksong produces polygon meshes with superior geometric properties including manifold topology watertight detection and consistent face normals which are critical for practical applications experimental results demonstrate the effectiveness of our approach showcasing not only intricate mesh generation but also significantly improved geometric integrity,0
we introduce gem3d a new deep topologyaware generative model of 3d shapes the key ingredient of our method is a neural skeletonbased representation encoding information on both shape topology and geometry through a denoising diffusion probabilistic model our method first generates skeletonbased representations following the medial axis transform mat then generates surfaces through a skeletondriven neural implicit formulation the neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations we discuss applications of our method in shape synthesis and point cloud reconstruction tasks and evaluate our method both qualitatively and quantitatively we demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the stateoftheart also involving challenging scenarios of reconstructing and synthesizing structurally complex highgenus shape surfaces from thingi10k and shapenet,0
a point cloud serves as a representation of the surface of a threedimensional 3d shape deep generative models have been adapted to model their variations typically using a map from a balllike set of latent variables however previous approaches did not pay much attention to the topological structure of a point cloud despite that a continuous map cannot express the varying numbers of holes and intersections moreover a point cloud is often composed of multiple subparts and it is also difficult to express in this study we propose chartpointflow a flowbased generative model with multiple latent labels for 3d point clouds each label is assigned to points in an unsupervised manner then a map conditioned on a label is assigned to a continuous subset of a point cloud similar to a chart of a manifold this enables our proposed model to preserve the topological structure with clear boundaries whereas previous approaches tend to generate blurry point clouds and fail to generate holes the experimental results demonstrate that chartpointflow achieves stateoftheart performance in terms of generation and reconstruction compared with other point cloud generators moreover chartpointflow divides an object into semantic subparts using charts and it demonstrates superior performance in case of unsupervised segmentation,0
in recent years the demand for 3d content has grown exponentially with intelligent upgrading of interactive media extended reality xr and metaverse industries in order to overcome the limitation of traditional manual modeling approaches such as laborintensive workflows and prolonged production cycles revolutionary advances have been achieved through the convergence of novel 3d representation paradigms and artificial intelligence generative technologies in this survey we conduct a systematically review of the cuttingedge achievements in static 3d object and scene generation as well as establish a comprehensive technical framework through systematic categorization specifically we initiate our analysis with mainstream 3d object representations followed by indepth exploration of two principal technical pathways in object generation datadriven supervised learning methods and deep generative modelbased approaches regarding scene generation we focus on three dominant paradigms layoutguided compositional synthesis 2d priorbased scene generation and ruledriven modeling finally we critically examine persistent challenges in 3d generation and propose potential research directions for future investigation this survey aims to provide readers with a structured understanding of stateoftheart 3d generation technologies while inspiring researchers to undertake more exploration in this domain,0
simulation is increasingly being used for generating large labelled datasets in many machine learning problems recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task usually relying on reinforcelike gradient estimators however these approaches are very expensive as they treat the entire data generation model training and validation pipeline as a blackbox and require multiple costly objective evaluations at each iteration we propose an efficient alternative for optimal synthetic data generation based on a novel differentiable approximation of the objective this allows us to optimize the simulator which may be nondifferentiable requiring only one objective evaluation at each iteration with a little overhead we demonstrate on a stateoftheart photorealistic renderer that the proposed method finds the optimal data distribution faster up to 50times with significantly reduced training data generation up to 30times and better accuracy 87 on realworld test datasets than previous methods,0
diffusion models have achieved great success in generating 2d images however the quality and generalizability of 3d content generation remain limited stateoftheart methods often require largescale 3d assets for training which are challenging to collect in this work we introduce kiss3dgen keep it simple and straightforward in 3d generation an efficient framework for generating editing and enhancing 3d objects by repurposing a welltrained 2d image diffusion model for 3d generation specifically we finetune a diffusion model to generate 3d bundle image a tiled representation composed of multiview images and their corresponding normal maps the normal maps are then used to reconstruct a 3d mesh and the multiview images provide texture mapping resulting in a complete 3d model this simple method effectively transforms the 3d generation problem into a 2d image generation task maximizing the utilization of knowledge in pretrained diffusion models furthermore we demonstrate that our kiss3dgen model is compatible with various diffusion model techniques enabling advanced features such as 3d editing mesh and texture enhancement etc through extensive experiments we demonstrate the effectiveness of our approach showcasing its ability to produce highquality 3d models efficiently,0
texture map production is an important part of 3d modeling and determines the rendering quality recently diffusionbased methods have opened a new way for texture generation however restricted control flexibility and limited prompt modalities may prevent creators from producing desired results furthermore inconsistencies between generated multiview images often lead to poor texture generation quality to address these issues we introduce textbfflexpainter a novel texture generation pipeline that enables flexible multimodal conditional guidance and achieves highly consistent texture generation a shared conditional embedding space is constructed to perform flexible aggregation between different input modalities utilizing such embedding space we present an imagebased cfg method to decompose structural and style information achieving reference imagebased stylization leveraging the 3d knowledge within the image diffusion prior we first generate multiview images simultaneously using a grid representation to enhance global understanding meanwhile we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency finally a 3daware texture completion model combined with a texture enhancement model is used to generate seamless highresolution texture maps comprehensive experiments demonstrate that our framework significantly outperforms stateoftheart methods in both flexibility and generation quality,0
in order to solve the problems of long training time large consumption of computing resources and huge parameter amount of gan network in image generation this paper proposes an improved gan network model which is named faster projected gan based on projected gan the proposed network is mainly focuses on the improvement of generator of projected gan by introducing depth separable convolution dsc the number of parameters of the projected gan is reduced the training speed is accelerated and memory is saved experimental results show that on ffhq1k artpainting landscape and other fewshot image datasets a 20 speed increase and a 15 memory saving are achieved at the same time fid loss is less or no loss and the amount of model parameters is better controlled at the same time significant training speed improvement has been achieved in the small sample image generation task of special scenes such as earthquake scenes with few public datasets,0
the research topic of sketchtoportrait generation has witnessed a boost of progress with deep learning techniques the recently proposed stylegan architectures achieve stateoftheart generation ability but the original stylegan is not friendly for sketchbased creation due to its unconditional generation nature to address this issue we propose a direct conditioning strategy to better preserve the spatial information under the stylegan framework specifically we introduce spatially conditioned stylegan scstylegan for short which explicitly injects spatial constraints to the original stylegan generation process we explore two input modalities sketches and semantic maps which together allow users to express desired generation results more precisely and easily based on scstylegan we present drawinginstyles a novel drawing interface for nonprofessional users to easily produce highquality photorealistic face images with precise control either from scratch or editing existing ones qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions the usability and expressiveness of our system are confirmed by a user study,0
controllable highfidelity mesh editing remains a significant challenge in 3d content creation existing generative methods often struggle with complex geometries and fail to produce detailed results we propose craftmesh a novel framework for highfidelity generative mesh manipulation via poisson seamless fusion our key insight is to decompose mesh editing into a pipeline that leverages the strengths of 2d and 3d generative models we edit a 2d reference image then generate a regionspecific 3d mesh and seamlessly fuse it into the original model we introduce two core techniques poisson geometric fusion which utilizes a hybrid sdfmesh representation with normal blending to achieve harmonious geometric integration and poisson texture harmonization for visually consistent texture blending experimental results demonstrate that craftmesh outperforms stateoftheart methods delivering superior global consistency and local detail in complex editing tasks,0
we present a novel approach named omnicontrol for incorporating flexible spatial control signals into a textconditioned human motion generation model based on the diffusion process unlike previous methods that can only control the pelvis trajectory omnicontrol can incorporate flexible spatial control signals over different joints at different times with only one model specifically we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals at the same time realism guidance is introduced to refine all the joints to generate more coherent motion both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism by combining them omnicontrol generates motions that are realistic coherent and consistent with the spatial constraints experiments on humanml3d and kitml datasets show that omnicontrol not only achieves significant improvement over stateoftheart methods on pelvis control but also shows promising results when incorporating the constraints over other joints,0
textdriven humanobject interaction texttohoi generation is an emerging field with applications in animation video games virtual reality and robotics a key challenge in hoi generation is maintaining interaction consistency in long sequences existing texttomotionbased approaches such as discrete motion tokenization cannot be directly applied to hoi generation due to limited data in this domain and the complexity of the modality to address the problem of interaction consistency in long sequences we propose an autoregressive diffusion model ardhoi that predicts the next continuous token specifically we introduce a contrastive variational autoencoder cvae to learn a physically plausible space of continuous hoi tokens thereby ensuring that generated humanobject motions are realistic and natural for generating sequences autoregressively we develop a mambabased context encoder to capture and maintain consistent sequential actions additionally we implement an mlpbased denoiser to generate the subsequent token conditioned on the encoded context our model has been evaluated on the omomo and behave datasets where it outperforms existing stateoftheart methods in terms of both performance and inference speed this makes ardhoi a robust and efficient solution for textdriven hoi tasks,0
diffusion models have emerged as a popular method for 3d generation however it is still challenging for diffusion models to efficiently generate diverse and highquality 3d shapes in this paper we introduce octfusion which can generate 3d shapes with arbitrary resolutions in 25 seconds on a single nvidia 4090 gpu and the extracted meshes are guaranteed to be continuous and manifold the key components of octfusion are the octreebased latent representation and the accompanying diffusion models the representation combines the benefits of both implicit neural representations and explicit spatial octrees and is learned with an octreebased variational autoencoder the proposed diffusion model is a unified multiscale unet that enables weights and computation sharing across different octree levels and avoids the complexity of widely used cascaded diffusion schemes we verify the effectiveness of octfusion on the shapenet and objaverse datasets and achieve stateoftheart performances on shape generation tasks we demonstrate that octfusion is extendable and flexible by generating highquality color fields for textured mesh generation and highquality 3d shapes conditioned on text prompts sketches or category labels our code and pretrained models are available at,0
human motion synthesis is an important task in computer graphics and computer vision while focusing on various conditioning signals such as text action class or audio to guide the generation process most existing methods utilize skeletonbased pose representation requiring additional skinning to produce renderable meshes given that human motion is a complex interplay of bones joints and muscles considering solely the skeleton for generation may neglect their inherent interdependency which can limit the variability and precision of the generated results to address this issue we propose a shapeconditioned motion diffusion model smd which enables the generation of motion sequences directly in mesh format conditioned on a specified target mesh in smd the input meshes are transformed into spectral coefficients using graph laplacian to efficiently represent meshes subsequently we propose a spectraltemporal autoencoder stae to leverage crosstemporal dependencies within the spectral domain extensive experimental evaluations show that smd not only produces vivid and realistic motions but also achieves competitive performance in texttomotion and actiontomotion tasks when compared to stateoftheart methods,0
the emergence of deep generative models has recently enabled the automatic generation of massive amounts of graphical content both in 2d and in 3d generative adversarial networks gans and style control mechanisms such as adaptive instance normalization adain have proved particularly effective in this context culminating in the stateoftheart stylegan architecture while such models are able to learn diverse distributions provided a sufficiently large training set they are not wellsuited for scenarios where the distribution of the training data exhibits a multimodal behavior in such cases reshaping a uniform or normal distribution over the latent space into a complex multimodal distribution in the data domain is challenging and the generator might fail to sample the target distribution well furthermore existing unsupervised generative models are not able to control the mode of the generated samples independently of the other visual attributes despite the fact that they are typically disentangled in the training data in this paper we introduce ummgan a novel architecture designed to better model multimodal distributions in an unsupervised fashion building upon the stylegan architecture our network learns multiple modes in a completely unsupervised manner and combines them using a set of learned weights we demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones we further show that ummgan effectively disentangles between modes and style thereby providing an independent degree of control over the generated content,0
generating 3d scenes is still a challenging task due to the lack of readily available scene data most existing methods only produce partial scenes and provide limited navigational freedom we introduce a practical and scalable solution that uses 360 video as an intermediate scene representation capturing the fullscene context and ensuring consistent visual content throughout the generation we propose worldprompter a generative pipeline that synthesizes traversable 3d scenes from text prompts worldprompter incorporates a conditional 360 panoramic video generator capable of producing a 128frame video that simulates a person walking through and capturing a virtual environment the resulting video is then reconstructed as gaussian splats by a fast feedforward 3d reconstructor enabling a true walkable experience within the 3d scene experiments demonstrate that our panoramic video generation model trained with a mix of image and video data achieves convincing spatial and temporal consistency for static scenes this is validated by an average colmap matching rate of 946 allowing for highquality panoramic gaussian splat reconstruction and improved navigation throughout the scene qualitative and quantitative results also show it outperforms the stateoftheart 360 video generators and 3d scene generation models,0
explorable 3d world generation from a single image or text prompt forms a cornerstone of spatial intelligence recent works utilize video model to achieve widescope and generalizable 3d world generation however existing approaches often suffer from a limited scope in the generated scenes in this work we propose matrix3d a framework that utilize panoramic representation for widecoverage omnidirectional explorable 3d world generation that combines conditional video generation and panoramic 3d reconstruction we first train a trajectoryguided panoramic video diffusion model that employs scene mesh renders as condition to enable highquality and geometrically consistent scene video generation to lift the panorama scene video to 3d world we propose two separate methods 1 a feedforward large panorama reconstruction model for rapid 3d scene reconstruction and 2 an optimizationbased pipeline for accurate and detailed 3d scene reconstruction to facilitate effective training we also introduce the matrixpano dataset the first largescale synthetic collection comprising 116k highquality static panoramic video sequences with depth and trajectory annotations extensive experiments demonstrate that our proposed framework achieves stateoftheart performance in panoramic video generation and 3d world generation see more in,0
despite the success of generative adversarial networks gans in image synthesis there lacks enough understanding on what generative models have learned inside the deep generative representations and how photorealistic images are able to be composed of the layerwise stochasticity introduced in recent gans in this work we show that highlystructured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in stateoftheart gan models like stylegan and biggan by probing the layerwise representations with a broad set of semantics at different abstraction levels we are able to quantify the causality between the activations and semantics occurring in the output image such a quantification identifies the humanunderstandable variation factors learned by gans to compose scenes the qualitative and quantitative results further suggest that the generative representations learned by the gans with layerwise latent codes are specialized to synthesize different hierarchical semantics the early layers tend to determine the spatial layout and configuration the middle layers control the categorical objects and the later layers finally render the scene attributes as well as color scheme identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation,0
generative models have recently gained increasing attention in image generation and editing tasks however they often lack a direct connection to object geometry which is crucial in sensitive domains such as computational anatomy biology and robotics this paper presents a novel framework for image generation informed by geodesic dynamics igg in deformation spaces our igg model comprises two key components i an efficient autoencoder that explicitly learns the geodesic path of image transformations in the latent space and ii a latent geodesic diffusion model that captures the distribution of latent representations of geodesic deformations conditioned on text instructions by leveraging geodesic paths our method ensures smooth topologypreserving and interpretable deformations capturing complex variations in image structures while maintaining geometric consistency we validate the proposed igg on plant growth data and brain magnetic resonance imaging mri experimental results show that igg outperforms the stateoftheart image generationediting models with superior performance in generating realistic highquality images with preserved object topology and reduced artifacts our code is publicly available at,0
we present a generative model to synthesize 3d shapes as sets of handles lightweight proxies that approximate the original 3d shape for applications in interactive editing shape parsing and building compact 3d representations our model can generate handle sets with varying cardinality and different types of handles figure 1 key to our approach is a deep architecture that predicts both the parameters and existence of shape handles and a novel similarity measure that can easily accommodate different types of handles such as cuboids or spheremeshes we leverage the recent advances in semantic 3d annotation as well as automatic shape summarizing techniques to supervise our approach we show that the resulting shape representations are intuitive and achieve superior quality than previous stateoftheart finally we demonstrate how our method can be used in applications such as interactive shape editing completion and interpolation leveraging the latent space learned by our model to guide these tasks project page,0
highquality environment lighting is essential for creating immersive mobile augmented reality ar experiences however achieving visually coherent estimation for mobile ar is challenging due to several key limitations in ar device sensing capabilities including low camera fov and limited pixel dynamic ranges recent advancements in generative ai which can generate highquality images from different types of prompts including texts and images present a potential solution for highquality lighting estimation still to effectively use generative image diffusion models we must address two key limitations of content quality and slow inference in this work we design and implement a generative lighting estimation system called clear that can produce highquality diverse environment maps in the format of 360 hdr images specifically we design a twostep generation pipeline guided by ar environment context data to ensure the output aligns with the physical environments visual context and color appearance to improve the estimation robustness under different lighting conditions we design a realtime refinement component to adjust lighting estimation results on ar devices through a combination of quantitative and qualitative evaluations we show that clear outperforms stateoftheart lighting estimation methods on both estimation accuracy latency and robustness and is rated by 31 participants as producing better renderings for most virtual objects for example clear achieves 51 to 56 accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties clear produces lighting estimates of comparable or better quality in just 32 seconds over 110x faster than stateoftheart methods,0
holographic neareye displays offer ultracompact form factors for virtual and augmented reality systems but rely on advanced computergenerated holography cgh algorithms to convert 3d scenes into interference patterns that can be displayed on spatial light modulators slms gaussian wave splatting gws has recently emerged as a powerful cgh paradigm that allows for the conversion of gaussians a stateoftheart neural 3d representation into holograms however gws assumes smoothphase distributions over the gaussian primitives limiting their ability to model viewdependent effects and reconstruct accurate defocus blur and severely underutilizing the spacebandwidth product of the slm in this work we propose randomphase gws gwsrp to improve bandwidth utilization which has the effect of increasing eyebox size reconstructing accurate defocus blur and parallax and supporting timemultiplexed rendering to suppress speckle artifacts at the core of gwsrp are 1 a fundamentally new wavefront compositing procedure and 2 an alphablending scheme specifically designed for randomphase gaussian primitives ensuring physically correct color reconstruction and robust occlusion handling additionally we present the first formally derived algorithm for applying random phase to gaussian primitives grounded in rigorous statistical optics analysis and validated through practical neareye display applications through extensive simulations and experimental validations we demonstrate that these advancements collectively with timemultiplexing uniquely enables fullbandwith light field cgh that supports accurate accurate parallax and defocus yielding stateoftheart image quality and perceptually faithful 3d holograms for nextgeneration neareye displays,0
we introduce stablematerials a novel approach for generating photorealistic physicalbased rendering pbr materials that integrate semisupervised learning with latent diffusion models ldms our method employs adversarial training to distill knowledge from existing largescale image generation models minimizing the reliance on annotated data and enhancing the diversity in generation this distillation approach aligns the distribution of the generated materials with that of image textures from an sdxl model enabling the generation of novel materials that are not present in the initial training dataset furthermore we employ a diffusionbased refiner model to improve the visual quality of the samples and achieve highresolution generation finally we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps we detail the architecture and training process of stablematerials the integration of semisupervised training within existing ldm frameworks and show the advantages of our approach comparative evaluations with stateoftheart methods show the effectiveness of stablematerials highlighting its potential applications in computer graphics and beyond stablematerials is publicly available at,0
unlike many other works where authors are usually focused on one or two quality criteria the current manuscript which is a generalization of the article published in russian offers a multicriteria approach to the assessment of the shape quality of curves that constitute component parts of the surfaces used for the computer modelling of object shapes in various types of design based on the analysis of point particle motion along a curved path requirements for the quality of functional curves are proposed a high order of smoothness a minimum number of curvature extrema minimization of the maximum value of curvature and its variation rate minimization of the potential energy of the curve and aesthetic analysis from the standpoint of the laws of technical aesthetics the authors do not set themselves the task of giving a simple and precise mathematical definition of such curves on the contrary this category can include various curves that meet certain quality criteria the refinement and addition of which is possible in the near future engineering practice shows that quality criteria can change over time which does not diminish the need to develop multicriteria methods for assessing the quality of geometric shapes technical issues faced during edge rounding in 3d models that affect the quality of industrial design product shape have been reviewed as an example of the imperfection of existing cad systems,0
unsupervised learning of 3d human faces from unstructured 2d image data is an active research area while recent works have achieved an impressive level of photorealism they commonly lack control of lighting which prevents the generated assets from being deployed in novel environments to this end we introduce lumigan an unconditional generative adversarial network gan for 3d human faces with a physically based lighting module that enables relighting under novel illumination at inference time unlike prior work lumigan can create realistic shadow effects using an efficient visibility formulation that is learned in a selfsupervised manner lumigan generates plausible physical properties for relightable faces including surface normals diffuse albedo and specular tint without any ground truth data in addition to relightability we demonstrate significantly improved geometry generation compared to stateoftheart nonrelightable 3d gans and notably better photorealism than existing relightable gans,0
we present stable video materials 3d svim3d a framework to predict multiview consistent physically based rendering pbr materials given a single image recently video diffusion models have been successfully used to reconstruct 3d objects from a single image efficiently however reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits we extend a latent video diffusion model to output spatially varying pbr parameters and surface normals jointly with each generated view based on explicit camera control this unique setup allows for relighting and generating a 3d asset using our model as neural prior we introduce various mechanisms to this pipeline that improve quality in this illposed setting we show stateoftheart relighting and novel view synthesis performance on multiple objectcentric datasets our method generalizes to diverse inputs enabling the generation of relightable 3d assets useful in arvr movies games and other visual media,0
existing 3dfrom2d generators are typically designed for wellcurated singlecategory datasets where all the objects have approximately the same scale 3d location and orientation and the camera always points to the center of the scene this makes them inapplicable to diverse inthewild datasets of nonalignable scenes rendered from arbitrary camera poses in this work we develop a 3d generator with generic priors 3dgp a 3d synthesis framework with more general assumptions about the training data and show that it scales to very challenging datasets like imagenet our model is based on three new ideas first we incorporate an inaccurate offtheshelf depth estimator into 3d gan training via a special depth adaptation module to handle the imprecision then we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training finally we extend the recent ideas of transferring knowledge from pretrained classifiers into gans for patchwise trained models by employing a simple distillationbased technique on top of the discriminator it achieves more stable training than the existing methods and speeds up the convergence by at least 40 we explore our model on four datasets sdip dogs 256x256 sdip elephants 256x256 lsun horses 256x256 and imagenet 256x256 and demonstrate that 3dgp outperforms the recent stateoftheart in terms of both texture and geometry quality code and visualizations,0
perceptual image quality assessment iqa is the task of predicting the visual quality of an image as perceived by a human observer current stateoftheart techniques are based on deep representations trained in discriminative manner such representations may ignore visually important features if they are not predictive of class labels recent generative models successfully learn lowdimensional representations using autoencoding and have been argued to preserve better visual features here we leverage existing autoencoders and propose vaeqa a simple and efficient method for predicting image quality in the presence of a fullreference we evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets has fewer trainable parameters a smaller memory footprint and faster run time,0
in the field of 3d point cloud generation numerous 3d generative models have demonstrated the ability to generate diverse and realistic 3d shapes however the majority of these approaches struggle to generate controllable 3d point cloud shapes that meet userspecific requirements hindering the largescale application of 3d point cloud generation to address the challenge of lacking control in 3d point cloud generation we are the first to propose controlling the generation of point clouds by shape structures that comprise part existences and part adjacency relationships we manually annotate the adjacency relationships between the segmented parts of point cloud shapes thereby constructing a structuregraph representation based on this structuregraph representation we introduce strucadt a novel structurecontrollable point cloud generation model which consists of structuregraphnet module to extract structureaware latent features ccnf prior module to learn the distribution of the latent features controlled by the part adjacency and diffusion transformer module conditioned on the latent features and part adjacency to generate structureconsistent point cloud shapes experimental results demonstrate that our structurecontrollable 3d point cloud generation method produces highquality and diverse point cloud shapes enabling the generation of controllable point clouds based on userspecified shape structures and achieving stateoftheart performance in controllable point cloud generation on the shapenet dataset,0
this paper presents a new approach for 3d shape generation inversion and manipulation through a direct generative modeling on a continuous implicit representation in wavelet domain specifically we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3d shapes via truncated signed distance functions and multiscale biorthogonal wavelets then we design a pair of neural networks a diffusionbased generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details further we may jointly train an encoder network to learn a latent space for inverting shapes allowing us to enable a rich variety of wholeshape and regionaware shape manipulations both quantitative and qualitative experimental results manifest the compelling shape generation inversion and manipulation capabilities of our approach over the stateoftheart methods,0
recent progress in deep generative models has led to tremendous breakthroughs in image generation however while existing models can synthesize photorealistic images they lack an understanding of our underlying 3d world we present a new generative model visual object networks von synthesizing natural images of objects with a disentangled 3d representation inspired by classic graphics rendering pipelines we unravel our image formation process into three conditionally independent factorsshape viewpoint and textureand present an endtoend adversarial learning framework that jointly models 3d shapes and 2d images our model first learns to synthesize 3d shapes that are indistinguishable from real shapes it then renders the objects 25d sketches ie silhouette and depth map from its shape under a sampled viewpoint finally it learns to add realistic texture to these 25d sketches to generate natural images the von not only generates images that are more realistic than stateoftheart 2d image synthesis methods but also enables many 3d operations such as changing the viewpoint of a generated image editing of shape and texture linear interpolation in texture and shape space and transferring appearance across different objects and viewpoints,0
wholebody audiodriven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents with wideranging applications in virtual reality digital entertainment and remote communication existing approaches often generate audiodriven facial expressions and gestures independently which introduces a significant limitation the lack of seamless coordination between facial and gestural elements resulting in less natural and cohesive animations to address this limitation we propose asynfusion a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis the proposed method is built upon a dualbranch dit architecture which enables the parallel generation of facial expressions and gestures within the model we introduce a cooperative synchronization module to facilitate bidirectional feature interaction between the two modalities and an asynchronous lcm sampling strategy to reduce computational overhead while maintaining highquality outputs extensive experiments demonstrate that asynfusion achieves stateoftheart performance in generating realtime synchronized wholebody animations consistently outperforming existing methods in both quantitative and qualitative evaluations,0
research on video generation has recently made tremendous progress enabling highquality videos to be generated from text prompts or images adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it yet it remains challenging to generate a video of the same scene from multiple different camera trajectories solutions to this multivideo generation problem could enable largescale 3d scene generation with editable camera trajectories among other applications we introduce collaborative video diffusion cvd as an important step towards this vision the cvd framework includes a novel crossvideo synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism trained on top of a stateoftheart cameracontrol module for video generation cvd generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines as shown in extensive experiments project page,0
computeraided design cad is the most widely used modeling approach for technical design the typical starting point in these designs is 2d sketches which can later be extruded and combined to obtain complex threedimensional assemblies such sketches are typically composed of parametric primitives such as points lines and circular arcs augmented with geometric constraints linking the primitives such as coincidence parallelism or orthogonality sketches can be represented as graphs with the primitives as nodes and the constraints as edges training a model to automatically generate cad sketches can enable several novel workflows but is challenging due to the complexity of the graphs and the heterogeneity of the primitives and constraints in particular each type of primitive and constraint may require a record of different size and parameter types we propose sketchgen as a generative model based on a transformer architecture to address the heterogeneity problem by carefully designing a sequential language for the primitives and constraints that allows distinguishing between different primitive or constraint types and their parameters while encouraging our model to reuse information across related parameters encoding shared structure a particular highlight of our work is the ability to produce primitives linked via constraints that enables the final output to be further regularized via a constraint solver we evaluate our model by demonstrating constraint prediction for given sets of primitives and full sketch generation from scratch showing that our approach significantly out performs the stateoftheart in cad sketch generation,0
usergenerated cinematic creations are gaining popularity as our daily entertainment yet it is a challenge to master cinematography for producing immersive contents many existing automatic methods focus on roughly controlling predefined shot types or movement patterns which struggle to engage viewers with the circumstances of the actor realworld cinematographic rules show that directors can create immersion by comprehensively synchronizing the camera with the actor inspired by this strategy we propose a deep camera control framework that enables actorcamera synchronization in three aspects considering frame aesthetics spatial action and emotional status in the 3d virtual stage following ruleofthirds our framework first modifies the initial camera placement to position the actor aesthetically this adjustment is facilitated by a selfsupervised adjustor that analyzes frame composition via camera projection we then design a gan model that can adversarially synthesize finegrained camera movement based on the physical action and psychological state of the actor using an encoderdecoder generator to map kinematics and emotional variables into camera trajectories moreover we incorporate a regularizer to align the generated stylistic variances with specific emotional categories and intensities the experimental results show that our proposed method yields immersive cinematic videos of high quality both quantitatively and qualitatively live examples can be found in the supplementary video,0
numerous taskspecific variants of conditional generative adversarial networks have been developed for image completion yet a serious limitation remains that all existing algorithms tend to fail when handling largescale missing regions to overcome this challenge we propose a generic new approach that bridges the gap between imageconditional and recent modulated unconditional generative architectures via comodulation of both conditional and stochastic style representations also due to the lack of good quantitative metrics for image completion we propose the new pairedunpaired inception discriminative score pidsuids which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space experiments demonstrate superior performance in terms of both quality and diversity over stateoftheart methods in freeform image completion and easy generalization to imagetoimage translation code is available at,0
the recent availability and adaptability of texttoimage models has sparked a new era in many related domains that benefit from the learned text priors as well as highquality and fast generation capabilities one of which is texture generation for 3d objects although recent texture generation methods achieve impressive results by using texttoimage networks the combination of global consistency quality and speed which is crucial for advancing texture generation to realworld applications remains elusive to that end we introduce meta 3d texturegen a new feedforward method comprised of two sequential networks aimed at generating highquality and globally consistent textures for arbitrary geometries of any complexity degree in less than 20 seconds our method achieves stateoftheart results in quality and speed by conditioning a texttoimage model on 3d semantics in 2d space and fusing them into a complete and highresolution uv texture map as demonstrated by extensive qualitative and quantitative evaluations in addition we introduce a texture enhancement network that is capable of upscaling any texture by an arbitrary ratio producing 4k pixel resolution textures,0
we introduce rogr a novel approach that reconstructs a relightable 3d model of an object captured from multiple views driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations our method samples the appearance of the object under multiple lighting environments creating a dataset that is used to train a lightingconditioned neural radiance field nerf that outputs the objects appearance under any input environmental lighting the lightingconditioned nerf uses a novel dualbranch architecture to encode the general lighting effects and specularities separately the optimized lightingconditioned nerf enables efficient feedforward relighting under arbitrary environment maps without requiring perillumination optimization or light transport simulation we evaluate our approach on the established tensoir and stanfordorb datasets where it improves upon the stateoftheart on most metrics and showcase our approach on realworld object captures,0
we present primdiffusion the first diffusionbased framework for 3d human generation devising diffusion models for 3d human generation is difficult due to the intensive computational cost of 3d representations and the articulated topology of 3d humans to tackle these challenges our key insight is operating the denoising diffusion process directly on a set of volumetric primitives which models the human body as a number of small volumes with radiance and kinematic information this volumetric primitives representation marries the capacity of volumetric representations with the efficiency of primitivebased rendering our primdiffusion framework has three appealing properties 1 compact and expressive parameter space for the diffusion model 2 flexible 3d representation that incorporates human prior and 3 decoderfree rendering for efficient novelview and novelpose synthesis extensive experiments validate that primdiffusion outperforms stateoftheart methods in 3d human generation notably compared to ganbased methods our primdiffusion supports realtime rendering of highquality 3d humans at a resolution of 512times512 once the denoising process is done we also demonstrate the flexibility of our framework on trainingfree conditional generation such as texture transfer and 3d inpainting,0
diffusion models have transformed image generation yet controlling their outputs to reliably erase undesired concepts remains challenging existing approaches usually require taskspecific training and struggle to generalize across both concrete eg objects and abstract eg styles concepts we propose casteer crossattention steering a trainingfree framework for concept erasure in diffusion models using steering vectors to influence hidden representations dynamically casteer precomputes conceptspecific steering vectors by averaging neural activations from images generated for each target concept during inference it dynamically applies these vectors to suppress undesired concepts only when they appear ensuring that unrelated regions remain unaffected this selective activation enables precise contextaware erasure without degrading overall image quality this approach achieves effective removal of harmful or unwanted content across a wide range of visual concepts all without model retraining casteer outperforms stateoftheart concept erasure techniques while preserving unrelated content and minimizing unintended effects pseudocode is provided in the supplementary,0
the ability to generate diverse 3d articulated head avatars is vital to a plethora of applications including augmented reality cinematography and education recent work on textguided 3d object generation has shown great promise in addressing these needs these methods directly leverage pretrained 2d texttoimage diffusion models to generate 3dmultiviewconsistent radiance fields of generic objects however due to the lack of geometry and texture priors these methods have limited control over the generated 3d objects making it difficult to operate inside a specific domain eg human heads in this work we develop a new approach to textguided 3d head avatar generation to address this limitation our framework directly operates on the geometry and texture of an articulable 3d morphable model 3dmm of a head and introduces novel optimization procedures to update the geometry and texture while keeping the 2d and 3d facial features aligned the result is a 3d head avatar that is consistent with the text description and can be readily articulated using the deformation model of the 3dmm we show that our diffusionbased articulated head avatars outperform stateoftheart approaches for this task the latter are typically based on clip which is known to provide limited diversity of generation and accuracy for 3d object generation,0
generalized feedforward gaussian models have achieved significant progress in sparseview 3d reconstruction by leveraging prior knowledge from large multiview datasets however these models often struggle to represent highfrequency details due to the limited number of gaussians while the densification strategy used in perscene 3d gaussian splatting 3dgs optimization can be adapted to the feedforward models it may not be ideally suited for generalized scenarios in this paper we propose generative densification an efficient and generalizable method to densify gaussians generated by feedforward models unlike the 3dgs densification strategy which iteratively splits and clones raw gaussian parameters our method upsamples feature representations from the feedforward models and generates their corresponding fine gaussians in a single forward pass leveraging the embedded prior knowledge for enhanced generalization experimental results on both objectlevel and scenelevel reconstruction tasks demonstrate that our method outperforms stateoftheart approaches with comparable or smaller model sizes achieving notable improvements in representing fine details,0
we present a method to generate 3d objects in styles our method takes a text prompt and a style reference image as input and reconstructs a neural radiance field to synthesize a 3d model with the content aligning with the text prompt and the style following the reference image to simultaneously generate the 3d object and perform style transfer in one go we propose a stylized score distillation loss to guide a textto3d optimization process to output visually plausible geometry and appearance our stylized score distillation is based on a combination of an original pretrained texttoimage model and its modified sibling with the key and value features of selfattention layers manipulated to inject styles from the reference image comparisons with stateoftheart methods demonstrated the strong visual performance of our method further supported by the quantitative results from our user study,0
we introduce mystyle a personalized deep generative prior trained with a few shots of an individual mystyle allows to reconstruct enhance and edit images of a specific person such that the output is faithful to the persons key facial characteristics given a small reference set of portrait images of a person 100 we tune the weights of a pretrained stylegan face generator to form a local lowdimensional personalized manifold in the latent space we show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual moreover we demonstrate that we obtain a personalized generative prior and propose a unified approach to apply it to various illposed image enhancement problems such as inpainting and superresolution as well as semantic editing using the personalized generative prior we obtain outputs that exhibit highfidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set we demonstrate our method with fairuse images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome we evaluate our approach against fewshots baselines and show that our personalized prior quantitatively and qualitatively outperforms stateoftheart alternatives,0
embodied human communication encompasses both verbal speech and nonverbal information eg gesture and head movements recent advances in machine learning have substantially improved the technologies for generating synthetic versions of both of these types of data on the speech side texttospeech systems are now able to generate highly convincing spontaneoussounding speech using unscripted speech audio as the source material on the motion side probabilistic motiongeneration methods can now synthesise vivid and lifelike speechdriven 3d gesticulation in this paper we put these two stateoftheart technologies together in a coherent fashion for the first time concretely we demonstrate a proofofconcept system trained on a singlespeaker audio and motioncapture dataset that is able to generate both speech and fullbody gestures together from text input in contrast to previous approaches for joint speechandgesture generation we generate fullbody gestures from speech synthesis trained on recordings of spontaneous speech from the same person as the motioncapture data we illustrate our results by visualising gesture spaces and textspeechgesture alignments and through a demonstration video at,0
in the early stages of architectural design shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs generative artificial intelligence ai provides a promising solution to automate this transformation but ensuring multiview consistency remains a significant challenge to solve this issue we propose a novel threestage consistent image generation framework using generative ai models to generate architectural designs from shoebox model representations the proposed method enhances stateoftheart image generation diffusion models to generate multiview consistent architectural images we employ controlnet as the backbone and optimize it to accommodate multiview inputs of architectural shoebox models captured from predefined perspectives to ensure stylistic and structural consistency across multiview images we propose an image space loss module that incorporates style loss structural loss and angle alignment loss we then use depth estimation method to extract depth maps from the generated multiview images finally we use the paired data of the architectural images and depth maps as inputs to improve the multiview consistency via the depthaware 3d attention module experimental results demonstrate that the proposed framework can generate multiview architectural images with consistent style and structural coherence from shoebox model inputs,0
stateoftheart diffusion models can generate highly realistic images based on various conditioning like text segmentation and depth however an essential aspect often overlooked is the specific camera geometry used during image capture the influence of different optical systems on the final scene appearance is frequently overlooked this study introduces a framework that intimately integrates a texttoimage diffusion model with the particular lens geometry used in image rendering our method is based on a perpixel coordinate conditioning method enabling the control over the rendering geometry notably we demonstrate the manipulation of curvature properties achieving diverse visual effects such as fisheye panoramic views and spherical texturing using a single diffusion model,0
generating artistic portraits is a challenging problem in computer vision existing portrait stylization models that generate good quality results are based on imagetoimage translation and require abundant data from both source and target domains however without enough data these methods would result in overfitting in this work we propose ctlgan a new fewshot artistic portraits generation model with a novel contrastive transfer learning strategy we adapt a pretrained stylegan in the source domain to a target artistic domain with no more than 10 artistic faces to reduce overfitting to the few training examples we introduce a novel crossdomain triplet loss which explicitly encourages the target instances generated from different latent codes to be distinguishable we propose a new encoder which embeds real faces into z space and proposes a dualpath training strategy to better cope with the adapted decoder and eliminate the artifacts extensive qualitative quantitative comparisons and a user study show our method significantly outperforms stateofthearts under 10shot and 1shot settings and generates high quality artistic portraits the code will be made publicly available,0
humans can easily infer the underlying 3d geometry and texture of an object only from a single 2d image current computer vision methods can do this too but suffer from view generalization problems the models inferred tend to make poor predictions of appearance in novel views as for generalization problems in machine learning the difficulty is balancing singleview accuracy cf training error bias with novel view accuracy cf test error variance we describe a class of models whose geometric rigidity is easily controlled to manage this tradeoff we describe a cycle consistency loss that improves view generalization roughly a model from a generated view should predict the original view well view generalization of textures requires that models share texture information so a car seen from the back still has headlights because other cars have headlights we describe a cycle consistency loss that encourages model textures to be aligned so as to encourage sharing we compare our method against the stateoftheart method and show both qualitative and quantitative improvements,0
recent advances in generative modeling and tokenization have driven significant progress in texttomotion generation leading to enhanced quality and realism in generated motions however effectively leveraging textual information for conditional motion generation remains an open challenge we observe that current approaches primarily relying on fixedlength text embeddings eg clip for global semantic injection struggle to capture the composite nature of human motion resulting in suboptimal motion quality and controllability to address this limitation we propose the composite aware semantic injection mechanism casim comprising a compositeaware semantic encoder and a textmotion aligner that learns the dynamic correspondence between text and motion tokens notably casim is model and representationagnostic readily integrating with both autoregressive and diffusionbased methods experiments on humanml3d and kit benchmarks demonstrate that casim consistently improves motion quality textmotion alignment and retrieval scores across stateoftheart methods qualitative analyses further highlight the superiority of our compositeaware approach over fixedlength semantic injection enabling precise motion control from text prompts and stronger generalization to unseen text inputs,0
the recent success of pretrained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3d meshes in the wild however these models are trained in the screen space while converting them to a multiview consistent texture image poses a major obstacle to the output quality in this paper we propose a novel method to enforce multiview consistency our method is based on the observation that latent space in a pretrained diffusion model is noised separately for each camera view making it difficult to achieve multiview consistency by directly manipulating the latent codes based on the celebrated denoising diffusion implicit models ddim scheme we propose to use an optimizationbased colorfusion to enforce consistency and indirectly modify the latent codes by gradient backpropagation our method further relaxes the sequential dependency assumption among the camera views by evaluating on a series of general 3d models we find our simple approach improves consistency and overall quality of the generated textures as compared to competing stateofthearts our implementation is available at,0
current methods for generating 3d scene layouts from text predominantly follow a declarative paradigm where a large language model llm specifies highlevel constraints that are then resolved by a separate solver this paper challenges that consensus by introducing a more direct imperative approach we task an llm with generating a stepbystep program that iteratively places each object relative to those already in the scene this paradigm simplifies the underlying scene specification language enabling the creation of more complex varied and highly structured layouts that are difficult to express declaratively to improve the robustness we complement our method with a novel llmfree error correction mechanism that operates directly on the generated code iteratively adjusting parameters within the program to resolve collisions and other inconsistencies in forcedchoice perceptual studies human participants overwhelmingly preferred our imperative layouts choosing them over those from two stateoftheart declarative systems 82 and 94 of the time demonstrating the significant potential of this alternative paradigm finally we present a simple automated evaluation metric for 3d scene layout generation that correlates strongly with human judgment,0
biometric face morphing poses a critical challenge to identity verification systems undermining their security and robustness to address this issue we propose wafusion a novel framework combining wavelet decomposition and diffusion models to generate highquality realistic morphed face images efficiently wafusion leverages the structural details captured by wavelet transforms and the generative capabilities of diffusion models producing face morphs with minimal artifacts experiments conducted on feret frgc frll and wvu twin datasets demonstrate wafusions superiority over stateoftheart methods producing highresolution morphs with fewer artifacts our framework excels across key biometric metrics including the attack presentation classification error rate apcer bona fide presentation classification error rate bpcer and equal error rate eer this work sets a new benchmark in biometric morph generation offering a cuttingedge and efficient solution to enhance biometric security systems,0
we present text2gestures a transformerbased learning method to interactively generate emotive fullbody gestures for virtual agents aligned with natural language text inputs our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions also known as affective features we also consider the intended task corresponding to the text and the target virtual agents intended gender and handedness in our generation pipeline we train and evaluate our network on the mpi emotional body expressions database and observe that our network produces stateoftheart performance in generating gestures for virtual agents aligned with the text for narration or conversation our network can generate these gestures at interactive rates on a commodity gpu we conduct a webbased user study and observe that around 91 of participants indicated our generated gestures to be at least plausible on a fivepoint likert scale the emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions with a minimum pearson coefficient of 077 in the valence dimension,0
we propose a novel framework collage for generating collaborative agentobjectagent interactions by leveraging large language models llms and hierarchical motionspecific vectorquantized variational autoencoders vqvaes our model addresses the lack of rich datasets in this domain by incorporating the knowledge and reasoning abilities of llms to guide a generative diffusion model the hierarchical vqvae architecture captures different motionspecific characteristics at multiple levels of abstraction avoiding redundant concepts and enabling efficient multiresolution representation we introduce a diffusion model that operates in the latent space and incorporates llmgenerated motion planning cues to guide the denoising process resulting in promptspecific motion generation with greater control and diversity experimental results on the core4d and interhuman datasets demonstrate the effectiveness of our approach in generating realistic and diverse collaborative humanobjecthuman interactions outperforming stateoftheart methods our work opens up new possibilities for modeling complex interactions in various domains such as robotics graphics and computer vision,0
stateoftheart neural rendering methods optimize gaussian scene representations from a few photographs for novelview synthesis building on these representations we develop an efficient algorithm dubbed gaussian wave splatting to turn these gaussians into holograms unlike existing computergenerated holography cgh algorithms gaussian wave splatting supports accurate occlusions and viewdependent effects for photorealistic scenes by leveraging recent advances in neural rendering specifically we derive a closedform solution for a 2d gaussiantohologram transform that supports occlusions and alpha blending inspired by classic computer graphics techniques we also derive an efficient approximation of the aforementioned process in the fourier domain that is easily parallelizable and implement it using custom cuda kernels by integrating emerging neural rendering pipelines with holographic display technology our gaussianbased cgh framework paves the way for nextgeneration holographic displays,0
drawing freehand sketches of mechanical components on multimedia devices for aibased engineering modeling has become a new trend however its development is being impeded because existing works cannot produce suitable sketches for datadriven research these works either generate sketches lacking a freehand style or utilize generative models not originally designed for this task resulting in poor effectiveness to address this issue we design a twostage generative framework mimicking the human sketching behavior pattern called msformer which is the first time to produce humanoid freehand sketches tailored for mechanical components the first stage employs open cascade technology to obtain multiview contour sketches from mechanical components filtering perturbing signals for the ensuing generation process meanwhile we design a view selector to simulate viewpoint selection tasks during human sketching for picking out informationrich sketches the second stage translates contour sketches into freehand sketches by a transformerbased generator to retain essential modeling features as much as possible and rationalize stroke distribution we introduce a novel edgeconstraint stroke initialization furthermore we utilize a clip vision encoder and a new loss function incorporating the hausdorff distance to enhance the generalizability and robustness of the model extensive experiments demonstrate that our approach achieves stateoftheart performance for generating freehand sketches in the mechanical domain project page,0
we introduce clrwire a novel framework for 3d curvebased wireframe generation that integrates geometry and topology into a unified continuous latent representation unlike conventional methods that decouple vertices edges and faces clrwire encodes curves as neural parametric curves along with their topological connectivity into a continuous and fixedlength latent space using an attentiondriven variational autoencoder vae this unified approach facilitates joint learning and generation of both geometry and topology to generate wireframes we employ a flow matching model to progressively map gaussian noise to these latents which are subsequently decoded into complete 3d wireframes our method provides finegrained modeling of complex shapes and irregular topologies and supports both unconditional generation and generation conditioned on point cloud or image inputs experimental results demonstrate that compared with stateoftheart generative approaches our method achieves substantial improvements in accuracy novelty and diversity offering an efficient and comprehensive solution for cad design geometric reconstruction and 3d content creation,0
recent texttoimage generation methods provide a simple yet exciting conversion capability between text and image domains while these methods have incrementally improved the generated image fidelity and text relevancy several pivotal gaps remain unanswered limiting applicability and quality we propose a novel texttoimage method that addresses these gaps by i enabling a simple control mechanism complementary to text in the form of a scene ii introducing elements that substantially improve the tokenization process by employing domainspecific knowledge over key image regions faces and salient objects and iii adapting classifierfree guidance for the transformer use case our model achieves stateoftheart fid and human evaluation results unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels significantly improving visual quality through scene controllability we introduce several new capabilities i scene editing ii text editing with anchor scenes iii overcoming outofdistribution text prompts and iv story illustration generation as demonstrated in the story we wrote,0
fullbody gestures play a pivotal role in natural interactions and are crucial for achieving effective communication nevertheless most existing studies primarily focus on the gesture generation of speakers overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them this paper innovatively proposes an interdiffusion generation model of speakers and listeners for effective communication for the first time we integrate the fullbody gestures of listeners into the generation framework by devising a novel interdiffusion mechanism this model can accurately capture the complex interaction patterns between speakers and listeners during communication in the model construction process based on the advanced diffusion model architecture we innovatively introduce interaction conditions and the gan model to increase the denoising step size as a result when generating gesture sequences the model can not only dynamically generate based on the speakers speech information but also respond in realtime to the listeners feedback enabling synergistic interaction between the two abundant experimental results demonstrate that compared with the current stateoftheart gesture generation methods the model we proposed has achieved remarkable improvements in the naturalness coherence and speechgesture synchronization of the generated gestures in the subjective evaluation experiments users highly praised the generated interaction scenarios believing that they are closer to real life human communication situations objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators providing more powerful support for effective communication,0
scanning reallife scenes with modern registration devices typically gives incomplete point cloud representations primarily due to the limitations of partial scanning 3d occlusions and dynamic light conditions recent works on processing incomplete point clouds have always focused on point cloud completion however these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry we propose using generative pointbased nerf gpn to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud the repaired point cloud can achieve multiview consistency with the captured images at high spatial resolution for the finetunes of a single scene we optimize the global latent condition by incorporating an autodecoder architecture while retaining multiview consistency as a result the generated point clouds are smooth plausible and geometrically consistent with the partial scanning images extensive experiments on shapenet demonstrate that our works achieve competitive performances to the other stateoftheart point cloudbased neural scene rendering and editing performances,0
numerous attempts have been made to the task of personagnostic face swapping given its wide applications while existing methods mostly rely on tedious network and loss designs they still struggle in the information balancing between the source and target faces and tend to produce visible artifacts in this work we introduce a concise and effective framework named styleswap our core idea is to leverage a stylebased generator to empower highfidelity and robust face swapping thus the generators advantage can be adopted for optimizing identity similarity we identify that with only minimal modifications a stylegan2 architecture can successfully handle the desired information from both source and target additionally inspired by the torgb layers a swappingdriven mask branch is further devised to improve information blending furthermore the advantage of stylegan inversion can be adopted particularly a swappingguided id inversion strategy is proposed to optimize identity similarity extensive experiments validate that our framework generates highquality face swapping results that outperform stateoftheart methods both qualitatively and quantitatively,0
scalable vector graphics svg are essential xmlbased formats for versatile graphics offering resolution independence and scalability unlike raster images svgs use geometric shapes and support interactivity animation and manipulation via css and javascript current svg generation methods face challenges related to high computational costs and complexity in contrast human designers use componentbased tools for efficient svg creation inspired by this svgbuilder introduces a componentbased autoregressive model for generating highquality colored svgs from textual input it significantly reduces computational overhead and improves efficiency compared to traditional methods our model generates svgs up to 604 times faster than optimizationbased approaches to address the limitations of existing svg datasets and support our research we introduce colorsvg100k the first largescale dataset of colored svgs comprising 100000 graphics this dataset fills the gap in color information for svg generation models and enhances diversity in model training evaluation against stateoftheart models demonstrates svgbuilders superior performance in practical applications highlighting its efficiency and quality in generating complex svg graphics,0
recent advancements in implicit 3d representations and generative models have markedly propelled the field of 3d object generation forward however it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls which is crucial in fields like industrial design and manufacturing to bridge this gap we introduce a framework that employs large language models llms to generate textdriven 3d shapes manipulating 3d software via program synthesis we present 3dpremise a dataset specifically tailored for 3d parametric modeling of industrial shapes designed to explore stateoftheart llms within our proposed pipeline our work reveals effective generation strategies and delves into the selfcorrection capabilities of llms using a visual interface our work highlights both the potential and limitations of llms in 3d parametric modeling for industrial applications,0
we present a generalization of the bilateral filter that can be applied to featurepreserving smoothing of signals on images meshes and other domains within a single unified framework our discretization is competitive with stateoftheart smoothing techniques in terms of both accuracy and speed is easy to implement and has parameters that are straightforward to understand unlike previous bilateral filters developed for meshes and other irregular domains our construction reduces exactly to the image bilateral on rectangular domains and comes with a rigorous foundation in both the smooth and discrete settings these guarantees allow us to construct unconditionally convergent meanshift schemes that handle a variety of extremely noisy signals we also apply our framework to geometric edgepreserving effects like feature enhancement and show how it is related to local histogram techniques,0
this paper presents vqsgen a novel algorithm for highquality creative sketch generation recent approaches have framed the task as pixelbased generation either as a whole or partbypart neglecting the intrinsic and contextual relationships among individual strokes such as the shape and spatial positioning of both proximal and distant strokes to overcome these limitations we propose treating each stroke within a sketch as an entity and introducing a vectorquantized vq stroke representation for finegrained sketch generation our method follows a twostage framework in stage one we decouple each strokes shape and location information to ensure the vq representation prioritizes stroke shape learning in stage two we feed the precise and compact representation into an autodecoding transformer to incorporate stroke semantics positions and shapes into the generation process by utilizing tokenized stroke representation our approach generates strokes with high fidelity and facilitates novel applications such as text or class label conditioned generation and sketch completion comprehensive experiments demonstrate our method surpasses existing stateoftheart techniques on the creativesketch dataset underscoring its effectiveness,0
training audiotoimage generative models requires an abundance of diverse audiovisual pairs that are semantically aligned such data is almost always curated from inthewild videos given the crossmodal semantic correspondence that is inherent to them in this work we hypothesize that insisting on the absolute need for ground truth audiovisual correspondence is not only unnecessary but also leads to severe restrictions in scale quality and diversity of the data ultimately impairing its use in the modern generative models that is we propose a scalable image sonification framework where instances from a variety of highquality yet disjoint unimodal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern visionlanguage models to demonstrate the efficacy of this approach we use our sonified images to train an audiotoimage generative model that performs competitively against stateoftheart finally through a series of ablation studies we exhibit several intriguing auditory capabilities like semantic mixing and interpolation loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process,0
the rapid advancement of artificial intelligence generated content aigc techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions this capability substantially reduces human labor and production costs in traditional marketing workflows however existing aigc techniques either demand extensive finetuning for each referenced image to achieve high fidelity or they struggle to maintain fidelity across diverse products making them impractical for ecommerce and marketing industries to tackle this limitation we first construct adprod100k a largescale advertising image generation dataset a key innovation in its construction is our dual data augmentation strategy which fosters robust 3daware representations crucial for realistic and highfidelity image synthesis leveraging this dataset we propose refadgen a generation framework that achieves high fidelity through a decoupled design the framework enforces precise spatial control by injecting a product mask at the unet input and employs an efficient attention fusion module afm to integrate product features this design effectively resolves the fidelityefficiency dilemma present in existing methods extensive experiments demonstrate that refadgen achieves stateoftheart performance showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging realworld inthewild images this offers a scalable and costeffective alternative to traditional workflows code and datasets are publicly available at,0
this work addresses the problem of generating 3d holistic body motions from human speech given a speech recording we synthesize sequences of 3d body poses hand gestures and facial expressions that are realistic and diverse to achieve this we first build a highquality dataset of 3d holistic body meshes with synchronous speech we then define a novel speechtomotion generation framework in which the face body and hands are modeled separately the separated modeling stems from the fact that face articulation strongly correlates with human speech while body poses and hand gestures are less correlated specifically we employ an autoencoder for face motions and a compositional vectorquantized variational autoencoder vqvae for the body and hand motions the compositional vqvae is key to generating diverse results additionally we propose a crossconditional autoregressive model that generates body poses and hand gestures leading to coherent and realistic motions extensive experiments and user studies demonstrate that our proposed approach achieves stateoftheart performance both qualitatively and quantitatively our novel dataset and code will be released for research purposes at,0
in this work we introduce cadcoder a novel framework that reformulates texttocad as the generation of cadquery scripts a pythonbased parametric cad language this representation enables direct geometric validation a richer modeling vocabulary and seamless integration with existing llms to further enhance code validity and geometric fidelity we propose a twostage learning pipeline 1 supervised finetuning on paired textcadquery data and 2 reinforcement learning with group reward policy optimization grpo guided by a cadspecific reward comprising both a geometric reward chamfer distance and a format reward we also introduce a chainofthought cot planning process to improve model reasoning and construct a largescale highquality dataset of 110k textcadquery3d model triplets and 15k cot samples via an automated pipeline extensive experiments demonstrate that cadcoder enables llms to generate diverse valid and complex cad models directly from natural language advancing the state of the art of texttocad generation and geometric reasoning,0
estimating video depth in openworld scenarios is challenging due to the diversity of videos in appearance content motion camera movement and length we present depthcrafter an innovative method for generating temporally consistent long depth sequences with intricate details for openworld videos without requiring any supplementary information such as camera poses or optical flow the generalization ability to openworld videos is achieved by training the videotodepth model from a pretrained imagetovideo diffusion model through our meticulously designed threestage training strategy our training approach enables the model to generate depth sequences with variable lengths at one time up to 110 frames and harvest both precise depth details and rich content diversity from realistic and synthetic datasets we also propose an inference strategy that can process extremely long videos through segmentwise estimation and seamless stitching comprehensive evaluations on multiple datasets reveal that depthcrafter achieves stateoftheart performance in openworld video depth estimation under zeroshot settings furthermore depthcrafter facilitates various downstream applications including depthbased visual effects and conditional video generation,0
cospeech gestures gestures that accompany speech play an important role in human communication automatic cospeech gesture generation is thus a key enabling technology for embodied conversational agents ecas since humans expect ecas to be capable of multimodal communication research into gesture generation is rapidly gravitating towards datadriven methods unfortunately individual research efforts in the field are difficult to compare there are no established benchmarks and each study tends to use its own dataset motion visualisation and evaluation methodology to address this situation we launched the genea challenge a gesturegeneration challenge wherein participating teams built automatic gesturegeneration systems on a common dataset and the resulting systems were evaluated in parallel in a large crowdsourced user study using the same motionrendering pipeline since differences in evaluation outcomes between systems now are solely attributable to differences between the motiongeneration methods this enables benchmarking recent approaches against one another in order to get a better impression of the state of the art in the field this paper reports on the purpose design results and implications of our challenge,0
photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks gans yet the dark side of such deepfakes the malicious use of generated media raises concerns about visual misinformation while existing research work on deepfake detection demonstrates high accuracy it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques thus we seek a proactive and sustainable solution on deepfake detection that is agnostic to the evolution of generative models by introducing artificial fingerprints into the models our approach is simple and effective we first embed artificial fingerprints into training data then validate a surprising discovery on the transferability of such fingerprints from training data to generative models which in turn appears in the generated deepfakes experiments show that our fingerprinting solution 1 holds for a variety of cuttingedge generative models 2 leads to a negligible side effect on generation quality 3 stays robust against imagelevel and modellevel perturbations 4 stays hard to be detected by adversaries and 5 converts deepfake detection and attribution into trivial tasks and outperforms the recent stateoftheart baselines our solution closes the responsibility loop between publishing pretrained generative model inventions and their possible misuses which makes it independent of the current arms race code and models are available at,0
the current stateoftheart diffusion model has demonstrated excellent results in generating images however the images are monotonous and are mostly the result of the distribution of images of people in the training set making it challenging to generate multiple images for a fixed number of individuals this problem can often only be solved by finetuning the training of the model this means that each individualanimated character image must be trained if it is to be drawn and the hardware and cost of this training is often beyond the reach of the average user who accounts for the largest number of people to solve this problem the character image feature encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation in addition various details can be adjusted during the process using prompts unlike traditional imagetoimage models the character image feature encoder extracts only the relevant image features rather than information about the models composition or movements in addition the character image feature encoder can be adapted to different models after training the proposed model can be conveniently incorporated into the stable diffusion generation process without modifying the models ontology or used in combination with stable diffusion as a joint model,0
the utilization of the triplanebased radiance fields has gained attention in recent years due to its ability to effectively disentangle 3d scenes with a highquality representation and low computation cost a key requirement of this method is the precise input of camera poses however due to the local update property of the triplane a similar joint estimation as previous joint posenerf optimization works easily results in local minima to this end we propose the disentangled triplane generation module to introduce global feature context and smoothness into triplane learning which mitigates errors caused by local updating then we propose the disentangled plane aggregation to mitigate the entanglement caused by the common triplane feature aggregation during camera pose updating in addition we introduce a twostage warmstart training strategy to reduce the implicit constraints caused by the triplane generator quantitative and qualitative results demonstrate that our proposed method achieves stateoftheart performance in novel view synthesis with noisy or unknown camera poses as well as efficient convergence of optimization project page,0
we present a novel neural algorithm for performing highquality highresolution realtime novel view synthesis from a sparse set of input rgb images or videos streams our network both reconstructs the 3d scene and renders novel views at 1080p resolution at 30fps on an nvidia a100 our feedforward network generalizes across a wide variety of datasets and scenes and produces stateoftheart quality for a realtime method our quality approaches and in some cases surpasses the quality of some of the top offline methods in order to achieve these results we use a novel combination of several key concepts and tie them together into a cohesive and effective algorithm we build on previous works that represent the scene using semitransparent layers and use an iterative learned renderandrefine approach to improve those layers instead of flat layers our method reconstructs layered depth maps ldms that efficiently represent scenes with complex depth and occlusions the iterative update steps are embedded in a multiscale unetstyle architecture to perform as much compute as possible at reduced resolution within each update step to better aggregate the information from multiple input views we use a specialized transformerbased network component this allows the majority of the perinput image processing to be performed in the input image space as opposed to layer space further increasing efficiency finally due to the realtime nature of our reconstruction and rendering we dynamically create and discard the internal 3d geometry for each frame generating the ldm for each view taken together this produces a novel and effective algorithm for view synthesis through extensive evaluation we demonstrate that we achieve stateoftheart quality at realtime rates project page,0
in recent years 3d generation has made great strides in both academia and industry however generating 3d scenes from a single rgb image remains a significant challenge as current approaches often struggle to ensure both object generation quality and scene coherence in multiobject scenarios to overcome these limitations we propose a novel threestage framework for 3d scene generation with explicit geometric representations and highquality textural details via single imageguided model generation and spatial layout optimization our method begins with an image instance segmentation and inpainting phase which recovers missing details of occluded objects in the input images thereby achieving complete generation of foreground 3d assets subsequently our approach captures the spatial geometry of reference image by constructing pseudostereo viewpoint for camera parameter estimation and scene depth inference while employing a model selection strategy to ensure optimal alignment between the 3d assets generated in the previous step and the input finally through model parameterization and minimization of the chamfer distance between point clouds in 3d and 2d space our approach optimizes layout parameters to produce an explicit 3d scene representation that maintains precise alignment with input guidance image extensive experiments on multiobject scene image sets have demonstrated that our approach not only outperforms stateoftheart methods in terms of geometric accuracy and texture fidelity of individual generated 3d models but also has significant advantages in scene layout synthesis,0
head and gaze dynamics are crucial in expressive 3d facial animation for conveying emotion and intention however existing methods frequently address facial components in isolation overlooking the intricate coordination between gaze head motion and speech the scarcity of highquality gazeannotated datasets hinders the development of datadriven models capable of capturing realistic personalized gaze control to address these challenges we propose stygazetalk an audiodriven method that generates synchronized gaze and head motion styles we extract speakerspecific motion traits from gazehead sequences with a multilayer lstm structure incorporating a style encoder enabling the generation of diverse animation styles we also introduce a highprecision multimodal dataset comprising eyetracked gaze audio head pose and 3d facial parameters providing a valuable resource for training and evaluating head and gaze control models experimental results demonstrate that our method generates realistic temporally coherent and styleaware headgaze motions significantly advancing the stateoftheart in audiodriven facial animation,0
recent progress on deep learning has made it possible to automatically transform the screenshot of graphic user interface gui into code by using the encoderdecoder framework while the commonly adopted image encoder eg cnn network might be capable of extracting image features to the desired level interpreting these abstract image features into hundreds of tokens of code puts a particular challenge on the decoding power of the rnnbased code generator considering the code used for describing gui is usually hierarchically structured we propose a new attentionbased hierarchical code generation model which can describe gui images in a finer level of details while also being able to generate hierarchically structured code in consistency with the hierarchical layout of the graphic elements in the gui our model follows the encoderdecoder framework all the components of which can be trained jointly in an endtoend manner the experimental results show that our method outperforms other current stateoftheart methods on both a publicly available guicode dataset as well as a dataset established by our own,0
we present makeatexture a new framework that efficiently synthesizes highresolution texture maps from textual prompts for given 3d geometries our approach progressively generates textures that are consistent across multiple viewpoints with a depthaware inpainting diffusion model in an optimized sequence of viewpoints determined by an automatic view selection algorithm a significant feature of our method is its remarkable efficiency achieving a full texture generation within an endtoend runtime of just 307 seconds on a single nvidia h100 gpu significantly outperforming existing methods such an acceleration is achieved by optimizations in the diffusion model and a specialized backprojection method moreover our method reduces the artifacts in the backprojection phase by selectively masking out nonfrontal faces and internal faces of opensurfaced objects experimental results demonstrate that makeatexture matches or exceeds the quality of other stateoftheart methods our work significantly improves the applicability and practicality of texture generation models for realworld 3d content creation including interactive creation and textguided texture editing,0
with the growing demand for highfidelity 3d models from 2d images existing methods still face significant challenges in accurately reproducing finegrained geometric details due to limitations in domain gaps and inherent ambiguities in rgb images to address these issues we propose hi3dgen a novel framework for generating highfidelity 3d geometry from images via normal bridging hi3dgen consists of three key components 1 an imagetonormal estimator that decouples the lowhigh frequency image pattern with noise injection and dualstream training to achieve generalizable stable and sharp estimation 2 a normaltogeometry learning approach that uses normalregularized latent diffusion learning to enhance 3d geometry generation fidelity and 3 a 3d data synthesis pipeline that constructs a highquality dataset to support training extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details outperforming stateoftheart methods in terms of fidelity our work provides a new direction for highfidelity 3d geometry generation from images by leveraging normal maps as an intermediate representation,0
one major challenge in science is to make all results potentially reproducible thus along with the raw data every step from basic processing of the data evaluation to the generation of the figures has to be documented as clearly as possible while there are many programming libraries that cover the basic processing and plotting steps eg matplotlib in python no library yet addresses the reproducible composing of single plots into meaningful figures for publication thus up to now it is still stateoftheart to generate publishable figures using imageprocessing or vectordrawing software leading to unwanted alterations of the presented data in the worst case and to figure quality reduction in the best case pylustrator a open source library based on the matplotlib aims to fill this gap and provides a tool to easily generate the code necessary to compose publication figures from single plots it provides a graphical user interface where the user can interactively compose the figures all changes are tracked and converted to code that is automatically integrated into the calling script file thus this software provides the missing link from raw data to the complete plot published in scientific journals and thus contributes to the transparency of the complete evaluation procedure,0
this paper presents an unpaired method for creating line drawings from photographs current methods often rely on high quality paired datasets to generate line drawings however these datasets often have limitations due to the subjects of the drawings belonging to a specific domain or in the amount of data collected although recent work in unsupervised imagetoimage translation has shown much progress the latest methods still struggle to generate compelling line drawings we observe that line drawings are encodings of scene information and seek to convey 3d shape and semantic meaning we build these observations into a set of objectives and train an image translation to map photographs into line drawings we introduce a geometry loss which predicts depth information from the image features of a line drawing and a semantic loss which matches the clip features of a line drawing with its corresponding photograph our approach outperforms stateoftheart unpaired image translation and line drawing generation methods on creating line drawings from arbitrary photographs for code and demo visit our webpage carolineecgithubioinformativedrawings,0
generative adversarial networks have been successfully applied to inpainting in natural images however the current stateoftheart models have not yet been widely adopted in the medical imaging domain in this paper we investigate the performance of three recently published deep learning based inpainting models context encoders semantic image inpainting and the contextual attention model applied to chest xrays as the chest exam is the most commonly performed radiological procedure we train these generative models on 12m 128 times 128 patches from 60k healthy xrays and learn to predict the center 64 times 64 region in each patch we test the models on both the healthy and abnormal radiographs we evaluate the results by visual inspection and comparing the psnr scores the outputs of the models are in most cases highly realistic we show that the methods have potential to enhance and detect abnormalities in addition we perform a 2afc observer study and show that an experienced human observer performs poorly in detecting inpainted regions particularly those generated by the contextual attention model,0
we introduce sdmnet a deep generative neural network which produces structured deformable meshes specifically the network is trained to generate a spatial arrangement of closed deformable mesh parts which respect the global part structure of a shape collection eg chairs airplanes etc our key observation is that while the overall structure of a 3d shape can be complex the shape can usually be decomposed into a set of parts each homeomorphic to a box and the finerscale geometry of the part can be recovered by deforming the box the architecture of sdmnet is that of a twolevel variational autoencoder vae at the part level a partvae learns a deformable model of part geometries at the structural level we train a structured parts vae spvae which jointly learns the part structure of a shape collection and the part geometries ensuring a coherence between global shape structure and surface details through extensive experiments and comparisons with the stateoftheart deep generative models of shapes we demonstrate the superiority of sdmnet in generating meshes with visual quality flexible topology and meaningful structures which benefit shape interpolation and other subsequently modeling tasks,0
generative diffusion models have advanced image editing with highquality results and intuitive interfaces such as prompts and semantic drawing however these interfaces lack precise control and the associated methods typically specialize on a single editing task we introduce a versatile generative workflow that operates in an intrinsicimage latent space enabling semantic local manipulation with pixel precision for a range of editing operations building atop the rgbx diffusion framework we address key challenges of identity preservation and intrinsicchannel entanglement by incorporating exact diffusion inversion and disentangled channel manipulation we enable precise efficient editing with automatic resolution of global illumination effects all without additional data collection or model finetuning we demonstrate stateoftheart performance across a variety of tasks on complex images including color and texture adjustments object insertion and removal global relighting and their combinations,0
propelled by the breakthrough in deep generative models audiotoimage generation has emerged as a pivotal crossmodal task that converts complex auditory signals into rich visual representations however previous works only focus on singlesource audio inputs for image generation ignoring the multisource characteristic in natural auditory scenes thus limiting the performance in generating comprehensive visual content to bridge this gap we propose a method called macs to conduct multisource audiotoimage generation to our best knowledge this is the first work that explicitly separates multisource audio to capture the rich audio components before image generation macs is a twostage method in the first stage multisource audio inputs are separated by a weakly supervised method where the audio and text labels are semantically aligned by casting into a common space using the large pretrained clap model we introduce a ranking loss to consider the contextual significance of the separated audio signals in the second stage effective image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a mlp layer we preprocess the llp dataset as the first full multisource audiotoimage generation benchmark the experiments are conducted on multisource mixedsource and singlesource audiotoimage generation tasks the proposed macs outperforms the current stateoftheart methods in 17 out of the 21 evaluation indexes on all tasks and delivers superior visual quality,0
the unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders autoregressive image models have been able to generate small images unconditionally but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail to address the former challenge we propose the subscale pixel network spn a conditional decoder architecture that generates an image as a sequence of subimages of equal size the spn compactly captures imagewide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models to address the latter challenge we propose to use multidimensional upscaling to grow an image in both size and depth via intermediate stages utilising distinct spns we evaluate spns on the unconditional generation of celebahq of size 256 and of imagenet from size 32 to 256 we achieve stateoftheart likelihood results in multiple settings set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets,0
we present a caricature generation framework based on shape and style manipulation using stylegan our framework dubbed stylecarigan automatically creates a realistic and detailed caricature from an input photo with optional controls on shape exaggeration degree and color stylization type the key component of our method is shape exaggeration blocks that are used for modulating coarse layer feature maps of stylegan to produce desirable caricature shape exaggerations we first build a layermixed stylegan for phototocaricature style conversion by swapping fine layers of the stylegan for photos to the corresponding layers of the stylegan trained to generate caricatures given an input photo the layermixed model produces detailed color stylization for a caricature but without shape exaggerations we then append shape exaggeration blocks to the coarse layers of the layermixed model and train the blocks to create shape exaggerations while preserving the characteristic appearances of the input experimental results show that our stylecarigan generates realistic and detailed caricatures compared to the current stateoftheart methods we demonstrate stylecarigan also supports other styleganbased image manipulations such as facial expression control,0
the research on developing planar curves to produce visually pleasing products ranges from electric appliances to car body design and indentifyingmodifying planar curves for special purposes namely for railway design highway design and robot trajectories have been progressing since 1970s the pattern of research in this field of study has branched to five major groups namely curve synthesis fairing process improvement in control of natural spiral construction of new type of planar curves and natural spiral fitting approximation techniques the purpose of is this paper is to briefly review recent progresses in computer aided geometric design cagd focusing on the topics states above,0
the generation of highquality 3d environments is crucial for industries such as gaming virtual reality and cinema yet remains resourceintensive due to the reliance on manual processes this study performs a systematic review of existing generative ai techniques for 3d scene generation analyzing their characteristics strengths limitations and potential for improvement by examining stateoftheart approaches it presents key challenges such as scene authenticity and the influence of textual inputs special attention is given to how ai can blend different stylistic domains while maintaining coherence the impact of training data on output quality and the limitations of current models in addition this review surveys existing evaluation metrics for assessing realism and explores how industry professionals incorporate ai into their workflows the findings of this study aim to provide a comprehensive understanding of the current landscape and serve as a foundation for future research on aidriven 3d content generation key findings include that advanced generative architectures enable highquality 3d content creation at a high computational cost effective multimodal integration techniques like crossattention and latent space alignment facilitate textto3d tasks and the quality and diversity of training data combined with comprehensive evaluation metrics are critical to achieving scalable robust 3d scene generation,0
in this work we present scenedreamer an unconditional generative model for unbounded 3d scenes which synthesizes largescale 3d landscapes from random noise our framework is learned from inthewild 2d image collections only without any 3d annotations at the core of scenedreamer is a principled learning paradigm comprising 1 an efficient yet expressive 3d scene representation 2 a generative scene parameterization and 3 an effective renderer that can leverage the knowledge from 2d images our approach begins with an efficient birdseyeview bev representation generated from simplex noise which includes a height field for surface elevation and a semantic field for detailed scene semantics this bev scene representation enables 1 representing a 3d scene with quadratic complexity 2 disentangled geometry and semantics and 3 efficient training moreover we propose a novel generative neural hash grid to parameterize the latent space based on 3d positions and scene semantics aiming to encode generalizable features across various scenes lastly a neural volumetric renderer learned from 2d image collections through adversarial training is employed to produce photorealistic images extensive experiments demonstrate the effectiveness of scenedreamer and superiority over stateoftheart methods in generating vivid yet diverse unbounded 3d worlds,0
we introduce a normalbased basrelief generation and stylization method which is motivated by the recent advancement in this topic creating basrelief from normal images has successfully facilitated basrelief modeling in image space however the use of normal images in previous work is often restricted to certain type of operations only this paper is intended to extend normalbased methods and construct basreliefs from normal images in a versatile way our method can not only generate a new normal image by combining various frequencies of existing normal images and details transferring but also build basreliefs from a single rgb image and its edgebased sketch image in addition we introduce an auxiliary function to represent a smooth base surface and generate a layered global shape to integrate above considerations into our framework we formulate the bas relief generation as a variational problem which can be solved by a screened poisson equation some advantages of our method are that it expands the basrelief shape space and generates diversified styles of results and that it is capable of transferring details from one region to other regions our method is easy to implement and produces goodquality basrelief models we experiment our method on a range of normal images and it compares favorably to other popular classic and stateoftheart methods,0
the recent advancements in textto3d generation mark a significant milestone in generative models unlocking new possibilities for creating imaginative 3d assets across various realworld scenarios while recent advancements in textto3d generation have shown promise they often fall short in rendering detailed and highquality 3d models this problem is especially prevalent as many methods base themselves on score distillation sampling sds this paper identifies a notable deficiency in sds that it brings inconsistent and lowquality updating direction for the 3d model causing the oversmoothing effect to address this we propose a novel approach called interval score matching ism ism employs deterministic diffusing trajectories and utilizes intervalbased score matching to counteract oversmoothing furthermore we incorporate 3d gaussian splatting into our textto3d generation pipeline extensive experiments show that our model largely outperforms the stateoftheart in quality and training efficiency,0
synthesizing photorealistic images and videos is at the heart of computer graphics and has been the focus of decades of research traditionally synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing which take specifically defined representations of geometry and material properties as input collectively these inputs define the actual scene and what is rendered and are referred to as the scene representation where a scene consists of one or more objects example scene representations are triangle meshes with accompanied textures eg created by an artist point clouds eg from a depth sensor volumetric grids eg from a ct scan or implicit surface functions eg truncated signed distance fields the reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering neural rendering is closely related and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from realworld observations neural rendering is a leap forward towards the goal of synthesizing photorealistic image and video content in recent years we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline this stateoftheart report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3d scene representations often now referred to as neural scene representations a key advantage of these methods is that they are 3dconsistent by design enabling applications such as novel viewpoint synthesis of a captured scene in addition to methods that handle static scenes we cover neural scene representations for modeling nonrigidly deforming objects,0
driving simulators play a large role in developing and testing new intelligent vehicle systems the visual fidelity of the simulation is critical for building visionbased algorithms and conducting human driver experiments low visual fidelity breaks immersion for humanintheloop driving experiments conventional computer graphics pipelines use detailed 3d models meshes textures and rendering engines to generate 2d images from 3d scenes these processes are laborintensive and they do not generate photorealistic imagery here we introduce a hybrid generative neural graphics pipeline for improving the visual fidelity of driving simulations given a 3d scene we partially render only important objects of interest such as vehicles and use generative adversarial processes to synthesize the background and the rest of the image to this end we propose a novel image formation strategy to form 2d semantic images from 3d scenery consisting of simple object models without textures these semantic images are then converted into photorealistic rgb images with a stateoftheart generative adversarial network gan trained on realworld driving scenes this replaces repetitiveness with randomly generated but photorealistic surfaces finally the partiallyrendered and gan synthesized images are blended with a blending gan we show that the photorealism of images generated with the proposed method is more similar to realworld driving datasets such as cityscapes and kitti than conventional approaches this comparison is made using semantic retention analysis and frechet inception distance fid measurements,0
recently the field of textguided 3d scene generation has garnered significant attention highquality generation that aligns with physical realism and high controllability is crucial for practical 3d scene applications however existing methods face fundamental limitations i difficulty capturing complex relationships between multiple objects described in the text ii inability to generate physically plausible scene layouts and iii lack of controllability and extensibility in compositional scenes in this paper we introduce layoutdreamer a framework that leverages 3d gaussian splatting 3dgs to facilitate highquality physically consistent compositional scene generation guided by text specifically given a text prompt we convert it into a directed scene graph and adaptively adjust the density and layout of the initial compositional 3d gaussians subsequently dynamic camera adjustments are made based on the training focal point to ensure entitylevel generation quality finally by extracting directed dependencies from the scene graph we tailor physical and layout energy to ensure both realism and flexibility comprehensive experiments demonstrate that layoutdreamer outperforms other compositional scene generation quality and semantic alignment methods specifically it achieves stateoftheart sota performance in the multiple objects generation metric of t3bench,0
scalable vector graphics svg is a popular format on the web and in the design industry however despite the great strides made in generative modeling svg has remained underexplored due to the discrete and complex nature of such data we introduce grimoire a textguided svg generative model that is comprised of two modules a visual shape quantizer vsq learns to map raster images onto a discrete codebook by reconstructing them as vector shapes and an autoregressive transformer art models the joint probability distribution over shape tokens positions and textual descriptions allowing us to generate vector graphics from natural language unlike existing models that require direct supervision from svg data grimoire learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data we demonstrate the effectiveness of our method by fitting grimoire for closed filled shapes on the mnist and for outline strokes on icon and font data surpassing previous imagesupervised methods in generative quality and vectorsupervised approach in flexibility,0
in this work we introduce unique3d a novel imageto3d framework for efficiently generating highquality 3d meshes from singleview images featuring stateoftheart generation fidelity and strong generalizability previous methods based on score distillation sampling sds can produce diversified 3d results by distilling 3d knowledge from large 2d diffusion models but they usually suffer from long percase optimization time with inconsistent issues recent works address the problem and generate better 3d results either by finetuning a multiview diffusion model or training a fast feedforward model however they still lack intricate textures and complex geometries due to inconsistency and limited generated resolution to simultaneously achieve high fidelity consistency and efficiency in single imageto3d we propose a novel framework unique3d that includes a multiview diffusion model with a corresponding normal diffusion model to generate multiview images with their normal maps a multilevel upscale process to progressively improve the resolution of generated orthographic multiviews as well as an instant and consistent mesh reconstruction algorithm called isomer which fully integrates the color and geometric priors into mesh results extensive experiments demonstrate that our unique3d significantly outperforms other imageto3d baselines in terms of geometric and textural details,0
inbetweening human motion generation aims to synthesize intermediate motions that transition between userspecified keyframes in addition to maintaining smooth transitions a crucial requirement of this task is to generate diverse motion sequences it is still challenging to maintain diversity particularly when it is necessary for the motions within a generated batch sampling to differ meaningfully from one another due to complex motion dynamics in this paper we propose a novel method termed the multicriteria guidance with inbetweening motion model mcgimm for inbetweening human motion generation a key strength of mcgimm lies in its plugandplay nature it enhances the diversity of motions generated by pretrained models without introducing additional parameters this is achieved by providing a sampling process of pretrained generative models with multicriteria guidance specifically mcgimm reformulates the sampling process of pretrained generative model as a multicriteria optimization problem and introduces an optimization process to explore motion sequences that satisfy multiple criteria eg diversity and smoothness moreover our proposed plugandplay multicriteria guidance is compatible with different families of generative models including denoised diffusion probabilistic models variational autoencoders and generative adversarial networks experiments on four popular human motion datasets demonstrate that mcgimm consistently stateoftheart methods in inbetweening motion generation task,0
we have witnessed rapid progress on 3daware image synthesis leveraging recent advances in generative visual models and neural rendering existing approaches however fall short in two ways first they may lack an underlying 3d representation or rely on viewinconsistent rendering hence synthesizing images that are not multiview consistent second they often depend upon representation network architectures that are not expressive enough and their results thus lack in image quality we propose a novel generative model named periodic implicit generative adversarial networks gan or pigan for highquality 3daware image synthesis gan leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as viewconsistent 3d representations with fine detail the proposed approach obtains stateoftheart results for 3daware image synthesis with multiple real and synthetic datasets,0
we propose relitlrm a large reconstruction model lrm for generating highquality gaussian splatting representations of 3d objects under novel illuminations from sparse 48 posed images captured under unknown static lighting unlike prior inverse rendering methods requiring dense captures and slow optimization often causing artifacts like incorrect highlights or shadow baking relitlrm adopts a feedforward transformerbased model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion the model is trained endtoend on synthetic multiview renderings of objects under varying known illuminations this architecture design enables to effectively decompose geometry and appearance resolve the ambiguity between material and lighting and capture the multimodal distribution of shadows and specularity in the relit appearance we show our sparseview feedforward relitlrm offers competitive relighting results to stateoftheart denseview optimizationbased baselines while being significantly faster our project page is available at,0
the introduction of diffusion models has brought significant advances to the field of audiodriven talking head generation however the extremely slow inference speed severely limits the practical implementation of diffusionbased talking head generation models in this study we propose read a realtime diffusiontransformerbased talking head generation framework our approach first learns a spatiotemporal highly compressed video latent space via a temporal vae significantly reducing the token count to accelerate generation to achieve better audiovisual alignment within this compressed latent space a pretrained speech autoencoder speechae is proposed to generate temporally compressed speech latent codes corresponding to the video latent space these latent representations are then modeled by a carefully designed audiotovideo diffusion transformer a2vdit backbone for efficient talking head synthesis furthermore to ensure temporal consistency and accelerated inference in extended generation we propose a novel asynchronous noise scheduler ans for both the training and inference processes of our framework the ans leverages asynchronous addnoise and asynchronous motionguided generation in the latent space ensuring consistency in generated video clips experimental results demonstrate that read outperforms stateoftheart methods by generating competitive talking head videos with significantly reduced runtime achieving an optimal balance between quality and speed while maintaining robust metric stability in longtime generation,0
training native 3d texture generative models remains a fundamental yet challenging problem largely due to the limited availability of largescale highquality 3d texture datasets this scarcity hinders generalization to realworld scenarios to address this most existing methods finetune foundation image generative models to exploit their learned visual priors however these approaches typically generate only multiview images and rely on postprocessing to produce uv texture maps an essential representation in modern graphics pipelines such twostage pipelines often suffer from error accumulation and spatial inconsistencies across the 3d surface in this paper we introduce seqtex a novel endtoend framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete uv texture maps unlike previous methods that model the distribution of uv textures in isolation seqtex reformulates the task as a sequence generation problem enabling the model to learn the joint distribution of multiview renderings and uv textures this design effectively transfers the consistent imagespace priors from video foundation models into the uv domain to further enhance performance we propose several architectural innovations a decoupled multiview and uv branch design geometryinformed attention to guide crossdomain feature alignment and adaptive token resolution to preserve fine texture details while maintaining computational efficiency together these components allow seqtex to fully utilize pretrained video priors and synthesize highfidelity uv texture maps without the need for postprocessing extensive experiments show that seqtex achieves stateoftheart performance on both imageconditioned and textconditioned 3d texture generation tasks with superior 3d consistency texturegeometry alignment and realworld generalization,0
we present zeroeggs a neural network framework for speechdriven gesture generation with zeroshot style control by example this means style can be controlled via only a short example motion clip even for motion styles unseen during training our model uses a variational framework to learn a style embedding making it easy to modify style through latent space manipulation or blending and scaling of style embeddings the probabilistic nature of our framework further enables the generation of a variety of outputs given the same input addressing the stochastic nature of gesture motion in a series of experiments we first demonstrate the flexibility and generalizability of our model to new speakers and styles in a user study we then show that our model outperforms previous stateoftheart techniques in naturalness of motion appropriateness for speech and style portrayal finally we release a highquality dataset of fullbody gesture motion including fingers with speech spanning across 19 different styles,0
3d scene generation plays a crucial role in gaming artistic creation virtual reality and many other domains however current 3d scene design still relies heavily on extensive manual effort from creators and existing automated methods struggle to generate opendomain scenes or support flexible editing as a result generating 3d worlds directly from text has garnered increasing attention in this paper we introduce holodeck 20 an advanced visionlanguageguided framework for 3d world generation with support for interactive scene editing based on human feedback holodeck 20 can generate diverse and stylistically rich 3d scenes eg realistic cartoon anime and cyberpunk styles that exhibit high semantic fidelity to finegrained input descriptions suitable for both indoor and opendomain environments holodeck 20 leverages visionlanguage models vlms to identify and parse the objects required in a scene and generates corresponding highquality assets via stateoftheart 3d generative models it then iteratively applies spatial constraints derived from the vlms to achieve semantically coherent and physically plausible layouts human evaluations and clipbased assessments demonstrate that holodeck 20 effectively generates highquality scenes closely aligned with detailed textual descriptions consistently outperforming baselines across indoor and opendomain scenarios additionally we provide editing capabilities that flexibly adapt to human feedback supporting layout refinement and styleconsistent object edits finally we present a practical application of holodeck 20 in procedural game modeling generating visually rich and immersive environments potentially boosting efficiency,0
scaling artistdesigned meshes to high triangle numbers remains challenging for autoregressive generative models existing transformerbased methods suffer from longsequence bottlenecks and limited quantization resolution primarily due to the large number of tokens required and constrained quantization granularity these issues prevent faithful reproduction of fine geometric details and structured density patterns we introduce meshmosaic a novel localtoglobal framework for artist mesh generation that scales to over 100k trianglessubstantially surpassing prior methods which typically handle only around 8k faces meshmosaic first segments shapes into patches generating each patch autoregressively and leveraging shared boundary conditions to promote coherence symmetry and seamless connectivity between neighboring regions this strategy enhances scalability to highresolution meshes by quantizing patches individually resulting in more symmetrical and organized mesh density and structure extensive experiments across multiple public datasets demonstrate that meshmosaic significantly outperforms stateoftheart methods in both geometric fidelity and user preference supporting superior detail representation and practical mesh generation for realworld applications,0
enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation demanding the integration of perceptual modeling and motion synthesis despite its significance this task remains largely unexplored most previous works have primarily focused on mapping modalities like speech audio and music to generate human motion as of yet these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion to bridge this gap and enable highquality modeling of human movements in response to spatial audio we introduce the first comprehensive spatial audiodriven human motion sam dataset which contains diverse and highquality spatial audio and motion data for benchmarking we develop a simple yet effective diffusionbased generative framework for human motion generation driven by spatial audio termed mospa which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism once trained mospa can generate diverse realistic human motions conditioned on varying spatial audio inputs we perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking where our method achieves stateoftheart performance on this task our code and model are publicly available at,0
we present gen3c a generative video model with precise camera control and temporal 3d consistency prior video models already generate realistic videos but they tend to leverage little 3d information leading to inconsistencies such as objects popping in and out of existence camera control if implemented at all is imprecise because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera in contrast gen3c is guided by a 3d cache point clouds obtained by predicting the pixelwise depth of seed images or previously generated frames when generating the next frames gen3c is conditioned on the 2d renderings of the 3d cache with the new camera trajectory provided by the user crucially this means that gen3c neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose the model instead can focus all its generative power on previously unobserved regions as well as advancing the scene state to the next frame our results demonstrate more precise camera control than prior work as well as stateoftheart results in sparseview novel view synthesis even in challenging settings such as driving scenes and monocular dynamic video results are best viewed in videos check out our webpage,0
internet of things iot has become a popular paradigm to fulfil needs of the industry such as asset tracking resource monitoring and automation as security mechanisms are often neglected during the deployment of iot devices they are more easily attacked by complicated and large volume intrusion attacks using advanced techniques artificial intelligence ai has been used by the cyber security community in the past decade to automatically identify such attacks however deep learning methods have yet to be extensively explored for intrusion detection systems ids specifically for iot most recent works are based on time sequential models like lstm and there is short of research in cnns as they are not naturally suited for this problem in this article we propose a novel solution to the intrusion attacks against iot devices using cnns the data is encoded as the convolutional operations to capture the patterns from the sensors data along time that are useful for attacks detection by cnns the proposed method is integrated with two classical cnns resnet and efficientnet where the detection performance is evaluated the experimental results show significant improvement in both true positive rate and false positive rate compared to the baseline using lstm,0
the integration of internet of things iot applications in our daily lives has led to a surge in data traffic posing significant security challenges iot applications using cloud and edge computing are at higher risk of cyberattacks because of the expanded attack surface from distributed edge and cloud services the vulnerability of iot devices and challenges in managing security across interconnected systems leading to oversights this led to the rise of mlbased solutions for intrusion detection systems idss which have proven effective in enhancing network security and defending against diverse threats however mlbased ids in iot systems encounters challenges particularly from noisy redundant and irrelevant features in varied iot datasets potentially impacting its performance therefore reducing such features becomes crucial to enhance system performance and minimize computational costs this paper focuses on improving the effectiveness of mlbased ids at the edge level by introducing a novel method to find a balanced tradeoff between cost and accuracy through the creation of informative features in a twotier edgeuser iot environment a hybrid binary quantuminspired artificial bee colony and genetic programming algorithm is utilized for this purpose three iot intrusion detection datasets namely nslkdd unswnb15 and botiot are used for the evaluation of the proposed approach,0
the internet of things iot propagates the paradigm of interconnecting billions of heterogeneous devices by various manufacturers to enable iot applications the communication between iot devices follows specifications defined by standard developing organizations in this paper we present a case study that investigates disclosed insecurities of the popular iot standard zigbee and derive general lessons about security economics in iot standardization efforts we discuss the motivation of iot standardization efforts that are primarily driven from an economic perspective in which large investments in security are not considered necessary since the consumers do not reward them success at the market is achieved by being quicktomarket providing functional features and offering easy integration for complementors nevertheless manufacturers should not only consider economic reasons but also see their responsibility to protect humans and technological infrastructures from being threatened by insecure iot products in this context we propose a number of recommendations to strengthen the security design in future iot standardization efforts ranging from the definition of a precise security model to the enforcement of an update policy,0
the deployment of internet of things iot systems in defense and national security faces some limitations that can be addressed with edge computing approaches the edge computing and iot paradigms combined bring potential benefits since they confront the limitations of traditional centralized cloud computing approaches which enable easy scalability realtime applications or mobility support but whose use poses certain risks in aspects like cybersecurity this chapter identifies scenarios in which defense and national security can leverage commercial offtheshelf cots edge iot capabilities to deliver greater survivability to warfighters or first responders while lowering costs and increasing operational efficiency and effectiveness in addition it presents the general design of a tactical edge iot communications architecture it identifies the open challenges for a widespread adoption and provides research guidelines and some recommendations for enabling costeffective edge iot for defense and national security,0
internet service providers isps and individual users of internet of things iot play a vital role in securing iot however encouraging them to do so is hard our study investigates isps and individuals attitudes towards the security of iot the obstacles they face and their incentives to keep iot secure drawing evidence from japan due to the complex interactions of the stakeholders we follow an iterative methodology where we present issues and potential solutions to our stakeholders in turn for isps we survey 27 isps in japan followed by a workshop with representatives from government and 5 isps based on the findings from this we conduct semistructured interviews with 20 participants followed by a more quantitative survey with 328 participants we review these results in a second workshop with representatives from government and 7 isps the appreciation of challenges by each party has lead to findings that are supported by all stakeholders securing iot devices is neither users nor isps priority individuals are keen on more interventions both from the government as part of regulation and from isps in terms of filtering malicious traffic participants are willing to pay for enhanced monitoring and filtering while isps do want to help users there appears to be a lack of effective technology to aid them isps would like to see more public recognition for their efforts but internally they struggle with executive buyin and effective means to communicate with their customers the majority of barriers and incentives are external to isps and individuals demonstrating the complexity of keeping iot secure and emphasizing the need for relevant stakeholders in the iot ecosystem to work in tandem,0
the rapid expansion of the internet of things iot has revolutionized modern industries by enabling smart automation and real time connectivity however this evolution has also introduced complex cybersecurity challenges due to the heterogeneous resource constrained and distributed nature of these environments to address these challenges this research presents cst afnet a novel dual attention based deep learning framework specifically designed for robust intrusion detection in iot networks the model integrates multi scale convolutional neural networks cnns for spatial feature extraction bidirectional gated recurrent units bigrus for capturing temporal dependencies and a dual attention mechanism channel and temporal attention to enhance focus on critical patterns in the data the proposed method was trained and evaluated on the edge iiotset dataset a comprehensive and realistic benchmark containing more than 22 million labeled instances spanning 15 attack types and benign traffic collected from a seven layer industrial testbed our proposed model achieves outstanding accuracy for both 15 attack types and benign traffic cst afnet achieves 9997 percent accuracy moreover this model demonstrates exceptional performance with macro averaged precision recall and f1 score all above 993 percent experimental results show that cst afnet achieves superior detection accuracy significantly outperforming traditional deep learning models the findings confirm that cst afnet is a powerful and scalable solution for real time cyber threat detection in complex iot and iiot environments paving the way for more secure intelligent and adaptive cyber physical systems,0
cyber intrusion attacks that compromise the users critical and sensitive data are escalating in volume and intensity especially with the growing connections between our daily life and the internet the large volume and high complexity of such intrusion attacks have impeded the effectiveness of most traditional defence techniques while at the same time the remarkable performance of the machine learning methods especially deep learning in computer vision had garnered research interests from the cyber security community to further enhance and automate intrusion detections however the expensive data labeling and limitation of anomalous data make it challenging to train an intrusion detector in a fully supervised manner therefore intrusion detection based on unsupervised anomaly detection is an important feature too in this paper we propose a threestage deep learning anomaly detection based network intrusion attack detection framework the framework comprises an integration of unsupervised kmeans clustering semisupervised ganomaly and supervised learning cnn algorithms we then evaluated and showed the performance of our implemented framework on three benchmark datasets nslkdd cicids2018 and toniot,0
the growing interest in the internet of things iot applications is associated with an augmented volume of security threats in this vein the intrusion detection systems ids have emerged as a viable solution for the detection and prevention of malicious activities unlike the signaturebased detection approaches machine learningbased solutions are a promising means for detecting unknown attacks however the machine learning models need to be accurate enough to reduce the number of false alarms more importantly they need to be trained and evaluated on realistic datasets such that their efficacy can be validated on realtime deployments many solutions proposed in the literature are reported to have high accuracy but are ineffective in real applications due to the nonrepresentativity of the dataset used for training and evaluation of the underlying models on the other hand some of the existing solutions overcome these challenges but yield low accuracy which hampers their implementation for commercial tools these solutions are majorly based on single learners and are therefore directly affected by the intrinsic limitations of each learning algorithm the novelty of this paper is to use the most realistic dataset available for intrusion detection called nslkdd and combine multiple learners to build ensemble learners that increase the accuracy of the detection furthermore a deployment architecture in a fogtothings environment that employs two levels of classifications is proposed in such architecture the first level performs an anomaly detection which reduces the latency of the classification substantially while the second level executes attack classifications enabling precise prevention measures finally the experimental results demonstrate the effectiveness of the proposed ids in comparison with the other stateofthearts on the nslkdd dataset,0
this work investigates the possibilities enabled by federated learning concerning iot malware detection and studies security issues inherent to this new learning paradigm in this context a framework that uses federated learning to detect malware affecting iot devices is presented nbaiot a dataset modeling network traffic of several real iot devices while affected by malware has been used to evaluate the proposed framework both supervised and unsupervised federated models multilayer perceptron and autoencoder able to detect malware affecting seen and unseen iot devices of nbaiot have been trained and evaluated furthermore their performance has been compared to two traditional approaches the first one lets each participant locally train a model using only its own data while the second consists of making the participants share their data with a central entity in charge of training a global model this comparison has shown that the use of more diverse and large data as done in the federated and centralized methods has a considerable positive impact on the model performance besides the federated models while preserving the participants privacy show similar results as the centralized ones as an additional contribution and to measure the robustness of the federated approach an adversarial setup with several malicious participants poisoning the federated model has been considered the baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to different attacks even with a single adversary the performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios these functions provide a significant improvement against malicious participants but more efforts are still needed to make federated approaches robust,0
the various types of communication technologies and mobility features in internet of things iot on the one hand enable fruitful and attractive applications but on the other hand facilitates malware propagation thereby raising new challenges on handling iotempowered malware for cyber security comparing with the malware propagation control scheme in traditional wireless networks where nodes can be directly repaired and secured in iot compromised end devices are difficult to be patched alternatively blocking malware via patching intermediate nodes turns out to be a more feasible and practical solution specifically patching intermediate nodes can effectively prevent the proliferation of malware propagation by securing infrastructure links and limiting malware propagation to local devicetodevice dissemination this article proposes a novel trafficaware patching scheme to select important intermediate nodes to patch which applies to the iot system with limited patching resources and response time constraint experiments on realworld trace datasets in iot networks are conducted to demonstrate the advantage of the proposed trafficaware patching scheme in alleviating malware propagation,0
many current approaches to the design of intrusion detection systems apply feature selection in a static nonadaptive fashion these methods often neglect the dynamic nature of network data which requires to use adaptive feature selection techniques in this paper we present a simple technique based on incremental learning of support vector machines in order to rank the features in real time within a streaming model for network data some illustrative numerical experiments with two popular benchmark datasets show that our approach allows to adapt to the changes in normal network behaviour and novel attack patterns which have not been experienced before,0
internet of things iot is becoming truly ubiquitous in our everyday life but it also faces unique security challenges intrusion detection is critical for the security and safety of a wireless iot network this paper discusses the humanintheloop active learning approach for wireless intrusion detection we first present the fundamental challenges against the design of a successful intrusion detection system ids for wireless iot network we then briefly review the rudimentary concepts of active learning and propose its employment in the diverse applications of wireless intrusion detection experimental example is also presented to show the significant performance improvement of the active learning method over traditional supervised learning approach while machine learning techniques have been widely employed for intrusion detection the application of humanintheloop machine learning that leverages both machine and human intelligence to intrusion detection of iot is still in its infancy we hope this article can assist the readers in understanding the key concepts of active learning and spur further research in this area,0
internet of things iot devices routinely have security issues but are the platform designers providing enough support to iot developers for them to easily implement security features for their platforms we surveyed the documentation code and guidance from nine iot manufacturers to look at what guidance they provided for implementing three security features required by several security standards secure boot device identity keys and unique per device passwords we find that more needs to be done to support developers if we want them to adopt security features especially in the face of incoming legislation that will require developers to implement them,0
intrusion detection systems are evolving into intelligent systems that perform data analysis searching for anomalies in their environment the development of deep learning technologies opened the door to build more complex and effective threat detection models however training those models may be computationally infeasible in most internet of things devices current approaches rely on powerful centralized servers that receive data from all their parties violating basic privacy constraints and substantially affecting response times and operational costs due to the huge communication overheads to mitigate these issues federated learning emerged as a promising approach where different agents collaboratively train a shared model neither exposing training data to others nor requiring a computeintensive centralized infrastructure this paper focuses on the application of federated learning approaches in the field of intrusion detection both technologies are described in detail and current scientific progress is reviewed and categorized finally the paper highlights the limitations present in recent works and presents some future directions for this technology,0
with increasingly sophisticated cybersecurity threats and rising demand for network automation autonomous cybersecurity mechanisms are becoming critical for securing modern networks the rapid expansion of internet of things iot systems amplifies these challenges as resourceconstrained iot devices demand scalable and efficient security solutions in this work an innovative intrusion detection system ids utilizing automated machine learning automl and multiobjective optimization moo is proposed for autonomous and optimized cyberattack detection in modern networking environments the proposed ids framework integrates two primary innovative techniques optimized importance and percentagebased automated feature selection oipautofs and optimized performance confidence and efficiencybased combined algorithm selection and hyperparameter optimization opcecash these components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency this work presents the first ids framework that integrates all four automl stages and employs multiobjective optimization to jointly optimize detection effectiveness efficiency and confidence for deployment in resourceconstrained systems experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed mooautoml ids outperforms stateoftheart idss establishing a new benchmark for autonomous efficient and optimized security for networks designed to support iot and edge environments with resource constraints the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments,0
intrusion detection has focused primarily on detecting cyberattacks at the eventlevel since there is such a large volume of network data and attacks are minimal machine learning approaches have focused on improving accuracy and reducing false positives but this has frequently resulted in overfitting in addition the volume of intrusion detection alerts is large and creates fatigue in the human analyst who must review them this research addresses the problems associated with eventlevel intrusion detection and the large volumes of intrusion alerts by applying active learning and cyber situation awareness this paper includes the results of two experiments using the unswnb15 dataset the first experiment evaluated sampling approaches for querying the oracle as part of active learning it then trained a random forest classifier using the samples and evaluated its results the second experiment applied cyber situation awareness by aggregating the detection results of the first experiment and calculating the probability that a computer system was part of a cyberattack this research showed that moving the perspective of eventlevel alerts to the probability that a computer system was part of an attack improved the accuracy of detection and reduced the volume of alerts that a human analyst would need to review,0
security is of primary importance to vehicles the viability of performing remote intrusions onto the invehicle network has been manifested in regard to unmanned autonomous cars limited work has been done to detect intrusions for them while existing intrusion detection systems idss embrace limitations against strong adversaries in this paper we consider the very nature of autonomous car and leverage the road context to build a novel ids named road contextaware ids raids when a computercontrolled car is driving through continuous roads road contexts and genuine frames transmitted on the cars invehicle network should resemble a regular and intelligible pattern raids hence employs a lightweight machine learning model to extract road contexts from sensory information eg camera images and distance sensor values that are used to generate control signals for maneuvering the car with such ongoing road context raids validates corresponding frames observed on the invehicle network anomalous frames that substantially deviate from road context will be discerned as intrusions we have implemented a prototype of raids with neural networks and conducted experiments on a raspberry pi with extensive datasets and meaningful intrusion cases evaluations show that raids significantly outperforms stateoftheart ids without using road context by up to 999 accuracy and short response time,0
recent advances in internet of things iot have enabled myriad domains such as smart homes personal monitoring devices and enhanced manufacturing iot is now pervasivenew applications are being used in nearly every conceivable environment which leads to the adoption of devicebased interaction and automation however iot has also raised issues about the security and privacy of these digitally augmented spaces program analysis is crucial in identifying those issues yet the application and scope of program analysis in iot remains largely unexplored by the technical community in this paper we study privacy and security issues in iot that require programanalysis techniques with an emphasis on identified attacks against these systems and defenses implemented so far based on a study of five iot programming platforms we identify the key insights that result from research efforts in both the program analysis and security communities and relate the efficacy of programanalysis techniques to security and privacy issues we conclude by studying recent iot analysis systems and exploring their implementations through these explorations we highlight key challenges and opportunities in calibrating for the environments in which iot systems will be used,0
there is a growing trend of cyberattacks against internet of things iot devices moreover the sophistication and motivation of those attacks is increasing the vast scale of iot diverse hardware and software and being typically placed in uncontrolled environments make traditional it security mechanisms such as signaturebased intrusion detection and prevention systems challenging to integrate they also struggle to cope with the rapidly evolving iot threat landscape due to long delays between the analysis and publication of the detection rules machine learning methods have shown faster response to emerging threats however model training architectures like cloud or edge computing face multiple drawbacks in iot settings including network overhead and data isolation arising from the large scale and heterogeneity that characterizes these networks this work presents an architecture for training unsupervised models for network intrusion detection in large distributed iot and industrial iot iiot deployments we leverage federated learning fl to collaboratively train between peers and reduce isolation and network overhead problems we build upon it to include an unsupervised device clustering algorithm fully integrated into the fl pipeline to address the heterogeneity issues that arise in fl settings the architecture is implemented and evaluated using a testbed that includes various emulated iotiiot devices and attackers interacting in a complex network topology comprising 100 emulated devices 30 switches and 10 routers the anomaly detection models are evaluated on real attacks performed by the testbeds threat actors including the entire mirai malware lifecycle an additional botnet based on the merlin command and control server and other redteaming tools performing scanning activities and multiple attacks targeting the emulated devices,0
iot contextsharing platforms are an essential component of todays interconnected iot deployments with their security affecting the entire deployment and the critical infrastructure adopting iot we report on a lack of systematic approach to the security of iot contextsharing platforms and propose the need for a methodological and systematic alternative to evaluate the existing solutions and develop securebydesign solutions we have identified the key components of a generic iot contextsharing platform and propose using mitre attck for threat modelling of such platforms,0
recent advances in technology have made our work easier compare to earlier times computer network is growing day by day but while discussing about the security of computers and networks it has always been a major concerns for organizations varying from smaller to larger enterprises it is true that organizations are aware of the possible threats and attacks so they always prepare for the safer side but due to some loopholes attackers are able to make attacks intrusion detection is one of the major fields of research and researchers are trying to find new algorithms for detecting intrusions clustering techniques of data mining is an interested area of research for detecting possible intrusions and attacks this paper presents a new clustering approach for anomaly intrusion detection by using the approach of kmedoids method of clustering and its certain modifications the proposed algorithm is able to achieve high detection rate and overcomes the disadvantages of kmeans algorithm,0
the continuous strengthening of the security posture of iot ecosystems is vital due to the increasing number of interconnected devices and the volume of sensitive data shared the utilisation of machine learning ml capabilities in the defence against iot cyber attacks has many potential benefits however the currently proposed frameworks do not consider data privacy secure architectures andor scalable deployments of iot ecosystems in this paper we propose a hierarchical blockchainbased federated learning framework to enable secure and privacypreserved collaborative iot intrusion detection we highlight and demonstrate the importance of sharing cyber threat intelligence among interorganisational iot networks to improve the models detection capabilities the proposed mlbased intrusion detection framework follows a hierarchical federated learning architecture to ensure the privacy of the learning process and organisational data the transactions model updates and processes will run on a secure immutable ledger and the conformance of executed tasks will be verified by the smart contract we have tested our solution and demonstrated its feasibility by implementing it and evaluating the intrusion detection performance using a key iot data set the outcome is a securely designed mlbased intrusion detection system capable of detecting a wide range of malicious activities while preserving data privacy,0
with the expansion of the internet of things iot the number of security incidents due to insecure and misconfigured iot devices is increasing especially on the consumer market manufacturers focus on new features and early releases at the expense of a comprehensive security strategy hence experts have started calling for regulation of the iot consumer market while policymakers are seeking for suitable regulatory approaches we investigate how manufacturers can be incentivized to increase sustainable security efforts for iot products we propose mandatory security update labels that inform consumers during buying decisions about the willingness of the manufacturer to provide security updates in the future mandatory means that the labels explicitly state when security updates are not guaranteed we conducted a user study with more than 1400 participants to assess the importance of security update labels for the consumer choice by means of a conjoint analysis the results show that the availability of security updates until which date the updates are guaranteed accounts for 8 to 35 impact on overall consumers choice depending on the perceived security risk of the product category for products with a high perceived security risk this availability is twice as important as other highranked product attributes moreover provisioning time for security updates how quickly the product will be patched after a vulnerability is discovered additionally accounts for 7 to 25 impact on consumers choices the proposed labels are intuitively understood by consumers do not require product assessments by third parties before release and have a potential to incentivize manufacturers to provide sustainable security support,0
the rapid proliferation of internet of things iot devices across multiple sectors has escalated serious network security concerns this has prompted ongoing research in machine learning mlbased intrusion detection systems idss for cyberattack classification traditional ml models require data transmission from iot devices to a centralized server for traffic analysis raising severe privacy concerns to address this issue researchers have studied federated learning flbased idss that train models across iot devices while keeping their data localized however the heterogeneity of data stemming from distinct vulnerabilities of devices and complexity of attack vectors poses a significant challenge to the effectiveness of fl models while current research focuses on adapting various ml models within the fl framework they fail to effectively address the issue of attack class imbalance among devices which significantly degrades the classification accuracy of minority attacks to overcome this challenge we introduce fedmade a novel dynamic aggregation method which clusters devices by their traffic patterns and aggregates local models based on their contributions towards overall performance we evaluate fedmade against other fl algorithms designed for noniid data and observe up to 7107 improvement in minority attack classification accuracy we further show that fedmade is robust to poisoning attacks and incurs only a 47 503 seconds latency overhead in each communication round compared to fedavg without increasing the computational load of iot devices,0
due to their rapid growth and deployment internet of things iot devices have become a central aspect of our daily lives however they tend to have many vulnerabilities which can be exploited by an attacker unsupervised techniques such as anomaly detection can help us secure the iot devices however an anomaly detection model must be trained for a long time in order to capture all benign behaviors this approach is vulnerable to adversarial attacks since all observations are assumed to be benign while training the anomaly detection model in this paper we propose ciota a lightweight framework that utilizes the blockchain concept to perform distributed and collaborative anomaly detection for devices with limited resources ciota uses blockchain to incrementally update a trusted anomaly detection model via selfattestation and consensus among iot devices we evaluate ciota on our own distributed iot simulation platform which consists of 48 raspberry pis to demonstrate ciotas ability to enhance the security of each device and the security of the network as a whole,0
the convergence of the internet of things iot and quantum computing is redefining the security paradigm of interconnected digital systems classical cryptographic algorithms such as rsa elliptic curve cryptography ecc and advanced encryption standard aes have long provided the foundation for securing iot communication however the emergence of quantum algorithms such as shors and grovers threatens to render these techniques vulnerable necessitating the development of quantumresilient alternatives this chapter examines the implications of quantum computing for iot security and explores strategies for building cryptographically robust systems in the postquantum era it presents an overview of postquantum cryptographic pqc families including latticebased codebased hashbased and multivariate approaches analyzing their potential for deployment in resourceconstrained iot environments in addition quantumbased methods such as quantum key distribution qkd and quantum random number generators qrngs are discussed for their ability to enhance confidentiality and privacy through physicsbased security guarantees the chapter also highlights issues of privacy management regulatory compliance and standardization emphasizing the need for collaborative efforts across academia industry and governance overall it provides a comprehensive perspective on security iot ecosystems against quantum threats and ensures resilience in the next generation of intelligent networks,0
protecting internet of things iot devices against cyber attacks is imperative owing to inherent security vulnerabilities these vulnerabilities can include a spectrum of sophisticated attacks that pose significant damage to both individuals and organizations employing robust security measures like intrusion detection systems idss is essential to solve these problems and protect iot systems from such attacks in this context our proposed ids model consists on a combination of convolutional neural network cnn and long shortterm memory lstm deep learning dl models this fusion facilitates the detection and classification of iot traffic into binary categories benign and malicious activities by leveraging the spatial feature extraction capabilities of cnn for pattern recognition and the sequential memory retention of lstm for discerning complex temporal dependencies in achieving enhanced accuracy and efficiency in assessing the performance of our proposed model the authors employed the new ciciot2023 dataset for both training and final testing while further validating the models performance through a conclusive testing phase utilizing the cicids2017 dataset our proposed model achieves an accuracy rate of 9842 accompanied by a minimal loss of 00275 false positive ratefpr is equally important reaching 917 with an f1score of 9857 these results demonstrate the effectiveness of our proposed cnnlstm ids model in fortifying iot environments against potential cyber threats,0
while the benefits of 6genabled internet of things iot are numerous providing highspeed lowlatency communication that brings new opportunities for innovation and forms the foundation for continued growth in the iot industry it is also important to consider the security challenges and risks associated with the technology in this paper we propose a twostage intrusion detection framework for securing iots which is based on two detectors in the first stage we propose an adversarial training approach using generative adversarial networks gan to help the first detector train on robust features by supplying it with adversarial examples as validation sets consequently the classifier would perform very well against adversarial attacks then we propose a deep learning dl model for the second detector to identify intrusions we evaluated the proposed approachs efficiency in terms of detection accuracy and robustness against adversarial attacks experiment results with a new cyber security dataset demonstrate the effectiveness of the proposed methodology in detecting both intrusions and persistent adversarial examples with a weighted avg of 96 95 95 and 95 for precision recall f1score and accuracy respectively,0
the last decades have been characterized by unprecedented technological advances many of them powered by modern technologies such as artificial intelligence ai and machine learning ml the world has become more digitally connected than ever but we face major challenges one of the most significant is cybercrime which has emerged as a global threat to governments businesses and civil societies the pervasiveness of digital technologies combined with a constantly shifting technological foundation has created a complex and powerful playground for cybercriminals which triggered a surge in demand for intelligent threat detection systems based on machine and deep learning this paper investigates aibased cyber threat detection to protect our modern digital ecosystems the primary focus is on evaluating mlbased classifiers and ensembles for anomalybased malware detection and network intrusion detection and how to integrate those models in the context of network security mobile security and iot security the discussion highlights the challenges when deploying and integrating aienabled cybersecurity solutions into existing enterprise systems and it infrastructures including options to overcome those challenges finally the paper provides future research directions to further increase the security and resilience of our modern digital industries infrastructures and ecosystems,0
the internet of things has rapidly transformed the 21st century enhancing decisionmaking processes and introducing innovative consumer services such as payasyouuse models the integration of smart devices and automation technologies has revolutionized every aspect of our lives from health services to the manufacturing industry and from the agriculture sector to mining alongside the positive aspects it is also essential to recognize the significant safety security and trust concerns in this technological landscape this chapter serves as a comprehensive guide for newcomers interested in the iot domain providing a foundation for making future contributions specifically it discusses the overview historical evolution key characteristics advantages architectures taxonomy of technologies and existing applications in major iot domains in addressing prevalent issues and challenges in designing and deploying iot applications the chapter examines security threats across architectural layers ethical considerations user privacy concerns and trustrelated issues this discussion equips researchers with a solid understanding of diverse iot aspects providing a comprehensive understanding of iot technology along with insights into the extensive potential and impact of this transformative field,0
attacks against the internet of things iot are rising as devices applications and interactions become more networked and integrated the increase in cyberattacks that target iot networks poses a considerable vulnerability and threat to the privacy security functionality and availability of critical systems which leads to operational disruptions financial losses identity thefts and data breaches to efficiently secure iot devices realtime detection of intrusion systems is critical especially those using machine learning to identify threats and mitigate risks and vulnerabilities this paper investigates the latest research on machine learningbased intrusion detection strategies for iot security concentrating on realtime responsiveness detection accuracy and algorithm efficiency key studies were reviewed from all wellknown academic databases and a taxonomy was provided for the existing approaches this review also highlights existing research gaps and outlines the limitations of current iot security frameworks to offer practical insights for future research directions and developments,0
a significant increase in the number of interconnected devices and data communication through wireless networks has given rise to various threats risks and security concerns internet of things iot applications is deployed in almost every field of daily life including sensitive environments the edge computing paradigm has complemented iot applications by moving the computational processing near the data sources among various security models machine learning ml based intrusion detection is the most conceivable defense mechanism to combat the anomalous behavior in edgeenabled iot networks the ml algorithms are used to classify the network traffic into normal and malicious attacks intrusion detection is one of the challenging issues in the area of network security the research community has proposed many intrusion detection systems however the challenges involved in selecting suitable algorithms to provide security in edgeenabled iot networks exist in this paper a comparative analysis of conventional machine learning classification algorithms has been performed to categorize the network traffic on nslkdd dataset using jupyter on pycharm tool it can be observed that multilayer perception mlp has dependencies between input and output and relies more on network configuration for intrusion detection therefore mlp can be more appropriate for edgebased iot networks with a better training time of 12 seconds and testing accuracy of 79,0
this paper proposes a novel selfsupervised intrusion detection ssid framework which enables a fully online deep learning dl based intrusion detection system ids that requires no human intervention or prior offline learning the proposed framework analyzes and labels incoming traffic packets based only on the decisions of the ids itself using an autoassociative deep random neural network and on an online estimate of its statistically measured trustworthiness the ssid framework enables ids to adapt rapidly to timevarying characteristics of the network traffic and eliminates the need for offline data collection this approach avoids human errors in data labeling and human labor and computational costs of model training and data collection the approach is experimentally evaluated on public datasets and compared with wellknown machine learning and deep learning models showing that this ssid framework is very useful and advantageous as an accurate and online learning dlbased ids for iot systems,0
we analyze sets of intrusion detection records observed on the networks of several large nonresidential organizations protected by a form of intrusion detection and prevention service our analyses reveal that the process of intrusion detection in these networks exhibits a significant degree of burstiness as well as strong memory with burstiness and memory properties that are comparable to those of natural processes driven by threshold effects but different from bursty human activities we explore timeseries models of these observable network security incidents based on partially observed data using a hidden markov model with restricted hidden states which we fit using markov chain monte carlo techniques we examine the output of the fitted model with respect to its statistical properties and demonstrate that the model adequately accounts for intrinsic bursting within observed network incidents as a result of alternation between two or more stochastic processes while our analysis does not lead directly to new detection capabilities the practical implications of gaining better understanding of the observed burstiness are significant and include opportunities for quantifying a networks risks and defensive efforts,0
the widespread adoption of internet of things has led to many security issues post the miraibased ddos attack in 2016 which compromised iot devices a host of new malware using mirais leaked source code and targeting iot devices have cropped up eg satori reaper amnesia masuta etc these malware exploit software vulnerabilities to infect iot devices instead of open telnet ports like mirai making them more difficult to block using existing solutions such as firewalls in this research we present edima a distributed modular solution which can be used towards the detection of iot malware network activity in largescale networks eg isp enterprise networks during the scanninginfecting phase rather than during an attack edima employs machine learning algorithms for edge devices traffic classification a packet traffic feature vector database a policy module and an optional packet subsampling module we evaluate the classification performance of edima through testbed experiments and present the results obtained,0
nowadays the internet of things iot is widely employed and its usage is growing exponentially because it facilitates remote monitoring predictive maintenance and datadriven decision making especially in the healthcare and industrial sectors however iot devices remain vulnerable due to their resource constraints and difficulty in applying security patches consequently various cybersecurity attacks are reported daily such as denial of service particularly in iotdriven solutions most attack detection methodologies are based on machine learning ml techniques which can detect attack patterns however the focus is more on identification rather than considering the impact of ml algorithms on computational resources this paper proposes a green methodology to identify iot malware networking attacks based on flow privacypreserving statistical features in particular the hyperparameters of three treebased models decision trees random forest and extratrees are optimized based on energy consumption and testtime performance in terms of matthews correlation coefficient our results show that models maintain high performance and detection accuracy while consistently reducing power usage in terms of watthours wh this suggests that onpremise mlbased intrusion detection systems are suitable for iot and other resourceconstrained devices,0
the rapid expansion of internet of things iot networks has introduced new security challenges necessitating efficient and reliable methods for intrusion detection in this study a detection framework based on hyperdimensional computing hdc is proposed to identify and classify network intrusions using the nslkdd dataset a standard benchmark for intrusion detection systems by leveraging the capabilities of hdc including highdimensional representation and efficient computation the proposed approach effectively distinguishes various attack categories such as dos probe r2l and u2r while accurately identifying normal traffic patterns comprehensive evaluations demonstrate that the proposed method achieves an accuracy of 9954 significantly outperforming conventional intrusion detection techniques making it a promising solution for iot network security this work emphasizes the critical role of robust and precise intrusion detection in safeguarding iot systems against evolving cyber threats,0
the digital transformation faces tremendous security challenges in particular the growing number of cyberattacks targeting internet of things iot systems restates the need for a reliable detection of malicious network activity this paper presents a comparative analysis of supervised unsupervised and reinforcement learning techniques on nine malware captures of the iot23 dataset considering both binary and multiclass classification scenarios the developed models consisted of support vector machine svm extreme gradient boosting xgboost light gradient boosting machine lightgbm isolation forest iforest local outlier factor lof and a deep reinforcement learning drl model based on a double deep qnetwork ddqn adapted to the intrusion detection context the most reliable performance was achieved by lightgbm nonetheless iforest displayed good anomaly detection results and the drl model demonstrated the possible benefits of employing this methodology to continuously improve the detection overall the obtained results indicate that the analyzed techniques are well suited for iot intrusion detection,0
the upcoming internet of things iot is foreseen to encompass massive numbers of connected devices smart objects and cyberphysical systems due to the largescale and massive deployment of devices it is deemed infeasible to safeguard 100 of the devices with stateoftheart security countermeasures hence largescale iot has inevitable loopholes for network intrusion and malware infiltration even worse exploiting the high density of devices and direct wireless connectivity malware infection can stealthily propagate through susceptible ie unsecured devices and form an epidemic outbreak without being noticed to security administration a malware outbreak enables adversaries to compromise large population of devices which can be exploited to launch versatile cyber and physical malicious attacks in this context we utilize spatial firewalls to safeguard the iot from malware outbreak in particular spatial firewalls are computationally capable devices equipped with stateoftheart security and antimalware programs that are spatially deployed across the network to filter the wireless traffic in order to detect and thwart malware propagation using tools from percolation theory we prove that there exists a critical density of spatial firewalls beyond which malware outbreak is impossible this in turns safeguards the iot from malware epidemics regardless of the infectiontreatment rates to this end a tractable upper bound for the critical density of spatial firewalls is obtained furthermore we characterize the relative communications ranges of the spatial firewalls and iot devices to ensure secure network connectivity the percentage of devices secured by the firewalls is also characterized,0
the need for secure internet of things iot devices is growing as iot devices are becoming more integrated into vital networks many systems rely on these devices to remain available and provide reliable service denial of service attacks against iot devices are a real threat due to the fact these low power devices are very susceptible to denialofservice attacks machine learning enabled network intrusion detection systems are effective at identifying new threats but they require a large amount of data to work well there are many network traffic data sets but very few that focus on iot network traffic within the iot network data sets there is a lack of coap denial of service data we propose a novel data set covering this gap we develop a new data set by collecting network traffic from real coap denial of service attacks and compare the data on multiple different machine learning classifiers we show that the data set is effective on many classifiers,0
adversarial attacks have been widely studied in the field of computer vision but their impact on network security applications remains an area of open research as iot 5g and ai continue to converge to realize the promise of the fourth industrial revolution industry 40 security incidents and events on iot networks have increased deep learning techniques are being applied to detect and mitigate many of such security threats against iot networks feedforward neural networks fnn have been widely used for classifying intrusion attacks in iot networks in this paper we consider a variant of the fnn known as the selfnormalizing neural network snn and compare its performance with the fnn for classifying intrusion attacks in an iot network our analysis is performed using the botiot dataset from the cyber range lab of the center of unsw canberra cyber in our experimental results the fnn outperforms the snn for intrusion detection in iot networks based on multiple performance metrics such as accuracy precision and recall as well as multiclassification metrics such as cohens kappa score however when tested for adversarial robustness the snn demonstrates better resilience against the adversarial samples from the iot dataset presenting a promising future in the quest for safer and more secure deep learning in iot networks,0
in recent years networked iot systems have revolutionized connectivity portability and functionality offering a myriad of advantages however these systems are increasingly targeted by adversaries due to inherent security vulnerabilities and limited computational and storage resources malicious applications commonly known as malware pose a significant threat to iot devices and networks while numerous malware detection techniques have been proposed existing approaches often overlook the resource constraints inherent in iot environments assuming abundant resources for detection tasks this oversight is compounded by ongoing workloads such as sensing and ondevice computations further diminishing available resources for malware detection to address these challenges we present a novel resource and workloadaware malware detection framework integrated with distributed computing for iot networks our approach begins by analyzing available resources for malware detection using a lightweight regression model depending on resource availability ongoing workload executions and communication costs the malware detection task is dynamically allocated either ondevice or offloaded to neighboring iot nodes with sufficient resources to safeguard data integrity and user privacy rather than transferring the entire malware detection task the classifier is partitioned and distributed across multiple nodes and subsequently integrated at the parent node for comprehensive malware detection experimental analysis demonstrates the efficacy of our proposed technique achieving a remarkable speedup of 98x compared to ondevice inference while maintaining a high malware detection accuracy of 967,0
the field of iot has blossomed and is positively influencing many application domains in this paper we bring out the unique challenges this field poses to research in computer systems and networking the unique challenges arise from the unique characteristics of iot systems such as the diversity of application domains where they are used and the increasingly demanding protocols they are being called upon to run such as video and lidar processing on constrained resources onnode and network we show how these open challenges can benefit from foundations laid in other areas such as 5g cellular protocols ml model reduction and deviceedgecloud offloading we then discuss the unique challenges for reliability security and privacy posed by iot systems due to their salient characteristics which include heterogeneity of devices and protocols dependence on the physical environment and the close coupling with humans we again show how the open research challenges benefit from reliability security and privacy advancements in other areas we conclude by providing a vision for a desirable end state for iot systems,0
in internet of things iot systems with security demands there is often a need to distribute sensitive information such as encryption keys digital signatures or login credentials etc among the devices so that it can be retrieved for confidential purposes at a later moment however this information cannot be entrusted to any one device since the failure of that device or an attack on it will jeopardize the security of the entire network even if the information is divided among devices there is still the danger that an attacker can compromise a group of devices and expose the sensitive information in this work we design and implement a secure and robust scheme to enable the distribution of sensitive information in iot networks the proposed approach has two important properties 1 it uses threshold secret sharing tss to split the information into pieces distributed among all devices in the system and so the information can only be retrieved collaboratively by groups of devices and 2 it ensures the privacy and integrity of the information even when attackers hijack a large number of devices and use them in concert specifically all the compromised devices can be identified the confidentiality of information is kept and authenticity of the secret can be guaranteed,0
blacklists are a widelyused internet security mechanism to protect internet users from financial scams malicious web pages and other cyber attacks based on blacklisted urls in this demo we introduce phishchain a transparent and decentralized system to blacklisting phishing urls at present publicprivate domain blacklists such as phishtank cryptoscamdb and apwg are maintained by a centralized authority but operate in a crowd sourcing fashion to create a manually verified blacklist periodically in addition to being a single point of failure the blacklisting process utilized by such systems is not transparent we utilize the blockchain technology to support transparency and decentralization where no single authority is controlling the blacklist and all operations are recorded in an immutable distributed ledger further we design a page rank based truth discovery algorithm to assign a phishing score to each url based on crowd sourced assessment of urls as an incentive for voluntary participation we assign skill points to each user based on their participation in url verification,0
software defined internet of things sdiot networks profits from centralized management and interactive resource sharing which enhances the efficiency and scalability of iot applications but with the rapid growth in services and applications it is vulnerable to possible attacks and faces severe security challenges intrusion detection has been widely used to ensure network security but classical detection means are usually signaturebased or explicitbehaviorbased and fail to detect unknown attacks intelligently which are hard to satisfy the requirements of sdiot networks in this paper we propose an aibased twostage intrusion detection empowered by software defined technology it flexibly captures network flows with a globle view and detects attacks intelligently through applying ai algorithms we firstly leverage bat algorithm with swarm division and differential mutation to select typical features then we exploit random forest through adaptively altering the weights of samples using weighted voting mechanism to classify flows evaluation results prove that the modified intelligent algorithms select more important features and achieve superior performance in flow classification it is also verified that intelligent intrusion detection shows better accuracy with lower overhead comparied with existing solutions,0
information about the privacy and security of internet of things iot devices is not readily available to consumers who want to consider it before making purchase decisions while legislators have proposed adding succinct consumer accessible labels they do not provide guidance on the content of these labels in this paper we report on the results of a series of interviews and surveys with privacy and security experts as well as consumers where we explore and test the design space of the content to include on an iot privacy and security label we conduct an expert elicitation study by following a threeround delphi process with 22 privacy and security experts to identify the factors that experts believed are important for consumers when comparing the privacy and security of iot devices to inform their purchase decisions based on how critical experts believed each factor is in conveying risk to consumers we distributed these factors across two layersa primary layer to display on the product package itself or prominently on a website and a secondary layer available online through a web link or a qr code we report on the experts rationale and arguments used to support their choice of factors moreover to study how consumers would perceive the privacy and security information specified by experts we conducted a series of semistructured interviews with 15 participants who had purchased at least one iot device smart home device or wearable based on the results of our expert elicitation and consumer studies we propose a prototype privacy and security label to help consumers make more informed iotrelated purchase decisions,0
smart homes are one of the most promising applications of the emerging internet of things iot technology with the growing number of iot related devices such as smart thermostats smart fridges smart speaker smart light bulbs and smart locks smart homes promise to make our lives easier and more comfortable however the increased deployment of such smart devices brings an increase in potential security risks and home privacy breaches in order to overcome such risks intrusion detection systems are presented as pertinent tools that can provide networklevel protection for smart devices deployed in home environments these systems monitor the network activities of the smart homeconnected devices and focus on alerting suspicious or malicious activity they also can deal with detected abnormal activities by hindering the impostors in accessing the victim devices however the employment of such systems in the context of a smart home can be challenging due to the devices hardware limitations which may restrict their ability to counter the existing and emerging attack vectors therefore this paper proposes an experimental comparison between the widely used opensource nidss namely snort suricata and bro ids to find the most appropriate one for smart homes in term of detection accuracy and resources consumption including cp and memory utilization experimental results show that suricata is the best performing nids for smart homes,0
in the recent years we have witnessed a huge growth in the number of internet of things iot and edge devices being used in our everyday activities this demands the security of these devices from cyber attacks to be improved to protect its users for years machine learning ml techniques have been used to develop network intrusion detection systems nids with the aim of increasing their reliabilityrobustness among the earlier ml techniques dt performed well in the recent years deep learning dl techniques have been used in an attempt to build more reliable systems in this paper a deep learning enabled long short term memory lstm autoencoder and a 13feature deep neural network dnn models were developed which performed a lot better in terms of accuracy on unswnb15 and botiot datsets hence we proposed lbdmids where we developed nids models based on variants of lstms namely stacked lstm and bidirectional lstm and validated their performance on the unswnb15 and botiot datasets this paper concludes that these variants in lbdmids outperform classic ml techniques and perform similarly to the dnn models that have been suggested in the past,0
the integration of the internet of things iot connects a number of intelligent devices with a minimum of human interference that can interact with one another iot is rapidly emerging in the areas of computer science however new security problems were posed by the crosscutting design of the multidisciplinary elements and iot systems involved in deploying such schemes ineffective is the implementation of security protocols ie authentication encryption application security and access network for iot systems and their essential weaknesses in security current security approaches can also be improved to protect the iot environment effectively in recent years deep learning dl machine learning ml has progressed significantly in various critical implementations therefore dlml methods are essential to turn iot systems protection from simply enabling safe contact between iot systems to intelligence systems in security this review aims to include an extensive analysis of ml systems and stateoftheart developments in dl methods to improve enhanced iot device protection methods on the other hand various new insights in machine and deep learning for iot securities illustrate how it could help future research iot protection risks relating to emerging or essential threats are identified as well as future iot device attacks and possible threats associated with each surface we then carefully analyze dl and ml iot protection approaches and present each approachs benefits possibilities and weaknesses this review discusses a number of potential challenges and limitations the future works recommendations and suggestions of dlml in iot security are also included,0
blockchains and smart contracts are an emerging promising technology that has received considerable attention we use the blockchain technology and in particular ethereum to implement a largescale eventbased internet of things iot control system we argue that the distributed nature of the ledger as well as ethereums capability of parallel execution of replicated smart contracts provide the sought after automation generality flexibility resilience and high availability we design a realistic blockchainbased iot architecture using existing technologies while by taking into consideration the characteristics and limitations of iot devices and applications furthermore we leverage blockchains immutability and ethereums support for custom tokens to build a robust and efficient tokenbased access control mechanism our evaluation shows that our solution is viable and offers significant security and usability advantages,0
the use of autonomous vehicles avs is a promising technology in intelligent transportation systems itss to improve safety and driving efficiency vehicletoeverything v2x technology enables communication among vehicles and other infrastructures however avs and internet of vehicles iov are vulnerable to different types of cyberattacks such as denial of service spoofing and sniffing attacks in this paper an intelligent intrusion detection system ids is proposed based on treestructure machine learning models the results from the implementation of the proposed intrusion detection system on standard data sets indicate that the system has the ability to identify various cyberattacks in the av networks furthermore the proposed ensemble learning and feature selection approaches enable the proposed system to achieve high detection rate and low computational cost simultaneously,0
iot as a domain has grown so much in the last few years that it rivals that of the mobile network environments in terms of data volumes as well as cybersecurity threats the confidentiality and privacy of data within iot environments have become very important areas of security research within the last few years more and more security experts are interested in designing robust ids systems to protect iot environments as a supplement to the more traditional security methods given that iot devices are resourceconstrained and have a heterogeneous protocol stack most traditional intrusion detection approaches dont work well within these schematic boundaries this has led security researchers to innovate at the intersection of machine learning and ids to solve the shortcomings of nonlearning based ids systems in the iot ecosystem despite various ml algorithms already having high accuracy with iot datasets we can see a lack of sufficient production grade models this survey paper details a comprehensive summary of the latest learningbased approaches used in iot intrusion detection systems and conducts a thorough critical review of these systems potential pitfalls in ml pipelines challenges from an ml perspective and discusses future research scope and recommendations,0
the proliferation of internet of things iot devices has expanded the attack surface necessitating efficient intrusion detection systems idss for network protection this paper presents flare a featurebased lightweight aggregation for robust evaluation of iot intrusion detection to address the challenges of securing iot environments through feature aggregation techniques flare utilizes a multilayered processing approach incorporating session flow and timebased slidingwindow data aggregation to analyze network behavior and capture vital features from iot network traffic data we perform extensive evaluations on iot data generated from our laboratory experimental setup to assess the effectiveness of the proposed aggregation technique to classify attacks in iot ids we employ four supervised learning models and two deep learning models we validate the performance of these models in terms of accuracy precision recall and f1score our results reveal that incorporating the flare aggregation technique as a foundational step in feature engineering helps lay a structured representation and enhances the performance of complex endtoend models making it a crucial step in iot ids pipeline our findings highlight the potential of flare as a valuable technique to improve performance and reduce computational costs of endtoend ids implementations thereby fostering more robust iot intrusion detection systems,0
nand flash memorybased iot devices inherently suffer from data retention issues in iot security these retention issues are significant and require a robust solution for secure deletion secure deletion methods can be categorized into offchip and onchip schemes offchip secure deletion schemes based on blocklevel erasure operations are unable to perform realtime trim operations consequently they are vulnerable to hacking threats on the other hand onchip secure deletion schemes enable realtime trim operations by performing deletion on a pagebypage basis however the onchip scheme introduces a challenge of program disturbance for neighboring page data the proposed onchip deletion scheme tackles this problem by utilizing ecc code modulation through a partial program operation this approach significantly reduces the program disturbance issue associated with neighboring page data moreover the proposed code modulation secure deletion scheme allows for realtime verification of the deletion of original data,0
in this paper a dataset of iot network traffic is presented our dataset was generated by utilising the gotham testbed an emulated largescale internet of things iot network designed to provide a realistic and heterogeneous environment for network security research the testbed includes 78 emulated iot devices operating on various protocols including mqtt coap and rtsp network traffic was captured in packet capture pcap format using tcpdump and both benign and malicious traffic were recorded malicious traffic was generated through scripted attacks covering a variety of attack types such as denial of service dos telnet brute force network scanning coap amplification and various stages of command and control cc communication the data were subsequently processed in python for feature extraction using the tshark tool and the resulting data was converted to comma separated values csv format and labelled the data repository includes the raw network traffic in pcap format and the processed labelled data in csv format our dataset was collected in a distributed manner where network traffic was captured separately for each iot device at the interface between the iot gateway and the device our dataset was collected in a distributed manner where network traffic was separately captured for each iot device at the interface between the iot gateway and the device with its diverse traffic patterns and attack scenarios this dataset provides a valuable resource for developing intrusion detection systems and security mechanisms tailored to complex largescale iot environments the dataset is publicly available at zenodo,0
for smart homes to be safe homes they must be designed with security in mind yet despite the widespread proliferation of connected digital technologies in the home environment there is a lack of research evaluating the security vulnerabilities and potential risks present within these systems our research presents a comprehensive methodology for conducting systematic iot security attacks intercepting network traffic and evaluating the security risks of smart home devices we perform hundreds of automated experiments using 11 popular commercial iot devices when deployed in a testbed exposed to a series of real deployed attacks flooding port scanning and os scanning our findings indicate that these devices are vulnerable to security attacks and our results are relevant to the security research community device engineers and the users who rely on these technologies in their daily lives,0
modern vehicles including autonomous vehicles and connected vehicles have adopted an increasing variety of functionalities through connections and communications with other vehicles smart devices and infrastructures however the growing connectivity of the internet of vehicles iov also increases the vulnerabilities to network attacks to protect iov systems against cyber threats intrusion detection systems idss that can identify malicious cyberattacks have been developed using machine learning ml approaches to accurately detect various types of attacks in iov networks we propose a novel ensemble ids framework named leader class and confidence decision ensemble lccde it is constructed by determining the bestperforming ml model among three advanced ml algorithms xgboost lightgbm and catboost for every class or type of attack the class leader models with their prediction confidence values are then utilized to make accurate decisions regarding the detection of various types of cyberattacks experiments on two public iov security datasets carhacking and cicids2017 datasets demonstrate the effectiveness of the proposed lccde for intrusion detection on both intravehicle and external networks,0
the rapid development of the internet of things iot has enabled novel usercentred applications including many in safetycritical areas such as healthcare smart environment security and emergency response systems the diversity in iot manufacturers standards and devices creates a combinatorial explosion of such deployment scenarios leading to increased security and safety threats due to the difficulty of managing such heterogeneity in almost every iot deployment wireless gateways are crucial for interconnecting iot devices and providing services yet they are vulnerable to external threats and serve as key entry points for largescale iot attacks memorybased vulnerabilities are among the most serious threats in software with no universal solution yet available legacy memory protection mechanisms such as canaries relro nx and fortify have enhanced memory safety but remain insufficient for comprehensive protection emerging technologies like armmte cheri and rust are based on more universal and robust securebydesign sbd memory safety principles yet each entails different tradeoffs in hardware or code modifications given the challenges of balancing security levels with associated overheads in iot systems this paper explores the impact of memory safety on the iot domain through an empirical largescale analysis of memoryrelated vulnerabilities in modern wireless gateways our results show that memory vulnerabilities constitute the majority of iot gateway threats underscoring the necessity for sbd solutions with the choice of memoryprotection technology depending on specific use cases and associated overheads,0
with the advancement of technology devices which are considered nontraditional in terms of internet capabilities are now being embedded in microprocessors to communicate and these devices are known as iot devices this technology has enabled household devices to have the ability to communicate with the internet and a network comprising of such device can create a home iot network such iot devices are resource constrained and lack highlevel security protocols thus security becomes a major issue for such network systems one way to secure the networks is through reliable authentication protocols and data transfer mechanism as the household devices are controllable by the users remotely they are accessed over the internet therefore there should also be a method to make the communication over the internet between iot devices and the users more secured this paper proposes a twophase authentication protocol for authentication purposes and a vpn based secure channel creation for the communication of the devices in the network furthermore the paper discusses the elliptic curve cryptography as a viable alternative to rsa for a more efficient key exchange mechanism for lowpowered iot devices in the network,0
the widespread integration of iot devices has greatly improved connectivity and computational capabilities facilitating seamless communication across networks despite their global deployment iot devices are frequently targeted for security breaches due to inherent vulnerabilities among these threats malware poses a significant risk to iot devices the lack of builtin security features and limited resources present challenges for implementing effective malware detection techniques on iot devices moreover existing methods assume access to all device resources for malware detection which is often not feasible for iot devices deployed in critical realworld scenarios to overcome this challenge this study introduces a novel approach to malware detection tailored for iot devices leveraging resource and workload awareness inspired by model parallelism initially the device assesses available resources for malware detection using a lightweight regression model based on resource availability ongoing workload and communication costs the malware detection task is dynamically allocated either ondevice or offloaded to neighboring iot nodes with sufficient resources to uphold data integrity and user privacy instead of transferring the entire malware detection task the classifier is divided and distributed across multiple nodes then integrated at the parent node for detection experimental results demonstrate that this proposed technique achieves a significant speedup of 98 x compared to ondevice inference while maintaining a high malware detection accuracy of 967,0
the internet of things iot with its high degree of interconnectivity and limited computational resources is particularly vulnerable to a wide range of cyber threats intrusion detection systems ids have been extensively studied to enhance iot security and machine learningbased ids mlids show considerable promise for detecting malicious activity however their effectiveness is often constrained by poor adaptability to emerging threats and the issue of catastrophic forgetting during continuous learning to address these challenges we propose citadel a selfsupervised continual learning framework designed to extract robust representations from benign data while preserving longterm knowledge through optimized memory consolidation mechanisms citadel integrates a tabulartoimage transformation module a memoryaware masked autoencoder for selfsupervised representation learning and a novelty detection component capable of identifying anomalies without dependence on labeled attack data our design enables the system to incrementally adapt to emerging behaviors while retaining its ability to detect previously observed threats experiments on multiple intrusion datasets demonstrate that citadel achieves up to a 729 improvement over the vaebased lifelong anomaly detector vlad in key detection and retention metrics highlighting its effectiveness in dynamic iot environments,0
as the number of iot devices has increased rapidly iot botnets have exploited the vulnerabilities of iot devices however it is still challenging to detect the initial intrusion on iot devices prior to massive attacks recent studies have utilized power sidechannel information to identify this intrusion behavior on iot devices but still lack accurate models in realtime for ubiquitous botnet detection we proposed the first online intrusion detection system called deepauditor for iot devices via power auditing to develop the realtime system we proposed a lightweight power auditing device called power auditor we also designed a distributed cnn classifier for online inference in a laboratory setting in order to protect data leakage and reduce networking redundancy we then proposed a privacypreserved inference protocol via packed homomorphic encryption and a sliding window protocol in our system the classification accuracy and processing time were measured and the proposed classifier outperformed a baseline classifier especially against unseen patterns we also demonstrated that the distributed cnn design is secure against any distributed components overall the measurements were shown to the feasibility of our realtime distributed system for intrusion detection on iot devices,0
in past three decades almost everything has changed in the field of malware and malware analysis from malware created as proof of some security concept and malware created for financial gain to malware created to sabotage infrastructure in this work we will focus on history and evolution of malware and describe most important malwares,0
intrusion detection systems ids have been the industry standard for securing iot networks against known attacks to increase the capability of an ids researchers proposed the concept of blockchainbased collaborativeids cids wherein blockchain acts as a decentralised platform allowing collaboration between cids nodes to share intrusion related information such as intrusion alarms and detection rules however proposals in blockchainbased cids overlook the importance of continuous evaluation of the trustworthiness of each node and generally work based on the assumption that the nodes are always honest in this paper we propose a decentralised cids that emphasises the importance of building trust between cids nodes in our proposed solution each cids node exchanges detection rules to help other nodes detect new types of intrusion our architecture offloads the trust computation to the blockchain and utilises a decentralised storage to host the shared trustworthy detection rules ensuring scalability our implementation in a labscale testbed shows that the our solution is feasible and performs within the expected benchmarks of the ethereum platform,0
intrusion detection systems idss have played a significant role in detecting and preventing cyberattacks within traditional computing systems it is not surprising that the same technology is being applied to secure internet of things iot networks from cyber threats the limited computational resources available on iot devices make it challenging to deploy conventional computingbased idss the idss designed for iot environments must also demonstrate high classification performance utilize lowcomplexity models and be of a small size despite significant progress in iotbased intrusion detection developing models that both achieve high classification performance and maintain reduced complexity remains challenging in this study we propose a hybrid cnn architecture composed of a lightweight cnn and bidirectional lstm bilstm to enhance the performance of ids on the unswnb15 dataset the proposed model is specifically designed to run onboard resourceconstrained iot devices and meet their computation capability requirements despite the complexity of designing a model that fits the requirements of iot devices and achieves higher accuracy our proposed model outperforms the existing research efforts in the literature by achieving an accuracy of 9728 for binary classification and 9691 for multiclassification,0
this article provides a survey on what can be called postquantum iot systems iot systems protected from the currently known quantum computing attacks the main postquantum cryptosystems and initiatives are reviewed the most relevant iot architectures and challenges are analyzed and the expected future trends are indicated thus this paper is aimed at providing a wide view of postquantum iot security and give useful guidelines to the future postquantum iot developers,0
facilitated by messaging protocols mp many home devices are connected to the internet bringing convenience and accessibility to customers however most deployed mps on iot platforms are fragmented and are not implemented carefully to support secure communication to the best of our knowledge there is no systematic solution to perform automatic security checks on mp implementations yet to bridge the gap we present mpinspector the first automatic and systematic solution for vetting the security of mp implementations mpinspector combines model learning with formal analysis and operates in three stages a using parameter semantics extraction and interaction logic extraction to automatically infer the state machine of an mp implementation b generating security properties based on meta properties and the state machine and c applying automatic property based formal verification to identify property violations we evaluate mpinspector on three popular mps including mqtt coap and amqp implemented on nine leading iot platforms it identifies 252 property violations leveraging which we further identify eleven types of attacks under two realistic attack scenarios in addition we demonstrate that mpinspector is lightweight the average overhead of endtoend analysis is 45 hours and effective with a precision of 100 in identifying property violations,0
internet of things iot and its applications are the most popular research areas at present the characteristics of iot on one side make it easily applicable to reallife applications whereas on the other side expose it to cyber threats denial of service dos is one of the most catastrophic attacks against iot in this paper we investigate the prospects of using machine learning classification algorithms for securing iot against dos attacks a comprehensive study is carried on the classifiers which can advance the development of anomalybased intrusion detection systems idss performance assessment of classifiers is done in terms of prominent metrics and validation methods popular datasets cidds001 unswnb15 and nslkdd are used for benchmarking classifiers friedman and nemenyi tests are employed to analyze the significant differences among classifiers statistically in addition raspberry pi is used to evaluate the response time of classifiers on iot specific hardware we also discuss a methodology for selecting the best classifier as per application requirements the main goals of this study are to motivate iot security researchers for developing idss using ensemble learning and suggesting appropriate methods for statistical assessment of classifiers performance,0
in recent years the evolution of machine learning techniques has significantly impacted the field of intrusion detection particularly within the context of the internet of things iot as iot networks expand the need for robust security measures to counteract potential threats has become increasingly critical this paper introduces a hybrid intrusion detection system ids that synergistically combines kolmogorovarnold networks kans with the xgboost algorithm our proposed ids leverages the unique capabilities of kans which utilize learnable activation functions to model complex relationships within data alongside the powerful ensemble learning techniques of xgboost known for its high performance in classification tasks this hybrid approach not only enhances the detection accuracy but also improves the interpretability of the model making it suitable for dynamic and intricate iot environments experimental evaluations demonstrate that our hybrid ids achieves an impressive detection accuracy exceeding 99 in distinguishing between benign and malicious activities additionally we were able to achieve f1 scores precision and recall that exceeded 98 furthermore we conduct a comparative analysis against traditional multilayer perceptron mlp networks assessing performance metrics such as precision recall and f1score the results underscore the efficacy of integrating kans with xgboost highlighting the potential of this innovative approach to significantly strengthen the security framework of iot networks,0
malwares are the key means leveraged by threat actors in the cyber space for their attacks there is a large array of commercial solutions in the market and significant scientific research to tackle the challenge of the detection and defense against malwares at the same time attackers also advance their capabilities in creating polymorphic and metamorphic malwares to make it increasingly challenging for existing solutions to tackle this issue we propose a methodology to perform malware detection and family attribution the proposed methodology first performs the extraction of opcodes from malwares in each family and constructs their respective opcode graphs we explore the use of clustering algorithms on the opcode graphs to detect clusters of malwares within the same malware family such clusters can be seen as belonging to different subfamily groups opcode graph signatures are built from each detected cluster hence for each malware family a group of signatures is generated to represent the family these signatures are used to classify an unknown sample as benign or belonging to one the malware families we evaluate our methodology by performing experiments on a dataset consisting of both benign files and malware samples belonging to a number of different malware families and comparing the results to existing approach,0
the rapid expansion of the internet of things iot has introduced significant security challenges necessitating efficient and adaptive intrusion detection systems ids traditional ids models often overlook the temporal characteristics of network traffic limiting their effectiveness in early threat detection we propose a transformerbased early intrusion detection system eids that incorporates dynamic temporal positional encodings to enhance detection accuracy while maintaining computational efficiency by leveraging network flow timestamps our approach captures both sequence structure and timing irregularities indicative of malicious behaviour additionally we introduce a data augmentation pipeline to improve model robustness evaluated on the ciciot2023 dataset our method outperforms existing models in both accuracy and earliness we further demonstrate its realtime feasibility on resourceconstrained iot devices achieving lowlatency inference and minimal memory footprint,0
network security morning nsm is essential for any cybersecurity system where the average cost of a cyber attack is 11 million no matter how secure a system it will eventually fail without proper and continuous monitoring no wonder that the cybersecurity market is expected to grow up to 1704 billion in 2022 however the majority of legacy industries do not invest in nsm implementation until it is too late due to the initial and operation costs and static unutilized resources thus this paper proposes a novel dynamic internet of things iot architecture for an industrial nsm that features a low installation and operation cost low power consumption intelligent organization behavior and environmentally friendly operation as a case study the system is implemented in a midrange oil a gas manufacturing facility in the southern states with more than 300 machines and servers over three remote locations and a production plant that features a challenging atmosphere condition the proposed system successfully shows a significant saving 65 in power consumption acquires onetenth of the installation cost develops an intelligent operation expert system tool as well as saves the environment from more than 500mg of co2 pollution per hour promoting green iot systems,0
an application of software known as an intrusion detection system ids employs machine algorithms to identify network intrusions selective logging safeguarding privacy reputationbased defense against numerous attacks and dynamic response to threats are a few of the problems that intrusion identification is used to solve the biological system known as iot has seen a rapid increase in high dimensionality and information traffic selfprotective mechanisms like intrusion detection systems idss are essential for defending against a variety of attacks on the other hand the functional and physical diversity of iot ids systems causes significant issues these attributes make it troublesome and unrealistic to completely use all iot elements and properties for ids selfsecurity for peculiaritybased ids this study proposes and implements a novel component selection and extraction strategy our strategy a fiveml algorithm modelbased ids for machine learningbased networks with proper hyperparamater tuning is presented in this paper by examining how the most popular feature selection methods and classifiers are combined such as knearest neighbors knn classifier decision tree dt classifier random forest rf classifier gradient boosting classifier and ada boost classifier the random forest rf classifier had the highest accuracy of 9939 the knearest neighbor knn classifier exhibited the lowest performance among the evaluated models achieving an accuracy of 9484 this studys models have a significantly higher performance rate than those used in previous studies indicating that they are more reliable,0
an intrusion detection system ids aims to alert users of incoming attacks by deploying a detector that monitors network traffic continuously as an effort to increase detection capabilities a set of independent ids detectors typically work collaboratively to build intelligence of holistic network representation which is referred to as collaborative intrusion detection system cids however developing an effective cids particularly for the iot ecosystem raises several challenges recent trends and advances in blockchain technology which provides assurance in distributed trust and secure immutable storage may contribute towards the design of effective cids in this poster abstract we present our ongoing work on a decentralized cids for iot which is based on blockchain technology we propose an architecture that provides accountable trust establishment which promotes incentives and penalties and scalable intrusion information storage by exchanging bloom filters we are currently implementing a proofofconcept of our modular architecture in a local testbed and evaluate its effectiveness in detecting common attacks in iot networks and the associated overhead,0
the vast increase of internet of things iot technologies and the everevolving attack vectors have increased cybersecurity risks dramatically a common approach to implementing aibased intrusion detection systems idss in distributed iot systems is in a centralised manner however this approach may violate data privacy and prohibit ids scalability therefore intrusion detection solutions in iot ecosystems need to move towards a decentralised direction federated learning fl has attracted significant interest in recent years due to its ability to perform collaborative learning while preserving data confidentiality and locality nevertheless most flbased ids for iot systems are designed under unrealistic data distribution conditions to that end we design an experiment representative of the real world and evaluate the performance of an flbased ids for our experiments we rely on toniot a realistic iot network traffic dataset associating each ip address with a single fl client additionally we explore pretraining and investigate various aggregation methods to mitigate the impact of data heterogeneity lastly we benchmark our approach against a centralised solution the comparison shows that the heterogeneous nature of the data has a considerable negative impact on the models performance when trained in a distributed manner however in the case of a pretrained initial global fl model we demonstrate a performance improvement of over 20 f1score compared to a randomly initiated global model,0
phishing is a persistent cybersecurity threat in todays digital landscape this paper introduces phishsense1b a refined version of the llamaguard31b model specifically tailored for phishing detection and reasoning this adaptation utilizes lowrank adaptation lora and the guardreasoner finetuning methodology we outline our lorabased finetuning process describe the balanced dataset comprising phishing and benign emails and highlight significant performance improvements over the original model our findings indicate that phishsense1b achieves an impressive 975 accuracy on a custom dataset and maintains strong performance with 70 accuracy on a challenging realworld dataset this performance notably surpasses both unadapted models and bertbased detectors additionally we examine current stateoftheart detection methods compare promptengineering with finetuning strategies and explore potential deployment scenarios,0
malware detection in iot environments necessitates robust methodologies this study introduces a cnnlstm hybrid model for iot malware identification and evaluates its performance against established methods leveraging kfold crossvalidation the proposed approach achieved 955 accuracy surpassing existing methods the cnn algorithm enabled superior learning model construction and the lstm classifier exhibited heightened accuracy in classification comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model highlighting its potential for enhancing iot security the study advocates for future exploration of svms as alternatives emphasizes the need for distributed detection strategies and underscores the importance of predictive analyses for a more powerful iot security this research serves as a platform for developing more resilient security measures in iot ecosystems,0
since the advent of the internet of things iot exchanging vast amounts of information has increased the number of security threats in networks as a result intrusion detection based on deep learning dl has been developed to achieve high throughput and high precision unlike general deep learningbased scenarios iot networks contain benign traffic far more than abnormal traffic with some rare attacks however most existing studies have been focused on sacrificing the detection rate of the majority class in order to improve the detection rate of the minority class in classimbalanced iot networks although this way can reduce the false negative rate of minority classes it both wastes resources and reduces the credibility of the intrusion detection systems to address this issue we propose a lightweight framework named s2cganids the proposed framework leverages the distribution characteristics of network traffic to expand the number of minority categories in both data space and feature space resulting in a substantial increase in the detection rate of minority categories while simultaneously ensuring the detection precision of majority categories to reduce the impact of sparsity on the experiments the cicids2017 numeric dataset is utilized to demonstrate the effectiveness of the proposed method the experimental results indicate that our proposed approach outperforms the superior method in both precision and recall particularly with a 102 improvement in the f1score,0
traditional techniques to detect malware infections were not meant to be used by the enduser and current malware removal tools and security software cannot handle the heterogeneity of iot devices in this paper we design develop and evaluate a tool called nurse to fill this information gap ie enabling endusers to detect iotmalware infections in their home networks nurse follows a modular approach to analyze iot traffic as captured by means of an arp spoofing technique which does not require any network modification or specific hardware thus nurse provides zeroconfiguration iot traffic analysis within everybodys reach after testing nurse in 83 different iot network scenarios with a wide variety of iot device types results show that nurse identifies malwareinfected iot devices with high accuracy 867 using device network behavior and contacted destinations,0
the widespread integration of internet of things iot devices across all facets of life has ushered in an era of interconnectedness creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems however traditional security systems are designed with a closedworld perspective and often face challenges in dealing with the everevolving threat landscape where new and unfamiliar attacks are constantly emerging in this paper we introduce a framework aimed at mitigating the open set recognition osr problem in the realm of network intrusion detection systems nids tailored for iot environments our framework capitalizes on imagebased representations of packetlevel data extracting spatial and temporal patterns from network traffic additionally we integrate stacking and subclustering techniques enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior the empirical results prominently underscore the frameworks efficacy boasting an impressive 88 detection rate for previously unseen attacks when compared against existing approaches and recent advancements future work will perform extensive experimentation across various openness levels and attack scenarios further strengthening the adaptability and performance of our proposed solution in safeguarding iot environments,0
internet of things iot is a disruptive technology with applications across diverse domains such as transportation and logistics systems smart grids smart homes connected vehicles and smart cities alongside the growth of these infrastructures the volume and variety of attacks on these infrastructures has increased highlighting the significance of distinct protection mechanisms intrusion detection is one of the distinguished protection mechanisms with notable recent efforts made to establish effective intrusion detection for iot and iov however unique characteristics of such infrastructures including battery power bandwidth and processors overheads and the network dynamics can influence the operation of an intrusion detection system this paper presents a comprehensive study of existing intrusion detection systems for iot systems including emerging systems such as internet of vehicles iov the paper analyzes existing systems in three aspects computational overhead energy consumption and privacy implications based on a rigorous analysis of the existing intrusion detection approaches the paper also identifies open challenges for an effective and collaborative design of intrusion detection system for resourceconstrained iot system in general and its applications such as iov these efforts are envisaged to highlight state of the art with respect to intrusion detection for iot and open challenges requiring specific efforts to achieve efficient intrusion detection within these systems,0
the internet of things iot is an extension of the traditional internet which allows a very large number of smart devices such as home appliances network cameras sensors and controllers to connect to one another to share information and improve user experiences current iot devices are typically microcomputers for domainspecific computations rather than traditional functionspecific embedded devices therefore many existing attacks targeted at traditional computers connected to the internet may also be directed at iot devices for example ddos attacks have become very common in iot environments as these environments currently lack basic security monitoring and protection mechanisms as shown by the recent mirai and brickerbot iot botnets in this paper we propose a novel lightweight approach for detecting ddos malware in iot environmentswe firstly extract onechannel grayscale images converted from binaries and then utilize a lightweight convolutional neural network for classifying iot malware families the experimental results show that the proposed system can achieve 940 accuracy for the classification of goodware and ddos malware and 818 accuracy for the classification of goodware and two main malware families,0
internet of things devices have seen a rapid growth and popularity in recent years with many more ordinary devices gaining network capability and becoming part of the ever growing iot network with this exponential growth and the limitation of resources it is becoming increasingly harder to protect against security threats such as malware due to its evolving faster than the defence mechanisms can handle with the traditional security systems are not able to detect unknown malware as they use signaturebased methods in this paper we aim to address this issue by introducing a novel iot malware traffic analysis approach using neural network and binary visualisation the prime motivation of the proposed approach is to faster detect and classify new malware zeroday malware the experiment results show that our method can satisfy the accuracy requirement of practical application,0
the rapid increase in the use of iot devices brings many benefits to the digital society ranging from improved efficiency to higher productivity however the limited resources and the open nature of these devices make them vulnerable to various cyber threats a single compromised device can have an impact on the whole network and lead to major security and physical damages this paper explores the potential of using network profiling and machine learning to secure iot against cyberattacks the proposed anomalybased intrusion detection solution dynamically and actively profiles and monitors all networked devices for the detection of iot device tampering attempts as well as suspicious network transactions any deviation from the defined profile is considered to be an attack and is subject to further analysis raw traffic is also passed on to the machine learning classifier for examination and identification of potential attacks performance assessment of the proposed methodology is conducted on the cybertrust testbed using normal and malicious network traffic the experimental results show that the proposed anomaly detection system delivers promising results with an overall accuracy of 9835 and 098 of falsepositive alarms,0
as iot devices continue to proliferate their reliability is increasingly constrained by security concerns in response researchers have developed diverse malware analysis techniques to detect and classify iot malware these techniques typically rely on extracting features at different levels from iot applications giving rise to a wide range of feature extraction methods however current approaches still face significant challenges when applied in practice this survey provides a comprehensive review of feature extraction techniques for iot malware analysis from multiple perspectives we first examine static and dynamic feature extraction methods followed by hybrid approaches we then explore feature representation strategies based on graph learning finally we compare the strengths and limitations of existing techniques highlight open challenges and outline promising directions for future research,0
with the emergence of iot internet of things huge amounts of sensitive data are being processed and transmitted everyday in edge devices with little to no security due to their aggressive power management schemes it is a common and necessary technique to make a backup of their program states and other necessary data in a nonvolatile memory nvm before going to sleep or low power mode however this memory is often left unprotected as adding robust security measures tends to be expensive for these resource constrained systems in this paper we propose a lightweight security system for nvm during low power mode this security architecture uses the memristor an emerging nanoscale device which is used to build hardware security primitives like puf physical unclonable function based encryptiondecryption true random number generators trng and memory integrity checking a reliability enhancement technique for this puf is also proposed which shows how this system would work even with lessthan100 reliable puf responses together with all these techniques we have established a dual layer security protocol data encryptionintegrity check which provides reasonable security to an embedded processor while being very lightweight in terms of area power and computation time a complete system design is demonstrated with 65nm cmos and emerging memristive technology with this we have provided a detailed and accurate estimation of resource overhead analysis of the security of the whole system is also provided,0
many iot use cases demand both secure storage and secure communication resourceconstrained devices cannot afford having one set of crypto protocols for storage and another for communication lightweight application layer security standards are being developed for iot communication extending these protocols for secure storage can significantly reduce communication latency and local processing we present blend combining secure storage and communication by storing iot data as precomputed encrypted network packets unlike local methods blend not only eliminates separate crypto for secure storage needs but also eliminates a need for realtime crypto operations reducing the communication latency significantly our evaluation shows that compared with a local solution blend reduces send latency from 630 microseconds to 110 microseconds per packet blend enables pki based key management while being sufficiently lightweight for iot blend doesnt need modifications to communication standards used when extended for secure storage and can therefore preserve underlying protocols security guarantees,0
maintaining security in iot systems depends on intrusion detection since these networks sensitivity to cyberattacks is growing based on the iot23 dataset this study explores the use of several machine learning ml and deep learning dl along with the hybrid models for binary and multiclass intrusion detection the standalone machine and deep learning models like random forest rf extreme gradient boosting xgboost artificial neural network ann knearest neighbors knn support vector machine svm and convolutional neural network cnn were used furthermore two hybrid models were created by combining machine learning techniques rf xgboost adaboost knn and svm and these hybrid models were voting based hybrid classifier where one is for binary and the other one is for multiclass classification these models vi were tested using precision recall accuracy and f1score criteria and compared the performance of each model this work thoroughly explains how hybrid standalone ml and dl techniques could improve ids intrusion detection system in terms of accuracy and scalability in iot internet of things,0
this work explores the evaluation of a machine learning anomaly detector using custommade parameterizable malware in an internet of things iot ecosystem it is assumed that the malware has infected and resides on the linux router that serves other devices on the network as depicted in figure 1 this iot ecosystem was developed as a testbed to evaluate the efficacy of a behaviorbased anomaly detector the malware consists of three types of custommade malware ransomware cryptominer and keylogger which all have exfiltration capabilities to the network the parameterization of the malware gives the malware samples multiple degrees of freedom specifically relating to the rate and size of data exfiltration the anomaly detector uses feature sets crafted from system calls and network traffic and uses a support vector machine svm for behavioralbased anomaly detection the custommade malware is used to evaluate the situations where the svm is effective as well as the situations where it is not effective,0
the exponential growth of the internet of things iot has significantly increased the complexity and volume of cybersecurity threats necessitating the development of advanced scalable and interpretable security frameworks this paper presents an innovative comprehensive framework for realtime iot attack detection and response that leverages machine learning ml explainable ai xai and large language models llm by integrating xai techniques such as shap shapley additive explanations and lime local interpretable modelagnostic explanations with a modelindependent architecture we ensure our frameworks adaptability across various ml algorithms additionally the incorporation of llms enhances the interpretability and accessibility of detection decisions providing system administrators with actionable humanunderstandable explanations of detected threats our endtoend framework not only facilitates a seamless transition from model development to deployment but also represents a realworld application capability that is often lacking in existing research based on our experiments with the ciciot2023 dataset citeneto2023ciciot2023 gemini and openai llms demonstrate unique strengths in attack mitigation gemini offers precise focused strategies while openai provides extensive indepth security measures incorporating shap and lime algorithms within xai provides comprehensive insights into attack detection emphasizing opportunities for model improvement through detailed feature analysis finetuning and the adaptation of misclassifications to enhance accuracy,0
in recent years malware detection has become an active research topic in the area of internet of things iot security the principle is to exploit knowledge from large quantities of continuously generated malware existing algorithms practice available malware features for iot devices and lack realtime prediction behaviors more research is thus required on malware detection to cope with realtime misclassification of the input iot data motivated by this in this paper we propose an adversarial selfsupervised architecture for detecting malware in iot networks setti considering samples of iot network traffic that may not be labeled in the setti architecture we design three selfsupervised attack techniques namely selfmds gselfmds and aselfmds the selfmds method considers the iot input data and the adversarial sample generation in realtime the gselfmds builds a generative adversarial network model to generate adversarial samples in the selfsupervised structure finally aselfmds utilizes three wellknown perturbation sample techniques to develop adversarial malware and inject it over the selfsupervised architecture also we apply a defence method to mitigate these attacks namely adversarial selfsupervised training to protect the malware detection architecture against injecting the malicious samples to validate the attack and defence algorithms we conduct experiments on two recent iot datasets iot23 and nbiot comparison of the results shows that in the iot23 dataset the selfmds method has the most damaging consequences from the attackers point of view by reducing the accuracy rate from 98 to 74 in the nbiot dataset the aselfmds method is the most devastating algorithm that can plunge the accuracy rate from 98 to 77,0
the rapid proliferation of internet of things iot devices has transformed numerous industries by enabling seamless connectivity and datadriven automation however this expansion has also exposed iot networks to increasingly sophisticated security threats including adversarial attacks targeting artificial intelligence ai and machine learning mlbased intrusion detection systems ids to deliberately evade detection induce misclassification and systematically undermine the reliability and integrity of security defenses to address these challenges we propose a novel adversarial detection model that enhances the robustness of iot ids against adversarial attacks through shapley additive explanations shapbased fingerprinting using shaps deepexplainer we extract attribution fingerprints from network traffic features enabling the ids to reliably distinguish between clean and adversarially perturbed inputs by capturing subtle attribution patterns the model becomes more resilient to evasion attempts and adversarial manipulations we evaluated the model on a standard iot benchmark dataset where it significantly outperformed a stateoftheart method in detecting adversarial attacks in addition to enhanced robustness this approach improves model transparency and interpretability thereby increasing trust in the ids through explainable ai,0
the rapid evolution of mobile networks from 5g to 6g has necessitated the development of autonomous network management systems such as zerotouch networks ztns however the increased complexity and automation of these networks have also escalated cybersecurity risks existing intrusion detection systems idss leveraging traditional machine learning ml techniques have shown effectiveness in mitigating these risks but they often require extensive manual effort and expert knowledge to address these challenges this paper proposes an automated machine learning automlbased autonomous ids framework towards achieving autonomous cybersecurity for nextgeneration networks to achieve autonomous intrusion detection the proposed automl framework automates all critical procedures of the data analytics pipeline including data preprocessing feature engineering model selection hyperparameter tuning and model ensemble specifically it utilizes a tabular variational autoencoder tvae method for automated data balancing treebased ml models for automated feature selection and base model learning bayesian optimization bo for hyperparameter optimization and a novel optimized confidencebased stacking ensemble ocse method for automated model ensemble the proposed automlbased ids was evaluated on two public benchmark network security datasets cicids2017 and 5gnidd and demonstrated improved performance compared to stateoftheart cybersecurity methods this research marks a significant step towards fully autonomous cybersecurity in nextgeneration networks potentially revolutionizing network security applications,0
in critical iot environments such as smart homes and industrial systems effective intrusion detection systems ids are essential for ensuring security however developing robust ids solutions remains a significant challenge traditional machine learningbased ids models typically require large datasets but data sharing is often limited due to privacy and security concerns federated learning fl presents a promising alternative by enabling collaborative model training without sharing raw data despite its advantages fl still faces key challenges such as data heterogeneity noniid data and high energy and computation costs particularly for resource constrained iot devices to address these issues this paper proposes optiflids a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption it also incorporates a customized aggregation method to better handle pruned models that differ due to noniid data distributions experiments conducted on three recent iot ids datasets toniot xiiotid and idsiot2024 demonstrate that optiflids maintains strong detection performance while improving energy efficiency making it wellsuited for deployment in realworld iot environments,0
the rapid expansion of the internet of things iot has intensified security challenges notably from distributed denial of service ddos attacks launched by compromised resourceconstrained devices traditional defenses are often illsuited for the iot paradigm creating a need for lightweight highperformance edgebased solutions this paper presents the design implementation and evaluation of an iot security framework that leverages the extended berkeley packet filter ebpf and the express data path xdp for inkernel mitigation of ddos attacks the system uses a ratebased detection algorithm to identify and block malicious traffic at the earliest stage of the network stack the framework is evaluated using both dockerbased simulations and realworld deployment on a raspberry pi 4 showing over 97 mitigation effectiveness under a 100 mbps flood legitimate traffic remains unaffected and system stability is preserved even under attack these results confirm that ebpfxdp provides a viable and highly efficient solution for hardening iot edge devices against volumetric network attacks,0
the novel internet of things iot paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications increasing systems complexity and the need for reliability and autonomy in this context both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered although machinebased service level agreements sla have been recently leveraged to establish and share policies in cloudbased scenarios and also in the iot context the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task to tackle this problem we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in iot according to the user needs through the use of a reinforcement learning based solution a user agent inside the environment is trained to choose the best smart objects granting access to the target services moreover the solution is designed to guarantee deadline requirements and user security and privacy needs finally to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis,0
the exponential expansion of iot and 5gadvanced applications has enlarged the attack surface for ddos malware and zeroday intrusions we propose an intrusion detection system that fuses a convolutional neural network cnn a bidirectional lstm bilstm and an autoencoder ae bottleneck within a privacypreserving federated learning fl framework the cnnbilstm branch captures local and gated crossfeature interactions while the ae emphasizes reconstructionbased anomaly sensitivity training occurs across edge devices without sharing raw data on unswnb15 binary the fused model attains auc 9959 percent and f1 9736 percent confusionmatrix analysis shows balanced error rates with high precision and recall average inference time is approximately 00476 ms per sample on our test hardware which is well within the less than 10 ms urllc budget supporting edge deployment we also discuss explainability drift tolerance and fl considerations for compliant scalable 5gadvanced iot security,0
in todays business environment it is difficult to imagine a workplace without access to the web yet a variety of email born viruses spyware adware trojan horses phishing attacks directory harvest attacks dos attacks and other threats combine to attack businesses and customers this paper is an attempt to review phishing a constantly growing and evolving threat to internet based commercial transactions various phishing approaches that include vishing spear phishng pharming keyloggers malware web trojans and others will be discussed this paper also highlights the latest phishing analysis made by antiphishing working group apwg and korean internet security center,0
the popularity of dynamic malware analysis has grown significantly as it enables analysts to observe the behavior of executing samples thereby enhancing malware detection and classification decisions with the continuous increase in new malware variants there is an urgent need for an automated malware analysis engine capable of accurately identifying malware samples in this paper we provide a brief overview of malware detection and classification methodologies moreover we introduce a novel framework tailored for the dynamic analysis environment called the incremental malware detection and classification framework imdcf imdcf offers a comprehensive solution for generalpurpose malware detection and classification achieving an accuracy rate of 9649 while maintaining a simple architecture,0
internet of things is growing rapidly with many connected devices now available to consumers with this growth the iot apps that manage the devices from smartphones raise significant security concerns typically these apps are secured via sensitive credentials such as email and password that need to be validated through specific servers thus requiring permissions to access the internet unfortunately even when developers are wellintentioned such apps can be nontrivial to secure so as to guarantee that users credentials do not leak to unauthorized servers on the internet for example if the app relies on thirdparty libraries as many do those libraries can potentially capture and leak sensitive credentials bugs in the applications can also result in exploitable vulnerabilities that leak credentials this paper presents our work inprogress on a prototype that enables developers to control how information flows within the app from sensitive ui data to specific servers we extend flowfence to enforce finegrained information flow policies on sensitive ui data,0
modern vehicles including connected vehicles and autonomous vehicles nowadays involve many electronic control units connected through intravehicle networks to implement various functionalities and perform actions modern vehicles are also connected to external networks through vehicletoeverything technologies enabling their communications with other vehicles infrastructures and smart devices however the improving functionality and connectivity of modern vehicles also increase their vulnerabilities to cyberattacks targeting both intravehicle and external networks due to the large attack surfaces to secure vehicular networks many researchers have focused on developing intrusion detection systems idss that capitalize on machine learning methods to detect malicious cyberattacks in this paper the vulnerabilities of intravehicle and external networks are discussed and a multitiered hybrid ids that incorporates a signaturebased ids and an anomalybased ids is proposed to detect both known and unknown attacks on vehicular networks experimental results illustrate that the proposed system can detect various types of known attacks with 9999 accuracy on the canintrusiondataset representing the intravehicle network data and 9988 accuracy on the cicids2017 dataset illustrating the external vehicular network data for the zeroday attack detection the proposed system achieves high f1scores of 0963 and 0800 on the above two datasets respectively the average processing time of each data packet on a vehiclelevel machine is less than 06 ms which shows the feasibility of implementing the proposed system in realtime vehicle systems this emphasizes the effectiveness and efficiency of the proposed ids,0
the volume of malware and the number of attacks in iot devices are rising everyday which encourages security professionals to continually enhance their malware analysis tools researchers in the field of cyber security have extensively explored the usage of sophisticated analytics and the efficiency of malware detection with the introduction of new malware kinds and attack routes security experts confront considerable challenges in developing efficient malware detection and analysis solutions in this paper a different view of malware analysis is considered and the risk level of each sample feature is computed and based on that the risk level of that sample is calculated in this way a criterion is introduced that is used together with accuracy and fpr criteria for malware analysis in iot environment in this paper three malware detection methods based on visualization techniques called the clustering approach the probabilistic approach and the deep learning approach are proposed then in addition to the usual machine learning criteria namely accuracy and fpr a proposed criterion based on the risk of samples has also been used for comparison with the results showing that the deep learning approach performed better in detecting malware,0
with the increasing number and sophistication of malware attacks malware detection systems based on machine learning ml grow in importance at the same time many popular ml models used in malware classification are supervised solutions these supervised classifiers often do not generalize well to novel malware therefore they need to be retrained frequently to detect new malware specimens which can be timeconsuming our work addresses this problem in a hybrid framework of theoretical quantum ml combined with feature selection strategies to reduce the data size and malware classifier training time the preliminary results show that vqc with xgboost selected features can get a 7891 test accuracy on the simulator the average accuracy for the model trained using the features selected with xgboost was 74 1135 on the ibm 5 qubits machines,0
the internet has caused tremendous changes since its appearance in the 1980s and now the internet of things iot seems to be doing the same the potential of iot has made it the center of attention for many people but where some see an opportunity to contribute others may see iot networks as a target to be exploited the high number of iot devices makes them the perfect setup for staging denialofservice attacks dos that can have devastating consequences this renders the need for cybersecurity measures such as intrusion detection systems idss evident the aim of this paper is to build an ids using the big data platform apache spark apache spark was used along with its ml library mllib and the botiot dataset the ids was then tested and evaluated based on fmeasure f1 as was the standard when evaluating imbalanced data two rounds of tests were performed a partial dataset for minimizing bias and the full botiot dataset for exploring big data and ml capabilities in a security setting for the partial dataset the random forest algorithm had the highest performance for binary classification at an average f1 measure of 997 as well as 996 for main category classification and an 885 f1 measure for sub category classification as for the complete dataset the decision tree algorithm scored the highest f1 measures for all conducted tests 979 for binary classification 79 for main category classification and 77 for sub category classification,0
intrusion detection systems ids for the internet of things iot systems can use aibased models to ensure secure communications iot systems tend to have many connected devices producing massive amounts of data with high dimensionality which requires complex models complex models have notorious problems such as overfitting low interpretability and high computational complexity adding model complexity penalty ie regularization can ease overfitting but it barely helps interpretability and computational efficiency feature engineering can solve these issues hence it has become critical for ids in largescale iot systems to reduce the size and dimensionality of data resulting in less complex models with excellent performance smaller data storage and fast detection this paper proposes a new feature engineering method called lemda light feature engineering based on the mean decrease in accuracy lemda applies exponential decay and an optional sensitivity factor to select and create the most informative features the proposed method has been evaluated and compared to other feature engineering methods using three iot datasets and four aiml models the results show that lemda improves the f1 score performance of all the ids models by an average of 34 and reduces the average training and detection times in most cases,0
social engineering attacks delivered via email commonly known as phishing represent a persistent cybersecurity threat leading to significant organizational incidents and data breaches although many organizations train employees on phishing often mandated by compliance requirements the realworld effectiveness of this training remains debated to contribute to evidencebased cybersecurity policy we conducted a largescale reproduction study n 12511 at a usbased financial technology firm our experimental design refined prior work by comparing training modalities in operational environments validating nists standardized phishing difficulty measurement and introducing novel organizationallevel temporal resilience metrics echoing prior work training interventions showed no significant main effects on click rates p0450 or reporting rates p0417 with negligible effect sizes however we found that the nist phish scale predicted user behavior with click rates increasing from 70 for easy lures to 150 for hard lures our organizationallevel resilience result was mixed 3655 of campaigns achieved inoculation patterns where reports preceded clicks but training did not significantly improve organizationallevel temporal protection in summary our results confirm the ineffectiveness of current phishing training approaches while offering a refined study design for future work,0
internet of things iot brings new challenges to the security solutions of computer networks so far intrusion detection system ids is one of the effective security tools but the vast amount of data that is generated by heterogeneous protocols and things alongside the constrained resources of the hosts make some of the present ids schemes defeated to grant idss the ability of working in the iot environments in this paper we propose a new distributed dimension reduction scheme which addresses the limited resources challenge a novel autoencoder ae designed and it learns to generate a latent space then the constrained hostsprobes use the generated weights to lower the dimension with a single operation the compressed data is transferred to a central ids server to verify the traffic type this scheme aims to lower the needed bandwidth to transfer data by compressing it and also reduce the overhead of the compression task in the hosts the proposed scheme is evaluated on three wellknown network traffic datasets unswnb15 toniot20 and nslkdd and the results show that we can have a 3dimensional latent space about 90 compression without any remarkable fall in ids detection accuracy,0
a smart home connects tens of home devices to the internet where an iot cloud runs various home automation applications while bringing unprecedented convenience and accessibility it also introduces various security hazards to users prior research studied smart home security from several aspects however we found that the complexity of the interactions among the participating entities ie devices iot clouds and mobile apps has not yet been systematically investigated in this work we conducted an indepth analysis of five widelyused smart home platforms combining firmware analysis network traffic interception and blackbox testing we reverseengineered the details of the interactions among the participating entities based on the details we inferred three legitimate state transition diagrams for the three entities respectively using these state machines as a reference model we identified a set of unexpected state transitions to confirm and trigger the unexpected state transitions we implemented a set of phantom devices to mimic a real device by instructing the phantom devices to intervene in the normal entityentity interactions we have discovered several new vulnerabilities and a spectrum of attacks against realworld smart home platforms,0
security and privacy are primary concerns in iot management security breaches in iot resources such as smart sensors can leak sensitive data and compromise the privacy of individuals effective iot management requires a comprehensive approach to prioritize access security and data privacy protection digital twins create virtual representations of iot resources blockchain adds decentralization transparency and reliability to iot systems this research integrates digital twins and blockchain to manage access to iot data streaming digital twins are used to encapsulate data access and view configurations access is enabled on digital twins not on iot resources directly trust structures programmed as smart contracts are the ones that manage access to digital twins consequently iot resources are not exposed to third parties and access security breaches can be prevented blockchain has been used to validate digital twins and store their configuration the research presented in this paper enables multitenant access and customization of data streaming views and abstracts the complexity of data access management this approach provides access and configuration security and data privacy protection,0
as iot devices become widely it is crucial to protect them from malicious intrusions however the data scarcity of iot limits the applicability of traditional intrusion detection methods which are highly datadependent to address this in this paper we propose the openset dandelion network osdn based on unsupervised heterogeneous domain adaptation in an openset manner the osdn model performs intrusion knowledge transfer from the knowledgerich source network intrusion domain to facilitate more accurate intrusion detection for the datascarce target iot intrusion domain under the openset setting it can also detect newlyemerged target domain intrusions that are not observed in the source domain to achieve this the osdn model forms the source domain into a dandelionlike feature space in which each intrusion category is compactly grouped and different intrusion categories are separated ie simultaneously emphasising intercategory separability and intracategory compactness the dandelionbased target membership mechanism then forms the target dandelion then the dandelion angular separation mechanism achieves better intercategory separability and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner to promote intracategory compactness the discriminating sampled dandelion mechanism is used assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge a semantic dandelion correction mechanism emphasises easilyconfused categories and guides better intercategory separability holistically these mechanisms form the osdn model that effectively performs intrusion knowledge transfer to benefit iot intrusion detection comprehensive experiments on several intrusion datasets verify the effectiveness of the osdn model outperforming three stateoftheart baseline methods by 169,0
security concerns for iot applications have been alarming because of their widespread use in different enterprise systems the potential threats to these applications are constantly emerging and changing and therefore sophisticated and dependable defense solutions are necessary against such threats with the rapid development of iot networks and evolving threat types the traditional machine learningbased ids must update to cope with the security requirements of the current sustainable iot environment in recent years deep learning and deep transfer learning have progressed and experienced great success in different fields and have emerged as a potential solution for dependable network intrusion detection however new and emerging challenges have arisen related to the accuracy efficiency scalability and dependability of the traditional ids in a heterogeneous iot setup this manuscript proposes a deep transfer learningbased dependable ids model that outperforms several existing approaches the unique contributions include effective attribute selection which is best suited to identify normal and attack scenarios for a small amount of labeled data designing a dependable deep transfer learningbased resnet model and evaluating considering realworld data to this end a comprehensive experimental performance evaluation has been conducted extensive analysis and performance evaluation show that the proposed model is robust more efficient and has demonstrated better performance ensuring dependability,0
the increasing use of internet of things iot devices has led to a rise in security related concerns regarding iot networks the surveillance cameras in iot networks are vulnerable to security threats such as brute force and zeroday attacks which can lead to unauthorized access by hackers and potential spying on the users activities moreover these cameras can be targeted by denial of service dos attacks which will make it unavailable for the user the proposed ai based framework will leverage machine learning algorithms to analyze network traffic and detect anomalous behavior allowing for quick detection and response to potential intrusions the framework will be trained and evaluated using realworld datasets to learn from past security incidents and improve its ability to detect potential intrusion,0
interaction between devices people and the internet has given birth to a new digital communication model the internet of things iot the seamless network of these smart devices is the core of this iot model however on the other hand integrating smart devices to constitute a network introduces many security challenges these connected devices have created a security blind spot where cybercriminals can easily launch an attack to compromise the devices using malware proliferation techniques therefore malware detection is considered a lifeline for the survival of iot devices against cyberattacks this study proposes a novel iot malware detection architecture imda using squeezing and boosting dilated convolutional neural network cnn the proposed architecture exploits the concepts of edge and smoothing multipath dilated convolutional operations channel squeezing and boosting in cnn edge and smoothing operations are employed with splittransformmerge stm blocks to extract local structure and minor contrast variation in the malware images stm blocks performed multipath dilated convolutional operations which helped recognize the global structure of malware patterns additionally channel squeezing and merging helped to get the prominent reduced and diverse feature maps respectively channel squeezing and boosting are applied with the help of stm block at the initial middle and final levels to capture the texture variation along with the depth for the sake of malware pattern hunting the proposed architecture has shown substantial performance compared with the customized cnn models the proposed imda has achieved accuracy 9793 f1score 09394 precision 09864 mcc 0 8796 recall 08873 aucpr 09689 and aucroc 09938,0
the internet of things iot faces tremendous security challenges machine learning models can be used to tackle the growing number of cyberattack variations targeting iot systems but the increasing threat posed by adversarial attacks restates the need for reliable defense strategies this work describes the types of constraints required for a realistic adversarial cyberattack example and proposes a methodology for a trustworthy adversarial robustness analysis with a realistic adversarial evasion attack vector the proposed methodology was used to evaluate three supervised algorithms random forest rf extreme gradient boosting xgb and light gradient boosting machine lgbm and one unsupervised algorithm isolation forest ifor constrained adversarial examples were generated with the adaptative perturbation pattern method a2pm and evasion attacks were performed against models created with regular and adversarial training even though rf was the least affected in binary classification xgb consistently achieved the highest accuracy in multiclass classification the obtained results evidence the inherent susceptibility of treebased algorithms and ensembles to adversarial evasion attacks and demonstrates the benefits of adversarial training and a security by design approach for a more robust iot network intrusion detection and cyberattack classification,0
critical role of internet of things iot in various domains like smart city healthcare supply chain and transportation has made them the target of malicious attacks past works in this area focused on centralized intrusion detection system ids assuming the existence of a central entity to perform data analysis and identify threats however such ids may not always be feasible mainly due to spread of data across multiple sources and gathering at central node can be costly also the earlier works primarily focused on improving true positive rate tpr and ignored the false positive rate fpr which is also essential to avoid unnecessary downtime of the systems in this paper we first present an architecture for ids based on hybrid ensemble model named phec which gives improved performance compared to stateoftheart architectures we then adapt this model to a federated learning framework that performs local training and aggregates only the model parameters next we propose noisetolerant phec in centralized and federated settings to address the labelnoise problem the proposed idea uses classifiers using weighted convex surrogate loss functions natural robustness of knn classifier towards noisy data is also used in the proposed architecture experimental results on four benchmark datasets drawn from various security attacks show that our model achieves high tpr while keeping fpr low on noisy and clean data further they also demonstrate that the hybrid ensemble models achieve performance in federated settings close to that of the centralized settings,0
smartphones contain information that is more sensitive and personal than those found on computers and laptops with an increase in the versatility of smartphone functionality more data has become vulnerable and exposed to attackers successful mobile malware attacks could steal a users location photos or even banking information due to a lack of postattack strategies firms also risk going out of business due to data theft thus there is a need besides just detecting malware intrusion in smartphones but to also identify the data that has been stolen to assess aid in recovery and prevent future attacks in this paper we propose an accessible nonintrusive machine learning solution to not only detect malware intrusion but also identify the type of data stolen for any app under supervision we do this with android usage data obtained by utilising publicly available data collection framework sherlock we test the performance of our architecture for multiple users on realworld data collected using the same framework our architecture exhibits less than 9 inaccuracy in detecting malware and can classify with 83 certainty on the type of data that is being stolen,0
due to its simple installation and connectivity the internet of things iot is susceptible to malware attacks being able to operate autonomously as iot devices have become more prevalent they have become the most tempting targets for malware weak guessable or hardcoded passwords and a lack of security measures contribute to these vulnerabilities along with insecure network connections and outdated update procedures to understand iot malware current methods and analysis using static methods are ineffective the field of deep learning has made great strides in recent years due to their tremendous data mining learning and expression capabilities cybersecurity has enjoyed tremendous growth in recent years as a result malware analysts will not have to spend as much time analyzing malware in this paper we propose a novel detection and analysis method that harnesses the power and simplicity of decision trees the experiments are conducted using a real word dataset malevis which is a publicly available dataset based on the results we show that our proposed approach outperforms existing stateoftheart solutions in that it achieves 9723 precision and 9589 recall in terms of detection and classification a specificity of 9658 f1score of 9640 an accuracy of 9643,0
intrusion detection systems ids are key components for securing critical infrastructures capable of detecting malicious activities on networks or hosts the procedure of implementing a ids for internet of things iot networks is not without challenges due to the variability of these systems and specifically the difficulty in accessing data the specifics of these very constrained devices render the design of an ids capable of dealing with the varied attacks a very challenging problem and a very active research subject in the current state of literature a number of approaches have been proposed to improve the efficiency of intrusion detection catering to some of these limitations such as resource constraints and mobility in this article we review works on ids specifically for these kinds of devices from 2008 to 2018 collecting a total of 51 different ids papers we summarise the current themes of the field summarise the techniques employed to train and deploy the idss and provide a qualitative evaluations of these approaches while these works provide valuable insights and solutions for subparts of these constraints we discuss the limitations of these solutions as a whole in particular what kinds of attacks these approaches struggle to detect and the setup limitations that are unique to this kind of system we find that although several paper claim novelty of their approach little inter paper comparisons have been made that there is a dire need for sharing of datasets and almost no shared code repositories consequently raising the need for a thorough comparative evaluation,0
modern vehicles including autonomous vehicles and connected vehicles are increasingly connected to the external world which enables various functionalities and services however the improving connectivity also increases the attack surfaces of the internet of vehicles iov causing its vulnerabilities to cyberthreats due to the lack of authentication and encryption procedures in vehicular networks intrusion detection systems idss are essential approaches to protect modern vehicle systems from network attacks in this paper a transfer learning and ensemble learningbased ids is proposed for iov systems using convolutional neural networks cnns and hyperparameter optimization techniques in the experiments the proposed ids has demonstrated over 9925 detection rates and f1scores on two wellknown public benchmark iov security datasets the carhacking dataset and the cicids2017 dataset this shows the effectiveness of the proposed ids for cyberattack detection in both intravehicle and external vehicular networks,0
message queuing brokers are a fundamental building block of the internet of things commonly used to store and forward messages from publishing clients to subscribing clients often a single trusted broker offers secured eg tls and unsecured connections but relays messages regardless of their inbound and outbound protection such mixed mode is facilitated for the sake of efficiency since tls is quite a burden for mqtt implementations on class0 iot devices such a broker thus transparently interconnects securely and insecurely connected devices we argue that such mixed mode operation can actually be a significant security problem clients can only control the security level of their own connection to the broker but they cannot enforce any protection towards other clients we describe an enhancement of such a publishsubscribe mechanism to allow for enforcing specified security levels of publishers or subscribers by only forwarding messages via connections which satisfy the desired security levels for example a client publishing a message over a secured channel can instruct the broker to forward the message exclusively to subscribers that are securely connected we prototypically implemented our solution for the mqtt protocol and provide detailed overhead measurements,0
the number of iot devices in use is increasing rapidly and so is the number of iot applications as in any new technology the rapid development means rapid increase in security threats and attack surfaces iot security has proven to be challenging throughout the past few years however another challenging task is to prevent iot devices from becoming a tool used by malicious attackers to break into other systems in this paper we present a conceptual design in which iot devices are used as tools in bruteforce attacks to break encryption keys of block ciphers the proposed design shows that with adequate number of iot devices employed in the attack the attack can succeed in breaking largekey block ciphers,0
the exponential growth of the internet of things iot has led to the emergence of substantial security concerns with iot networks becoming the primary target for cyberattacks this study examines the potential of kolmogorovarnold networks kans as an alternative to conventional machine learning models for intrusion detection in iot networks the study demonstrates that kans which employ learnable activation functions outperform traditional mlps and achieve competitive accuracy compared to stateoftheart models such as random forest and xgboost while offering superior interpretability for intrusion detection in iot networks,0
the use of machine learning has become a significant part of malware detection efforts due to the influx of new malware an ever changing threat landscape and the ability of machine learning methods to discover meaningful distinctions between malicious and benign software antivirus vendors have also begun to widely utilize malware classifiers based on dynamic and static malware analysis features therefore a malware author might make evasive binary modifications against machine learning models as part of the malware development life cycle to execute an attack successfully this makes the studying of possible classifier evasion strategies an essential part of cyber defense against malice to this extent we stage a grey box setup to analyze a scenario where the malware author does not know the target classifier algorithm and does not have access to decisions made by the classifier but knows the features used in training in this experiment a malicious actor trains a surrogate model using the ember2018 dataset to discover binary mutations that cause an instance to be misclassified via a monte carlo tree search then mutated malware is sent to the victim model that takes the place of an antivirus api to test whether it can evade detection,0
internet of things iot is becoming more frequently used in more applications as the number of connected devices is in a rapid increase more connected devices result in bigger challenges in terms of scalability maintainability and most importantly security especially when it comes to 5g networks the security aspect of iot devices is an infant field which is why it is our focus in this paper multiple iot device manufacturers do not consider securing the devices they produce for different reasons like cost reduction or to avoid using energyharvesting components such potentially malicious devices might be exploited by the adversary to do multiple harmful attacks therefore we developed a system that can recognize malicious behavior of a specific iot node on the network through convolutional neural network and monitoring we were able to provide malware detection for iot using a central node that can be installed within the network the achievement shows how such models can be generalized and applied easily to any network while clearing out any stigma regarding deep learning techniques,0
many iotinternet of things systems run android systems or androidlike systems with the continuous development of machine learning algorithms the learningbased android malware detection system for iot devices has gradually increased however these learningbased detection models are often vulnerable to adversarial samples an automated testing framework is needed to help these learningbased malware detection systems for iot devices perform security analysis the current methods of generating adversarial samples mostly require training parameters of models and most of the methods are aimed at image data to solve this problem we propose a textbftesting framework for textbflearningbased textbfandroid textbfmalware textbfdetection systemstlamd for iot devices the key challenge is how to construct a suitable fitness function to generate an effective adversarial sample without affecting the features of the application by introducing genetic algorithms and some technical improvements our test framework can generate adversarial samples for the iot android application with a success rate of nearly 100 and can perform blackbox testing on the system,0
a city is a large human settlement that serves the people who live there and a smart city is a concept of how cities might better serve their residents through new forms of technology in this paper we focus on four major smart city domains according to maslows hierarchy of needs smart utility smart transportation smart homes and smart healthcare numerous iot applications have been developed to achieve the intelligence that we desire in our smart domains ranging from personal gadgets such as health trackers and smart watches to largescale industrial iot systems such as nuclear and energy management systems however many of the existing smart city iot solutions can be made better by considering the suitability of their security strategies inappropriate system security designs generally occur in two scenarios first system designers recognize the importance of security but are unsure of where when or how to implement it and second system designers try to fit traditional security designs to meet the smart city security context thus the objective of this paper is to provide application designers with the missing security link they may need to improve their security designs by evaluating the specific context of each smart city domain and the contextspecific security requirements we aim to provide directions on when where and how they should implement security strategies and the possible security challenges they need to consider in addition we present a new perspective on security issues in smart cities from a datacentric viewpoint by referring to the reference architecture the activitynetworkthings antcentric architecture built upon the concept of security in a zerotrust environment by doing so we reduce the security risks posed by new system interactions or unanticipated user behaviors while avoiding the hassle of regularly upgrading security models,0
with the increase of iot devices and technologies coming into service malware has risen as a challenging threat with increased infection rates and levels of sophistication without strong security mechanisms a huge amount of sensitive data is exposed to vulnerabilities and therefore easily abused by cybercriminals to perform several illegal activities thus advanced network security mechanisms that are able of performing a realtime traffic analysis and mitigation of malicious traffic are required to address this challenge we are proposing a novel iot malware traffic analysis approach using deep learning and visual representation for faster detection and classification of new malware zeroday malware the detection of malicious network traffic in the proposed approach works at the package level significantly reducing the time of detection with promising results due to the deep learning technologies used to evaluate our proposed method performance a dataset is constructed which consists of 1000 pcap files of normal and malware traffic that are collected from different network traffic sources the experimental results of residual neural network resnet50 are very promising providing a 9450 accuracy rate for detection of malware traffic,0
malware intrusion is problematic for internet of things iot and artificial intelligence of things aiot devices as they often reside in an ecosystem of connected devices such as a smart home if any devices are infected the whole ecosystem can be compromised although various machine learning ml models are deployed to detect malware and network intrusion generally speaking robust highaccuracy models tend to require resources not found in all iot devices compared to less robust models defined by weak learners in order to combat this issue fadhilla proposed a metalearner ensemble model comprised of less robust prediction results inherent with weak learner ml models to produce a highly robust metalearning ensemble model the main problem with the prior research is that it cannot be deployed in lowend aiot devices due to the limited resources comprising processing power storage and memory the required libraries quickly exhaust lowend aiot devices resources hence this research aims to optimize the proposed super learner metalearning ensemble model to make it viable for lowend aiot devices we show the library and ml model memory requirements associated with each optimization stage and emphasize that optimization of current ml models is necessitated for lowend aiot devices our results demonstrate that we can obtain similar accuracy and false positive rate fpr metrics from highend aiot devices running the derived ml model with a lower inference duration and smaller memory footprint,0
industrial internet of things iiot systems have become integral to smart manufacturing yet their growing connectivity has also exposed them to significant cybersecurity threats traditional intrusion detection systems ids often rely on centralized architectures that raise concerns over data privacy latency and single points of failure in this work we propose a novel federated learningenhanced blockchain framework flbcid for privacypreserving intrusion detection tailored for iiot environments our architecture combines federated learning fl to ensure decentralized model training with blockchain technology to guarantee data integrity trust and tamper resistance across iiot nodes we design a lightweight intrusion detection model collaboratively trained using fl across edge devices without exposing sensitive data a smart contractenabled blockchain system records model updates and anomaly scores to establish accountability experimental evaluations using the toniot and nbaiot datasets demonstrate the superior performance of our framework achieving 973 accuracy while reducing communication overhead by 41 compared to baseline centralized methods our approach ensures privacy scalability and robustnesscritical for secure industrial operations the proposed flbcid system provides a promising solution for enhancing trust and privacy in modern iiot security architectures,0
the current amount of iot devices and their limitations has come to serve as a motivation for malicious entities to take advantage of such devices and use them for their own gain to protect against cyberattacks in iot devices machine learning techniques can be applied to intrusion detection systems moreover privacy related issues associated with centralized approaches can be mitigated through federated learning this work proposes a hostbased intrusion detection systems that leverages federated learning and multilayer perceptron neural networks to detected cyberattacks on iot devices with high accuracy and enhancing data privacy protection,0
the rapid progress in technology innovation usage and distribution has increased in the last decade the rapid growth of the internet of things iot systems worldwide has increased network security challenges created by malicious third parties thus reliable intrusion detection and network forensics systems that consider security concerns and iot systems limitations are essential to protect such systems iot botnet attacks are one of the significant threats to enterprises and individuals thus this paper proposed an economic deep learningbased model for detecting iot botnet attacks along with different types of attacks the proposed model achieved higher accuracy than the stateoftheart detection models using a smaller implementation budget and accelerating the training and detecting processes,0
the internet of things iot is a new computing paradigm that spans wearable devices homes hospitals cities transportation and critical infrastructure building security into this new computing paradigm is a major technical challenge today however what are the security problems in iot that we can solve using existing security principles and what are the new problems and challenges in this space that require new security mechanisms this article summarizes the intellectual similarities and differences between classic information technology security research and iot security research,0
malware is a type of malicious program that replicate from host machine and propagate through network it has been considered as one type of computer attack and intrusion that can do a variety of malicious activity on a computer this paper addresses the current trend of malware detection techniques and identifies the significant criteria in each technique to improve malware detection in intrusion detection system ids several existing techniques are analyzing from 48 various researches and the capability criteria of malware detection technique have been reviewed from the analysis a new generic taxonomy of malware detection technique have been proposed named hybrid malware detection technique hybrid mdt which consists of hybrid signature and anomaly detection technique and hybrid specification based and anomaly detection technique to complement the weaknesses of the existing malware detection technique in detecting known and unknown attack as well as reducing false alert before and during the intrusion occur,0
internet of things iot is the interconnection of heterogeneous smart devices through the internet with diverse application areas the huge number of smart devices and the complexity of networks has made it impossible to secure the data and communication between devices various conventional security controls are insufficient to prevent numerous attacks against these informationrich devices along with enhancing existing approaches a peripheral defence intrusion detection system ids proved efficient in most scenarios however conventional ids approaches are unsuitable to mitigate continuously emerging zeroday attacks intelligent mechanisms that can detect unfamiliar intrusions seems a prospective solution this article explores popular attacks against iot architecture and its relevant defence mechanisms to identify an appropriate protective measure for different networking practices and attack categories besides a security framework for iot architecture is provided with a list of security enhancement techniques,0
as internet of things devices become prevalent using intrusion detection to protect iot from malicious intrusions is of vital importance however the data scarcity of iot hinders the effectiveness of traditional intrusion detection methods to tackle this issue in this paper we propose the adaptive birecommendation and selfimproving network abrsi based on unsupervised heterogeneous domain adaptation hda the abrsi transfers enrich intrusion knowledge from a datarich network intrusion source domain to facilitate effective intrusion detection for datascarce iot target domains the abrsi achieves finegrained intrusion knowledge transfer via adaptive birecommendation matching matching the birecommendation interests of two recommender systems and the alignment of intrusion categories in the shared feature space form a mutualbenefit loop besides the abrsi uses a selfimproving mechanism autonomously improving the intrusion knowledge transfer from four ways a hard pseudo label voting mechanism jointly considers recommender system decision and label relationship information to promote more accurate hard pseudo label assignment to promote diversity and target data participation during intrusion knowledge transfer target instances failing to be assigned with a hard pseudo label will be assigned with a probabilistic soft pseudo label forming a hybrid pseudolabelling strategy meanwhile the abrsi also makes soft pseudolabels globally diverse and individually certain finally an error knowledge learning mechanism is utilised to adversarially exploit factors that causes detection ambiguity and learns through both current and previous error knowledge preventing error knowledge forgetfulness holistically these mechanisms form the abrsi model that boosts iot intrusion detection accuracy via hdaassisted intrusion knowledge transfer,0
internet of things iot that integrate a variety of devices into networks to provide advanced and intelligent services have to protect user privacy and address attacks such as spoofing attacks denial of service attacks jamming and eavesdropping in this article we investigate the attack model for iot systems and review the iot security solutions based on machine learning techniques including supervised learning unsupervised learning and reinforcement learning we focus on the machine learning based iot authentication access control secure offloading and malware detection schemes to protect data privacy in this article we discuss the challenges that need to be addressed to implement these machine learning based security schemes in practical iot systems,0
the widespread adoption of the internet of things iot has raised a new challenge for developers since it is prone to known and unknown cyberattacks due to its heterogeneity flexibility and close connectivity to defend against such security breaches researchers have focused on building sophisticated intrusion detection systems idss using machine learning ml techniques although these algorithms notably improve detection performance they require excessive computing power and resources which are crucial issues in iot networks considering the recent trends of decentralized data processing and computing systems consequently many optimization techniques have been incorporated with these ml models specifically a special category of optimizer adopted from the behavior of living creatures and different aspects of natural phenomena known as metaheuristic algorithms has been a central focus in recent years and brought about remarkable results considering this vital significance we present a comprehensive and systematic review of various applications of metaheuristics algorithms in developing a machine learningbased ids especially for iot a significant contribution of this study is the discovery of hidden correlations between these optimization techniques and machine learning models integrated with stateoftheart iotidss in addition the effectiveness of these metaheuristic algorithms in different applications such as feature selection parameter or hyperparameter tuning and hybrid usages are separately analyzed moreover a taxonomy of existing iotidss is proposed furthermore we investigate several critical issues related to such integration our extensive exploration ends with a discussion of promising optimization algorithms and technologies that can enhance the efficiency of iotidss,0
the rapid growth of the internet of things iot has revolutionized industries enabling unprecedented connectivity and functionality however this expansion also increases vulnerabilities exposing iot networks to increasingly sophisticated cyberattacks intrusion detection systems ids are crucial for mitigating these threats and recent advancements in machine learning ml offer promising avenues for improvement this research explores a hybrid approach combining several standalone ml models such as random forest rf xgboost knearest neighbors knn and adaboost in a votingbased hybrid classifier for effective iot intrusion detection this ensemble method leverages the strengths of individual algorithms to enhance accuracy and address challenges related to data complexity and scalability using the widelycited iot23 dataset a prominent benchmark in iot cybersecurity research we evaluate our hybrid classifiers for both binary and multiclass intrusion detection problems ensuring a fair comparison with existing literature results demonstrate that our proposed hybrid models designed for robustness and scalability outperform standalone approaches in iot environments this work contributes to the development of advanced intelligent ids frameworks capable of addressing evolving cyber threats,0
the number and variety of internetconnected devices have grown enormously in the past few years presenting new challenges to security and privacy research has shown that network adversaries can use traffic rate metadata from consumer iot devices to infer sensitive user activities shaping traffic flows to fit distributions independent of user activities can protect privacy but this approach has seen little adoption due to required developer effort and overhead bandwidth costs here we present a python library for iot developers to easily integrate privacypreserving traffic shaping into their products the library replaces standard networking functions with versions that automatically obfuscate device traffic patterns through a combination of payload padding fragmentation and randomized cover traffic our library successfully preserves user privacy and requires approximately 4 kbs overhead bandwidth for iot devices with low send rates or high latency tolerances this overhead is reasonable given normal internet speeds in american homes and is an improvement on the bandwidth requirements of existing solutions,0
in this paper we present an endtoend view of iot security and privacy and a case study our contribution is threefold first we present our endtoend view of an iot system and this view can guide risk assessment and design of an iot system we identify 10 basic iot functionalities that are related to security and privacy based on this view we systematically present security and privacy requirements in terms of iot system software networking and big data analytics in the cloud second using the endtoend view of iot security and privacy we present a vulnerability analysis of the edimax ip camera system we are the first to exploit this system and have identified various attacks that can fully control all the cameras from the manufacturer our realworld experiments demonstrate the effectiveness of the discovered attacks and raise the alarms again for the iot manufacturers third such vulnerabilities found in the exploit of edimax cameras and our previous exploit of edimax smartplugs can lead to another wave of mirai attacks which can be either botnets or worm attacks to systematically understand the damage of the mirai malware we model propagation of the mirai and use the simulations to validate the modeling the work in this paper raises the alarm again for the iot device manufacturers to better secure their products in order to prevent malware attacks like mirai,0
the goal of this chapter is to illuminate the operational frameworks key actors and significant cybersecurity implications of the malware as a service maas ecosystem highlighting the transformation of malware proliferation into a serviceoriented model the chapter discusses how maas democratises access to sophisticated cyberattack capabilities enabling even those with minimal technical knowledge to execute catastrophic cyberattacks the discussion extends to the roles within the maas ecosystem including malware developers affiliates initial access brokers and the essential infrastructure providers that support these nefarious activities the study emphasises the profound challenges maas poses to traditional cybersecurity defences rendered ineffective against the constantly evolving and highly adaptable threats generated by maas platforms with the increase in malware sophistication there is a parallel call for a paradigm shift in defensive strategies advocating for dynamic analysis behavioural detection and the integration of ai and machine learning techniques by exploring the intricacies of the maas ecosystem including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals the ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the maas model,0
internet of things iot is transforming human lives by paving the way for the management of physical devices on the edge these interconnected iot objects share data for remote accessibility and can be vulnerable to open attacks and illegal access intrusion detection methods are commonly used for the detection of such kinds of attacks but with these methods the performanceaccuracy is not optimal this work introduces a novel intrusion detection approach based on an ensemblebased voting classifier that combines multiple traditional classifiers as a base learner and gives the vote to the predictions of the traditional classifier in order to get the final prediction to test the effectiveness of the proposed approach experiments are performed on a set of seven different iot devices and tested for binary attack classification and multiclass attack classification the results illustrate prominent accuracies on global positioning system gps sensors and weather sensors to 96 and 97 and for other machine learning algorithms to 85 and 87 respectively furthermore comparison with other traditional machine learning methods validates the superiority of the proposed algorithm,0
software updates are critical for ensuring systems remain free of bugs and vulnerabilities while they are in service while many internet of things iot devices are capable of outlasting desktops and mobile phones their software update practices are not yet well understood despite a large body of research aiming to create new methodologies for keeping iot devices up to date this paper discusses efforts towards characterizing the iot software update landscape through networklevel analysis of iot device traffic our results suggest that vendors do not currently follow security best practices and that software update standards while available are not being deployed,0
internetofthings iot devices are nowadays massively integrated in daily life homes factories or public places this technology offers attractive services to improve the quality of life as well as new economic markets through the exploitation of the collected data however these connected objects have also become attractive targets for attackers because their current security design is often weak or flawed as illustrated by several vulnerabilities such as mirai blueborne etc this paper presents a novel approach for detecting intrusions in smart spaces such as smarthomes or smartfactories that is based on the monitoring and profiling of radio communications at the physical layer using machine learning techniques the approach is designed to be independent of the large and heterogeneous set of wireless communication protocols typically implemented by connected objects such as wifi bluetooth zigbee bluetoothlowenergy ble or proprietary communication protocols the main concepts of the proposed approach are presented together with an experimental case study illustrating its feasibility based on data collected during the deployment of the intrusion detection approach in a smart home under reallife conditions,0
the internet of things iot has been introduced as a breakthrough technology that integrates intelligence into everyday objects enabling high levels of connectivity between them as the iot networks grow and expand they become more susceptible to cybersecurity attacks a significant challenge in current intrusion detection systems for iot includes handling imbalanced datasets where labeled data are scarce particularly for new and rare types of cyber attacks existing literature often fails to detect such underrepresented attack classes this paper introduces a novel intrusion detection approach designed to address these challenges by integrating self supervised learning ssl few shot learning fsl and random forest rf our approach excels in learning from limited and imbalanced data and enhancing detection capabilities the approach starts with a deep infomax model trained to extract key features from the dataset these features are then fed into a prototypical network to generate discriminate embedding subsequently an rf classifier is employed to detect and classify potential malware including a range of attacks that are frequently observed in iot networks the proposed approach was evaluated through two different datasets malevis and wsnds which demonstrate its superior performance with accuracies of 9860 and 9956 precisions of 9879 and 9956 recalls of 9860 and 9956 and f1scores of 9863 and 9956 respectively,0
in this paper we propose a framework called contegotee to secure internetofthings iot edge devices with timing requirements from control spoofing attacks where an adversary sends malicious control signals to the actuators we use a trusted computing base available in commodity processors such as arm trustzone and propose an invariant checking mechanism to ensure the security and safety of the physical system a working prototype of contegotee was developed using embedded linux kernel we demonstrate the feasibility of our approach for a robotic vehicle running on an armbased platform,0
network intrusion detection systems and antivirus software are essential in detecting malicious network traffic and attacks such as denialofservice and malwares each attack worm or virus has its own distinctive signature signaturebased intrusion detection and antivirus systems depend on pattern matching to look for possible attack signatures pattern matching is a very complex task which requires a lot of time memory and computing resources softwarebased intrusion detection is not fast enough to match high network speeds and the increasing number of attacks in this paper we propose special purpose hardware for wumanber pattern matching algorithm fpgas form an excellent choice because of their massively parallel structure reprogrammable logic and memory resources the hardware is designed in verilog and implemented using xilinx ise for evaluation we dope network traffic traces collected using wireshark with 2500 signatures from the clamav virus definitions database experimental results show high speed that reaches up to 216 mbps in addition we evaluate time device usage and power consumption,0
internet of things iot networks have become an increasingly attractive target of cyberattacks powerful machine learning ml models have recently been adopted to implement network intrusion detection systems to protect iot networks for the successful training of such ml models selecting the right data features is crucial maximising the detection accuracy and computational efficiency this paper comprehensively analyses feature sets importance and predictive power for detecting network attacks three feature selection algorithms chisquare information gain and correlation have been utilised to identify and rank data features the attributes are fed into two ml classifiers deep feedforward and random forest to measure their attack detection performance the experimental evaluation considered three datasets unswnb15 csecicids2018 and toniot in their proprietary flow format in addition the respective variants in netflow format were also considered ie nfunswnb15 nfcsecicids2018 and nftoniot the experimental evaluation explored the marginal benefit of adding individual features our results show that the accuracy initially increases rapidly with adding features but converges quickly to the maximum this demonstrates a significant potential to reduce the computational and storage cost of intrusion detection systems while maintaining nearoptimal detection accuracy this has particular relevance in iot systems with typically limited computational and storage resources,0
recent innovations in the design of computer viruses have led to new tradeoffs for the attacker multiple variants of a malware may spread at different rates and have different levels of visibility to the network in this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth we show that in the meanfield deterministic regime this spreadstealth tradeoff is optimized by computationally simple singlethreshold policies specifically we show that only one variant of the malware is spread by the attacker at each time as there exists a time up to which the attacker prioritizes maximizing the spread of the malware and after which she prioritizes stealth,0
antiphishing aims to detect phishing contentdocuments in a pool of textual data this is an important problem in cybersecurity that can help to guard users from fraudulent information natural language processing nlp offers a natural solution for this problem as it is capable of analyzing the textual content to perform intelligent recognition in this work we investigate stateoftheart techniques for text categorization in nlp to address the problem of antiphishing for emails ie predicting if an email is phishing or not these techniques are based on deep learning models that have attracted much attention from the community recently in particular we present a framework with hierarchical long shortterm memory networks hlstms and attention mechanisms to model the emails simultaneously at the word and the sentence level our expectation is to produce an effective model for antiphishing and demonstrate the effectiveness of deep learning for problems in cybersecurity,0
the authentication and authorization for constrained environments ace framework provides finegrained access control in the internet of things where devices are resourceconstrained and with limited connectivity the ace framework defines separate profiles to specify how exactly entities interact and what security and communication protocols to use this paper presents the novel ace ipsec profile which specifies how a client establishes a secure ipsec channel with a resource server contextually using the ace framework to enforce authorized access to remote resources the profile makes it possible to establish ipsec security associations either through their direct provisioning or through the standard ikev2 protocol we provide the first open source implementation of the ace ipsec profile for the contiki os and test it on the resourceconstrained zolertia firefly platform our experimental performance evaluation confirms that the ipsec profile and its operating modes are affordable and deployable also on constrained iot platforms,0
the rapid integration of internet of things iot devices into enterprise environments presents significant security challenges many iot devices are released to the market with minimal security measures often harbouring an average of 25 vulnerabilities per device to enhance cybersecurity measures and aid system administrators in managing iot patches more effectively we propose an innovative framework that predicts the time it will take for a vulnerable iot device to receive a fix or patch we developed a survival analysis model based on the accelerated failure time aft approach implemented using the xgboost ensemble regression model to predict when vulnerable iot devices will receive fixes or patches by constructing a comprehensive iot vulnerabilities database that combines public and private sources we provide insights into affected devices vulnerability detection dates published cves patch release dates and associated twitter activity trends we conducted thorough experiments evaluating different combinations of features including fundamental device and vulnerability data national vulnerability database nvd information such as cve cwe and cvss scores transformed textual descriptions into sentence vectors and the frequency of twitter trends related to cves our experiments demonstrate that the proposed model accurately predicts the time to fix for iot vulnerabilities with data from vuldb and nvd proving particularly effective incorporating twitter trend data offered minimal additional benefit this framework provides a practical tool for organisations to anticipate vulnerability resolutions improve iot patch management and strengthen their cybersecurity posture against potential threats,0
currently the iot ecosystem is comprised of fully connected smart devices that exchange data to provide more automated precise and fast decisions this idealised situation can only be accomplished if a system for data transactions is processed efficiently and security is ensured with high scalability and practicability the integrity of data must be maintained during the exchange or transfer of data between entities we propose to make a application called datchain that responds to the above situation the application stores data sensed by the iot sensors in the backend after encrypting it and when the data is required for any purpose it can be exchanged using a suitable blockchain network that can keep up with the transfer rate even at high traffic in a secure environment,0
one of the biggest problems with the internet technology is the unwanted spam emails the well disguised phishing email comes in as part of the spam and makes its entry into the inbox quite frequently nowadays while phishing is normally considered a consumer issue the fraudulent tactics the phishers use are now intimidating the corporate sector as well in this paper we analyze the various aspects of phishing attacks and draw on some possible defenses as countermeasures we initially address the different forms of phishing attacks in theory and then look at some examples of attacks in practice along with their common defenses we also highlight some recent statistical data on phishing scam to project the seriousness of the problem finally some specific phishing countermeasures at both the user level and the organization level are listed and a multilayered antiphishing proposal is presented to round up our studies,0
data scarcity hinders the usability of datadependent algorithms when tackling iot intrusion detection iid to address this we utilise the data rich network intrusion detection nid domain to facilitate more accurate intrusion detection for iid domains in this paper a geometric graph alignment gga approach is leveraged to mask the geometric heterogeneities between domains for better intrusion knowledge transfer specifically each intrusion domain is formulated as a graph where vertices and edges represent intrusion categories and categorywise interrelationships respectively the overall shape is preserved via a confused discriminator incapable to identify adjacency matrices between different intrusion domain graphs a rotation avoidance mechanism and a centre point matching mechanism is used to avoid graph misalignment due to rotation and symmetry respectively besides categorywise semantic knowledge is transferred to act as vertexlevel alignment to exploit the target data a pseudolabel election mechanism that jointly considers network prediction geometric property and neighbourhood information is used to produce finegrained pseudolabel assignment upon aligning the intrusion graphs geometrically from different granularities the transferred intrusion knowledge can boost iid performance comprehensive experiments on several intrusion datasets demonstrate stateoftheart performance of the gga approach and validate the usefulness of gga constituting components,0
the rise of new complex attacks scenarios in internet of things iot environments necessitate more advanced and intelligent cyber defense techniques such as various intrusion detection systems idss which are responsible for detecting and mitigating malicious activities in iot networks without human intervention to address this issue deep reinforcement learning drl has been proposed in recent years to automatically tackle intrusionsattacks in this paper a comprehensive survey of drlbased ids on iot is presented furthermore in this survey the stateoftheart drlbased ids methods have been classified into five categories including wireless sensor network wsn deep qnetwork dqn healthcare hybrid and other techniques in addition the most crucial performance metrics namely accuracy recall precision false negative rate fnr false positive rate fpr and fmeasure are detailed in order to evaluate the performance of each proposed method the paper provides a summary of datasets utilized in the studies as well,0
with the proliferation of the internet and smart devices iot technology has seen significant advancements and has become an integral component of smart homes urban security smart logistics and other sectors iot facilitates realtime monitoring of critical production indicators enabling businesses to detect potential quality issues anticipate equipment malfunctions and refine processes thereby minimizing losses and reducing costs furthermore iot enhances realtime asset tracking optimizing asset utilization and management however the expansion of iot has also led to a rise in cybercrimes with devices increasingly serving as vectors for malicious attacks as the number of iot devices grows there is an urgent need for robust network security measures to counter these escalating threats this paper introduces a deep learning model incorporating lstm and attention mechanisms a pivotal strategy in combating cybercrime in iot networks our experiments conducted on datasets including iot23 botiot iot network intrusion mqtt and mqttset demonstrate that our proposed method outperforms existing baselines,0
this paper presents a new network intrusion detection system nids based on graph neural networks gnns gnns are a relatively new subfield of deep neural networks which can leverage the inherent structure of graphbased data training and evaluation data for nidss are typically represented as flow records which can naturally be represented in a graph format in this paper we propose egraphsage a gnn approach that allows capturing both the edge features of a graph as well as the topological information for network intrusion detection in iot networks to the best of our knowledge our proposal is the first successful practical and extensively evaluated approach of applying gnns on the problem of network intrusion detection for iot using flowbased data our extensive experimental evaluation on four recent nids benchmark datasets shows that our approach outperforms the stateoftheart in terms of key classification metrics which demonstrates the potential of gnns in network intrusion detection and provides motivation for further research,0
this chapter draws from across the foregoing chapters discussing many core hdi approaches and disciplinary perspectives to consider the specific application of hdi in home network security while much work has considered the challenges of securing in home iot devices and their communications especially for those with limited power or computational capacity scant attention has been paid by the research community to home network security and its acceptability and usability from the viewpoint of ordinary citizens it will be clear that we need a radical transformation in our approach to designing domestic networking infrastructure to guard against widespread cyberattacks that threaten to counter the benefits of the iot our aim has to be to defend against enemies inside the walls to protect critical functionality in the home against rogue devices and prevent the proliferation of disruptive widescale iot ddos attacks that are already occurring,0
current research on users perspectives of cyber security and privacy related to traditional and smart devices at home is very active but the focus is often more on specific modern devices such as mobile and smart iot devices in a home context in addition most were based on smallerscale empirical studies such as online surveys and interviews we endeavour to fill these research gaps by conducting a largerscale study based on a realworld dataset of 413985 tweets posted by nonexpert users on twitter in six months of three consecutive years january and february in 2019 2020 and 2021 two machine learningbased classifiers were developed to identify the 413985 tweets we analysed this dataset to understand nonexpert users cyber security and privacy perspectives including the yearly trend and the impact of the covid19 pandemic we applied topic modelling sentiment analysis and qualitative analysis of selected tweets in the dataset leading to various interesting findings for instance we observed a 54 increase in nonexpert users tweets on cyber security andor privacy related topics in 2021 compared to before the start of global covid19 lockdowns january 2019 to february 2020 we also observed an increased level of helpseeking tweets during the covid19 pandemic our analysis revealed a diverse range of topics discussed by nonexpert users across the three years including vpns wifi smartphones laptops smart home devices financial security and security and privacy issues involving different stakeholders overall negative sentiment was observed across almost all topics nonexpert users discussed on twitter in all the three years our results confirm the multifaceted nature of nonexpert users perspectives on cyber security and privacy and call for more holistic comprehensive and nuanced research on different facets of such perspectives,0
the factories of the future require efficient interconnection of their physical machines into the cyber space to cope with the emerging need of an increased uptime of machines higher performance rates an improved level of productivity and a collective collaboration along the supply chain with the rapid growth of the internet of things iot and its application in industrial areas the so called industrial internet of things iiotindustry 40 emerged however further to the rapid growth of iotiiot systems cyber attacks are an emerging threat and simple manual security testing can often not cope with the scale of large iotiiot networks in this paper we suggest to extract metadata from commonly used diagrams and models in a typical software development process to automate the process of threat modelling security analysis and penetration testing without detailed prior security knowledge in that context we present requirements and recommendations for metadata in iotiiot models that are needed as necessary input parameters of security assurance tools,0
the internet of things iot brings connectivity to about every objects found in the physical space it extends connectivity to everyday objects from connected fridges cars and cities the iot creates opportunities in numerous domains however this increase in connectivity creates many prominent challenges this paper provides a survey of some of the major issues challenging the widespread adoption of the iot particularly it focuses on the interoperability management security and privacy issues in the iot it is concluded that there is a need to develop a multifaceted technology approach to iot security management and privacy,0
internetofthings iot is perpetually revolutionizing our daily life and rapidly transforming physical objects into an ubiquitous connected ecosystem due to their massive deployment and moderate security levels those devices face a lot of security management and control challenges their classical centralized architecture is still cloaking vulnerabilities and anomalies that can be exploited by hackers for spying eavesdropping and taking control of the network in this paper we propose to improve the iot architecture with additional security features using artificial intelligence ai and blockchain technology we propose a novel architecture based on permissioned blockchain technology in order to build a scalable and decentralized endtoend secure iot system furthermore we enhance the iot system security with an aicomponent at the gateway level to detect and classify suspected activities malware and cyberattacks using machine learning techniques simulations and practical implementation show that the proposed architecture delivers high performance against cyberattacks,0
iot application domains device diversity and connectivity are rapidly growing iot devices control various functions in smart homes and buildings smart cities and smart factories making these devices an attractive target for attackers on the other hand the large variability of different application scenarios and inherent heterogeneity of devices make it very challenging to reliably detect abnormal iot device behaviors and distinguish these from benign behaviors existing approaches for detecting attacks are mostly limited to attacks directly compromising individual iot devices or require predefined detection policies they cannot detect attacks that utilize the control plane of the iot system to trigger actions in an unintendedmalicious context eg opening a smart lock while the smart home residents are absent in this paper we tackle this problem and propose argus the first selflearning intrusion detection system for detecting contextual attacks on iot environments in which the attacker maliciously invokes iot device actions to reach its goals argus monitors the contextual setting based on the state and actions of iot devices in the environment an unsupervised deep neural network dnn is used for modeling the typical contextual device behavior and detecting actions taking place in abnormal contextual settings this unsupervised approach ensures that argus is not restricted to detecting previously known attacks but is also able to detect new attacks we evaluated argus on heterogeneous realworld smarthome settings and achieve at least an f1score of 9964 for each setup with a false positive rate fpr of at most 003,0
the application of machine learning ml algorithms are massively scalingup due to rapid digitization and emergence of new tecnologies like internet of things iot in todays digital era we can find ml algorithms being applied in the areas of healthcare iot engineering finance and so on however all these algorithms need to be trained in order to predictsolve a particular problem there is high possibility of tampering the training datasets and produce biased results hence in this article we have proposed blockchain based solution to secure the datasets generated from iot devices for ehealth applications the proposed blockchain based solution uses using private cloud to tackle the aforementioned issue for evaluation we have developed a system that can be used by dataset owners to secure their data,0
as the internet continues to be populated with new devices and emerging technologies the attack surface grows exponentially technology is shifting towards a profitdriven internet of things market where security is an afterthought traditional defending approaches are no longer sufficient to detect both known and unknown attacks to high accuracy machine learning intrusion detection systems have proven their success in identifying unknown attacks with high precision nevertheless machine learning models are also vulnerable to attacks adversarial examples can be used to evaluate the robustness of a designed model before it is deployed further using adversarial examples is critical to creating a robust model designed for an adversarial environment our work evaluates both traditional machine learning and deep learning models robustness using the botiot dataset our methodology included two main approaches first label poisoning used to cause incorrect classification by the model second the fast gradient sign method used to evade detection measures the experiments demonstrated that an attacker could manipulate or circumvent detection with significant probability,0
malware a persistent cybersecurity threat increasingly targets interconnected digital systems such as desktop mobile and iot platforms through sophisticated attack vectors by exploiting these vulnerabilities attackers compromise the integrity and resilience of modern digital ecosystems to address this risk security experts actively employ machine learning or deep learningbased strategies integrating static dynamic or hybrid approaches to categorize malware instances despite their advantages these methods have inherent drawbacks and malware variants persistently evolve with increased sophistication necessitating advancements in detection strategies visualizationbased techniques are emerging as scalable and interpretable solutions for detecting and understanding malicious behaviors across diverse platforms including desktop mobile iot and distributed systems as well as through analysis of network packet capture files in this comprehensive survey of more than 100 highquality research articles we evaluate existing visualizationbased approaches applied to malware detection and classification as a first contribution we propose a new allencompassing framework to study the landscape of visualizationbased malware detection techniques within this framework we systematically analyze stateoftheart approaches across the critical stages of the malware detection pipeline by analyzing not only the single techniques but also how they are combined to produce the final solution we shed light on the main challenges in visualizationbased approaches and provide insights into the advancements and potential future directions in this critical field,0
the proliferation of iot devices has significantly increased network vulnerabilities creating an urgent need for effective intrusion detection systems ids machine learningbased ids mlids offer advanced detection capabilities but rely on labeled attack data which limits their ability to identify unknown threats selfsupervised learning ssl presents a promising solution by using only normal data to detect patterns and anomalies this paper introduces safe a novel framework that transforms tabular network intrusion data into an imagelike format enabling masked autoencoders maes to learn robust representations of network behavior the features extracted by the maes are then incorporated into a lightweight novelty detector enhancing the effectiveness of anomaly detection experimental results demonstrate that safe outperforms the stateoftheart anomaly detection method scale learningbased deep anomaly detection method slad by up to 262 and surpasses the stateoftheart sslbased network intrusion detection approach anomale by up to 235 in f1score,0
in this paper we propose a joint semantic transfer network jstn towards effective intrusion detection for largescale scarcely labelled iot domain as a multisource heterogeneous domain adaptation mshda method the jstn integrates a knowledge rich network intrusion ni domain and another smallscale iot intrusion ii domain as source domains and preserves intrinsic semantic properties to assist target ii domain intrusion detection the jstn jointly transfers the following three semantics to learn a domaininvariant and discriminative feature representation the scenario semantic endows source ni and ii domain with characteristics from each other to ease the knowledge transfer process via a confused domain discriminator and categorical distribution knowledge preservation it also reduces the sourcetarget discrepancy to make the shared feature space domaininvariant meanwhile the weighted implicit semantic transfer boosts discriminability via a finegrained knowledge preservation which transfers the source categorical distribution to the target domain the sourcetarget divergence guides the importance weighting during knowledge preservation to reflect the degree of knowledge learning additionally the hierarchical explicit semantic alignment performs centroidlevel and representativelevel alignment with the help of a geometric similarityaware pseudolabel refiner which exploits the value of unlabelled target ii domain and explicitly aligns feature representations from a global and local perspective in a concentrated manner comprehensive experiments on various tasks verify the superiority of the jstn against stateoftheart comparing methods on average a 103 of accuracy boost is achieved the statistical soundness of each constituting component and the computational efficiency are also verified,0
graph neural network gnnbased network intrusion detection systems nids are often evaluated on single datasets limiting their ability to generalize under distribution drift furthermore their adversarial robustness is typically assessed using synthetic perturbations that lack realism this measurement gap leads to an overestimation of gnnbased nids resilience to address the limitations we propose textbfrealiot a comprehensive framework for robustness evaluation of gnnbased nids in iot environments our framework presents a methodology that creates a unified dataset from canonical datasets to assess generalization under drift in addition it features a novel intrusion dataset collected from a physical iot testbed which captures network traffic and attack scenarios under realworld settings furthermore using realiot we explore the usage of large language models llms to analyze network data and mitigate the impact of adversarial examples by filtering suspicious flows our evaluations using realiot reveal performance drops in gnn models compared to results from standard benchmarks quantifying their susceptibility to drift and realistic attacks we also demonstrate the potential of llmbased filtering to enhance robustness these findings emphasize the necessity of realistic threat modeling and rigorous measurement practices for developing resilient iot intrusion detection systems,0
journalists have long been the targets of both physical and cyberattacks from wellresourced adversaries internet of things iot devices are arguably a new avenue of threat towards journalists through both targeted and generalised cyberphysical exploitation this study comprises three parts first we interviewed 11 journalists and surveyed 5 further journalists to determine the extent to which journalists perceive threats through the iot particularly via consumer iot devices second we surveyed 34 cyber security experts to establish if and how laypeople can combat iot threats third we compared these findings to assess journalists knowledge of threats and whether their protective mechanisms would be effective against experts depictions and predictions of iot threats our results indicate that journalists generally are unaware of iotrelated risks and are not adequately protecting themselves this considers cases where they possess iot devices or where they enter iotenabled environments eg at work or home expert recommendations spanned both immediate and longterm mitigation methods including practical actions that are technical and sociopolitical in nature however all proposed individual mitigation methods are likely to be shortterm solutions with 26 of 34 765 of cyber security experts responding that within the next five years it will not be possible for the public to optout of interaction with the iot,0
as the internet of things iot emerges over the next decade developing secure communication for iot devices is of paramount importance achieving endtoend encryption for largescale iot systems like smart buildings or smart cities is challenging because multiple principals typically interact indirectly via intermediaries meaning that the recipient of a message is not known in advance this paper proposes jedi joining encryption and delegation for iot a manytomany endtoend encryption protocol for iot jedi encrypts and signs messages endtoend while conforming to the decoupled communication model typical of iot systems jedis keys support expiry and finegrained access to data common in iot furthermore jedi allows principals to delegate their keys restricted in expiry or scope to other principals thereby granting access to data and managing access control in a scalable distributed way through careful protocol design and implementation jedi can run across the spectrum of iot devices including ultra lowpower deeply embedded sensors severely constrained in cpu memory and energy consumption we apply jedi to an existing iot messaging system and demonstrate that its overhead is modest,0
embedded devices are specialised devices designed for one or only a few purposes they are often part of a larger system through wired or wireless connection those embedded devices that are connected to other computers or embedded systems through the internet are called internet of things iot for short devices with their widespread usage and their insufficient protection these devices are increasingly becoming the target of malware attacks companies often cut corners to save manufacturing costs or misconfigure when producing these devices this can be lack of software updates ports left open or security defects by design although these devices may not be as powerful as a regular computer their large number makes them suitable candidates for botnets other types of iot devices can even cause health problems since there are even pacemakers connected to the internet this means that without sufficient defence even directed assaults are possible against people the goal of this thesis project is to provide better security for these devices with the help of machine learning algorithms and reverse engineering tools specifically i study the applicability of controlflow related data of executables for malware detection i present a malware detection method with two phases the first phase extracts controlflow related data using static binary analysis the second phase classifies binary executables as either malicious or benign using a neural network model i train the model using a dataset of malicious and benign arm applications,0
we consider the future cyber security of industrial control systems as best as we can see much of this future unfolds in the context of the internet of things iot in fact we envision that all industrial and infrastructure environments and cyberphysical systems in general will take the form reminiscent of what today is referred to as the iot iot is envisioned as multitude of heterogeneous devices densely interconnected and communicating with the objective of accomplishing a diverse range of objectives often collaboratively one can argue that in the relatively near future the iot construct will subsume industrial plants infrastructures housing and other systems that today are controlled by ics and scada systems in the iot environments cybersecurity will derive largely from system agility movingtarget defenses cybermaneuvering and other autonomous or semiautonomous behaviors cyber security of iot may also benefit from new design methods for mixedtrusted systems and from big data analytics predictive and autonomous,0
to date a large number of research papers have been written on the classification of malware its identification classification into different families and the distinction between malware and goodware these works have been based on captured malware samples and have attempted to analyse malware and goodware using various techniques including techniques from the field of artificial intelligence for example neural networks have played a significant role in these classification methods some of this work also deals with analysing malware using its visualisation these works usually convert malware samples capturing the structure of malware into image structures which are then the object of image processing in this paper we propose a very unconventional and novel approach to malware visualisation based on dynamic behaviour analysis with the idea that the images which are visually very interesting are then used to classify malware concerning goodware our approach opens an extensive topic for future discussion and provides many new directions for research in malware analysis and classification as discussed in conclusion the results of the presented experiments are based on a database of 6 589 997 goodware 827 853 potentially unwanted applications and 4 174 203 malware samples provided by eset and selected experimental data images generating polynomial formulas and software generating images are available on github for interested readers thus this paper is not a comprehensive compact study that reports the results obtained from comparative experiments but rather attempts to show a new direction in the field of visualisation with possible applications in malware analysis,0
with the rapid growth of iot devices ensuring robust network security has become a critical challenge traditional intrusion detection systems idss often face limitations in detecting sophisticated attacks within highdimensional and complex data environments this paper presents a novel approach to network anomaly detection using hyperdimensional computing hdc techniques specifically applied to the nslkdd dataset the proposed method leverages the efficiency of hdc in processing largescale data to identify both known and unknown attack patterns the model achieved an accuracy of 9155 on the kddtrain subset outperforming traditional approaches these comparative evaluations underscore the models superior performance highlighting its potential in advancing anomaly detection for iot networks and contributing to more secure and intelligent cybersecurity solutions,0
in the most intrusion detection systems ids a system tries to learn characteristics of different type of attacks by analyzing packets that sent or received in network these packets have a lot of features but not all of them is required to be analyzed to detect that specific type of attack detection speed and computational cost is another vital matter here because in these types of problems datasets are very huge regularly in this paper we tried to propose a very simple and fast feature selection method to eliminate features with no helpful information on them result faster learning in process of redundant feature omission we compared our proposed method with three most successful similarity based feature selection algorithm including correlation coefficient least square regression error and maximal information compression index after that we used recommended features by each of these algorithms in two popular classifiers including bayes and knn classifier to measure the quality of the recommendations experimental result shows that although the proposed method cant outperform evaluated algorithms with high differences in accuracy but in computational cost it has huge superiority over them,0
the growing adoption of the internet of things iot has brought a significant increase in attacks targeting those devices machine learning ml methods have shown promising results for intrusion detection however the scarcity of iot datasets remains a limiting factor in developing mlbased security systems for iot scenarios static datasets get outdated due to evolving iot architectures and threat landscape meanwhile the testbeds used to generate them are rarely published this paper presents the gotham testbed a reproducible and flexible security testbed extendable to accommodate new emulated devices services or attackers gotham is used to build an iot scenario composed of 100 emulated devices communicating via mqtt coap and rtsp protocols among others in a topology composed of 30 switches and 10 routers the scenario presents three threat actors including the entire mirai botnet lifecycle and additional redteaming tools performing dos scanning and attacks targeting iot protocols the testbed has many purposes including a cyber range testing security solutions and capturing network and application data to generate datasets we hope that researchers can leverage and adapt gotham to include other devices stateoftheart attacks and topologies to share scenarios and datasets that reflect the current iot settings and threat landscape,0
the prevalence of iot devices makes them an ideal target for attackers to reduce the risk of attacks vendors routinely deliver security updates patches for their devices the delivery of security updates becomes challenging due to the issue of scalability as the number of devices may grow much quicker than vendors distribution systems previous studies have suggested a permissionless and decentralized blockchainbased network in which nodes can host and deliver security updates thus the addition of new nodes scales out the network however these studies do not provide an incentive for nodes to join the network making it unlikely for nodes to freely contribute their hosting space bandwidth and computation resources in this paper we propose a novel decentralized iot software update delivery network in which participating nodes referred to as distributors are compensated by vendors with digital currency for delivering updates to devices upon the release of a new security update a vendor will make a commitment to provide digital currency to distributors that deliver the update the commitment will be made with the use of smart contracts and hence will be public binding and irreversible the smart contract promises compensation to any distributor that provides proofofdistribution which is unforgeable proof that a single update was delivered to a single device a distributor acquires the proofofdistribution by exchanging a security update for a device signature using the zeroknowledge contingent payment zkcp trustless data exchange protocol eliminating the need for trust between the security update distributor and the security consumer iot device by providing fair compensation can significantly increase the number of distributors thus facilitating rapid scale out,0
the rapid growth of the internet of things iot has expanded opportunities for innovation but also increased exposure to botnetdriven cyberattacks conventional detection methods often struggle with scalability privacy and adaptability in resourceconstrained iot environments to address these challenges we present a lightweight and privacypreserving botnet detection framework based on federated learning this approach enables distributed devices to collaboratively train models without exchanging raw data thus maintaining user privacy while preserving detection accuracy a communicationefficient aggregation strategy is introduced to reduce overhead ensuring suitability for constrained iot networks experiments on benchmark iot botnet datasets demonstrate that the framework achieves high detection accuracy while substantially reducing communication costs these findings highlight federated learning as a practical path toward scalable secure and privacyaware intrusion detection for iot ecosystems,0
internet of things iot is defined as the connection between places and physical objects ie things over the internetnetwork via smart computing devices traditionally we learn about the iot ecosystemproblems by conducting surveys of iot developerspractitioners another way to learn is by analyzing iot developer discussions in popular online developer forums like stack overflow so however we are aware of no such studies that focused on iot developers security and mlrelated discussions in so this paper offers the results of preliminary study of iot developer discussions in so we find around 12 of sentences contain security discussions while around 012 sentences contain ml related discussions we find that iot developers discussing security issues frequently inquired about how the shared data can be stored shared and transferred securely across iot devices and users we also find that iot developers are interested to adopt deep neural networkbased ml models into their iot devices but they find it challenging to accommodate those into their resourceconstrained iot devices our findings offer implications for iot vendors and researchers to develop and design novel techniques for improved security and ml adoption into iot devices,0
consumer iot devices may suffer malware attacks and be recruited into botnets or worse there is evidence that generic advice to device owners to address iot malware can be successful but this does not account for emerging forms of persistent iot malware less is known about persistent malware which resides on persistent storage requiring targeted manual effort to remove it this paper presents a field study on the removal of persistent iot malware by consumers we partnered with an isp to contrast remediation times of 760 customers across three malware categories windows malware nonpersistent iot malware and persistent iot malware we also contacted isp customers identified as having persistent iot malware on their networkattached storage devices specifically qsnatch we found that persistent iot malware exhibits a mean infection duration many times higher than windows or mirai malware qsnatch has a survival probability of 30 after 180 days whereby most if not all other observed malware types have been removed for interviewed device users qsnatch infections lasted longer so are apparently more difficult to get rid of yet participants did not report experiencing difficulty in following notification instructions we see two factors driving this paradoxical finding first most users reported having high technical competency also we found evidence of planning behavior for these tasks and the need for multiple notifications our findings demonstrate the critical nature of interventions from outside for persistent malware since automatic scan of an av tool or a power cycle like we are used to for windows malware and mirai infections will not solve persistent iot malware infections,0
background the recent surge in phishing attacks keeps undermining the effectiveness of the traditional antiphishing blacklist approaches ondevice antiphishing solutions are gaining popularity as they offer faster phishing detection locally aim we aim to eliminate the delay in recognizing and recording phishing campaigns in databases via ondevice solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawlers scan to finish additionally utilizing operating systemspecific resources and frameworks we aim to minimize the impact on system performance and depend on local processing to protect user privacy method we propose a phishing detection solution that uses a combination of computer vision and ondevice machine learning models to analyze websites in real time our referencebased approach analyzes the visual content of webpages identifying phishing attempts through layout analysis credential input areas detection and brand impersonation criteria combination results our case study shows its feasible to perform background processing ondevice continuously for the case of the web browser requiring the resource use of 16 of a single cpu core and less than 84mb of ram on apple m1 while maintaining the accuracy of brand logo detection at 466 comparable with baselines and of credential requiring page detection at 981 improving the baseline by 31 within the test dataset conclusions our results demonstrate the potential of ondevice realtime phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest eg email clients and messenger windows,0
the internet of things iot has emerged as a foundational paradigm supporting a range of applications including healthcare education agriculture smart homes and more recently enterprise systems however significant advancements in iot networks have been impeded by security vulnerabilities and threats that if left unaddressed could hinder the deployment and operation of iot based systems detecting unwanted activities within the iot is crucial as it directly impacts confidentiality integrity and availability consequently intrusion detection has become a fundamental research area and the focus of numerous studies an intrusion detection system ids is essential to the iots alarm mechanisms enabling effective security management this paper examines iot security and introduces an intelligent twolayer intrusion detection system for iot machine learning techniques power the systems intelligence with a two layer structure enhancing intrusion detection by selecting essential features the system maintains detection accuracy while minimizing processing overhead the proposed method for intrusion detection in iot is implemented in two phases in the first phase the grasshopper optimization algorithm goa is applied for feature selection in the second phase the support vector machine svm algorithm is used to detect intrusions the method was implemented in matlab and the nslkdd dataset was used for evaluation simulation results show that the proposed method improves accuracy compared to other approaches,0
the routing protocol for lowpower and lossy networks rpl has become the de facto routing standard for resourceconstrained iot systems but its lightweight design exposes critical vulnerabilities to a wide range of routinglayer attacks such as hello flood decreased rank and version number manipulation traditional countermeasures including protocollevel modifications and machine learning classifiers can achieve high accuracy against known threats yet they fail when confronted with novel or zeroday attacks unless fully retrained an approach that is impractical for dynamic iot environments in this paper we investigate incremental learning as a practical and adaptive strategy for intrusion detection in rplbased networks we systematically evaluate five model families including ensemble models and deep learning models our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats all while reducing training time compared to full retraining by combining five diverse models with attackspecific analysis forgetting behavior and time efficiency this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving rplbased iot networks,0
this study explores the application of quantum machine learning qml algorithms to enhance cybersecurity threat detection particularly in the classification of malware and intrusion detection within highdimensional datasets classical machine learning approaches encounter limitations when dealing with intricate obfuscated malware patterns and extensive network intrusion data to address these challenges we implement and evaluate various qml algorithms including quantum neural networks qnn quantum support vector machines qsvm and hybrid quantum convolutional neural networks qcnn for malware detection tasks our experimental analysis utilized two datasets the intrusion dataset comprising 150 samples with 56 memorybased features derived from volatility framework analysis and the obfuscatedmalmem2022 dataset containing 58596 samples with 57 features representing benign and malicious software remarkably our qml methods demonstrated superior performance compared to classical approaches achieving accuracies of 95 for qnn and 94 for qsvm these quantumenhanced methods leveraged quantum superposition and entanglement principles to accurately identify complex patterns within highly obfuscated malware samples that were imperceptible to classical methods to further advance malware analysis we propose a novel realtime malware analysis framework that incorporates quantum feature extraction using quantum fourier transform quantum feature maps and classification using variational quantum circuits this system integrates explainable ai methods including gradcam and scorecam algorithms to provide interpretable insights into the quantum decisionmaking processes,0
supply chain security threats pose new challenges to security risk modeling techniques for complex ict systems such as the iot with established techniques drawn from attack trees and reliability analysis providing needed points of reference graphbased analysis can provide a framework for considering the role of suppliers in such systems we present such a framework here while highlighting the need for a componentcentered model given resource limitations when applying this model to existing systems we study various classes of uncertainties in model development including structural uncertainties and uncertainties in the magnitude of estimated event probabilities using case studies we find that structural uncertainties constitute a greater challenge to model utility and as such should receive particular attention best practices in the face of these uncertainties are proposed,0
with the rapid growth of the internetofthings iot concerns about the security of iot devices have become prominent several vendors are producing ipconnected devices for home and small office networks that often suffer from flawed security designs and implementations they also tend to lack mechanisms for firmware updates or patches that can help eliminate security vulnerabilities securing networks where the presence of such vulnerable devices is given requires a brownfield approach applying necessary protection measures within the network so that potentially vulnerable devices can coexist without endangering the security of other devices in the same network in this paper we present iot sentinel a system capable of automatically identifying the types of devices being connected to an iot network and enabling enforcement of rules for constraining the communications of vulnerable devices so as to minimize damage resulting from their compromise we show that iot sentinel is effective in identifying device types and has minimal performance overhead,0
despite its technological benefits internet of things iot has cyber weaknesses due to the vulnerabilities in the wireless medium machine learning mlbased methods are widely used against cyber threats in iot networks with promising performance advanced persistent threat apt is prominent for cybercriminals to compromise networks and it is crucial to longterm and harmful characteristics however it is difficult to apply mlbased approaches to identify apt attacks to obtain a promising detection performance due to an extremely small percentage among normal traffic there are limited surveys to fully investigate apt attacks in iot networks due to the lack of public datasets with all types of apt attacks it is worth to bridge the stateoftheart in network attack detection with apt attack detection in a comprehensive review article this survey article reviews the security challenges in iot networks and presents the wellknown attacks apt attacks and threat models in iot systems meanwhile signaturebased anomalybased and hybrid intrusion detection systems are summarized for iot networks the article highlights statistical insights regarding frequently applied mlbased methods against network intrusion alongside the number of attacks types detected finally open issues and challenges for common network intrusion and apt attacks are presented for future research,0
malware remains a big threat to cyber security calling for machine learning based malware detection while promising such detectors are known to be vulnerable to evasion attacks ensemble learning typically facilitates countermeasures while attackers can leverage this technique to improve attack effectiveness as well this motivates us to investigate which kind of robustness the ensemble defense or effectiveness the ensemble attack can achieve particularly when they combat with each other we thus propose a new attack approach named mixture of attacks by rendering attackers capable of multiple generative methods and multiple manipulation sets to perturb a malware example without ruining its malicious functionality this naturally leads to a new instantiation of adversarial training which is further geared to enhancing the ensemble of deep neural networks we evaluate defenses using android malware detectors against 26 different attacks upon two practical datasets experimental results show that the new adversarial training significantly enhances the robustness of deep neural networks against a wide range of attacks ensemble methods promote the robustness when base classifiers are robust enough and yet ensemble attacks can evade the enhanced malware detectors effectively even notably downgrading the virustotal service,0
internet of things iot is the epitome of sustainable development it has facilitated the development of smart systems industrialization and the stateoftheart quality of life iot architecture is one of the essential baselines of understanding the widespread adoption security issues are very crucial for any technical infrastructure since iot comprises heterogeneous devices its security issues are diverse too various security attacks can be responsible for compromising confidentiality integrity and availability in this paper at first the iot architecture is described briefly after that the components of iot are explained with perspective to various iot based applications and services finally various security issues including recommended solutions are elaborately described and the potential research challenges and future research directions,0
the internet of things iot is intended for ubiquitous connectivity among different entities or things while its purpose is to provide effective and efficient solutions security of the devices and network is a challenging issue the number of devices connected along with the adhoc nature of the system further exacerbates the situation therefore security and privacy has emerged as a significant challenge for the iot in this paperwe aim to provide a thorough survey related to the privacy and security challenges of the iot this document addresses these challenges from the perspective of technologies and architecture used this work focuses also in iot intrinsic vulnerabilities as well as the security challenges of various layers based on the security principles of data confidentiality integrity and availability this survey analyzes articles published for the iot at the time and relates it to the security conjuncture of the field and its projection to the future,0
iot devices are known to be vulnerable to various cyberattacks such as data exfiltration and the execution of flooding attacks as part of a ddos attack when it comes to detecting such attacks using network traffic analysis it has been shown that some attack scenarios are not always equally easy to detect if they involve different iot models that is when targeted at some iot models a given attack can be detected rather accurately while when targeted at others the same attack may result in too many false alarms in this research we attempt to explain this variability of iot attack detectability and devise a risk assessment method capable of addressing a key question how easy is it for an anomalybased network intrusion detection system to detect a given cyberattack involving a specific iot model in the process of addressing this question we a investigate the predictability of iot network traffic b present a novel taxonomy for iot attack detection which also encapsulates traffic predictability aspects c propose an expertbased attack detectability estimation method which uses this taxonomy to derive a detectability score termed dscore for a given combination of iot model and attack scenario and d empirically evaluate our method while comparing it with a datadriven method,0
trusted execution environments tees embedded in iot devices provide a deployable solution to secure iot applications at the hardware level by design in tees the trusted operating system trusted os is the primary component it enables the tee to use securitybased design techniques such as data encryption and identity authentication once a trusted os has been exploited the tee can no longer ensure security however trusted oses for iot devices have received little security analysis which is challenging from several perspectives 1 trusted oses are closedsource and have an unfavorable environment for sending test cases and collecting feedback 2 trusted oses have complex data structures and require a stateful workflow which limits existing vulnerability detection tools to address the challenges we present syztrust the first stateaware fuzzing framework for vetting the security of resourcelimited trusted oses syztrust adopts a hardwareassisted framework to enable fuzzing trusted oses directly on iot devices as well as tracking state and code coverage noninvasively syztrust utilizes composite feedback to guide the fuzzer to effectively explore more states as well as to increase the code coverage we evaluate syztrust on trusted oses from three major vendors samsung tsinglink cloud and ali cloud these systems run on cortex m2333 mcus which provide the necessary abstraction for embedded tees we discovered 70 previously unknown vulnerabilities in their trusted oses receiving 10 new cves so far furthermore compared to the baseline syztrust has demonstrated significant improvements including 66 higher code coverage 651 higher state coverage and 31 improved vulnerabilityfinding capability we report all discovered new vulnerabilities to vendors and open source syztrust,0
the increased popularity of iot devices have made them lucrative targets for attackers due to insecure product development practices these devices are often vulnerable even to very trivial attacks and can be easily compromised due to the sheer number and heterogeneity of iot devices it is not possible to secure the iot ecosystem using traditional endpoint and network security solutions to address the challenges and requirements of securing iot devices in edge networks we present iotkeeper which is a novel system capable of securing the network against any malicious activity in real time the proposed system uses a lightweight anomaly detection technique to secure both devicetodevice and devicetoinfrastructure communications while using limited resources available on the gateway it uses unlabeled network data to distinguish between benign and malicious traffic patterns observed in the network a detailed evaluation done with real world testbed shows that iotkeeper detects any device generating malicious traffic with high accuracy 0982 and low false positive rate 001 the results demonstrate that iotkeeper is lightweight responsive and can effectively handle complex d2d interactions without requiring explicit attack signatures or sophisticated hardware,0
the future internet of things iot will have a deep economical commercial and social impact on our lives the participating nodes in iot networks are usually resourceconstrained which makes them luring targets for cyber attacks in this regard extensive efforts have been made to address the security and privacy issues in iot networks primarily through traditional cryptographic approaches however the unique characteristics of iot nodes render the existing solutions insufficient to encompass the entire security spectrum of the iot networks this is at least in part because of the resource constraints heterogeneity massive realtime data generated by the iot devices and the extensively dynamic behavior of the networks therefore machine learning ml and deep learning dl techniques which are able to provide embedded intelligence in the iot devices and networks are leveraged to cope with different security problems in this paper we systematically review the security requirements attack vectors and the current security solutions for the iot networks we then shed light on the gaps in these security solutions that call for ml and dl approaches we also discuss in detail the existing ml and dl solutions for addressing different security problems in iot networks at last based on the detailed investigation of the existing solutions in the literature we discuss the future research directions for ml and dlbased iot security,0
the internet of things also known as the iot refers to the billions of devices around the world that are now connected to the internet collecting and sharing data the amount of data collected through iot sensors must be completely securely controlled to protect the information collected by iot sensors a lightweight method called discover the flooding attackrpl dfarpl has been proposed the proposed dfarpl method identifies intrusive nodes in several steps to exclude them from continuing routing operations thus in the dfarpl method it first builds a cluster and selects the most appropriate node as a cluster head in dodag then due to the vulnerability of the rpl protocol to flooding attacks it uses an ant colony algorithm aco using five steps to detect attacks use flooding to prevent malicious activity on the iot network in other words if it detects a node as malicious it puts that node on the detention list and quarantines it for a certain period of time the results obtained from the simulation show the superiority of the proposed method in terms of packet delivery rate detection rate false positive rate and false negative rate compared to irad and reato methods,0
in various scenarios achieving security between iot devices is challenging since the devices may have different dedicated communication standards resource constraints as well as various applications in this article we first provide requirements and existing solutions for iot security we then introduce a new reconfigurable security framework based on edge computing which utilizes a nearuser edge device ie security agent to simplify key management and offload the computational costs of security algorithms at iot devices this framework is designed to overcome the challenges including high computation costs low flexibility in key management and low compatibility in deploying new security algorithms in iot especially when adopting advanced cryptographic primitives we also provide the design principles of the reconfigurable security framework the exemplary security protocols for anonymous authentication and secure data access control and the performance analysis in terms of feasibility and usability the reconfigurable security framework paves a new way to strength iot security by edge computing,0
iot devices are increasingly utilized in critical infrastructure enterprises and households there are several sophisticated cyberattacks that have been reported and many networks have proven vulnerable to both active and passive attacks by leaking private information allowing unauthorized access and being open to denial of service attacks this paper aims firstly to assist network operators to understand the need for an iot network security solution and then secondly to survey iot network attack vectors cyber threats and countermeasures with a focus on improving the robustness of existing security solutions our first contribution highlights viewpoints on iot security from the perspective of stakeholders such as manufacturers service providers consumers and authorities we discuss the differences between iot and it systems the need for iot security solutions and we highlight the key components required for iot network security system architecture for our second contribution we survey the types of iot attacks by grouping them based on their impact we discuss various attack techniques threats and shortfalls of existing countermeasures with an intention to enable future research into improving iot network security,0
blockchain bc a byproduct of bitcoin cryptocurrency has gained immense and wide scale popularity for its applicability in various diverse domains especially in multifaceted nonmonetary systems by adopting cryptographic techniques such as hashing and asymmetric encryption along with distributed consensus approach a blockchain based distributed ledger not only becomes highly secure but also immutable and thus eliminates the need for any thirdparty intermediators on the contrary innumerable iot internet of things devices are increasingly being added to the network this phenomenon poses higher risk in terms of security and privacy it is thus extremely important to address the security aspects of the growing iot ecosystem this paper explores the applicability of bc for ensuring enhanced security and privacy in the iot ecosystem recent research articles and projects or applications were surveyed to assess the implementation of bc for iot security and identify associated challenges and propose solutions for bc enabled enhanced security for the iot ecosystem,0
edge computingbased nextgeneration wireless networks ngwniot offer enhanced bandwidth capacity for largescale service provisioning but remain vulnerable to evolving cyber threats existing intrusion detection and prevention methods provide limited security as adversaries continually adapt their attack strategies we propose a dynamic attack detection and prevention approach to address this challenge first blockchainbased authentication uses the deoxys authentication algorithm daa to verify iot device legitimacy before data transmission next a bistage intrusion detection system is introduced the first stage uses signaturebased detection via an improved random forest irf algorithm in contrast the second stage applies featurebased anomaly detection using a diffusion convolution recurrent neural network dcrnn to ensure quality of service qos and maintain service level agreements sla trustaware service migration is performed using heapbased optimization hbo additionally ondemand virtual highinteraction honeypots deceive attackers and extract attack patterns which are securely stored using the bimodal lattice signature scheme bliss to enhance signaturebased intrusion detection systems ids the proposed framework is implemented in the ns3 simulation environment and evaluated against existing methods across multiple performance metrics including accuracy attack detection rate false negative rate precision recall roc curve memory usage cpu usage and execution time experimental results demonstrate that the framework significantly outperforms existing approaches reinforcing the security of ngwnenabled iot ecosystems,0
this paper presents a novel approach to intrusion detection by integrating traditional signaturebased methods with the contextual understanding capabilities of the gpt2 large language model llm as cyber threats become increasingly sophisticated particularly in distributed heterogeneous and resourceconstrained environments such as those enabled by the internet of things iot the need for dynamic and adaptive intrusion detection systems idss becomes increasingly urgent while traditional methods remain effective for detecting known threats they often fail to recognize new and evolving attack patterns in contrast gpt2 excels at processing unstructured data and identifying complex semantic relationships making it wellsuited to uncovering subtle zeroday attack vectors we propose a hybrid ids framework that merges the robustness of signaturebased techniques with the adaptability of gpt2driven semantic analysis experimental evaluations on a representative intrusion dataset demonstrate that our model enhances detection accuracy by 63 reduces false positives by 90 and maintains near realtime responsiveness these results affirm the potential of language model integration to build intelligent scalable and resilient cybersecurity defences suited for modern connected environments,0
with the everincreasing reliance on digital networks for various aspects of modern life ensuring their security has become a critical challenge intrusion detection systems play a crucial role in ensuring network security actively identifying and mitigating malicious behaviours however the relentless advancement of cyberthreats has rendered traditionalclassical approaches insufficient in addressing the sophistication and complexity of attacks this paper proposes a novel 3stage intrusion detection system inspired by a simplified version of the lockheed martin cyber kill chain to detect advanced multistep attacks the proposed approach consists of three models each responsible for detecting a group of attacks with common characteristics the detection outcome of the first two stages is used to conduct a feasibility study on the possibility of predicting attacks in the third stage using the ton iot dataset we achieved an average of 94 f1score among different stages outperforming the benchmark approaches based on randomforest model finally we comment on the feasibility of this approach to be integrated in a realworld system and propose various possible future work,0
the internet of things iot has already changed our daily lives by integrating smart devices together towards delivering high quality services to its clients these devices when integrated together form a network through which massive amount of data can be produced transferred and shared a critical concern is the security and integrity of such a complex platform to ensure the sustainability and reliability of these iotbased systems blockchain is an emerging technology that has demonstrated its unique features and capabilities for different problems and application domains including iotbased systems this survey paper reviews the adaptation of blockchain in the context of iot to represent how this technology is capable of addressing the integration and security problems of devices connected to iot systems the innovation of this survey is that we present a survey based upon the integration approaches and security issues of iot data and discuss the role of blockchain in connection with these issues,0
in an era where the internet of things iot intersects increasingly with generative artificial intelligence ai this article scrutinizes the emergent security risks inherent in this integration we explore how generative ai drives innovation in iot and we analyze the potential for data breaches when using generative ai and the misuse of generative ai technologies in iot ecosystems these risks not only threaten the privacy and efficiency of iot systems but also pose broader implications for trust and safety in aidriven environments the discussion in this article extends to strategic approaches for mitigating these risks including the development of robust security protocols the multilayered security approaches and the adoption of ai technological solutions through a comprehensive analysis this article aims to shed light on the critical balance between embracing ai advancements and ensuring stringent security in iot providing insights into the future direction of these intertwined technologies,0
the proliferation of iot devices in smart homes hospitals and enterprise networks is widespread and continuing to increase in a superlinear manner with this unprecedented growth how can one assess the security of an iot network holistically in this article we explore two dimensions of security assessment using vulnerability information of iot devices and their underlying components textitcompositional security scores and siem logs captured from the communications and operations of such devices in a network textitdynamic activity metrics to propose the notion of an textitattack circuit these measures are used to evaluate the security of iot devices and the overall iot network demonstrating the effectiveness of attack circuits as practical tools for computing security metrics exploitability impact and risk to confidentiality integrity and availability of heterogeneous networks we propose methods for generating attack circuits with inputoutput pairs constructed from cves using natural language processing nlp and with weights computed using standard security scoring procedures as well as efficient optimization methods for evaluating attack circuits our system provides insight into possible attack paths an adversary may utilize based on their exploitability impact or overall risk we have performed experiments on iot networks to demonstrate the efficacy of the proposed techniques,0
security has become ubiquitous in every domain today as newly emerging malware pose an everincreasing perilous threat to systems consequently honeypots are fast emerging as an indispensible forensic tool for the analysis of malicious network traffic honeypots can be considered to be traps for hackers and intruders and are generally deployed complimentary to intrusion detection systems ids and intrusion prevention systems ips in a network they help system administrators perform a rigorous analysis of external and internal attacks on their networks they are also used by security firms and research labs to capture the latest variants of malware however honeypots would serve a slightly different purpose in our proposed system we intend to use honeypots for generating and broadcasting instant cures for new and unknown malware in a network the cures which will be in the form of onthefly antimalware signatures would spread in a fashion that is similar to the way malware spreads across networks the most striking advantage of implementing this technology is that an effective initial control can be exercised on malware proposed system would be capable of providing cures for new fatal viruses which have not yet been discovered by prime security firms of the world,0
the use of machine learning ml models in cybersecurity solutions requires highquality data that is stripped of redundant missing and noisy information by selecting the most relevant features data integrity and model efficiency can be significantly improved this work evaluates the feature sets provided by a combination of different feature selection methods namely information gain chisquared test recursive feature elimination mean absolute deviation and dispersion ratio in multiple iot network datasets the influence of the smaller feature sets on both the classification performance and the training time of ml models is compared with the aim of increasing the computational efficiency of iot intrusion detection overall the most impactful features of each dataset were identified and the ml models obtained higher computational efficiency while preserving a good generalization showing little to no difference between the sets,0
the internet of things iot integrates billions of smart devices that can communicate with one another with minimal human intervention it is one of the fastest developing fields in the history of computing with an estimated 50 billion devices by the end of 2020 on the one hand iot play a crucial role in enhancing several reallife smart applications that can improve life quality on the other hand the crosscutting nature of iot systems and the multidisciplinary components involved in the deployment of such systems introduced new security challenges implementing security measures such as encryption authentication access control network security and application security for iot devices and their inherent vulnerabilities is ineffective therefore existing security methods should be enhanced to secure the iot system effectively machine learning and deep learning mldl have advanced considerably over the last few years and machine intelligence has transitioned from laboratory curiosity to practical machinery in several important applications consequently mldl methods are important in transforming the security of iot systems from merely facilitating secure communication between devices to securitybased intelligence systems the goal of this work is to provide a comprehensive survey of ml dl methods that can be used to develop enhanced security methods for iot systems iot security threats that are related to inherent or newly introduced threats are presented and various potential iot system attack surfaces and the possible threats related to each surface are discussed we then thoroughly review mldl methods for iot security and present the opportunities advantages and shortcomings of each method we discuss the opportunities and challenges involved in applying mldl to iot security these opportunities and challenges can serve as potential future research directions,0
the use of iot in society is perhaps already ubiquitous with a vast attack surface offering multiple opportunities for malicious actors this short paper first presents an introduction to iot and its security issues including an overview of iot layer models and topologies iot standardisation efforts and protocols the focus then moves to iot vulnerabilities and specific suggestions for mitigations this works intended audience are those relatively new to iot though with existing networkrelated knowledge it is concluded that device resource constraints and a lack of iot standards are significant issues research opportunities exist to develop efficient iot ids and energysaving cryptography techniques lightweight enough to reasonably deploy the need for standardised protocols and channelbased security solutions is clear underpinned by legislative directives to ensure high standards that prevent costcutting on the device manufacturing side,0
the internet of things iot market is rapidly growing and is expected to double from 2020 to 2025 the increasing use of iot devices particularly in smart homes raises crucial concerns about user privacy and security as these devices often handle sensitive and critical information inadequate security designs and implementations by iot vendors can lead to significant vulnerabilities to address these iot device vulnerabilities institutions and organizations have published iot security best practices bps to guide manufacturers in ensuring the security of their products however there is currently no standardized approach for evaluating the effectiveness of individual bp recommendations this leads to manufacturers investing effort in implementing less effective bps while potentially neglecting measures with greater impact in this paper we propose a methodology for evaluating the security impact of iot bps and ranking them based on their effectiveness in protecting against security threats our approach involves translating identified bps into concrete test cases that can be applied to realworld iot devices to assess their effectiveness in mitigating vulnerabilities we applied this methodology to evaluate the security impact of nine commodity iot products discovering 18 vulnerabilities by empirically assessing the actual impact of bps on device security iot designers and implementers can prioritize their security investments more effectively improving security outcomes and optimizing limited security budgets,0
iot devices are increasingly deployed in daily life many of these devices are however vulnerable due to insecure design implementation and configuration as a result many networks already have vulnerable iot devices that are easy to compromise this has led to a new category of malware specifically targeting iot devices however existing intrusion detection techniques are not effective in detecting compromised iot devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved in this paper we present dot an autonomous selflearning distributed system for detecting compromised iot devices effectively in contrast to prior work dot uses a novel selflearning approach to classify devices into device types and build normal communication profiles for each of these that can subsequently be used to detect anomalous deviations in communication patterns dot utilizes a federated learning approach for aggregating behavior profiles efficiently to the best of our knowledge it is the first system to employ a federated learning approach to anomalydetectionbased intrusion detection consequently dot can cope with emerging new and unknown attacks we systematically and extensively evaluated more than 30 offtheshelf iot devices over a long term and show that dot is highly effective 956 detection rate and fast 257 ms at detecting devices compromised by for instance the infamous mirai malware dot reported no false alarms when evaluated in a realworld smart home deployment setting,0
the internet of things iot has become a guiding technology behind automation and smart computing one of the major concerns with the iot systems is the lack of privacy and security preserving schemes for controlling access and ensuring the security of the data a majority of security issues arise because of the centralized architecture of iot systems another concern is the lack of proper authentication and access control schemes to moderate access to information generated by the iot devices so the question that arises is how to ensure the identity of the equipment or the communicating node the answer to secure operations in a trustless environment brings us to the decentralized solution of blockchain a lot of research has been going on in the area of convergence of iot and blockchain and it has resulted in some remarkable progress in addressing some of the significant issues in the iot arena this work reviews the challenges and threats in the iot environment and how integration with blockchain can resolve some of them,0
as new technologies such as the internet of things iot are integrated into critical national infrastructures cni new cybersecurity threats emerge that require specific security solutions approaches used for analysis include the modelling and simulation of critical infrastructure systems using attributes functionalities operations and behaviours to support various security analysis viewpoints recognising and appropriately managing associated security risks with several critical infrastructure protection approaches available the question of how to effectively model the complex behaviour of interconnected cni elements and to configure their protection as a systemofsystems remains a challenge using a systematic review approach existing critical infrastructure protection approaches tools and techniques are examined to determine their suitability given trends like iot and effective security modelling and analysis issues it is found that empiricalbased agentbased system dynamicsbased and networkbased modelling are more commonly applied than economicbased and equationbased techniques and empiricalbased modelling is the most widely used the energy and transportation critical infrastructure sectors reflect the most responsive sectors and no one critical infrastructure protection cip approach tool technique methodology or framework provides a fitforall capacity for allround attribute modelling and simulation of security risks typically deciding factors for cip choices to adopt are often dominated by tradeoffs between complexity of use and popularity of approach as well as between specificity and generality of application in sectors,0
the continuous rise in the adoption of emerging technologies such as internet of things iot by businesses has brought unprecedented opportunities for innovation and growth however due to the distinct characteristics of these emerging iot technologies like realtime data processing selfconfiguration interoperability and scalability they have also introduced some unique cybersecurity challenges such as malware attacks advanced persistent threats apts dos ddos denial of service distributed denial of service attacks and insider threats as a result of these challenges there is an increased need for improved cybersecurity approaches and efficient management solutions to ensure the privacy and security of communication within iot networks one proposed security approach is the utilization of trustbased systems and is the focus of this study this research paper presents a systematic literature review on the trustbased cybersecurity security approaches for iot a total of 23 articles were identified that satisfy the review criteria we highlighted the common trustbased mitigation techniques in existence for dealing with these threats and grouped them into three major categories namely observationbased knowledgebased clusterbased systems finally several open issues were highlighted and future research directions presented,0
the everincreasing security vulnerabilities in the internetofthings iot systems require improved threat detection approaches this paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis temporal support learning and focused feature extraction the proposed attentionbased model benefits from a hybrid cnnbilstm architecture and achieves 99 classification accuracy in detecting botnet attacks utilizing the nbaiot dataset while maintaining high precision and recall across various scenarios the proposed models performance is further validated by key parameters such as mathews correlation coefficient and cohens kappa correlation coefficient the closetoideal results for these parameters demonstrate the proposed models ability to detect botnet attacks accurately and efficiently in practical settings and on unseen data the proposed model proved to be a powerful defence mechanism for iot networks to face emerging security challenges,0
in recent years as intrusion attacks on iot networks have grown exponentially there is an immediate need for sophisticated intrusion detection systems idss a vast majority of current idss are datadriven which means that one of the most important aspects of this area of research is the quality of the data acquired from iot network traffic two of the most cited intrusion detection datasets are the kddcup99 and the nslkdd the main goal of our project was to conduct a robust comparison of both datasets by evaluating the performance of various machine learning ml classifiers trained on them with a larger set of classification metrics than previous researchers from our research we were able to conclude that the nslkdd dataset is of a higher quality than the kddcup99 dataset as the classifiers trained on it were on average 2018 less accurate this is because the classifiers trained on the kddcup99 dataset exhibited a bias towards the redundancies within it allowing them to achieve higher accuracies,0
with the development of artificial intelligence algorithms like deep learning models and the successful applications in many different fields further similar trails of deep learning technology have been made in cyber security area it shows the preferable performance not only in academic security research but also in industry practices when dealing with part of cyber security issues by deep learning methods compared to those conventional rules especially for the malware detection and classification tasks it saves generous time cost and promotes the accuracy for a total pipeline of malware detection system in this paper we construct special deep neural network ie maldeepnet tbmalnet and ibmalnet for malware dynamic behavior classification tasks then we build the family clustering algorithm based on deep learning and fulfil related testing except that we also design a novel malware prediction model which could detect the malware coming in future through the mal generative adversarial network malgan implementation all those algorithms present fairly considerable value in related datasets afterwards,0
proliferation of iot devices in society demands a renewed focus on securing the use and maintenance of such systems iotbased systems will have a great impact on society and therefore such systems must have guaranteed resilience we introduce cryptographicbased building blocks that strive to ensure that distributed iot networks remain in a healthy condition throughout their lifecycle our presented solution utilizes deterministic and interlinked smart contracts on the ethereum blockchain to enforce secured management and maintenance for hardened iot devices a key issue investigated is the protocol development for securing iot device deployments and means for communicating securely with devices by supporting values of openness automation and provenance we can introduce novel means that reduce the threats of surveillance and theft while also improving operator accountability and trust in iot technology,0
the internet of things refers to the network of devices connected to the internet and can communicate with each other the term things is to refer nonconventional devices that are usually not connected to the internet the network of such devices or things is growing at an enormous rate the security and privacy of the data flowing through these things is a major concern the devices are low powered and the conventional encryption algorithms are not suitable to be employed on these devices in this correspondence a survey of the contemporary lightweight encryption algorithms suitable for use in the iot environment has been presented,0
the internet of things iot is becoming an integral part of our modern lives as we converge towards a world surrounded by ubiquitous connectivity the inherent complexity presented by the vast iot ecosystem ends up in an insufficient understanding of individual system components and their interactions leading to numerous security challenges in order to create a secure iot platform from the ground up there is a need for a unifying operating system os that can act as a cornerstone regulating the development of stable and secure solutions in this paper we present a classification of the security challenges stemming from the manifold aspects of iot development we also specify security requirements to direct the secure development of an unifying iot os to resolve many of those ensuing challenges survey of several modern iot oss confirm that while the developers of the oss have taken many alternative approaches to implement security we are far from engineering an adequately secure and unified architecture more broadly the study presented in this paper can help address the growing need for a secure and unified platform to base iot development on and assure the safe secure and reliable operation of iot in critical domains,0
the internet of things iot is an emerging concept that directly links to the billions of physical items or things that are connected to the internet and are all gathering and exchanging information between devices and systems however iot devices were not built with security in mind which might lead to security vulnerabilities in a multidevice system traditionally we investigated iot issues by polling iot developers and specialists this technique however is not scalable since surveying all iot developers is not feasible another way to look into iot issues is to look at iot developer discussions on major online development forums like stack overflow so however finding discussions that are relevant to iot issues is challenging since they are frequently not categorized with iotrelated terms in this paper we present the iot security dataset a domainspecific dataset of 7147 samples focused solely on iot security discussions as there are no automated tools to label these samples we manually labeled them we further employed multiple transformer models to automatically detect security discussions through rigorous investigations we found that iot security discussions are different and more complex than traditional security discussions we demonstrated a considerable performance loss up to 44 of transformer models on crossdomain datasets when we transferred knowledge from a generalpurpose dataset opiner supporting our claim thus we built a domainspecific iot security detector with an f1score of 069 we have made the dataset public in the hope that developers would learn more about the security discussion and vendors would enhance their concerns about product security,0
in recent years the increase in nonwindows malware threats had turned the focus of the cybersecurity community research works on hunting windows pebased malwares are maturing whereas the developments on linux malware threat hunting are relatively scarce with the advent of the internet of things iot era smart devices that are getting integrated into human life have become a hackers highway for their malicious activities the iot devices employ various unixbased architectures that follow elf executable and linkable format as their standard binary file specification this study aims at providing a comprehensive survey on the latest developments in crossarchitectural iot malware detection and classification approaches aided by a modern taxonomy we discuss the feature representations feature extraction techniques and machine learning models employed in the surveyed works we further provide more insights on the practical challenges involved in crossarchitectural iot malware threat hunting and discuss various avenues to instill potential future research,0
in the rapidly evolving landscape of the iot the security of connected devices has become a paramount concern this paper explores the concept of proactive threat hunting as a pivotal strategy for enhancing the security and sustainability of iot systems proactive threat hunting is an alternative to traditional reactive security measures that analyses iot networks continuously and in advance to find and eliminate threats before they occure by improving the security posture of iot devices this approach significantly contributes to extending iot operational lifespan and reduces environmental impact by integrating security metrics similar to the common vulnerability scoring system cvss into consumer platforms this paper argues that proactive threat hunting can elevate user awareness about the security of iot devices this has the potential to impact consumer choices and encourage a securityconscious mindset in both the manufacturing and user communities through a comprehensive analysis this study demonstrates how proactive threat hunting can contribute to the development of a more secure sustainable and useraware iot ecosystem,0
iot is a dynamic network of interconnected things that communicate and exchange data where security is a significant issue previous studies have mainly focused on attack classifications and open issues rather than presenting a comprehensive overview on the existing threats and vulnerabilities this knowledge helps analyzing the network in the early stages even before any attack takes place in this paper the researchers have proposed different security aspects and a novel bayesian security aspects dependency graph for iot bsagiot to illustrate their relations the proposed bsagiot is a generic model applicable to any iot network and contains aspects from five categories named data access control standard network and loss this proposed bayesian security aspect graph bsag presents an overview of the security aspects in any given iot network the purpose of bsagiot is to assist security experts in analyzing how a successful compromise andor a failed breach could impact the overall security and privacy of the respective iot network in addition root cause identification of security challenges how they affect one another their impact on iot networks via topological sorting and risk assessment could be achieved hence to demonstrate the feasibility of the proposed method experimental results with various scenarios has been presented in which the security aspects have been quantified based on the network configurations the results indicate the impact of the aspects on each other and how they could be utilized to mitigate andor eliminate the security and privacy deficiencies in iot networks,0
there have been significant issues given the iot with heterogeneity of billions of devices and with a large amount of data this paper proposed an innovative design of the internet of things iot environment intrusion detection system or ids using deep learningintegrated convolutional neural networks cnn and long shortterm memory lstm networks our model based on the cicids2017 dataset achieved an accuracy of 9952 in classifying network traffic as either benign or malicious the realtime processing capability scalability and low false alarm rate in our model surpass some traditional ids approaches and therefore prove successful for application in todays iot networks the development and the performance of the model with possible applications that may extend to other related fields of adaptive learning techniques and crossdomain applicability are discussed the research involving deep learning for iot cybersecurity offers a potent solution for significantly improving network security,0
in the field of computer science and information technology internet of things iot is one of the emerging technologies in iot environment several devices are interconnected and transmit data among them there may be some security vulnerability arise within the iot environment till date iot has not been widely accepted due to its security flaws hence to keep the iot environment most robust we propose a stable security framework of iot with elliptic curve cryptography ecc using dna encoding the ecc is most lightweight cryptography technique among other well known public key cryptography techniques to increase encryption complexity dna encoding mechanism of dna computing with ecc is preceded,0
besides intels sgx technology there are longrunning discussions on how trusted computing technologies can be used to cloak malware past research showed example methods of malicious activities utilising flicker trusted platform module and recently integrating with enclaves we observe two ambiguous methodologies of malware development being associated with sgx and it is crucial to systematise their details one methodology is to use the core sgx ecosystem to cloak malware potentially affecting a large number of systems the second methodology is to create a custom enclave not adhering to base assumptions of sgx creating a demonstration code of malware behaviour with these incorrect assumptions remaining local without any impact we examine what malware aims to do in realworld scenarios and stateofart techniques in malware evasion we present multiple limitations of maintaining the sgxassisted malware and evading it from antimalware mechanisms the limitations make sgx enclaves a poor choice for achieving a successful malware campaign we systematise twelve misconceptions myths outlining how an overfitmalware using sgx weakens malwares existing abilities we find the differences by comparing sgx assistance for malware with nonsgx malware ie malware in the wild in our paper we conclude that the use of hardware enclaves does not increase the preexisting attack surface enables no new infection vector and does not contribute any new methods to the stealthiness of malware,0
there is an expectation that users of home iot devices will be able to secure those devices but they may lack information about what they need to do in february 2022 we launched a web service that scans users iot devices to determine how secure they are the service aims to diagnose and remediate vulnerabilities and malware infections of iot devices of japanese users this paper reports on findings from operating this service drawn from three studies 1 the engagement of 114747 users between february 2022 may 2024 2 a largescale evaluation survey among service users n4103 and 3 an investigation and targeted survey n90 around the remediation actions of users of nonsecure devices during the operation we notified 417 036 users that one or more of their devices were detected as vulnerable and 171 015 users that one of their devices was infected with malware the service found no issues for 99 of users still 96 of all users evaluated the service positively most often for it providing reassurance being free of charge and short diagnosis time of the 171 users with malware infections 67 returned to the service later for a new check with 59 showing improvement of the 417 users with vulnerable devices 151 users revisited and rediagnosed where 75 showed improvement we report on lessons learned including a consideration of the capabilities that nonexpert users will assume of a security scan,0
bitcoin has emerged as a revolutionary payment system with its decentralized ledger concept however it has significant problems such as high transaction fees and long confirmation times lightning network ln which was introduced much later solves most of these problems with an innovative concept called offchain payments with this advancement bitcoin has become an attractive venue to perform micropayments which can also be adopted in many iot applications eg toll payments nevertheless it is not feasible to host ln and bitcoin on iot devices due to the storage memory and processing requirements therefore in this paper we propose an efficient and secure protocol that enables an iot device to use ln through an untrusted gateway node the gateway hosts ln and bitcoin nodes and can open close ln channels send ln payments on behalf of the iot device this delegation approach is powered by a 22threshold scheme that requires the iot device and the ln gateway to jointly perform all ln operations which in turn secures both parties funds specifically we propose to thresholdize lns bitcoin public and private keys as well as its commitment points with these and several other protocol level changes iot device is protected against revoked state broadcast collusion and ransom attacks we implemented the proposed protocol by changing lns source code and thoroughly evaluated its performance using a raspberry pi our evaluation results show that computational and communication delays associated with the protocol are negligible to the best of our knowledge this is the first work that implemented threshold cryptography in ln,0
iot is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life as such ensuring the security of iot devices is crucial iot devices can differ from traditional computing thereby the design and implementation of proper security measures can be challenging in iot devices we observed that iot developers discuss their securityrelated challenges in developer forums like stack overflowso however we find that iot security discussions can also be buried inside nonsecurity discussions in so in this paper we aim to understand the challenges iot developers face while applying security practices and techniques to iot devices we have two goals 1 develop a model that can automatically find securityrelated iot discussions in so and 2 study the model output to learn about iot developer securityrelated challenges first we download 53k posts from so that contain discussions about iot second we manually labeled 5919 sentences from 53k posts as 1 or 0 third we use this benchmark to investigate a suite of deep learning transformer models the best performing model is called secbot fourth we apply secbot on the entire posts and find around 30k security related sentences fifth we apply topic modeling to the securityrelated sentences then we label and categorize the topics sixth we analyze the evolution of the topics in so we found that 1 secbot is based on the retraining of the deep learning model roberta secbot offers the best f1score of 0935 2 there are six error categories in misclassified samples by secbot secbot was mostly wrong when the keywordscontexts were ambiguous eg gateway can be a security gateway or a simple gateway 3 there are 9 security topics grouped into three categories software hardware and network and 4 the highest number of topics belongs to software security followed by network security,0
internetofthings iot devices are vulnerable to malware and require new mitigation techniques due to their limited resources to that end previous research has used periodic remote attestation ra or traffic analysis ta to detect malware in iot devices however ra is expensive and ta only raises suspicion without confirming malware presence to solve this we design madea the first system that blends ra and ta to offer a comprehensive approach to malware detection for the iot ecosystem ta builds profiles of expected packet traces during benign operations of each device and then uses them to detect malware from network traffic in realtime ra confirms the presence or absence of malware on the device madea achieves 100 true positive rate it also outperforms other approaches with 160x faster detection time finally without madea effective periodic ra can consume at least 14x the amount of energy that a device needs in one hour,0
modern scientific advancements often contribute to the introduction and refinement of neverbeforeseen technologies this can be quite the task for humans to maintain and monitor and as a result our society has become reliant on machine learning to assist in this task with new technology comes new methods and thus new ways to circumvent existing cyber security measures this study examines the effectiveness of three distinct internet of things cyber security algorithms currently used in industry today for malware and intrusion detection random forest rf supportvector machine svm and knearest neighbor knn each algorithm was trained and tested on the aposemat iot23 dataset which was published in january 2020 with the earliest of captures from 2018 and latest from 2019 the rf svm and knn reached peak accuracies of 9296 8623 and 9148 respectively in intrusion detection and 9227 8352 and 8980 in malware detection it was found all three algorithms are capable of being effectively utilized for the current landscape of iot cyber security in 2021,0
this study investigates the performance of two open source intrusion detection systems idss namely snort and suricata for accurately detecting the malicious traffic on computer networks snort and suricata were installed on two different but identical computers and the performance was evaluated at 10 gbps network speed it was noted that suricata could process a higher speed of network traffic than snort with lower packet drop rate but it consumed higher computational resources snort had higher detection accuracy and was thus selected for further experiments it was observed that the snort triggered a high rate of false positive alarms to solve this problem a snort adaptive plugin was developed to select the best performing algorithm for snort adaptive plugin an empirical study was carried out with different learning algorithms and support vector machine svm was selected a hybrid version of svm and fuzzy logic produced a better detection accuracy but the best result was achieved using an optimised svm with firefly algorithm with fpr false positive rate as 86 and fnr false negative rate as 22 which is a good result the novelty of this work is the performance comparison of two idss at 10 gbps and the application of hybrid and optimised machine learning algorithms to snort,0
internet of things iot has been rapidly growing in the past few years in all life disciplines iot provides automation and smart control to its users in different domains such as home automation healthcare systems automotive and many more given the tremendous number of connected iot devices this growth leads to enormous automatic interactions among sizeable iot apps in their environment making iot apps more intelligent and more enjoyable to their users but some unforeseen interactions of iot apps and any potential malicious behaviour can seriously cause insecure and unsafe consequences to its users primarily nonexperts who lack the required knowledge regarding the potential impact of their iot automation processes in this paper we study the problem of security and safety verification of iot systems we survey techniques that utilize program analysis to verify iot applications security and safety properties the study proposes a set of categorization and classification attributes to enhance our understanding of the research landscape in this domain moreover we discuss the main challenges considered in the surveyed work and potential solutions that could be adopted to ensure the security and safety of iot systems,0
a technology platform that is gradually bridging the gap between object visibility and remote accessibility is the internet of things iot rapid deployment of this application can significantly transform the health housing and power distribution and generation sectors etc it has considerably changed the power sector regarding operations services optimization power distribution asset management and aided in engaging customers to reduce energy consumption despite its societal opportunities and the benefits it presents the power generation sector is bedeviled with many security challenges on the critical infrastructure this review discusses the security challenges posed by iot in power generation and critical infrastructure to achieve this the authors present the various iot applications particularly on the grid infrastructure from an empirical literature perspective the authors concluded by discussing how the various entities in the sector can overcome these security challenges to ensure an exemplary future iot implementation on the power critical infrastructure value chain,0
internet of things iot has become the most promising technology for service automation monitoring and interconnection etc however the security and privacy issues caused by iot arouse concerns recent research focuses on addressing security issues by looking inside platform and apps in this work we creatively change the angle to consider security problems from a wireless context perspective we propose a novel framework called iotgaze which can discover potential anomalies and vulnerabilities in the iot system via wireless traffic analysis by sniffing the encrypted wireless traffic iotgaze can automatically identify the sequential interaction of events between apps and devices we discover the temporal event dependencies and generate the wireless context for the iot system meanwhile we extract the iot context which reflects users expectation from iot apps descriptions and user interfaces if the wireless context does not match the expected iot context iotgaze reports an anomaly furthermore iotgaze can discover the vulnerabilities caused by the interapp interaction via hidden channels such as temperature and illuminance we provide a proofofconcept implementation and evaluation of our framework on the samsung smartthings platform the evaluation shows that iotgaze can effectively discover anomalies and vulnerabilities thereby greatly enhancing the security of iot systems,0
this paper provides an overview of the internet of things iot and its significance it discusses the concept of maninthemiddle mitm attacks in detail including their causes potential solutions and challenges in detecting and preventing such attacks the paper also addresses the current issues related to iot security and explores future methods and facilities for improving detection and prevention mechanisms against mitm,0
the internet of things iot is one of the emerging technologies that has grabbed the attention of researchers from academia and industry the idea behind internet of things is the interconnection of internet enabled things or devices to each other and to humans to achieve some common goals in near future iot is expected to be seamlessly integrated into our environment and human will be wholly solely dependent on this technology for comfort and easy life style any security compromise of the system will directly affect human life therefore security and privacy of this technology is foremost important issue to resolve in this paper we present a thorough study of security problems in iot and classify possible cyberattacks on each layer of iot architecture we also discuss challenges to traditional security solutions such as cryptographic solutions authentication mechanisms and key management in iot device authentication and access controls is an essential area of iot security which is not surveyed so far we spent our efforts to bring the state of the art device authentication and access control techniques on a single paper,0
a large number of network security breaches in iot networks have demonstrated the unreliability of current network intrusion detection systems nidss consequently network interruptions and loss of sensitive data have occurred which led to an active research area for improving nids technologies in an analysis of related works it was observed that most researchers aim to obtain better classification results by using a set of untried combinations of feature reduction fr and machine learning ml techniques on nids datasets however these datasets are different in feature sets attack types and network design therefore this paper aims to discover whether these techniques can be generalised across various datasets six ml models are utilised a deep feed forward dff convolutional neural network cnn recurrent neural network rnn decision tree dt logistic regression lr and naive bayes nb the accuracy of three feature extraction fe algorithms principal component analysis pca autoencoder ae and linear discriminant analysis lda are evaluated using three benchmark datasets unswnb15 toniot and csecicids2018 although pca and ae algorithms have been widely used the determination of their optimal number of extracted dimensions has been overlooked the results indicate that no clear fe method or ml model can achieve the best scores for all datasets the optimal number of extracted dimensions has been identified for each dataset and lda degrades the performance of the ml models on two datasets the variance is used to analyse the extracted dimensions of lda and pca finally this paper concludes that the choice of datasets significantly alters the performance of the applied techniques we believe that a universal benchmark feature set is needed to facilitate further advancement and progress of research in this field,0
as the number of iot devices increases security concerns become more prominent the impact of threats can be minimized by deploying network intrusion detection system nids by monitoring network traffic detecting and discovering intrusions and issuing security alerts promptly most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them thus limiting the monitoring of complex iot network attack events besides anomalous traffic in real networks accounts for only a small fraction which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult in this paper we propose an egconmix method based on egraphsage incorporating a data augmentation module to fix the problem of data imbalance in addition we incorporate contrastive learning to discern the difference between normal and malicious traffic samples facilitating the extraction of key features extensive experiments on two publicly available datasets demonstrate the superior intrusion detection performance of egconmix compared to stateoftheart methods remarkably it exhibits significant advantages in terms of training speed and accuracy for largescale graphs,0
the booming internet of things iot market has drawn tremendous interest from cyber attackers the centralized cloudbased iot service architecture has serious limitations in terms of security availability and scalability and is subject to single points of failure spof recently accommodating iot services on blockchains has become a trend for better security privacy and reliability however blockchains shortcomings of high cost low throughput and long latency make it unsuitable for iot applications in this paper we take a retrospection of existing blockchainbased iot solutions and propose a framework for efficient blockchain and iot integration following the framework we design a novel blockchainassisted decentralized iot remote accessing system rsiot which has the advantage of defending iot devices against zeroday attacks without relying on any trusted thirdparty by introducing incentives and penalties enforced by smart contracts our work enables an economic approach to thwarting the majority of attackers who aim to achieve monetary gains our work presents an example of how blockchain can be used to ensure the fairness of service trading in a decentralized environment and punish misbehaviors objectively we show the security of rsiot via detailed security analyses finally we demonstrate its scalability efficiency and usability through a proofofconcept implementation on the ethereum testnet blockchain,0
millions of vulnerable consumer iot devices in home networks are the enabler for cyber crimes putting user privacy and internet security at risk internet service providers isps are best poised to play key roles in mitigating risks by automatically inferring active iot devices per household and notifying users of vulnerable ones developing a scalable inference method that can perform robustly across thousands of home networks is a nontrivial task this paper focuses on the challenges of developing and applying datadriven inference models when labeled data of device behaviors is limited and the distribution of data changes concept drift across time and space domains our contributions are threefold 1 we collect and analyze network traffic of 24 types of consumer iot devices from 12 real homes over six weeks to highlight the challenge of temporal and spatial concept drifts in network behavior of iot devices 2 we analyze the performance of two inference strategies namely global inference a model trained on a combined set of all labeled data from training homes and contextualized inference several models each trained on the labeled data from a training home in the presence of concept drifts and 3 to manage concept drifts we develop a method that dynamically applies the closest model from a set to network traffic of unseen homes during the testing phase yielding better performance in 20 of scenarios,0
intrusion detection systems idss play a critical role in protecting billions of iot devices from malicious attacks however the idss for iot devices face inherent challenges of iot systems including the heterogeneity of iot datadevices the high dimensionality of training data and the imbalanced data moreover the deployment of idss on iot systems is challenging and sometimes impossible due to the limited resources such as memorystorage and computing capability of typical iot devices to tackle these challenges this article proposes a novel deep neural networkarchitecture called constrained twin variational autoencoder ctvae that can feed classifiers of idss with more separabledistinguishable and lowerdimensional representation data additionally in comparison to the stateoftheart neural networks used in idss ctvae requires less memorystorage and computing power hence making it more suitable for iot ids systems extensive experiments with the 11 most popular iot botnet datasets show that ctvae can boost around 1 in terms of accuracy and fscore in detection attack compared to the stateoftheart machine learning and representation learning methods whilst the running time for attack detection is lower than 2e6 seconds and the model size is lower than 1 mb we also further investigate various characteristics of ctvae in the latent space and in the reconstruction representation to demonstrate its efficacy compared with current wellknown methods,0
denial of service dos attacks constitute a major security threat to todays internet this challenge is especially pertinent to the internet of things iot as devices have less computing power memory and security mechanisms to mitigate dos attacks this paper presents a model that mimics the unique characteristics of a network of iot devices including components of the system implementing crypto puzzles a dos mitigation technique we created an imitation of a dos attack on the system and conducted a quantitative analysis to simulate the impact such an attack may potentially exert upon the system assessing the trade off between security and throughput in the iot system we model this through stochastic model checking in prism and provide evidence that supports this as a valuable method to compare the efficiency of different implementations of iot systems exemplified by a case study,0
the web is experiencing an explosive growth in the last years new technologies are introduced at a very fastpace with the aim of narrowing the gap between webbased applications and traditional desktop applications the results are web applications that look and feel almost like desktop applications while retaining the advantages of being originated from the web however these advancements come at a price the same technologies used to build responsive pleasant and fullyfeatured web applications can also be used to write web malware able to escape detection systems in this article we present new obfuscation techniques based on some of the features of the upcoming html5 standard which can be used to deceive malware detection systems the proposed techniques have been experimented on a reference set of obfuscated malware our results show that the malware rewritten using our obfuscation techniques go undetected while being analyzed by a large number of detection systems the same detection systems were able to correctly identify the same malware in its original unobfuscated form we also provide some hints about how the existing malware detection systems can be modified in order to cope with these new techniques,0
with the proliferation of internet of things iot devices ensuring secure communications has become imperative due to their low cost and embedded nature many of these devices operate with computational and energy constraints neglecting the potential security vulnerabilities that they may bring this workinprogress is focused on designing secure communication among remote servers and embedded iot devices to balance security robustness and energy efficiency the proposed approach uses lightweight cryptography optimizing device performance and security without overburdening their limited resources our architecture stands out for integrating edge servers and a central name server allowing secure and decentralized authentication and efficient connection transitions between different edge servers this architecture enhances the scalability of the iot network and reduces the load on each server distributing the responsibility for authentication and key management,0
malware affecting internet of things iot devices is rapidly growing due to the relevance of this paradigm in realworld scenarios specialized literature has also detected a trend towards multipurpose malware able to execute different malicious actions such as remote control data leakage encryption or code hiding among others protecting iot devices against this kind of malware is challenging due to their wellknown vulnerabilities and limitation in terms of cpu memory and storage to improve it the moving target defense mtd paradigm was proposed a decade ago and has shown promising results but there is a lack of iot mtd solutions dealing with multipurpose malware thus this work proposes four mtd mechanisms changing iot devices network data and runtime environment to mitigate multipurpose malware furthermore it presents a lightweight and iotoriented mtd framework to decide what when and how the mtd mechanisms are deployed finally the efficiency and effectiveness of the framework and mtd mechanisms are evaluated in a realworld scenario with one iot spectrum sensor affected by multipurpose malware,0
security and privacy of the users have become significant concerns due to the involvement of the internet of things iot devices in numerous applications cyber threats are growing at an explosive pace making the existing security and privacy measures inadequate hence everyone on the internet is a product for hackers consequently machine learning ml algorithms are used to produce accurate outputs from large complex databases where the generated outputs can be used to predict and detect vulnerabilities in iotbased systems furthermore blockchain bc techniques are becoming popular in modern iot applications to solve security and privacy issues several studies have been conducted on either ml algorithms or bc techniques however these studies target either security or privacy issues using ml algorithms or bc techniques thus posing a need for a combined survey on efforts made in recent years addressing both security and privacy issues using ml algorithms and bc techniques in this paper we provide a summary of research efforts made in the past few years starting from 2008 to 2019 addressing security and privacy issues using ml algorithms and bctechniques in the iot domain first we discuss and categorize various security and privacy threats reported in the past twelve years in the iot domain then we classify the literature on security and privacy efforts based on ml algorithms and bc techniques in the iot domain finally we identify and illuminate several challenges and future research directions in using ml algorithms and bc techniques to address security and privacy issues in the iot domain,0
the internetofthings iot is an imminent and corporal technology that enables the connectivity of smart physical devices with virtual objects contriving in distinct platforms with the help of the internet the iot is under massive experimentation to operate in a distributed manner making it favourable to be utilized in the healthcare ecosystem however un der the iot healthcare ecosystem ioths the nodes of the iot networks are unveiled to an aberrant level of security threats regulating an adequate volume of sensitive and personal data ioths undergoes various security challenges for which a distributed mechanism to address such concerns plays a vital role although blockchain having a distributed ledger is integral to solving security concerns in iothss it undergoes major problems including massive storage and computational requirements also holochain which has low computational and memory requirements lacks authentication distribution availability therefore this paper proposes a hybrid holochain and blockchainbased privacy perseverance and security framework for iothss that combines the benefits holochain and blockchain provide overcoming the computational memory and authentication challenges this framework is more suited for iot scenarios where resource needs to be optimally utilized comprehensive security and performance analysis is conducted to demonstrate the suitability and effectiveness of the proposed hybrid security approach for iothss in contrast to the blockchainonly or holochainonly based approaches,0
with the proliferation of the internet of things iot and the rising interconnectedness of devices network security faces significant challenges especially from anomalous activities while traditional machine learningbased intrusion detection systems mlids effectively employ supervised learning methods they possess limitations such as the requirement for labeled data and challenges with high dimensionality recent unsupervised mlids approaches such as autoencoders and generative adversarial networks gan offer alternative solutions but pose challenges in deployment onto resourceconstrained iot devices and in interpretability to address these concerns this paper proposes a novel federated unsupervised anomaly detection framework fedpca that leverages principal component analysis pca and the alternating directions method multipliers admm to learn common representations of distributed noniid datasets building on the fedpca framework we propose two algorithms fedpe in euclidean space and fedpg on grassmann manifolds our approach enables realtime threat detection and mitigation at the device level enhancing network resilience while ensuring privacy moreover the proposed algorithms are accompanied by theoretical convergence rates even under a subsampling scheme a novel result experimental results on the unswnb15 and toniot datasets show that our proposed methods offer performance in anomaly detection comparable to nonlinear baselines while providing significant improvements in communication and memory efficiency underscoring their potential for securing iot networks,0
the proliferation of vulnerable internetofthings iot devices has enabled largescale cyberattacks solutions like hestia and homesnitch have failed to comprehensively address iot security needs this research evaluates if wireguard an emerging vpn protocol can provide efficient security tailored for resourceconstrained iot systems we compared wireguards performance against standard protocols openvpn and ipsec in a simulated iot environment metrics measured included throughput latency and jitter during file transfers initial results reveal wireguards potential as a lightweight yet robust iot security solution despite disadvantages for wireguard in our experimental environment with further testing wireguards simplicity and low overhead could enable widespread vpn adoption to harden iot devices against attacks the protocols advantages in setup time performance and compatibility make it promising for integration especially on weak iot processors and networks,0
the internet of things iot has rapidly expanded across various sectors with consumer iot devices such as smart thermostats and security cameras experiencing growth although these devices improve efficiency and promise additional comfort they also introduce new security challenges common and easytoexplore vulnerabilities make iot devices prime targets for malicious actors upcoming mandatory security certifications offer a promising way to mitigate these risks by enforcing best practices and providing transparency regulatory bodies are developing iot security frameworks but a universal standard for largescale systematic security assessment is lacking existing manual testing approaches are expensive limiting their efficacy in the diverse and rapidly evolving iot domain this paper reviews current iot security challenges and assessment efforts identifies gaps and proposes a roadmap for scalable automated security assessment leveraging a modelbased testing approach and machine learning techniques to strengthen consumer iot security,0
intrusion detection systems ids play a crucial role in iot and network security by monitoring system data and alerting to suspicious activities machine learning ml has emerged as a promising solution for ids offering highly accurate intrusion detection however mlids solutions often overlook two critical aspects needed to build reliable systems continually changing data streams and a lack of attack labels streaming network traffic and associated cyber attacks are continually changing which can degrade the performance of deployed ml models labeling attack data such as zeroday attacks in realworld intrusion scenarios may not be feasible making the use of ml solutions that do not rely on attack labels necessary to address both these challenges we propose cndids a continual novelty detection ids framework which consists of i a learningbased feature extractor that continuously updates new feature representations of the system data and ii a novelty detector that identifies new cyber attacks by leveraging principal component analysis pca reconstruction our results on realistic intrusion datasets show that cndids achieves up to 61x fscore improvement and up to 65x improved forward transfer over the sota unsupervised continual learning algorithm our code will be released upon acceptance,0
the concepts of internet of things iot and cyber physical systems cps are closely related to each other iot is often used to refer to small interconnected devices like those in smart home while cps often refers to large interconnected devices like industry machines and smart cars in this paper we present a unified view of iot and cps from the perspective of network architecture iot and cps are similar given that they are based on either the osi model or tcpip model in both iot and cps networkingcommunication modules are attached to original things so that isolated things can be integrated into cyber space if needed actuators can also be integrated with a thing so as to control the thing with this unified view we can perform risk assessment of an iotcps system from six factors hardware networking operating system os software data and human to illustrate the use of such risk analysis framework we analyze an air quality monitoring network smart home using smart plugs and building automation system bas we also discuss challenges such as cost and secure os in iot security,0
this paper explores googles edge tpu for implementing a practical network intrusion detection system nids at the edge of iot based on a deep learning approach while there are a significant number of related works that explore machine learning based nids for the iot edge they generally do not consider the issue of the required computational and energy resources the focus of this paper is the exploration of deep learningbased nids at the edge of iot and in particular the computational and energy efficiency in particular the paper studies googles edge tpu as a hardware platform and considers the following three key metrics computation inference time energy efficiency and the traffic classification performance various scaled model sizes of two major deep neural network architectures are used to investigate these three metrics the performance of the edge tpubased implementation is compared with that of an energy efficient embedded cpu arm cortex a53 our experimental evaluation shows some unexpected results such as the fact that the cpu significantly outperforms the edge tpu for small model sizes,0
blockchain bc the technology behind the bitcoin cryptocurrency system is starting to be adopted for ensuring enhanced security and privacy in the internet of things iot ecosystem fervent research is currently being focused in both academia and industry in this domain proof of work pow a cryptographic puzzle plays a vital role in ensuring bc security by maintaining a digital ledger of transactions which are considered to be incorruptible furthermore bc uses a changeable public key pk to record the identity of users thus providing an extra layer of privacy not only in cryptocurrency has the successful adoption of the bc been implemented but also in multifaceted nonmonetary systems such as in distributed storage systems proof of location and healthcare recent research articles and projects or applications were surveyed to assess the implementation of the bc for iot security and identify associated challenges and propose solutions for bc enabled enhanced security for the iot ecosystem,0
in iotbased critical sectors 5g can provide more rapid connection speeds lower latency faster downloads and capability to connect more devices due to the introduction of new dynamics such as softwarization and virtualization 5genabled iot networks increase systems vulnerabilities to security threats due to these dynamics consequently adaptive cybersecurity solutions need to be developed for 5genabled iot applications to protect them against potential cyberattacks this task specifies new adaptive strategies of security intelligence with associated scenarios to meet the challenges of 5giot characteristics in this task we have also developed an autonomous adaptive security framework which can protect 5genabaled iot dynamically and autonomously the framework is based on a closed feedback loop of advanced analytics to monitor analyse and adapt to evolving threats to 5genanled iot applications,0
internet of things iot is being considered as the growth engine for industrial revolution 40 the combination of iot cloud computing and healthcare can contribute in ensuring wellbeing of people one important challenge of iot network is maintaining privacy and to overcome security threats this paper provides a systematic review of the security aspects of iot firstly the application of iot in industrial and medical service scenarios are described and the security threats are discussed for the different layers of iot healthcare architecture secondly different types of existing malware including spyware viruses worms keyloggers and trojan horses are described in the context of iot thirdly some of the recent malware attacks such as mirai echobot and reaper are discussed next a comparative discussion is presented on the effectiveness of different machine learning algorithms in mitigating the security threats it is found that the knearest neighbor knn machine learning algorithm exhibits excellent accuracy in detecting malware this paper also reviews different tools for ransomware detection classification and analysis finally a discussion is presented on the existing security issues open challenges and possible future scopes in ensuring iot security,0
increasingly malwares are becoming complex and they are spreading on networks targeting different infrastructures and personalend devices to collect modify and destroy victim information malware behaviors are polymorphic metamorphic persistent able to hide to bypass detectors and adapt to new environments and even leverage machine learning techniques to better damage targets thus it makes them difficult to analyze and detect with traditional endpoint detection and response intrusion detection and prevention systems to defend against malwares recent work has proposed different techniques based on signatures and machine learning in this paper we propose to use an algebraic topological approach called topologicalbased data analysis tda to efficiently analyze and detect complex malware patterns next we compare the different tda techniques ie persistence homology tomato tda mapper and existing techniques ie pca umap tsne using different classifiers including random forest decision tree xgboost and lightgbm we also propose some recommendations to deploy the bestidentified models for malware detection at scale results show that tda mapper combined with pca is better for clustering and for identifying hidden relationships between malware clusters compared to pca persistent diagrams are better to identify overlapping malware clusters with low execution time compared to umap and tsne for malware detection malware analysts can use random forest and decision tree with tsne and persistent diagram to achieve better performance and robustness on noised data,0
the machine learning approach is vital in internet of things iot malware traffic detection due to its ability to keep pace with the everevolving nature of malware machine learning algorithms can quickly and accurately analyze the vast amount of data produced by iot devices allowing for the realtime identification of malicious network traffic the system can handle the exponential growth of iot devices thanks to the usage of distributed systems like apache kafka and apache spark and intels oneapi software stack accelerates model inference speed making it a useful tool for realtime malware traffic detection these technologies work together to create a system that can give scalable performance and high accuracy making it a crucial tool for defending against cyber threats in smart communities and medical institutions,0
internet of things iot devices are progressively being utilised in a variety of edge applications to monitor and control home and industry infrastructure due to the limited compute and energy resources active security protections are usually minimal in many iot devices this has created a critical security challenge that has attracted researchers attention in the field of network security despite a large number of proposed network intrusion detection systems nidss there is limited research into practical iot implementations and to the best of our knowledge no edgebased nids has been demonstrated to operate on common lowpower chipsets found in the majority of iot devices such as the esp8266 this research aims to address this gap by pushing the boundaries on lowpower machine learning ml based nidss we propose and develop an efficient and lowpower mlbased nids and demonstrate its applicability for iot edge applications by running it on a typical smart light bulb we also evaluate our system against other proposed edgebased nidss and show that our model has a higher detection performance and is significantly faster and smaller and therefore more applicable to a wider range of iot edge devices,0
security in the internet of things iot remains a predominant area of concern this survey updates the state of the art covered in previous surveys and focuses on defending against threats rather than on the threats alone this area is less extensively covered by other surveys and warrants particular attention a lifecycle approach is adopted articulated to form a defence in depth strategy against malicious actors compromising an iot network laterally within it and from it this study highlights the challenges of each mitigation step emphasises novel perspectives and reconnects the discussed mitigation steps to the ground principles they seek to implement,0
broadly defined as the internet of things iot the growth of commodity devices that integrate physical processes with digital systems have changed the way we live play and work yet existing iot platforms cannot evaluate whether an iot app or environment is safe secure and operates correctly in this paper we present soteria a static analysis system for validating whether an iot app or iot environment collection of apps working in concert adheres to identified safety security and functional properties soteria operates in three phases a translation of platformspecific iot source code into an intermediate representation ir b extracting a state model from the ir c applying model checking to verify desired properties we evaluate soteria on 65 smartthings market apps through 35 properties and find nine 14 individual apps violate ten 29 properties further our study of combined app environments uncovered eleven property violations not exhibited in the isolated apps lastly we demonstrate soteria on maliot a novel opensource test suite containing 17 apps with 20 unique violations,0
the rapid proliferation of internet of things iot devices in recent years has resulted in a significant surge in the number of cyberattacks targeting these devices recent data indicates that the number of such attacks has increased by over 100 percent highlighting the urgent need for robust cybersecurity measures to mitigate these threats in addition a cyberattack will begin to spread malware across the network once it has successfully compromised an iot network however to mitigate this attack a new patch must be applied immediately in reality the time required to prepare and apply the new patch can vary significantly depending on the nature of the cyberattack in this paper we address the issue of how to mitigate cyberattacks before the new patch is applied by formulating an optimal control strategy that reduces the impact of malware propagation and minimise the number of infected devices across iot networks in the smart home a novel nodebased epidemiological model susceptible infected high infected low recover first and recover completesihilrfrc is established with immediate response state for the restricted environment after that the impact of malware on iot devices using both high and low infected rates will be analyzed finally to illustrate the main results several numerical analyses are carried out in addition to simulate the realworld scenario of iot networks in the smart home we built a dataset to be used in the experiments,0
the blockchain has found numerous applications in many areas with the expectation to significantly enhance their security the internet of things iot constitutes a prominent application domain of blockchain with a number of architectures having been proposed for improving not only security but also properties like transparency and auditability however many blockchain solutions suffer from inherent constraints associated with the consensus protocol used these constraints are mostly inherited by the permissionless setting eg computational power in proofofwork and become serious obstacles in a resourceconstrained iot environment moreover consensus protocols with low throughput or high latency are not suitable for iot networks where massive volumes of data are generated thus in this paper we focus on permissioned blockchain platforms and investigate the consensus protocols used aiming at evaluating their performance and fault tolerance as the main selection criteria for in principle highly insecure iot ecosystem the results of the paper provide new insights on the essential differences of various consensus protocols and their capacity to meet iot needs,0
the growing complexity of internet of things iot environments particularly in crossdomain data sharing presents significant security challenges existing datasharing schemes often rely on computationally expensive cryptographic operations and centralized key management limiting their effectiveness for resourceconstrained devices to address these issues we propose an efficient secure blockchainbased datasharing scheme first our scheme adopts a distributed key generation method which avoids single point of failure this method also allows independent pseudonym generation and key updates enhancing authentication flexibility while reducing computational overhead additionally the scheme provides a complete datasharing process covering data uploading storage and sharing while ensuring data traceability integrity and privacy security analysis shows that the proposed scheme is theoretically secure and resistant to various attacks while performance evaluations demonstrate lower computational and communication overhead compared to existing solutions making it both secure and efficient for iot applications,0
the rapid development of iot applications and their use in various fields of everyday life has resulted in an escalated number of different possible cyberthreats and has consequently raised the need of securing iot devices collecting cyberthreat intelligence eg zeroday vulnerabilities or trending exploits from various online sources and utilizing it to proactively secure iot systems or prepare mitigation scenarios has proven to be a promising direction in this work we focus on social media monitoring and investigate realtime cyberthreat intelligence detection from the twitter stream initially we compare and extensively evaluate six different machinelearning based classification alternatives trained with vulnerability descriptions and tested with realworld data from the twitter stream to identify the bestfitting solution subsequently based on our findings we propose a novel social media monitoring system tailored to the iot domain the system allows users to identify recenttrending vulnerabilities and exploits on iot devices finally to aid research on the field and support the reproducibility of our results we publicly release all annotated datasets created during this process,0
the internetofthings iot is an emerging and cognitive technology which connects a massive number of smart physical devices with virtual objects operating in diverse platforms through the internet iot is increasingly being implemented in distributed settings making footprints in almost every sector of our life unfortunately for healthcare systems the entities connected to the iot networks are exposed to an unprecedented level of security threats relying on a huge volume of sensitive and personal data iot healthcare systems are facing unique challenges in protecting data security and privacy although blockchain has posed to be the solution in this scenario thanks to its inherent distributed ledger technology dlt it suffers from major setbacks of increasing storage and computation requirements with the network size this paper proposes a holochainbased security and privacypreserving framework for iot healthcare systems that overcomes these challenges and is particularly suited for resource constrained iot scenarios the performance and thorough security analyses demonstrate that a holochainbased iot healthcare system is significantly better compared to blockchain and other existing systems,0
with rapid technological growth security attacks are drastically increasing in many crucial internetofthings iot applications such as healthcare and defense the early detection of security attacks plays a significant role in protecting huge resources an intrusion detection system is used to address this problem the signaturebased approaches fail to detect zeroday attacks so anomalybased detection particularly ai tools are becoming popular in addition the imbalanced dataset leads to biased results in machine learning ml models f1 score is an important metric to measure the accuracy of classlevel correct predictions the model may fail to detect the target samples if the f1 is considerably low it will lead to unrecoverable consequences in sensitive applications such as healthcare and defense so any improvement in the f1 score has significant impact on the resource protection in this paper we present a framework for mlbased intrusion detection system for an imbalanced dataset in this study the most recent dataset namely ciciot2023 is considered the random forest rf algorithm is used in the proposed framework the proposed approach improves 372 375 and 469 in precision recall and f1 score respectively with the existing method additionally for unsaturated classes ie classes with f1 score 099 f1 score improved significantly by 79 as a result the proposed approach is more suitable for iot security applications for efficient detection of intrusion and is useful in further studies,0
in this work an adaptive and robust nullspace projection arnsp scheme is proposed for secure transmission with artificial noise anaided directional modulation dm in wireless networks the proposed scheme is carried out in three steps firstly the directions of arrival doas of the signals from the desired user and eavesdropper are estimated by the root multiple signal classificaiton rootmusic algorithm and the related signaltonoise ratios snrs are estimated based on the ratio of the corresponding eigenvalue to the minimum eigenvalue of the covariance matrix of the received signals in the second step the value intervals of doa estimation errors are predicted based on the doa and snr estimations finally a robust nsp beamforming dm system is designed according to the aforeobtained estimations and predictions our examination shows that the proposed scheme can significantly outperform the conventional nonadaptive robust scheme and nonrobust nsp scheme in terms of achieving a much lower bit error rate ber at the desired user and a much higher secrecy rate sr in addition the ber and sr performance gains achieved by the proposed scheme relative to other schemes increase with the value range of doa estimation error,0
the internet of things iot is still in its infancy and has attracted much interest in many industrial sectors including medical fields logistics tracking smart cities and automobiles however as a paradigm it is susceptible to a range of significant intrusion threats this paper presents a threat analysis of the iot and uses an artificial neural network ann to combat these threats a multilevel perceptron a type of supervised ann is trained using internet packet traces then is assessed on its ability to thwart distributed denial of service ddosdos attacks this paper focuses on the classification of normal and threat patterns on an iot network the ann procedure is validated against a simulated iot network the experimental results demonstrate 994 accuracy and can successfully detect various ddosdos attacks,0
triggeraction programming tap is a popular enduser programming framework that can simplify the internet of things iot automation with simple triggeraction rules however it also introduces new security and safety threats a lot of advanced techniques have been proposed to address this problem rigorously reasoning about the security of a tapbased iot system requires a welldefined model and verification method both against rule semantics and physicalworld features eg concurrency rule latency extended action tardy attributes and connectionbased rule interactions which has been missing until now by analyzing these features we find 9 new types of rule interaction vulnerabilities and validate them on two commercial iot platforms we then present tapinspector a novel system to detect these interaction vulnerabilities in concurrent tapbased iot systems it automatically extracts tap rules from iot apps translates them into a hybrid model by model slicing and state compression and performs semantic analysis and model checking with various safety and liveness properties our experiments corroborate that tapinspector is practical it identifies 533 violations related to rule interaction from 1108 realworld market iot apps and is at least 60000 times faster than the baseline without optimization,0
over the last decade iot platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services because of easy accessibility and fastgrowing demand for smart devices and network iot is now facing more security challenges than ever before there are existing security measures that can be applied to protect iot however traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness thus a strongdynamically enhanced and up to date security system is required for nextgeneration iot system a huge technological advancement has been noticed in machine learning ml which has opened many possible research windows to address ongoing and future challenges in iot in order to detect attacks and identify abnormal behaviors of smart devices and networks ml is being utilized as a powerful technology to fulfill this purpose in this survey paper the architecture of iot is discussed following a comprehensive literature review on ml approaches the importance of security of iot in terms of different types of possible attacks moreover mlbased potential solutions for iot security has been presented and future challenges are discussed,0
a diverse set of internet of things iot devices are becoming an integrated part of daily lives and playing an increasingly vital role in various industry enterprise and agricultural settings the current iot ecosystem relies on several iot management platforms to manage and operate a large number of iot devices their data and their connectivity considering their key role these platforms must be properly secured against cyber attacks in this work we first explore the core operationsfeatures of leading platforms to design a framework to perform a systematic security evaluation of these platforms subsequently we use our framework to analyze a representative set of 52 iot management platforms including 42 webhosted and 10 locallydeployable platforms we discover a number of high severity unauthorized access vulnerabilities in 952 evaluated iot management platforms which could be abused to perform attacks such as remote iot sim deactivation iot sim overcharging and iot device data forgery more seriously we also uncover instances of broken authentication in 1352 platforms including complete account takeover on 852 platforms along with remote code execution on 252 platforms in effect 1752 platforms were affected by vulnerabilities that could lead to platformwide attacks overall vulnerabilities were uncovered in 33 platforms out of which 28 platforms responded to our responsible disclosure we were also assigned 11 cves and awarded bounty for our findings,0
machine learning algorithms can effectively classify malware through dynamic behavior but are susceptible to adversarial attacks existing attacks however often fail to find an effective solution in both the feature and problem spaces this issue arises from not addressing the intrinsic nondeterministic nature of malware namely executing the same sample multiple times may yield significantly different behaviors hence the perturbations computed for a specific behavior may be ineffective for others observed in subsequent executions in this paper we show how an attacker can augment their chance of success by leveraging a new and more efficient feature space algorithm for sequential data which we have named psfgsm and by adopting two problem space strategies specially tailored to address nondeterminism in the problem space we implement our novel algorithm and attack strategies in tarallo an endtoend adversarial framework that significantly outperforms previous works in both white and blackbox scenarios our preliminary analysis in a sandboxed environment and against two rnnbased malware detectors shows that tarallo achieves a success rate up to 99 on both feature and problem space attacks while significantly minimizing the number of modifications required for misclassification,0
there has been increasing interest in the potential of blockchain in enhancing the security of devices and systems such as internet of things iot in this paper we present a blockchainbased iot security architecture iotchain the threetier architecture comprises an authentication layer a blockchain layer and an application layer and is designed to achieve identity authentication access control privacy protection lightweight feature regional node fault tolerance denialofservice resilience and storage integrity we also evaluate the performance of iotchain to demonstrate its utility in an iot deployment,0
while the application of iot in smart technologies becomes more and more proliferated the pandemonium of its protocols becomes increasingly confusing more seriously severe security deficiencies of these protocols become evident as timeto market is a key factor which satisfaction comes at the price of a less thorough security design and testing this applies especially to the smart home domain where the consumerdriven market demands quick and cheap solutions this paper presents an overview of iot application domains and discusses the most important wireless iot protocols for smart home which are knxrf enocean zigbee zwave and thread finally it describes the security features of said protocols and compares them with each other giving advice on whose protocols are more suitable for a secure smart home,0
the internet of things iot is integrating the internet and smart devices in almost every domain such as home automation ehealthcare systems vehicular networks industrial control and military applications in these sectors sensory data which is collected from multiple sources and managed through intermediate processing by multiple nodes is used for decisionmaking processes ensuring data integrity and keeping track of data provenance is a core requirement in such a highly dynamic context since data provenance is an important tool for the assurance of data trustworthiness dealing with such requirements is challenging due to the limited computational and energy resources in iot networks this requires addressing several challenges such as processing overhead secure provenance bandwidth consumption and storage efficiency in this paper we propose zircon a novel zerowatermarking approach to establish endtoend data trustworthiness in an iot network in zircon provenance information is stored in a tamperproof centralized network database through watermarks generated at source node before transmission we provide an extensive security analysis showing the resilience of our scheme against passive and active attacks we also compare our scheme with existing works based on performance metrics such as computational time energy utilization and cost analysis the results show that zircon is robust against several attacks lightweight storage efficient and better in energy utilization and bandwidth consumption compared to prior art,0
in recent years the existence of a significant crossimpact between cloud computing and internet of things iot has lead to a dichotomy that gives raise to cloudassisted iot caiot and iotbased cloud iotbc although it is pertinent to study both technologies this paper focuses on caiot and especially its security issues which are inherited from both cloud computing and iot this study starts with reviewing existing relevant surveys noting their shortcomings which motivate a comprehensive survey in this area we proceed to highlight existing approaches towards the design of secure caiot scaiot along with related security challenges and controls we develop a layered architecture for scaiot furthermore we take a look at what the future may hold for scaiot with a focus on the role of artificial intelligenceai,0
blockchain technology has gained increasing popularity in the research of internet of things iot systems in the past decade as a distributed and immutable ledger secured by strong cryptography algorithms the blockchain brings a new perspective to secure iot systems many studies have been devoted to integrating blockchain into iot device management access control data integrity security and privacy in comparison the blockchainfacilitated iot communication is much less studied nonetheless we see the potential of blockchain in decentralizing and securing iot communications this paper proposes an innovative iot service platform powered by consortium blockchain technology the presented solution abstracts machinetomachine m2m and humantomachine h2m communications into services provided by iot devices then it materializes data exchange of the iot network through smart contracts and blockchain transactions additionally we introduce the auxiliary storage layer to the proposed platform to address various data storage requirements our proofofconcept implementation is tested against various workloads and connection sizes under different block configurations to evaluate the platforms transaction throughput latency and hardware utilization the experiment results demonstrate that our solution can maintain high performance under most testing scenarios and provide valuable insights on optimizing the blockchain configuration to achieve the best performance,0
the internet of things iot is giving a boost to a plethora of new opportunities for the robust and sustainable deployment of cyber physical systems the cornerstone of any iot system is the sensing devices these sensing devices have considerable resource constraints including insufficient battery capacity cpu capability and physical security because of such resource constraints designing lightweight cryptographic protocols is an opportunity remote user authentication ensures that two parties establish a secure and durable session key this study presents a lightweight and safe authentication strategy for the usergateway u gw iot network model the proposed system is designed leveraging elliptic curve cryptography ecc we undertake a formal security analysis with both the automated validation of internet security protocols avispa and burrows abadi needham ban logic tools and an information security assessment with the delev yao channel we use publish subscribe based message queuing telemetry transport mqtt protocol for communication additionally the performance analysis and comparison of security features show that the proposed scheme is resilient to well known cryptographic threats,0
robot systems are increasingly integrating into numerous avenues of modern life from cleaning houses to providing guidance and emotional support robots now work directly with humans due to their farreaching applications and progressively complex architecture they are being targeted by adversarial attacks such as sensoractuator attacks data spoofing malware and network intrusion therefore security for robotic systems has become crucial in this paper we address the underserved area of malware detection in robotic software since robots work in close proximity to humans often with direct interactions malware could have lifethreatening impacts hence we propose the robomal framework of static malware detection on binary executables to detect malware before it gets a chance to execute additionally we address the great paucity of data in this space by providing the robomal dataset comprising controller executables of a smallscale autonomous car the performance of the framework is compared against widely used supervised learning models gru cnn and ann notably the lstmbased robomal model outperforms the other models with an accuracy of 85 and precision of 87 in 10fold crossvalidation hence proving the effectiveness of the proposed framework,0
android the most popular mobile os has around 78 of the mobile market share due to its popularity it attracts many malware attacks in fact people have discovered around one million new malware samples per quarter and it was reported that over 98 of these new malware samples are in fact derivatives or variants from existing malware families in this paper we first show that runtime behaviors of malwares core functionalities are in fact similar within a malware family hence we propose a framework to combine runtime behavior with static structures to detect malware variants we present the design and implementation of monet which has a client and a backend server module the client module is a lightweight indevice app for behavior monitoring and signature generation and we realize this using two novel interception techniques the backend server is responsible for large scale malware detection we collect 3723 malware samples and top 500 benign apps to carry out extensive experiments of detecting malware variants and defending against malware transformation our experiments show that monet can achieve around 99 accuracy in detecting malware variants furthermore it can defend against 10 different obfuscation and transformation techniques while only incurs around 7 performance overhead and about 3 battery overhead more importantly monet will automatically alert users with intrusion details so to prevent further malicious behaviors,0
this is the era of smart devices or things which are fueling the growth of internet of things iot it is impacting every sphere around us making our life dependent on this technological feat it is of high concern that these smart things are being targeted by cyber criminals taking advantage of heterogeneity minuscule security features and vulnerabilities within these devices conventional centralized it security measures have limitations in terms of scalability and cost therefore these smart devices are required to be monitored closer to their location ideally at the edge of iot networks in this paper we explore how some security features can be implemented at the network edge to secure these smart devices we explain the importance of network function virtualization nfv in order to deploy security functions at the network edge to achieve this goal we introduce netra a novel lightweight dockerbased architecture for virtualizing network functions to provide iot security also we highlight the advantages of the proposed architecture over the standardized nfv architecture in terms of storage memory usage latency throughput load average scalability and explain why the standardized architecture is not suitable for iot we study the performance of proposed nfv based edge analysis for iot security and show that attacks can be detected with more than 95 accuracy in less than a second,0
as the de facto routing protocol for many internet of things iot networks nowadays and to assure the confidentiality and integrity of its control messages the routing protocol for low power and lossy networks rpl incorporates three modes of security the unsecured mode um preinstalled secure mode psm and the authenticated secure mode asm while the psm and asm are intended to protect against external routing attacks and some replay attacks through an optional replay protection mechanism recent research showed that rpl in psm is still vulnerable to many routing attacks both internal and external in this paper we propose a novel secure mode for rpl the chained secure mode csm based on the concept of intraflow network coding nc the csm is designed to enhance rpl resilience and mitigation capability against replay attacks while allowing the integration with external security measures such as intrusion detection systems idss the security and performance of the proposed csm were evaluated and compared against rpl in um and psm with and without the optional replay protection under several routing attacks the neighbor attack na wormhole wh and cloneid attack ca using average packet delivery rate pdr endtoend e2e latency and power consumption as metrics it showed that csm has better performance and more enhanced security than both the um and psm with the replay protection while mitigating both the na and wh attacks and significantly reducing the effect of the ca in the investigated scenarios,0
the rapid growth of technology has led to the creation of computing networks the applications of the internet of things are becoming more and more visible with the expansion and development of sensors and the use of a series of equipment to connect to the internet of course the growth of any network will also provide some challenges the main challenge of iot like any other network is its security in the field of security there are issues such as attack detection authentication encryption and the so on one of the most important attack is cyberattacks that disrupt the network usage one of the most important attacks on the iot is botnet attack the most important challenges of this topic include very high computational complexity lack of comparison with previous methods lack of scalability high execution time lack of review of the proposed approach in terms of accuracy to detect and classify attacks and intrusions using intrusion detection systems for the iot is an important step in identifying and detecting various attacks therefore an algorithm that can solve these challenges has provided a nearoptimal method using trainingbased models and algorithms such as deep dearningreinforcement learning and xgboost learning in combination drlxgboost models can be an interesting approach to overcoming previous weaknesses the data of this research is botiot2018,0
with the recent advances of iot internet of things new and more robust security frameworks are needed to detect and mitigate new forms of cyberattacks which exploit complex and heterogeneity iot networks as well as the existence of many vulnerabilities in iot devices with the rise of blockchain technologies service providers pay considerable attention to better understand and adopt blockchain technologies in order to have better secure and trusted systems for own organisations and their customers the present paper introduces a high level guide for the senior officials and decision makers in the organisations and technology managers for blockchain security framework by design principle for trust and adoption in iot environments the paper discusses cybertrust project blockchain technology development as a representative case study for offered security framework security and privacy by design approach is introduced as an important consideration in setting up the framework,0
in recent years the internet of things iot technology has led to the emergence of multiple smart applications in different vital sectors including healthcare education agriculture energy management etc iot aims to interconnect several intelligent devices over the internet such as sensors monitoring systems and smart appliances to control store exchange and analyze collected data the main issue in iot environments is that they can present potential vulnerabilities to be illegally accessed by malicious users which threatens the safety and privacy of gathered data to face this problem several recent works have been conducted using microservicesbased architecture to minimize the security threats and attacks related to iot data by employing microservices these works offer extensible reusable and reconfigurable security features in this paper we aim to provide a survey about microservicesbased approaches for securing iot applications this survey will help practitioners understand ongoing challenges and explore new and promising research opportunities in the iot security field to the best of our knowledge this paper constitutes the first survey that investigates the use of microservices technology for securing iot applications,0
context the increase in internet of things iot devices gives rise to an increase in deceptive manipulations by malicious actors these actors should be prevented from targeting the iot networks cybersecurity threats have evolved and become dynamically sophisticated such that they could exploit any vulnerability found in iot networks however with the introduction of the software defined network sdn in the iot networks as the central monitoring unit iot networks are less vulnerable and less prone to threats although the sdn itself is vulnerable to several threats objective to present a comprehensive and unbiased overview of the stateoftheart on iot networks security enhancement using sdn controllers method we review the current body of knowledge on enhancing the security of iot networks using sdn with a systematic mapping study sms following the established guidelines results the sms result comprises 33 primary studies analyzed against four major research questions the sms highlights current research trends and identifies gaps in the sdniot network security conclusion we conclude that the sdn controller architecture commonly used for securing iot networks is the centralized controller architecture however this architecture is not without its limitations additionally the predominant technique utilized for risk mitigation is machine learning,0
with the continuous development of industrial iot iiot technology network security is becoming more and more important and intrusion detection is an important part of its security however since the amount of attack traffic is very small compared to normal traffic this imbalance makes intrusion detection in it very difficult to address this imbalance an intrusion detection system called pretraining wasserstein generative adversarial network intrusion detection system pwgids is proposed in this paper this system is divided into two main modules 1 in this module we introduce the pretraining mechanism in the wasserstein generative adversarial network with gradient penalty wgangp for the first time firstly using the normal network traffic to train the wgangp and then inputting the imbalance data into the pretrained wgangp to retrain and generate the final required data 2 intrusion detection module we use lightgbm as the classification algorithm to detect attack traffic in iiot networks the experimental results show that our proposed pwgids outperforms other models with f1scores of 99 and 89 on the 2 datasets respectively and the pretraining mechanism we proposed can also be widely used in other gans providing a new way of thinking for the training of gans,0
internet of things iot is realized by the idea of free flow of information amongst various low power embedded devices that use internet to communicate with one another it is predicted that the iot will be widely deployed and it will find applicability in various domains of life demands of iot have lately attracted huge attention and organizations are excited about the business value of the data that will be generated by the iot paradigm on the other hand iot have various security and privacy concerns for the end users that limit its proliferation in this paper we have identified categorized and discussed various security challenges and state of the art efforts to resolve these challenges,0
the internet of things iot is undergoing rapid growth in the it industry but it continues to be associated with several security and privacy concerns as a result of its massive scale decentralised topology and resourceconstrained devices blockchain bc a distributed ledger technology used in cryptocurrency has attracted significant attention in the realm of iot security and privacy however adopting bc to iot is not straightforward in most cases due to overheads and delays caused by bc operations in this paper we apply a bc technology known as hyperledgder fabric to an iot network this technology introduces an executeorder technique for transactions that separates the transaction execution from consensus resulting in increased efficiency we demonstrate that our proposed iotbc architecture is sufficiently secure with regard to fundamental security goals ie confidentiality integrity and availability finally the simulation results are highlighted that shows the performance overheads associated with our approach are as minimal as those associated with the hyperledger fabric framework and negligible in terms of security and privacy,0
iot application providers increasingly use microservice architecture msa to develop applications that convert iot data into valuable information the independently deployable and scalable nature of microservices enables dynamic utilization of edge and cloud resources provided by various service providers thus improving performance however iot data security should be ensured during multidomain data processing and transmission among distributed and dynamically composed microservices the ability to implement granular security controls at the microservices level has the potential to solve this to this end edgecloud environments require intricate and scalable security frameworks that operate across multidomain environments to enforce various security policies during the management of microservices ie initial placement scaling migration and dynamic composition considering the sensitivity of the iot data to address the lack of such a framework we propose an architectural framework that uses policyascode to ensure secure microservice management within multidomain edgecloud environments the proposed framework contains a control plane to intelligently and dynamically utilise and configure cloudnative ie container orchestrators and service mesh technologies to enforce security policies we implement a prototype of the proposed framework using opensource cloudnative technologies such as docker kubernetes istio and open policy agent to validate the framework evaluations verify our proposed frameworks ability to enforce security policies for distributed microservices management thus harvesting the msa characteristics to ensure iot application security needs,0
consumer internet of things iot devices often leverage the local network to communicate with the corresponding companion app or other devices this has benefits in terms of efficiency since it offloads the cloud enisa and nist security guidelines underscore the importance of enabling default local communication for safety and reliability indeed an iot device should continue to function in case the cloud connection is not available while the security of clouddevice connections is typically strengthened through the usage of standard protocols local connectivity security is frequently overlooked neglecting the security of local communication opens doors to various threats including replay attacks in this paper we investigate this class of attacks by designing a systematic methodology for automatically testing iot devices vulnerability to replay attacks specifically we propose a tool named repliot able to test whether a replay attack is successful or not without prior knowledge of the target devices we perform thousands of automated experiments using popular commercial devices spanning various vendors and categories notably our study reveals that among these devices 51 of them do not support local connectivity thus they are not compliant with the reliability and safety requirements of the enisanist guidelines we find that 75 of the remaining devices are vulnerable to replay attacks with repliot having a detection accuracy of 0981 finally we investigate the possible causes of this vulnerability discussing possible mitigation strategies,0
the internet of things iot is a futuristic technology that promises to connect tons of devices via the internet as more individuals connect to the internet it is believed that communication will generate mountains of data iot is currently leveraging wireless sensor networks wsns to collect monitor and transmit data and sensitive data across wireless networks using sensor nodes wsns encounter a variety of threats posed by attackers including unauthorized access and data security especially in the context of the internet of things where small embedded devices with limited computational capabilities such as sensor nodes are expected to connect to a larger network as a result wsns are vulnerable to a variety of attacks furthermore implementing security is timeconsuming and selective as traditional security algorithms degrade network performance due to their computational complexity and inherent delays this paper describes an encryption algorithm that combines the secure iot sit algorithm with the security protocols for sensor networks spins security protocol to create the lightweight security algorithm lsa which addresses data security concerns while reducing power consumption in wsns without sacrificing performance,0
for years attack graphs have been an important tool for security assessment of enterprise networks but iot devices a new player in the it world might threat the reliability of this tool in this paper we review the challenges that must be addressed when using attack graphs to model and analyze enterprise networks that include iot devices in addition we propose novel ideas and countermeasures aimed at addressing these challenges,0
energy consumption of iot devices is a very important issue for this reason many techniques have been developed to allow iot nodes to be aware of the amount of available energy when energy is missing the device halts and saves its state one of those techniques is context saving relying on the use of nonvolatile memories nvm to store and restore the state of the device however this information as far as iot devices deal with security might be the target of attacks including tampering and theft of confidential data in this paper we propose a secure context saving seccs approach that provides a context saving procedure and a hardware module easy to implement inside a system on chip soc this approach provides both confidentiality and integrity to all the cpu content saved into the target nvm,0
the internet of things iot is smartifying our everyday life our starting point is iotlysa a calculus for describing iot systems and its static analysis which will be presented at coordination 2016 we extend the mentioned proposal in order to begin an investigation about security issues in particular for the static verification of secrecy and some other security properties,0
